examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 16:50:24,835] torch.distributed.run: [WARNING] 
[2024-12-02 16:50:24,835] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 16:50:24,835] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 16:50:24,835] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0][rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())


---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.output_layer.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 108.33
     rank  1: 107.62
     rank  2: 79.06
     rank  3: 77.14
     rank  4: 47.93
     rank  5: 52.23
     rank  6: 32.95
     rank  7: 52.20
     rank  8: 54.75
     rank  9: 54.34
     rank 10: 64.18
     rank 11: 64.24
     rank 12: 34.23
     rank 13: 43.53
     rank 14: 32.61
     rank 15: 29.09
     rank 16: 31.07
     rank 17: 45.43
     rank 18: 36.83
     rank 19: 33.08
  train/valid/test-data-iterators-setup:
     rank  0: 1134.97
     rank  1: 1134.81
     rank  2: 1135.08
     rank  3: 1134.79
     rank  4: 1134.76
     rank  5: 1134.85
     rank  6: 1134.93
     rank  7: 1134.83
     rank  8: 1232.46
     rank  9: 1232.48
     rank 10: 1232.44
     rank 11: 1233.58
     rank 12: 1232.49
     rank 13: 1232.51
     rank 14: 1232.54
     rank 15: 1232.53
     rank 16: 1232.64
     rank 17: 1232.80
     rank 18: 1232.80
     rank 19: 1232.66
 [2024-12-02 16:51:13] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21107.2 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.108734E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10086.0 | max reserved: 10086.0[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10114.0 | max reserved: 10114.0

[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10062.0 | max reserved: 10062.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
times across ranks (ms):
  forward-backward:
     rank  0: 21036.57
     rank  1: 21036.44
     rank  2: 21036.84
     rank  3: 21036.74
     rank  4: 21040.57
     rank  5: 21040.37
     rank  6: 21040.67
     rank  7: 21040.59
     rank  8: 21040.35
     rank  9: 21040.16
     rank 10: 21040.44
     rank 11: 21040.33
     rank 12: 21040.41
     rank 13: 21040.19
     rank 14: 21040.28
     rank 15: 21040.30
     rank 16: 21040.08
     rank 17: 21039.99
     rank 18: 21039.96
     rank 19: 21040.11
  forward-compute:
     rank  0: 4880.48
     rank  1: 4861.69
     rank  2: 4886.34
     rank  3: 4866.04
     rank  4: 5439.91
     rank  5: 5424.27
     rank  6: 5448.86
     rank  7: 5430.52
     rank  8: 5130.56
     rank  9: 5117.74
     rank 10: 5145.04
     rank 11: 5125.37
     rank 12: 5010.86
     rank 13: 5002.38
     rank 14: 5017.52
     rank 15: 4999.69
     rank 16: 4850.78
     rank 17: 4860.62
     rank 18: 4866.84
     rank 19: 4854.18
  backward-compute:
     rank  0: 51.62
     rank  1: 52.23
     rank  2: 50.02
     rank  3: 53.38
     rank  4: 3318.81
     rank  5: 3320.50
     rank  6: 3321.18
     rank  7: 3324.08
     rank  8: 3298.94
     rank  9: 3303.70
     rank 10: 3305.82
     rank 11: 3303.10
     rank 12: 3292.45
     rank 13: 3296.02
     rank 14: 3294.31
     rank 15: 3293.63
     rank 16: 3500.84
     rank 17: 3493.87
     rank 18: 3497.35
     rank 19: 3494.02
  pure-backward-compute:
     rank  0: 50.92
     rank  1: 51.38
     rank  2: 49.17
     rank  3: 52.65
     rank  4: 3317.42
     rank  5: 3319.55
     rank  6: 3319.80
     rank  7: 3323.04
     rank  8: 3297.55
     rank  9: 3302.57
     rank 10: 3304.77
     rank 11: 3301.87
     rank 12: 3290.77
     rank 13: 3294.95
     rank 14: 3292.28
     rank 15: 3292.63
     rank 16: 3497.91
     rank 17: 3491.68
     rank 18: 3495.42
     rank 19: 3490.95
  batch-generator:
     rank  0: 1121.28
     rank  1: 1108.50
     rank  2: 1136.69
     rank  3: 1116.22
     rank  4: 1226.83
     rank  5: 1212.47
     rank  6: 1239.41
     rank  7: 1219.95
     rank  8: 1081.88
     rank  9: 1070.61
     rank 10: 1098.91
     rank 11: 1081.31
     rank 12: 1206.73
     rank 13: 1200.16
     rank 14: 1220.24
     rank 15: 1200.54
     rank 16: 894.13
     rank 17: 898.89
     rank 18: 908.67
     rank 19: 894.78
  forward-recv:
     rank  4: 4034.99
     rank  5: 4038.56
     rank  6: 4036.80
     rank  7: 4036.85
     rank  8: 6789.78
     rank  9: 6791.35
     rank 10: 6784.43
     rank 11: 6790.32
     rank 12: 9313.32
     rank 13: 9321.78
     rank 14: 9313.51
     rank 15: 9322.19
     rank 16: 11754.31
     rank 17: 11758.27
     rank 18: 11747.75
     rank 19: 11758.04
  forward-send:
     rank  0: 7723.84
     rank  1: 7742.45
     rank  2: 7715.58
     rank  3: 7739.04
     rank  4: 4642.22
     rank  5: 4656.18
     rank  6: 4630.80
     rank  7: 4655.14
     rank  8: 2287.91
     rank  9: 2300.49
     rank 10: 2282.06
     rank 11: 2300.52
     rank 12: 33.68
     rank 13: 37.91
     rank 14: 27.55
     rank 15: 37.53
  backward-recv:
     rank  0: 1632.10
     rank  1: 1632.32
     rank  2: 1633.43
     rank  3: 1632.20
     rank  4: 596.61
     rank  5: 597.16
     rank  6: 597.49
     rank  7: 597.40
     rank  8: 393.04
     rank  9: 392.65
     rank 10: 393.75
     rank 11: 393.38
     rank 12: 197.08
     rank 13: 197.45
     rank 14: 196.82
     rank 15: 197.96
  backward-send:
     rank  4: 22.69
     rank  5: 22.08
     rank  6: 21.70
     rank  7: 21.56
     rank  8: 31.34
     rank  9: 31.51
     rank 10: 30.84
     rank 11: 30.94
     rank 12: 20.91
     rank 13: 20.55
     rank 14: 20.67
     rank 15: 20.38
     rank 16: 10.50
     rank 17: 10.33
     rank 18: 10.21
     rank 19: 10.60
  forward-send-backward-recv:
     rank  0: 6694.83
     rank  1: 6693.97
     rank  2: 6697.20
     rank  3: 6691.74
     rank  4: 2829.18
     rank  5: 2829.33
     rank  6: 2826.97
     rank  7: 2824.68
     rank  8: 2638.39
     rank  9: 2635.70
     rank 10: 2636.47
     rank 11: 2636.93
     rank 12: 2451.72
     rank 13: 2450.64
     rank 14: 2450.29
     rank 15: 2454.82
  backward-send-forward-recv:
     rank  4: 87.74
     rank  5: 86.01
     rank  6: 89.08
     rank  7: 84.15
     rank  8: 190.59
     rank  9: 189.33
     rank 10: 186.35
     rank 11: 183.60
     rank 12: 225.53
     rank 13: 222.03
     rank 14: 223.78
     rank 15: 222.88
     rank 16: 212.34
     rank 17: 208.10
     rank 18: 209.34
     rank 19: 211.92
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.07
     rank 12: 0.06
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.06
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.14
     rank  1: 0.12
     rank  2: 0.16
     rank  3: 0.15
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.06
     rank 14: 0.03
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.10
  all-grads-sync:
     rank  0: 42.26
     rank  1: 38.18
     rank  2: 45.95
     rank  3: 38.49
     rank  4: 47.17
     rank  5: 42.48
     rank  6: 42.47
     rank  7: 47.02
     rank  8: 47.57
     rank  9: 45.80
     rank 10: 44.07
     rank 11: 45.43
     rank 12: 43.77
     rank 13: 39.82
     rank 14: 40.85
     rank 15: 45.11
     rank 16: 2.82
     rank 17: 2.67
     rank 18: 2.58
     rank 19: 3.10
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.07
     rank  5: 0.09
     rank  6: 0.07
     rank  7: 0.06
     rank  8: 0.11
     rank  9: 0.10
     rank 10: 0.08
     rank 11: 0.08
     rank 12: 0.08
     rank 13: 0.07
     rank 14: 0.08
     rank 15: 0.08
     rank 16: 0.07
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.09
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.05
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.36
     rank  1: 2.23
     rank  2: 2.58
     rank  3: 2.45
     rank  4: 58.03
     rank  5: 58.81
     rank  6: 58.17
     rank  7: 58.08
     rank  8: 52.38
     rank  9: 52.34
     rank 10: 52.49
     rank 11: 53.09
     rank 12: 53.25
     rank 13: 52.93
     rank 14: 52.78
     rank 15: 52.68
     rank 16: 55.37
     rank 17: 56.05
     rank 18: 55.35
     rank 19: 59.89
  optimizer:
     rank  0: 3.68
     rank  1: 3.55
     rank  2: 3.90
     rank  3: 3.76
     rank  4: 59.32
     rank  5: 60.12
     rank  6: 59.48
     rank  7: 59.49
     rank  8: 53.66
     rank  9: 53.62
     rank 10: 53.80
     rank 11: 54.41
     rank 12: 54.56
     rank 13: 54.23
     rank 14: 54.09
     rank 15: 53.98
     rank 16: 56.68
     rank 17: 57.37
     rank 18: 56.66
     rank 19: 61.22
 [2024-12-02 16:51:21] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7958.6 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.285778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7872.77
     rank  1: 7872.77
     rank  2: 7872.76
     rank  3: 7872.76
     rank  4: 7876.91
     rank  5: 7876.94
     rank  6: 7876.97
     rank  7: 7876.96
     rank  8: 7876.63
     rank  9: 7876.61
     rank 10: 7876.62
     rank 11: 7876.61
     rank 12: 7876.64
     rank 13: 7876.63
     rank 14: 7876.63
     rank 15: 7876.60
     rank 16: 7876.88
     rank 17: 7876.83
     rank 18: 7876.83
     rank 19: 7876.89
  forward-compute:
     rank  0: 922.17
     rank  1: 926.61
     rank  2: 927.09
     rank  3: 929.59
     rank  4: 2868.08
     rank  5: 2873.31
     rank  6: 2870.48
     rank  7: 2877.38
     rank  8: 2788.88
     rank  9: 2790.72
     rank 10: 2793.05
     rank 11: 2795.13
     rank 12: 2759.80
     rank 13: 2766.29
     rank 14: 2758.77
     rank 15: 2762.52
     rank 16: 2908.40
     rank 17: 2912.21
     rank 18: 2915.58
     rank 19: 2909.43
  backward-compute:
     rank  0: 51.61
     rank  1: 52.15
     rank  2: 46.44
     rank  3: 50.98
     rank  4: 3317.69
     rank  5: 3317.11
     rank  6: 3319.55
     rank  7: 3319.07
     rank  8: 3290.41
     rank  9: 3296.66
     rank 10: 3291.35
     rank 11: 3294.57
     rank 12: 3289.54
     rank 13: 3291.78
     rank 14: 3294.28
     rank 15: 3290.34
     rank 16: 3500.86
     rank 17: 3502.29
     rank 18: 3502.70
     rank 19: 3499.79
  pure-backward-compute:
     rank  0: 50.98
     rank  1: 51.46
     rank  2: 45.80
     rank  3: 50.33
     rank  4: 3316.64
     rank  5: 3316.25
     rank  6: 3318.51
     rank  7: 3317.97
     rank  8: 3289.14
     rank  9: 3295.56
     rank 10: 3290.45
     rank 11: 3293.64
     rank 12: 3288.10
     rank 13: 3291.07
     rank 14: 3292.77
     rank 15: 3289.62
     rank 16: 3498.20
     rank 17: 3500.57
     rank 18: 3501.18
     rank 19: 3497.32
  batch-generator:
     rank  0: 50.69
     rank  1: 56.09
     rank  2: 57.05
     rank  3: 60.34
     rank  4: 72.95
     rank  5: 82.01
     rank  6: 77.46
     rank  7: 84.07
     rank  8: 53.87
     rank  9: 56.78
     rank 10: 58.63
     rank 11: 61.01
     rank 12: 51.48
     rank 13: 59.79
     rank 14: 56.88
     rank 15: 58.05
     rank 16: 55.29
     rank 17: 60.80
     rank 18: 64.34
     rank 19: 60.80
  forward-recv:
     rank  4: 80.49
     rank  5: 78.91
     rank  6: 80.57
     rank  7: 79.48
     rank  8: 276.05
     rank  9: 275.49
     rank 10: 276.53
     rank 11: 276.09
     rank 12: 451.50
     rank 13: 451.92
     rank 14: 451.18
     rank 15: 452.04
     rank 16: 621.64
     rank 17: 621.44
     rank 18: 621.71
     rank 19: 621.28
  forward-send:
     rank  0: 413.15
     rank  1: 409.95
     rank  2: 412.03
     rank  3: 410.45
     rank  4: 31.91
     rank  5: 31.23
     rank  6: 31.09
     rank  7: 30.76
     rank  8: 21.19
     rank  9: 21.03
     rank 10: 20.38
     rank 11: 20.51
     rank 12: 10.56
     rank 13: 10.24
     rank 14: 10.52
     rank 15: 9.89
  backward-recv:
     rank  0: 1633.68
     rank  1: 1633.57
     rank  2: 1635.22
     rank  3: 1634.84
     rank  4: 593.11
     rank  5: 591.83
     rank  6: 591.37
     rank  7: 589.17
     rank  8: 391.51
     rank  9: 391.31
     rank 10: 392.92
     rank 11: 392.73
     rank 12: 194.77
     rank 13: 194.43
     rank 14: 194.56
     rank 15: 195.64
  backward-send:
     rank  4: 21.17
     rank  5: 21.62
     rank  6: 22.36
     rank  7: 23.60
     rank  8: 31.60
     rank  9: 31.45
     rank 10: 30.56
     rank 11: 30.19
     rank 12: 20.75
     rank 13: 20.67
     rank 14: 20.86
     rank 15: 20.19
     rank 16: 10.47
     rank 17: 9.62
     rank 18: 10.44
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 4844.77
     rank  1: 4842.55
     rank  2: 4845.10
     rank  3: 4840.20
     rank  4: 867.95
     rank  5: 869.71
     rank  6: 866.87
     rank  7: 866.70
     rank  8: 697.50
     rank  9: 690.94
     rank 10: 697.91
     rank 11: 694.70
     rank 12: 543.99
     rank 13: 545.04
     rank 14: 540.08
     rank 15: 547.76
  backward-send-forward-recv:
     rank  4: 74.52
     rank  5: 73.07
     rank  6: 74.11
     rank  7: 70.68
     rank  8: 146.35
     rank  9: 146.17
     rank 10: 144.04
     rank 11: 142.65
     rank 12: 156.54
     rank 13: 151.41
     rank 14: 157.10
     rank 15: 153.15
     rank 16: 167.83
     rank 17: 165.63
     rank 18: 162.40
     rank 19: 167.99
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.30
     rank  1: 0.34
     rank  2: 0.36
     rank  3: 0.33
     rank  4: 2.56
     rank  5: 2.41
     rank  6: 2.38
     rank  7: 2.43
     rank  8: 2.18
     rank  9: 2.15
     rank 10: 2.17
     rank 11: 2.17
     rank 12: 2.33
     rank 13: 2.15
     rank 14: 2.31
     rank 15: 2.16
     rank 16: 2.54
     rank 17: 2.38
     rank 18: 2.41
     rank 19: 2.58
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.06
     rank  6: 0.04
     rank  7: 0.07
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.01
     rank  1: 2.10
     rank  2: 2.07
     rank  3: 2.02
     rank  4: 46.31
     rank  5: 46.40
     rank  6: 46.28
     rank  7: 46.49
     rank  8: 43.02
     rank  9: 42.98
     rank 10: 42.99
     rank 11: 42.96
     rank 12: 43.10
     rank 13: 42.99
     rank 14: 43.03
     rank 15: 43.00
     rank 16: 46.39
     rank 17: 46.36
     rank 18: 46.40
     rank 19: 46.46
  optimizer:
     rank  0: 2.66
     rank  1: 2.75
     rank  2: 2.72
     rank  3: 2.67
     rank  4: 46.96
     rank  5: 47.05
     rank  6: 46.92
     rank  7: 47.16
     rank  8: 43.67
     rank  9: 43.63
     rank 10: 43.63
     rank 11: 43.61
     rank 12: 43.75
     rank 13: 43.64
     rank 14: 43.68
     rank 15: 43.65
     rank 16: 47.04
     rank 17: 47.01
     rank 18: 47.05
     rank 19: 47.11
 [2024-12-02 16:51:29] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7959.6 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 3.205440E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7876.61
     rank  1: 7876.55
     rank  2: 7876.61
     rank  3: 7876.61
     rank  4: 7880.81
     rank  5: 7880.73
     rank  6: 7880.78
     rank  7: 7880.75
     rank  8: 7880.48
     rank  9: 7880.42
     rank 10: 7880.48
     rank 11: 7880.48
     rank 12: 7880.50
     rank 13: 7880.40
     rank 14: 7880.50
     rank 15: 7880.44
     rank 16: 7880.71
     rank 17: 7880.71
     rank 18: 7880.70
     rank 19: 7880.71
  forward-compute:
     rank  0: 911.90
     rank  1: 914.74
     rank  2: 921.18
     rank  3: 914.28
     rank  4: 2888.34
     rank  5: 2893.85
     rank  6: 2892.49
     rank  7: 2889.19
     rank  8: 2789.35
     rank  9: 2789.04
     rank 10: 2795.89
     rank 11: 2798.39
     rank 12: 2753.89
     rank 13: 2758.62
     rank 14: 2752.61
     rank 15: 2755.57
     rank 16: 2913.66
     rank 17: 2917.36
     rank 18: 2917.88
     rank 19: 2914.52
  backward-compute:
     rank  0: 51.94
     rank  1: 51.78
     rank  2: 45.26
     rank  3: 50.25
     rank  4: 3324.49
     rank  5: 3324.25
     rank  6: 3328.01
     rank  7: 3328.62
     rank  8: 3293.11
     rank  9: 3297.66
     rank 10: 3292.89
     rank 11: 3295.84
     rank 12: 3277.71
     rank 13: 3279.67
     rank 14: 3280.61
     rank 15: 3278.05
     rank 16: 3501.69
     rank 17: 3503.77
     rank 18: 3503.64
     rank 19: 3500.80
  pure-backward-compute:
     rank  0: 51.29
     rank  1: 51.10
     rank  2: 44.64
     rank  3: 49.60
     rank  4: 3323.36
     rank  5: 3323.50
     rank  6: 3327.08
     rank  7: 3327.08
     rank  8: 3291.89
     rank  9: 3296.32
     rank 10: 3292.02
     rank 11: 3295.10
     rank 12: 3276.46
     rank 13: 3278.86
     rank 14: 3279.26
     rank 15: 3277.30
     rank 16: 3499.31
     rank 17: 3502.12
     rank 18: 3502.15
     rank 19: 3498.36
  batch-generator:
     rank  0: 52.49
     rank  1: 56.25
     rank  2: 63.20
     rank  3: 57.09
     rank  4: 68.21
     rank  5: 76.24
     rank  6: 79.30
     rank  7: 76.73
     rank  8: 52.61
     rank  9: 55.11
     rank 10: 60.77
     rank 11: 63.06
     rank 12: 53.53
     rank 13: 58.91
     rank 14: 57.07
     rank 15: 58.29
     rank 16: 53.25
     rank 17: 58.71
     rank 18: 59.35
     rank 19: 58.63
  forward-recv:
     rank  4: 79.75
     rank  5: 79.54
     rank  6: 78.12
     rank  7: 79.49
     rank  8: 274.94
     rank  9: 274.21
     rank 10: 274.69
     rank 11: 274.80
     rank 12: 452.16
     rank 13: 453.01
     rank 14: 451.65
     rank 15: 453.32
     rank 16: 619.77
     rank 17: 619.50
     rank 18: 619.84
     rank 19: 619.31
  forward-send:
     rank  0: 415.19
     rank  1: 416.02
     rank  2: 411.51
     rank  3: 414.68
     rank  4: 31.31
     rank  5: 32.59
     rank  6: 30.00
     rank  7: 31.69
     rank  8: 20.73
     rank  9: 21.11
     rank 10: 19.92
     rank 11: 20.76
     rank 12: 10.55
     rank 13: 10.18
     rank 14: 10.55
     rank 15: 9.87
  backward-recv:
     rank  0: 1637.45
     rank  1: 1638.59
     rank  2: 1638.51
     rank  3: 1638.68
     rank  4: 594.34
     rank  5: 595.68
     rank  6: 597.11
     rank  7: 596.02
     rank  8: 394.45
     rank  9: 393.82
     rank 10: 394.77
     rank 11: 394.92
     rank 12: 196.50
     rank 13: 197.13
     rank 14: 196.47
     rank 15: 196.76
  backward-send:
     rank  4: 22.72
     rank  5: 22.44
     rank  6: 20.61
     rank  7: 21.89
     rank  8: 31.42
     rank  9: 31.32
     rank 10: 31.18
     rank 11: 30.76
     rank 12: 20.85
     rank 13: 20.15
     rank 14: 20.89
     rank 15: 20.56
     rank 16: 10.58
     rank 17: 10.02
     rank 18: 10.28
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 4853.22
     rank  1: 4848.77
     rank  2: 4852.65
     rank  3: 4852.50
     rank  4: 842.64
     rank  5: 845.05
     rank  6: 839.95
     rank  7: 838.11
     rank  8: 695.24
     rank  9: 691.95
     rank 10: 697.19
     rank 11: 693.30
     rank 12: 559.97
     rank 13: 561.67
     rank 14: 558.81
     rank 15: 564.48
  backward-send-forward-recv:
     rank  4: 74.78
     rank  5: 69.09
     rank  6: 73.07
     rank  7: 74.82
     rank  8: 146.89
     rank  9: 146.71
     rank 10: 142.29
     rank 11: 139.89
     rank 12: 156.52
     rank 13: 151.96
     rank 14: 156.92
     rank 15: 153.32
     rank 16: 168.73
     rank 17: 166.33
     rank 18: 165.50
     rank 19: 169.39
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.34
     rank  1: 0.29
     rank  2: 0.32
     rank  3: 0.31
     rank  4: 2.35
     rank  5: 2.30
     rank  6: 2.30
     rank  7: 2.30
     rank  8: 2.14
     rank  9: 2.19
     rank 10: 2.22
     rank 11: 2.21
     rank 12: 2.26
     rank 13: 2.14
     rank 14: 2.31
     rank 15: 2.14
     rank 16: 2.40
     rank 17: 2.42
     rank 18: 2.42
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.10
     rank  1: 2.01
     rank  2: 2.01
     rank  3: 2.01
     rank  4: 46.40
     rank  5: 46.30
     rank  6: 46.28
     rank  7: 46.31
     rank  8: 43.00
     rank  9: 43.04
     rank 10: 43.05
     rank 11: 43.01
     rank 12: 43.02
     rank 13: 42.96
     rank 14: 43.11
     rank 15: 42.97
     rank 16: 46.29
     rank 17: 46.32
     rank 18: 46.41
     rank 19: 46.39
  optimizer:
     rank  0: 2.67
     rank  1: 2.58
     rank  2: 2.58
     rank  3: 2.58
     rank  4: 46.98
     rank  5: 46.87
     rank  6: 46.86
     rank  7: 46.88
     rank  8: 43.57
     rank  9: 43.61
     rank 10: 43.62
     rank 11: 43.58
     rank 12: 43.59
     rank 13: 43.53
     rank 14: 43.68
     rank 15: 43.54
     rank 16: 46.87
     rank 17: 46.89
     rank 18: 46.98
     rank 19: 47.02
 [2024-12-02 16:51:37] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7980.4 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.865407E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7897.99
     rank  1: 7897.96
     rank  2: 7898.05
     rank  3: 7898.02
     rank  4: 7902.15
     rank  5: 7902.12
     rank  6: 7902.20
     rank  7: 7902.18
     rank  8: 7901.85
     rank  9: 7901.83
     rank 10: 7901.87
     rank 11: 7901.89
     rank 12: 7901.84
     rank 13: 7901.80
     rank 14: 7901.84
     rank 15: 7901.84
     rank 16: 7902.11
     rank 17: 7902.09
     rank 18: 7902.09
     rank 19: 7902.11
  forward-compute:
     rank  0: 916.11
     rank  1: 922.01
     rank  2: 921.56
     rank  3: 922.86
     rank  4: 2877.86
     rank  5: 2883.62
     rank  6: 2880.80
     rank  7: 2888.05
     rank  8: 2777.72
     rank  9: 2778.14
     rank 10: 2782.54
     rank 11: 2784.07
     rank 12: 2761.98
     rank 13: 2766.69
     rank 14: 2760.51
     rank 15: 2763.57
     rank 16: 2915.72
     rank 17: 2919.22
     rank 18: 2920.78
     rank 19: 2916.40
  backward-compute:
     rank  0: 51.05
     rank  1: 51.23
     rank  2: 46.68
     rank  3: 52.06
     rank  4: 3339.22
     rank  5: 3339.34
     rank  6: 3340.28
     rank  7: 3338.65
     rank  8: 3278.00
     rank  9: 3282.11
     rank 10: 3278.67
     rank 11: 3281.48
     rank 12: 3275.47
     rank 13: 3279.95
     rank 14: 3278.75
     rank 15: 3275.48
     rank 16: 3517.53
     rank 17: 3520.07
     rank 18: 3519.92
     rank 19: 3517.68
  pure-backward-compute:
     rank  0: 50.42
     rank  1: 50.57
     rank  2: 46.04
     rank  3: 51.38
     rank  4: 3338.22
     rank  5: 3338.53
     rank  6: 3339.26
     rank  7: 3337.73
     rank  8: 3276.82
     rank  9: 3280.98
     rank 10: 3277.76
     rank 11: 3280.48
     rank 12: 3274.07
     rank 13: 3279.20
     rank 14: 3277.45
     rank 15: 3274.74
     rank 16: 3515.13
     rank 17: 3518.50
     rank 18: 3518.41
     rank 19: 3515.11
  batch-generator:
     rank  0: 51.13
     rank  1: 58.03
     rank  2: 57.68
     rank  3: 59.87
     rank  4: 74.48
     rank  5: 79.91
     rank  6: 76.21
     rank  7: 85.22
     rank  8: 52.77
     rank  9: 55.69
     rank 10: 58.75
     rank 11: 60.10
     rank 12: 52.36
     rank 13: 58.82
     rank 14: 57.06
     rank 15: 58.16
     rank 16: 56.09
     rank 17: 61.30
     rank 18: 62.92
     rank 19: 61.71
  forward-recv:
     rank  4: 80.67
     rank  5: 78.89
     rank  6: 80.75
     rank  7: 80.48
     rank  8: 279.44
     rank  9: 279.08
     rank 10: 280.64
     rank 11: 279.47
     rank 12: 452.88
     rank 13: 453.02
     rank 14: 452.60
     rank 15: 453.87
     rank 16: 621.42
     rank 17: 621.34
     rank 18: 621.32
     rank 19: 620.86
  forward-send:
     rank  0: 413.83
     rank  1: 409.29
     rank  2: 413.44
     rank  3: 412.11
     rank  4: 31.79
     rank  5: 30.29
     rank  6: 31.58
     rank  7: 30.98
     rank  8: 21.16
     rank  9: 20.78
     rank 10: 20.22
     rank 11: 20.75
     rank 12: 10.49
     rank 13: 10.29
     rank 14: 10.44
     rank 15: 9.74
  backward-recv:
     rank  0: 1639.14
     rank  1: 1639.98
     rank  2: 1639.72
     rank  3: 1640.95
     rank  4: 599.11
     rank  5: 598.98
     rank  6: 598.41
     rank  7: 601.01
     rank  8: 399.29
     rank  9: 398.71
     rank 10: 399.63
     rank 11: 398.43
     rank 12: 198.38
     rank 13: 199.05
     rank 14: 198.16
     rank 15: 199.06
  backward-send:
     rank  4: 21.98
     rank  5: 22.45
     rank  6: 22.47
     rank  7: 21.71
     rank  8: 31.23
     rank  9: 31.24
     rank 10: 30.66
     rank 11: 31.40
     rank 12: 21.01
     rank 13: 20.43
     rank 14: 20.69
     rank 15: 20.50
     rank 16: 10.53
     rank 17: 10.45
     rank 18: 10.14
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 4870.40
     rank  1: 4868.31
     rank  2: 4869.70
     rank  3: 4863.51
     rank  4: 854.52
     rank  5: 856.62
     rank  6: 852.02
     rank  7: 852.85
     rank  8: 733.26
     rank  9: 729.55
     rank 10: 734.23
     rank 11: 731.42
     rank 12: 574.34
     rank 13: 576.10
     rank 14: 573.79
     rank 15: 579.63
  backward-send-forward-recv:
     rank  4: 74.64
     rank  5: 73.11
     rank  6: 74.62
     rank  7: 69.28
     rank  8: 146.35
     rank  9: 146.66
     rank 10: 142.73
     rank 11: 142.12
     rank 12: 155.01
     rank 13: 148.33
     rank 14: 155.38
     rank 15: 151.95
     rank 16: 168.40
     rank 17: 166.13
     rank 18: 164.71
     rank 19: 168.50
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.30
     rank  1: 0.29
     rank  2: 0.37
     rank  3: 0.36
     rank  4: 2.34
     rank  5: 2.30
     rank  6: 2.35
     rank  7: 2.53
     rank  8: 2.15
     rank  9: 2.17
     rank 10: 2.16
     rank 11: 2.21
     rank 12: 2.21
     rank 13: 2.20
     rank 14: 2.17
     rank 15: 2.16
     rank 16: 2.42
     rank 17: 2.40
     rank 18: 2.41
     rank 19: 2.53
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.00
     rank  1: 2.00
     rank  2: 2.11
     rank  3: 2.08
     rank  4: 46.34
     rank  5: 46.32
     rank  6: 46.32
     rank  7: 46.43
     rank  8: 43.00
     rank  9: 42.99
     rank 10: 42.99
     rank 11: 43.05
     rank 12: 43.07
     rank 13: 42.94
     rank 14: 43.01
     rank 15: 42.97
     rank 16: 46.43
     rank 17: 46.28
     rank 18: 46.33
     rank 19: 46.44
  optimizer:
     rank  0: 2.58
     rank  1: 2.59
     rank  2: 2.70
     rank  3: 2.67
     rank  4: 46.93
     rank  5: 46.90
     rank  6: 46.90
     rank  7: 47.02
     rank  8: 43.58
     rank  9: 43.58
     rank 10: 43.57
     rank 11: 43.63
     rank 12: 43.66
     rank 13: 43.53
     rank 14: 43.60
     rank 15: 43.56
     rank 16: 47.02
     rank 17: 46.86
     rank 18: 46.91
     rank 19: 47.02
 [2024-12-02 16:51:45] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7983.3 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.346552E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7901.14
     rank  1: 7901.15
     rank  2: 7901.17
     rank  3: 7901.19
     rank  4: 7905.34
     rank  5: 7905.33
     rank  6: 7905.38
     rank  7: 7905.33
     rank  8: 7905.01
     rank  9: 7905.02
     rank 10: 7905.08
     rank 11: 7905.02
     rank 12: 7905.02
     rank 13: 7905.02
     rank 14: 7905.10
     rank 15: 7905.01
     rank 16: 7905.29
     rank 17: 7905.31
     rank 18: 7905.28
     rank 19: 7905.29
  forward-compute:
     rank  0: 911.84
     rank  1: 915.18
     rank  2: 916.84
     rank  3: 923.72
     rank  4: 2875.40
     rank  5: 2880.41
     rank  6: 2877.27
     rank  7: 2886.81
     rank  8: 2779.18
     rank  9: 2779.45
     rank 10: 2783.11
     rank 11: 2786.56
     rank 12: 2753.15
     rank 13: 2758.85
     rank 14: 2751.76
     rank 15: 2754.56
     rank 16: 2926.42
     rank 17: 2930.75
     rank 18: 2932.35
     rank 19: 2927.15
  backward-compute:
     rank  0: 50.93
     rank  1: 51.29
     rank  2: 45.34
     rank  3: 50.53
     rank  4: 3339.67
     rank  5: 3341.95
     rank  6: 3342.86
     rank  7: 3341.07
     rank  8: 3281.10
     rank  9: 3285.09
     rank 10: 3281.17
     rank 11: 3282.45
     rank 12: 3280.44
     rank 13: 3282.63
     rank 14: 3285.20
     rank 15: 3281.63
     rank 16: 3515.05
     rank 17: 3516.67
     rank 18: 3516.85
     rank 19: 3515.12
  pure-backward-compute:
     rank  0: 50.30
     rank  1: 50.62
     rank  2: 44.72
     rank  3: 49.90
     rank  4: 3338.66
     rank  5: 3341.10
     rank  6: 3341.68
     rank  7: 3339.99
     rank  8: 3279.86
     rank  9: 3283.97
     rank 10: 3280.45
     rank 11: 3281.65
     rank 12: 3279.27
     rank 13: 3281.73
     rank 14: 3283.92
     rank 15: 3280.79
     rank 16: 3512.35
     rank 17: 3515.11
     rank 18: 3515.39
     rank 19: 3512.70
  batch-generator:
     rank  0: 50.54
     rank  1: 55.03
     rank  2: 56.90
     rank  3: 64.50
     rank  4: 62.80
     rank  5: 70.33
     rank  6: 71.52
     rank  7: 82.26
     rank  8: 54.46
     rank  9: 57.33
     rank 10: 58.80
     rank 11: 62.79
     rank 12: 51.07
     rank 13: 58.27
     rank 14: 55.15
     rank 15: 56.18
     rank 16: 66.49
     rank 17: 68.63
     rank 18: 70.24
     rank 19: 67.90
  forward-recv:
     rank  4: 77.71
     rank  5: 76.77
     rank  6: 77.36
     rank  7: 73.71
     rank  8: 275.95
     rank  9: 275.44
     rank 10: 276.55
     rank 11: 274.36
     rank 12: 451.93
     rank 13: 452.38
     rank 14: 451.79
     rank 15: 452.38
     rank 16: 617.81
     rank 17: 617.52
     rank 18: 617.85
     rank 19: 617.37
  forward-send:
     rank  0: 416.42
     rank  1: 413.56
     rank  2: 416.77
     rank  3: 408.85
     rank  4: 31.64
     rank  5: 30.60
     rank  6: 31.88
     rank  7: 28.93
     rank  8: 21.26
     rank  9: 21.07
     rank 10: 20.88
     rank 11: 20.65
     rank 12: 10.57
     rank 13: 10.17
     rank 14: 10.60
     rank 15: 9.94
  backward-recv:
     rank  0: 1638.16
     rank  1: 1639.54
     rank  2: 1640.18
     rank  3: 1638.78
     rank  4: 595.03
     rank  5: 594.49
     rank  6: 592.69
     rank  7: 595.77
     rank  8: 392.56
     rank  9: 391.96
     rank 10: 393.18
     rank 11: 393.36
     rank 12: 198.01
     rank 13: 198.50
     rank 14: 197.94
     rank 15: 198.21
  backward-send:
     rank  4: 21.70
     rank  5: 22.52
     rank  6: 22.62
     rank  7: 20.89
     rank  8: 31.67
     rank  9: 31.68
     rank 10: 30.16
     rank 11: 30.53
     rank 12: 20.97
     rank 13: 20.28
     rank 14: 20.65
     rank 15: 20.79
     rank 16: 10.62
     rank 17: 10.20
     rank 18: 10.06
     rank 19: 10.57
  forward-send-backward-recv:
     rank  0: 4876.56
     rank  1: 4874.92
     rank  2: 4875.64
     rank  3: 4873.14
     rank  4: 866.78
     rank  5: 868.44
     rank  6: 864.76
     rank  7: 866.44
     rank  8: 742.70
     rank  9: 739.51
     rank 10: 743.16
     rank 11: 742.28
     rank 12: 582.03
     rank 13: 582.35
     rank 14: 579.13
     rank 15: 585.79
  backward-send-forward-recv:
     rank  4: 75.56
     rank  5: 71.95
     rank  6: 74.05
     rank  7: 72.55
     rank  8: 145.91
     rank  9: 146.07
     rank 10: 143.60
     rank 11: 142.57
     rank 12: 156.40
     rank 13: 151.43
     rank 14: 155.51
     rank 15: 153.66
     rank 16: 167.51
     rank 17: 164.92
     rank 18: 163.03
     rank 19: 167.72
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.29
     rank  1: 0.29
     rank  2: 0.32
     rank  3: 0.30
     rank  4: 2.34
     rank  5: 2.36
     rank  6: 2.36
     rank  7: 2.32
     rank  8: 2.21
     rank  9: 2.19
     rank 10: 2.29
     rank 11: 2.15
     rank 12: 2.15
     rank 13: 2.17
     rank 14: 2.20
     rank 15: 2.15
     rank 16: 2.38
     rank 17: 2.40
     rank 18: 2.41
     rank 19: 2.37
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.11
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 1.99
     rank  2: 2.00
     rank  3: 2.01
     rank  4: 46.35
     rank  5: 46.36
     rank  6: 46.35
     rank  7: 46.32
     rank  8: 43.04
     rank  9: 43.00
     rank 10: 43.03
     rank 11: 42.95
     rank 12: 42.98
     rank 13: 43.03
     rank 14: 43.06
     rank 15: 42.99
     rank 16: 46.32
     rank 17: 46.26
     rank 18: 46.33
     rank 19: 46.39
  optimizer:
     rank  0: 2.59
     rank  1: 2.59
     rank  2: 2.60
     rank  3: 2.61
     rank  4: 46.95
     rank  5: 46.96
     rank  6: 46.96
     rank  7: 46.92
     rank  8: 43.64
     rank  9: 43.60
     rank 10: 43.63
     rank 11: 43.55
     rank 12: 43.58
     rank 13: 43.63
     rank 14: 43.66
     rank 15: 43.59
     rank 16: 46.92
     rank 17: 46.86
     rank 18: 46.93
     rank 19: 46.99
 [2024-12-02 16:51:53] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7951.2 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.426261E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7867.08
     rank  1: 7867.08
     rank  2: 7867.38
     rank  3: 7867.13
     rank  4: 7871.25
     rank  5: 7871.30
     rank  6: 7871.50
     rank  7: 7871.31
     rank  8: 7870.98
     rank  9: 7870.96
     rank 10: 7871.22
     rank 11: 7870.98
     rank 12: 7870.95
     rank 13: 7870.93
     rank 14: 7871.21
     rank 15: 7870.94
     rank 16: 7871.23
     rank 17: 7871.23
     rank 18: 7871.93
     rank 19: 7871.25
  forward-compute:
     rank  0: 914.39
     rank  1: 915.54
     rank  2: 918.18
     rank  3: 923.07
     rank  4: 2875.10
     rank  5: 2877.65
     rank  6: 2875.84
     rank  7: 2883.06
     rank  8: 2791.78
     rank  9: 2792.59
     rank 10: 2798.63
     rank 11: 2800.67
     rank 12: 2752.44
     rank 13: 2759.69
     rank 14: 2751.00
     rank 15: 2754.66
     rank 16: 2907.41
     rank 17: 2909.27
     rank 18: 2913.05
     rank 19: 2907.76
  backward-compute:
     rank  0: 50.16
     rank  1: 51.24
     rank  2: 45.79
     rank  3: 50.91
     rank  4: 3336.98
     rank  5: 3338.74
     rank  6: 3339.85
     rank  7: 3339.01
     rank  8: 3284.06
     rank  9: 3289.94
     rank 10: 3284.03
     rank 11: 3286.19
     rank 12: 3275.87
     rank 13: 3275.27
     rank 14: 3278.27
     rank 15: 3276.27
     rank 16: 3500.85
     rank 17: 3502.47
     rank 18: 3502.57
     rank 19: 3499.94
  pure-backward-compute:
     rank  0: 49.54
     rank  1: 50.58
     rank  2: 45.19
     rank  3: 50.27
     rank  4: 3336.03
     rank  5: 3337.86
     rank  6: 3338.81
     rank  7: 3338.07
     rank  8: 3282.60
     rank  9: 3288.83
     rank 10: 3283.28
     rank 11: 3285.41
     rank 12: 3274.66
     rank 13: 3274.58
     rank 14: 3277.08
     rank 15: 3275.52
     rank 16: 3498.68
     rank 17: 3500.89
     rank 18: 3501.11
     rank 19: 3497.64
  batch-generator:
     rank  0: 50.92
     rank  1: 52.99
     rank  2: 56.57
     rank  3: 62.09
     rank  4: 54.89
     rank  5: 61.58
     rank  6: 63.84
     rank  7: 76.01
     rank  8: 62.53
     rank  9: 63.38
     rank 10: 67.67
     rank 11: 69.90
     rank 12: 51.24
     rank 13: 60.32
     rank 14: 55.55
     rank 15: 57.08
     rank 16: 52.87
     rank 17: 56.44
     rank 18: 60.45
     rank 19: 57.81
  forward-recv:
     rank  4: 79.42
     rank  5: 78.66
     rank  6: 79.42
     rank  7: 78.11
     rank  8: 272.25
     rank  9: 272.22
     rank 10: 272.41
     rank 11: 271.91
     rank 12: 448.50
     rank 13: 449.08
     rank 14: 448.24
     rank 15: 449.07
     rank 16: 618.04
     rank 17: 617.73
     rank 18: 618.08
     rank 19: 617.77
  forward-send:
     rank  0: 415.04
     rank  1: 412.31
     rank  2: 413.40
     rank  3: 410.89
     rank  4: 32.43
     rank  5: 31.56
     rank  6: 31.32
     rank  7: 30.96
     rank  8: 21.29
     rank  9: 20.98
     rank 10: 20.58
     rank 11: 20.78
     rank 12: 10.64
     rank 13: 10.05
     rank 14: 10.66
     rank 15: 10.11
  backward-recv:
     rank  0: 1631.83
     rank  1: 1632.39
     rank  2: 1633.90
     rank  3: 1631.64
     rank  4: 589.01
     rank  5: 588.74
     rank  6: 589.13
     rank  7: 589.35
     rank  8: 391.53
     rank  9: 390.91
     rank 10: 392.22
     rank 11: 393.02
     rank 12: 194.74
     rank 13: 195.53
     rank 14: 194.70
     rank 15: 194.91
  backward-send:
     rank  4: 21.88
     rank  5: 22.17
     rank  6: 21.71
     rank  7: 20.62
     rank  8: 31.55
     rank  9: 31.34
     rank 10: 30.62
     rank 11: 30.34
     rank 12: 20.92
     rank 13: 19.98
     rank 14: 20.71
     rank 15: 20.76
     rank 16: 10.56
     rank 17: 10.31
     rank 18: 10.20
     rank 19: 10.67
  forward-send-backward-recv:
     rank  0: 4848.02
     rank  1: 4848.12
     rank  2: 4848.76
     rank  3: 4843.25
     rank  4: 840.79
     rank  5: 840.71
     rank  6: 836.76
     rank  7: 838.45
     rank  8: 697.81
     rank  9: 693.13
     rank 10: 699.36
     rank 11: 696.86
     rank 12: 561.17
     rank 13: 563.99
     rank 14: 560.02
     rank 15: 565.12
  backward-send-forward-recv:
     rank  4: 74.13
     rank  5: 73.24
     rank  6: 75.24
     rank  7: 70.98
     rank  8: 146.01
     rank  9: 145.65
     rank 10: 141.81
     rank 11: 140.33
     rank 12: 156.69
     rank 13: 150.60
     rank 14: 157.34
     rank 15: 153.54
     rank 16: 168.35
     rank 17: 167.58
     rank 18: 163.66
     rank 19: 169.19
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.16
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.29
     rank  1: 0.27
     rank  2: 0.31
     rank  3: 0.31
     rank  4: 2.29
     rank  5: 2.35
     rank  6: 2.33
     rank  7: 2.30
     rank  8: 2.25
     rank  9: 2.24
     rank 10: 2.16
     rank 11: 2.14
     rank 12: 2.17
     rank 13: 2.15
     rank 14: 2.17
     rank 15: 2.14
     rank 16: 2.51
     rank 17: 2.45
     rank 18: 2.98
     rank 19: 2.45
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.12
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.00
     rank  1: 1.99
     rank  2: 2.01
     rank  3: 2.01
     rank  4: 46.29
     rank  5: 46.32
     rank  6: 46.34
     rank  7: 46.28
     rank  8: 43.07
     rank  9: 43.02
     rank 10: 42.97
     rank 11: 42.97
     rank 12: 42.97
     rank 13: 42.94
     rank 14: 42.98
     rank 15: 42.97
     rank 16: 46.33
     rank 17: 46.32
     rank 18: 46.94
     rank 19: 46.37
  optimizer:
     rank  0: 3.23
     rank  1: 3.23
     rank  2: 3.25
     rank  3: 3.24
     rank  4: 47.53
     rank  5: 47.57
     rank  6: 47.57
     rank  7: 47.53
     rank  8: 44.31
     rank  9: 44.25
     rank 10: 44.21
     rank 11: 44.21
     rank 12: 44.20
     rank 13: 44.17
     rank 14: 44.22
     rank 15: 44.20
     rank 16: 47.56
     rank 17: 47.55
     rank 18: 48.15
     rank 19: 47.60
 [2024-12-02 16:52:01] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7961.1 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.166870E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7879.92
     rank  1: 7879.86
     rank  2: 7879.91
     rank  3: 7879.97
     rank  4: 7884.10
     rank  5: 7884.04
     rank  6: 7884.06
     rank  7: 7884.10
     rank  8: 7883.80
     rank  9: 7883.74
     rank 10: 7883.78
     rank 11: 7883.86
     rank 12: 7883.80
     rank 13: 7883.71
     rank 14: 7883.77
     rank 15: 7883.81
     rank 16: 7884.10
     rank 17: 7884.03
     rank 18: 7884.04
     rank 19: 7884.11
  forward-compute:
     rank  0: 923.61
     rank  1: 932.34
     rank  2: 927.76
     rank  3: 932.26
     rank  4: 2871.87
     rank  5: 2877.50
     rank  6: 2872.79
     rank  7: 2882.06
     rank  8: 2789.55
     rank  9: 2790.09
     rank 10: 2793.55
     rank 11: 2797.39
     rank 12: 2756.05
     rank 13: 2763.68
     rank 14: 2754.53
     rank 15: 2758.32
     rank 16: 2916.56
     rank 17: 2921.76
     rank 18: 2920.47
     rank 19: 2917.82
  backward-compute:
     rank  0: 50.45
     rank  1: 47.02
     rank  2: 45.82
     rank  3: 51.21
     rank  4: 3330.70
     rank  5: 3332.59
     rank  6: 3332.37
     rank  7: 3332.50
     rank  8: 3286.84
     rank  9: 3293.79
     rank 10: 3287.52
     rank 11: 3288.41
     rank 12: 3287.51
     rank 13: 3289.65
     rank 14: 3289.34
     rank 15: 3288.81
     rank 16: 3498.62
     rank 17: 3500.58
     rank 18: 3499.62
     rank 19: 3497.89
  pure-backward-compute:
     rank  0: 49.81
     rank  1: 46.37
     rank  2: 45.20
     rank  3: 50.57
     rank  4: 3329.63
     rank  5: 3331.69
     rank  6: 3331.27
     rank  7: 3331.70
     rank  8: 3285.71
     rank  9: 3292.72
     rank 10: 3286.78
     rank 11: 3287.59
     rank 12: 3286.25
     rank 13: 3288.99
     rank 14: 3288.18
     rank 15: 3288.07
     rank 16: 3495.93
     rank 17: 3499.02
     rank 18: 3497.67
     rank 19: 3495.44
  batch-generator:
     rank  0: 55.34
     rank  1: 63.96
     rank  2: 59.18
     rank  3: 64.30
     rank  4: 70.33
     rank  5: 76.86
     rank  6: 74.33
     rank  7: 82.49
     rank  8: 59.02
     rank  9: 60.72
     rank 10: 61.99
     rank 11: 65.83
     rank 12: 51.34
     rank 13: 62.02
     rank 14: 57.22
     rank 15: 58.17
     rank 16: 62.79
     rank 17: 67.40
     rank 18: 66.53
     rank 19: 66.38
  forward-recv:
     rank  4: 79.53
     rank  5: 77.26
     rank  6: 79.46
     rank  7: 79.26
     rank  8: 274.53
     rank  9: 273.64
     rank 10: 275.07
     rank 11: 274.51
     rank 12: 450.32
     rank 13: 450.48
     rank 14: 450.26
     rank 15: 450.84
     rank 16: 618.56
     rank 17: 618.55
     rank 18: 618.46
     rank 19: 618.23
  forward-send:
     rank  0: 412.44
     rank  1: 406.90
     rank  2: 412.08
     rank  3: 410.43
     rank  4: 32.65
     rank  5: 31.11
     rank  6: 32.49
     rank  7: 31.89
     rank  8: 21.19
     rank  9: 20.93
     rank 10: 20.68
     rank 11: 20.73
     rank 12: 10.56
     rank 13: 10.30
     rank 14: 10.52
     rank 15: 10.03
  backward-recv:
     rank  0: 1640.89
     rank  1: 1642.15
     rank  2: 1642.03
     rank  3: 1641.34
     rank  4: 597.66
     rank  5: 596.58
     rank  6: 598.14
     rank  7: 597.87
     rank  8: 398.22
     rank  9: 397.24
     rank 10: 398.23
     rank 11: 398.28
     rank 12: 196.31
     rank 13: 196.13
     rank 14: 196.32
     rank 15: 196.50
  backward-send:
     rank  4: 22.40
     rank  5: 22.49
     rank  6: 21.64
     rank  7: 21.35
     rank  8: 31.30
     rank  9: 31.20
     rank 10: 31.27
     rank 11: 31.10
     rank 12: 20.88
     rank 13: 20.81
     rank 14: 20.66
     rank 15: 20.79
     rank 16: 10.51
     rank 17: 10.15
     rank 18: 10.17
     rank 19: 10.62
  forward-send-backward-recv:
     rank  0: 4844.15
     rank  1: 4842.90
     rank  2: 4844.49
     rank  3: 4836.86
     rank  4: 851.17
     rank  5: 853.65
     rank  6: 849.64
     rank  7: 849.81
     rank  8: 699.38
     rank  9: 694.15
     rank 10: 700.31
     rank 11: 698.40
     rank 12: 552.67
     rank 13: 552.86
     rank 14: 551.18
     rank 15: 555.86
  backward-send-forward-recv:
     rank  4: 75.11
     rank  5: 72.81
     rank  6: 74.76
     rank  7: 68.62
     rank  8: 146.45
     rank  9: 146.36
     rank 10: 143.77
     rank 11: 141.34
     rank 12: 156.74
     rank 13: 150.35
     rank 14: 157.50
     rank 15: 153.38
     rank 16: 168.68
     rank 17: 165.03
     rank 18: 166.06
     rank 19: 169.17
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.28
     rank  1: 0.28
     rank  2: 0.30
     rank  3: 0.30
     rank  4: 2.61
     rank  5: 2.29
     rank  6: 2.30
     rank  7: 2.31
     rank  8: 2.25
     rank  9: 2.25
     rank 10: 2.17
     rank 11: 2.21
     rank 12: 2.17
     rank 13: 2.14
     rank 14: 2.16
     rank 15: 2.20
     rank 16: 2.45
     rank 17: 2.45
     rank 18: 2.47
     rank 19: 2.48
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.11
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 1.99
     rank  2: 2.00
     rank  3: 1.99
     rank  4: 46.27
     rank  5: 46.26
     rank  6: 46.25
     rank  7: 46.28
     rank  8: 43.02
     rank  9: 43.00
     rank 10: 43.00
     rank 11: 43.04
     rank 12: 43.02
     rank 13: 42.99
     rank 14: 43.02
     rank 15: 43.02
     rank 16: 46.37
     rank 17: 46.29
     rank 18: 46.47
     rank 19: 46.42
  optimizer:
     rank  0: 2.58
     rank  1: 2.58
     rank  2: 2.59
     rank  3: 2.58
     rank  4: 46.86
     rank  5: 46.85
     rank  6: 46.84
     rank  7: 46.86
     rank  8: 43.60
     rank  9: 43.59
     rank 10: 43.59
     rank 11: 43.64
     rank 12: 43.60
     rank 13: 43.57
     rank 14: 43.60
     rank 15: 43.60
     rank 16: 46.95
     rank 17: 46.88
     rank 18: 47.06
     rank 19: 47.01
 [2024-12-02 16:52:09] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7963.4 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.505956E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7879.79
     rank  1: 7879.82
     rank  2: 7879.80
     rank  3: 7879.89
     rank  4: 7883.92
     rank  5: 7883.94
     rank  6: 7883.95
     rank  7: 7884.01
     rank  8: 7883.64
     rank  9: 7883.66
     rank 10: 7883.69
     rank 11: 7883.78
     rank 12: 7883.65
     rank 13: 7883.65
     rank 14: 7883.66
     rank 15: 7883.73
     rank 16: 7883.96
     rank 17: 7883.88
     rank 18: 7883.89
     rank 19: 7883.95
  forward-compute:
     rank  0: 949.82
     rank  1: 955.46
     rank  2: 959.05
     rank  3: 956.49
     rank  4: 2879.54
     rank  5: 2881.73
     rank  6: 2885.03
     rank  7: 2888.56
     rank  8: 2790.88
     rank  9: 2790.22
     rank 10: 2796.79
     rank 11: 2792.90
     rank 12: 2759.94
     rank 13: 2766.66
     rank 14: 2758.92
     rank 15: 2761.35
     rank 16: 2921.90
     rank 17: 2926.67
     rank 18: 2924.75
     rank 19: 2921.60
  backward-compute:
     rank  0: 50.79
     rank  1: 46.92
     rank  2: 46.27
     rank  3: 52.39
     rank  4: 3325.88
     rank  5: 3328.69
     rank  6: 3328.28
     rank  7: 3323.96
     rank  8: 3283.58
     rank  9: 3289.87
     rank 10: 3284.61
     rank 11: 3285.85
     rank 12: 3292.93
     rank 13: 3294.73
     rank 14: 3293.70
     rank 15: 3293.28
     rank 16: 3492.86
     rank 17: 3494.80
     rank 18: 3494.54
     rank 19: 3492.39
  pure-backward-compute:
     rank  0: 50.16
     rank  1: 46.26
     rank  2: 45.65
     rank  3: 51.73
     rank  4: 3324.99
     rank  5: 3327.89
     rank  6: 3327.04
     rank  7: 3323.23
     rank  8: 3282.51
     rank  9: 3288.78
     rank 10: 3283.91
     rank 11: 3284.74
     rank 12: 3291.64
     rank 13: 3294.03
     rank 14: 3292.56
     rank 15: 3292.46
     rank 16: 3490.35
     rank 17: 3493.20
     rank 18: 3492.79
     rank 19: 3489.61
  batch-generator:
     rank  0: 50.07
     rank  1: 57.28
     rank  2: 60.37
     rank  3: 58.50
     rank  4: 66.37
     rank  5: 72.27
     rank  6: 81.40
     rank  7: 80.68
     rank  8: 51.96
     rank  9: 54.32
     rank 10: 59.04
     rank 11: 57.88
     rank 12: 52.32
     rank 13: 60.82
     rank 14: 57.15
     rank 15: 57.49
     rank 16: 58.27
     rank 17: 63.44
     rank 18: 61.77
     rank 19: 62.61
  forward-recv:
     rank  4: 78.94
     rank  5: 78.13
     rank  6: 75.60
     rank  7: 79.01
     rank  8: 273.15
     rank  9: 272.72
     rank 10: 271.67
     rank 11: 272.93
     rank 12: 451.33
     rank 13: 451.62
     rank 14: 451.10
     rank 15: 452.00
     rank 16: 621.87
     rank 17: 621.66
     rank 18: 621.78
     rank 19: 621.35
  forward-send:
     rank  0: 397.55
     rank  1: 395.84
     rank  2: 391.66
     rank  3: 396.81
     rank  4: 37.40
     rank  5: 37.05
     rank  6: 34.67
     rank  7: 37.10
     rank  8: 21.08
     rank  9: 21.03
     rank 10: 20.47
     rank 11: 21.00
     rank 12: 10.58
     rank 13: 10.30
     rank 14: 10.52
     rank 15: 10.01
  backward-recv:
     rank  0: 1633.02
     rank  1: 1633.85
     rank  2: 1633.31
     rank  3: 1631.74
     rank  4: 594.60
     rank  5: 593.46
     rank  6: 593.71
     rank  7: 594.89
     rank  8: 393.73
     rank  9: 393.73
     rank 10: 395.06
     rank 11: 395.08
     rank 12: 193.04
     rank 13: 193.78
     rank 14: 193.15
     rank 15: 193.32
  backward-send:
     rank  4: 22.43
     rank  5: 22.07
     rank  6: 22.21
     rank  7: 21.09
     rank  8: 31.77
     rank  9: 31.08
     rank 10: 31.01
     rank 11: 31.18
     rank 12: 20.93
     rank 13: 19.53
     rank 14: 20.97
     rank 15: 20.58
     rank 16: 10.50
     rank 17: 9.98
     rank 18: 10.38
     rank 19: 10.55
  forward-send-backward-recv:
     rank  0: 4842.04
     rank  1: 4840.91
     rank  2: 4842.99
     rank  3: 4835.54
     rank  4: 850.74
     rank  5: 853.13
     rank  6: 848.14
     rank  7: 851.95
     rank  8: 708.36
     rank  9: 703.93
     rank 10: 709.14
     rank 11: 706.21
     rank 12: 546.32
     rank 13: 547.15
     rank 14: 546.63
     rank 15: 550.93
  backward-send-forward-recv:
     rank  4: 74.11
     rank  5: 71.76
     rank  6: 74.91
     rank  7: 68.43
     rank  8: 146.41
     rank  9: 146.53
     rank 10: 143.48
     rank 11: 146.00
     rank 12: 157.38
     rank 13: 151.78
     rank 14: 157.91
     rank 15: 154.43
     rank 16: 167.88
     rank 17: 164.61
     rank 18: 166.10
     rank 19: 169.18
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.29
     rank  1: 0.33
     rank  2: 0.30
     rank  3: 0.37
     rank  4: 2.40
     rank  5: 2.28
     rank  6: 2.30
     rank  7: 2.33
     rank  8: 2.16
     rank  9: 2.16
     rank 10: 2.16
     rank 11: 2.26
     rank 12: 2.23
     rank 13: 2.15
     rank 14: 2.21
     rank 15: 2.19
     rank 16: 2.47
     rank 17: 2.30
     rank 18: 2.34
     rank 19: 2.46
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 2.06
     rank  2: 2.00
     rank  3: 2.10
     rank  4: 46.27
     rank  5: 46.26
     rank  6: 46.26
     rank  7: 46.38
     rank  8: 43.01
     rank  9: 43.01
     rank 10: 43.00
     rank 11: 43.19
     rank 12: 43.06
     rank 13: 42.97
     rank 14: 42.99
     rank 15: 43.11
     rank 16: 46.37
     rank 17: 46.27
     rank 18: 46.41
     rank 19: 46.38
  optimizer:
     rank  0: 2.60
     rank  1: 2.67
     rank  2: 2.61
     rank  3: 2.71
     rank  4: 46.89
     rank  5: 46.87
     rank  6: 46.87
     rank  7: 46.99
     rank  8: 43.62
     rank  9: 43.62
     rank 10: 43.65
     rank 11: 43.81
     rank 12: 43.67
     rank 13: 43.58
     rank 14: 43.60
     rank 15: 43.72
     rank 16: 46.98
     rank 17: 46.90
     rank 18: 47.04
     rank 19: 46.99
 [2024-12-02 16:52:17] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7966.0 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 8.697761E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7882.74
     rank  1: 7882.72
     rank  2: 7882.68
     rank  3: 7882.78
     rank  4: 7887.02
     rank  5: 7886.88
     rank  6: 7887.31
     rank  7: 7886.96
     rank  8: 7886.65
     rank  9: 7886.55
     rank 10: 7886.73
     rank 11: 7886.66
     rank 12: 7886.71
     rank 13: 7886.55
     rank 14: 7886.75
     rank 15: 7886.76
     rank 16: 7886.86
     rank 17: 7886.85
     rank 18: 7886.84
     rank 19: 7886.98
  forward-compute:
     rank  0: 910.81
     rank  1: 918.20
     rank  2: 918.44
     rank  3: 917.18
     rank  4: 2885.25
     rank  5: 2888.75
     rank  6: 2889.32
     rank  7: 2893.51
     rank  8: 2787.73
     rank  9: 2787.55
     rank 10: 2792.67
     rank 11: 2796.02
     rank 12: 2766.65
     rank 13: 2773.99
     rank 14: 2766.02
     rank 15: 2768.40
     rank 16: 2919.44
     rank 17: 2922.86
     rank 18: 2923.36
     rank 19: 2919.70
  backward-compute:
     rank  0: 51.70
     rank  1: 46.17
     rank  2: 46.70
     rank  3: 51.22
     rank  4: 3326.91
     rank  5: 3327.77
     rank  6: 3329.06
     rank  7: 3329.88
     rank  8: 3289.53
     rank  9: 3295.90
     rank 10: 3290.43
     rank 11: 3291.53
     rank 12: 3286.67
     rank 13: 3287.59
     rank 14: 3288.06
     rank 15: 3287.52
     rank 16: 3496.67
     rank 17: 3498.42
     rank 18: 3498.41
     rank 19: 3494.88
  pure-backward-compute:
     rank  0: 51.04
     rank  1: 45.53
     rank  2: 46.07
     rank  3: 50.58
     rank  4: 3326.03
     rank  5: 3326.98
     rank  6: 3327.88
     rank  7: 3329.10
     rank  8: 3288.45
     rank  9: 3294.83
     rank 10: 3289.58
     rank 11: 3290.74
     rank 12: 3285.58
     rank 13: 3286.91
     rank 14: 3286.96
     rank 15: 3286.78
     rank 16: 3494.43
     rank 17: 3496.84
     rank 18: 3496.69
     rank 19: 3492.40
  batch-generator:
     rank  0: 50.74
     rank  1: 59.32
     rank  2: 59.57
     rank  3: 59.13
     rank  4: 67.03
     rank  5: 73.99
     rank  6: 76.91
     rank  7: 80.28
     rank  8: 52.37
     rank  9: 55.18
     rank 10: 58.39
     rank 11: 62.04
     rank 12: 51.15
     rank 13: 60.15
     rank 14: 56.31
     rank 15: 56.28
     rank 16: 54.13
     rank 17: 58.87
     rank 18: 59.74
     rank 19: 60.53
  forward-recv:
     rank  4: 80.10
     rank  5: 79.30
     rank  6: 77.98
     rank  7: 79.90
     rank  8: 277.33
     rank  9: 277.08
     rank 10: 277.40
     rank 11: 277.24
     rank 12: 451.48
     rank 13: 451.64
     rank 14: 450.67
     rank 15: 452.17
     rank 16: 624.34
     rank 17: 624.17
     rank 18: 624.21
     rank 19: 623.99
  forward-send:
     rank  0: 422.46
     rank  1: 420.18
     rank  2: 418.13
     rank  3: 421.63
     rank  4: 33.80
     rank  5: 33.12
     rank  6: 32.07
     rank  7: 33.63
     rank  8: 21.02
     rank  9: 20.89
     rank 10: 19.82
     rank 11: 20.85
     rank 12: 10.55
     rank 13: 10.31
     rank 14: 10.52
     rank 15: 9.96
  backward-recv:
     rank  0: 1637.43
     rank  1: 1639.41
     rank  2: 1638.85
     rank  3: 1638.85
     rank  4: 591.08
     rank  5: 590.30
     rank  6: 589.78
     rank  7: 589.49
     rank  8: 393.93
     rank  9: 392.90
     rank 10: 393.84
     rank 11: 394.98
     rank 12: 196.52
     rank 13: 197.24
     rank 14: 196.51
     rank 15: 196.74
  backward-send:
     rank  4: 21.93
     rank  5: 22.29
     rank  6: 22.44
     rank  7: 21.84
     rank  8: 31.38
     rank  9: 31.29
     rank 10: 31.23
     rank 11: 30.00
     rank 12: 21.00
     rank 13: 19.94
     rank 14: 20.71
     rank 15: 20.72
     rank 16: 10.57
     rank 17: 10.13
     rank 18: 10.30
     rank 19: 10.65
  forward-send-backward-recv:
     rank  0: 4853.43
     rank  1: 4851.76
     rank  2: 4853.99
     rank  3: 4847.21
     rank  4: 852.14
     rank  5: 853.71
     rank  6: 849.96
     rank  7: 849.45
     rank  8: 704.89
     rank  9: 699.46
     rank 10: 705.60
     rank 11: 703.62
     rank 12: 545.49
     rank 13: 547.29
     rank 14: 545.07
     rank 15: 549.36
  backward-send-forward-recv:
     rank  4: 74.46
     rank  5: 72.74
     rank  6: 74.86
     rank  7: 69.74
     rank  8: 146.46
     rank  9: 146.64
     rank 10: 143.94
     rank 11: 140.35
     rank 12: 157.04
     rank 13: 150.69
     rank 14: 157.50
     rank 15: 153.82
     rank 16: 168.51
     rank 17: 166.20
     rank 18: 165.71
     rank 19: 169.78
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.05
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.05
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.32
     rank  1: 0.29
     rank  2: 0.33
     rank  3: 0.30
     rank  4: 2.36
     rank  5: 2.29
     rank  6: 2.56
     rank  7: 2.30
     rank  8: 2.15
     rank  9: 2.16
     rank 10: 2.21
     rank 11: 2.18
     rank 12: 2.21
     rank 13: 2.13
     rank 14: 2.23
     rank 15: 2.15
     rank 16: 2.41
     rank 17: 2.32
     rank 18: 2.36
     rank 19: 2.51
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.08
     rank  1: 2.00
     rank  2: 2.05
     rank  3: 2.01
     rank  4: 46.30
     rank  5: 46.56
     rank  6: 46.28
     rank  7: 46.29
     rank  8: 43.01
     rank  9: 43.03
     rank 10: 43.08
     rank 11: 42.96
     rank 12: 43.33
     rank 13: 42.96
     rank 14: 43.05
     rank 15: 43.01
     rank 16: 46.29
     rank 17: 46.32
     rank 18: 46.33
     rank 19: 46.42
  optimizer:
     rank  0: 2.61
     rank  1: 2.54
     rank  2: 2.59
     rank  3: 2.54
     rank  4: 46.83
     rank  5: 47.10
     rank  6: 46.81
     rank  7: 46.82
     rank  8: 43.55
     rank  9: 43.56
     rank 10: 43.62
     rank 11: 43.50
     rank 12: 43.87
     rank 13: 43.50
     rank 14: 43.58
     rank 15: 43.54
     rank 16: 46.83
     rank 17: 46.86
     rank 18: 46.86
     rank 19: 46.96
 [2024-12-02 16:52:25] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7966.1 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.343228E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7883.06
     rank  1: 7883.06
     rank  2: 7883.07
     rank  3: 7883.07
     rank  4: 7887.28
     rank  5: 7887.23
     rank  6: 7887.25
     rank  7: 7887.23
     rank  8: 7886.92
     rank  9: 7886.93
     rank 10: 7886.94
     rank 11: 7886.93
     rank 12: 7886.94
     rank 13: 7886.91
     rank 14: 7886.94
     rank 15: 7886.91
     rank 16: 7887.19
     rank 17: 7887.19
     rank 18: 7887.19
     rank 19: 7887.18
  forward-compute:
     rank  0: 907.41
     rank  1: 912.33
     rank  2: 911.09
     rank  3: 917.55
     rank  4: 2875.92
     rank  5: 2875.81
     rank  6: 2875.01
     rank  7: 2887.72
     rank  8: 2786.46
     rank  9: 2786.19
     rank 10: 2791.36
     rank 11: 2795.92
     rank 12: 2761.38
     rank 13: 2767.90
     rank 14: 2760.17
     rank 15: 2763.70
     rank 16: 2911.34
     rank 17: 2915.47
     rank 18: 2914.90
     rank 19: 2912.14
  backward-compute:
     rank  0: 51.24
     rank  1: 46.23
     rank  2: 46.38
     rank  3: 51.28
     rank  4: 3334.82
     rank  5: 3338.55
     rank  6: 3338.93
     rank  7: 3338.81
     rank  8: 3291.17
     rank  9: 3296.60
     rank 10: 3291.61
     rank 11: 3293.42
     rank 12: 3290.09
     rank 13: 3291.50
     rank 14: 3291.28
     rank 15: 3291.30
     rank 16: 3507.49
     rank 17: 3509.10
     rank 18: 3508.95
     rank 19: 3506.17
  pure-backward-compute:
     rank  0: 50.59
     rank  1: 45.59
     rank  2: 45.77
     rank  3: 50.63
     rank  4: 3333.87
     rank  5: 3337.67
     rank  6: 3337.47
     rank  7: 3338.09
     rank  8: 3290.00
     rank  9: 3295.40
     rank 10: 3290.89
     rank 11: 3292.68
     rank 12: 3288.98
     rank 13: 3290.77
     rank 14: 3290.19
     rank 15: 3290.56
     rank 16: 3505.33
     rank 17: 3507.41
     rank 18: 3507.24
     rank 19: 3503.97
  batch-generator:
     rank  0: 50.51
     rank  1: 56.59
     rank  2: 55.31
     rank  3: 62.51
     rank  4: 58.24
     rank  5: 59.68
     rank  6: 74.89
     rank  7: 81.17
     rank  8: 53.10
     rank  9: 55.26
     rank 10: 58.78
     rank 11: 62.94
     rank 12: 51.36
     rank 13: 59.63
     rank 14: 55.76
     rank 15: 57.06
     rank 16: 52.08
     rank 17: 58.24
     rank 18: 58.10
     rank 19: 57.52
  forward-recv:
     rank  4: 79.76
     rank  5: 79.64
     rank  6: 79.83
     rank  7: 78.70
     rank  8: 275.35
     rank  9: 275.00
     rank 10: 275.55
     rank 11: 275.32
     rank 12: 450.41
     rank 13: 450.75
     rank 14: 450.15
     rank 15: 450.70
     rank 16: 619.95
     rank 17: 619.89
     rank 18: 619.98
     rank 19: 619.37
  forward-send:
     rank  0: 418.37
     rank  1: 417.30
     rank  2: 417.49
     rank  3: 415.00
     rank  4: 31.96
     rank  5: 31.68
     rank  6: 31.24
     rank  7: 30.53
     rank  8: 21.05
     rank  9: 21.00
     rank 10: 20.45
     rank 11: 20.26
     rank 12: 10.53
     rank 13: 10.30
     rank 14: 10.51
     rank 15: 9.87
  backward-recv:
     rank  0: 1640.39
     rank  1: 1641.78
     rank  2: 1641.24
     rank  3: 1642.01
     rank  4: 595.56
     rank  5: 594.17
     rank  6: 593.97
     rank  7: 595.16
     rank  8: 393.95
     rank  9: 392.93
     rank 10: 394.78
     rank 11: 395.24
     rank 12: 197.62
     rank 13: 198.48
     rank 14: 197.61
     rank 15: 197.76
  backward-send:
     rank  4: 22.03
     rank  5: 22.62
     rank  6: 22.74
     rank  7: 21.56
     rank  8: 31.56
     rank  9: 31.56
     rank 10: 30.77
     rank 11: 30.24
     rank 12: 20.81
     rank 13: 19.92
     rank 14: 20.68
     rank 15: 20.74
     rank 16: 10.53
     rank 17: 10.31
     rank 18: 10.49
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 4859.19
     rank  1: 4858.83
     rank  2: 4860.84
     rank  3: 4851.04
     rank  4: 852.20
     rank  5: 854.54
     rank  6: 849.40
     rank  7: 849.47
     rank  8: 706.28
     rank  9: 701.57
     rank 10: 707.45
     rank 11: 704.63
     rank 12: 547.53
     rank 13: 548.78
     rank 14: 547.53
     rank 15: 551.24
  backward-send-forward-recv:
     rank  4: 74.00
     rank  5: 71.87
     rank  6: 74.72
     rank  7: 66.83
     rank  8: 146.29
     rank  9: 146.60
     rank 10: 143.05
     rank 11: 139.90
     rank 12: 157.36
     rank 13: 151.61
     rank 14: 157.94
     rank 15: 154.01
     rank 16: 168.87
     rank 17: 165.72
     rank 18: 166.11
     rank 19: 169.86
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.29
     rank  1: 0.29
     rank  2: 0.30
     rank  3: 0.30
     rank  4: 2.33
     rank  5: 2.28
     rank  6: 2.30
     rank  7: 2.28
     rank  8: 2.15
     rank  9: 2.19
     rank 10: 2.15
     rank 11: 2.14
     rank 12: 2.25
     rank 13: 2.16
     rank 14: 2.24
     rank 15: 2.15
     rank 16: 2.42
     rank 17: 2.35
     rank 18: 2.38
     rank 19: 2.43
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 2.00
     rank  2: 2.01
     rank  3: 2.00
     rank  4: 46.37
     rank  5: 46.30
     rank  6: 46.30
     rank  7: 46.30
     rank  8: 43.06
     rank  9: 43.05
     rank 10: 42.96
     rank 11: 42.98
     rank 12: 43.04
     rank 13: 42.96
     rank 14: 43.02
     rank 15: 43.00
     rank 16: 46.31
     rank 17: 46.30
     rank 18: 46.43
     rank 19: 46.38
  optimizer:
     rank  0: 2.50
     rank  1: 2.50
     rank  2: 2.52
     rank  3: 2.51
     rank  4: 46.88
     rank  5: 46.80
     rank  6: 46.80
     rank  7: 46.81
     rank  8: 43.56
     rank  9: 43.55
     rank 10: 43.46
     rank 11: 43.48
     rank 12: 43.54
     rank 13: 43.47
     rank 14: 43.52
     rank 15: 43.51
     rank 16: 46.81
     rank 17: 46.80
     rank 18: 46.95
     rank 19: 46.89
