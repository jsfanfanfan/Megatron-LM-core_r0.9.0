examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 22:10:39,803] torch.distributed.run: [WARNING] 
[2024-12-02 22:10:39,803] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 22:10:39,803] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 22:10:39,803] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]

---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.output_layer.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 108.68
     rank  1: 74.14
     rank  2: 74.30
     rank  3: 71.48
     rank  4: 61.23
     rank  5: 54.23
     rank  6: 60.92
     rank  7: 60.66
     rank  8: 31.67
     rank  9: 31.71
     rank 10: 46.18
     rank 11: 46.11
     rank 12: 44.77
     rank 13: 31.80
     rank 14: 44.82
     rank 15: 31.73
     rank 16: 53.08
     rank 17: 50.32
     rank 18: 62.44
     rank 19: 62.25
  train/valid/test-data-iterators-setup:
     rank  0: 1092.50
     rank  1: 1093.52
     rank  2: 1092.76
     rank  3: 1092.53
     rank  4: 1149.08
     rank  5: 1149.16
     rank  6: 1149.14
     rank  7: 1149.44
     rank  8: 1149.14
     rank  9: 1149.16
     rank 10: 1150.07
     rank 11: 1149.18
     rank 12: 1149.15
     rank 13: 1149.22
     rank 14: 1149.38
     rank 15: 1149.18
     rank 16: 1149.44
     rank 17: 1149.32
     rank 18: 1149.86
     rank 19: 1149.35
 [2024-12-02 22:11:29] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22327.5 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10090.0 | max reserved: 10090.0
times across ranks (ms):
  forward-backward:
     rank  0: 22257.11
     rank  1: 22256.97
     rank  2: 22256.84
     rank  3: 22256.87
     rank  4: 22261.57
     rank  5: 22261.20
     rank  6: 22261.60
     rank  7: 22261.29
     rank  8: 22261.00
     rank  9: 22260.87
     rank 10: 22260.98
     rank 11: 22260.88
     rank 12: 22261.01
     rank 13: 22260.80
     rank 14: 22261.12
     rank 15: 22261.10
     rank 16: 22261.08
     rank 17: 22260.79
     rank 18: 22260.83
     rank 19: 22260.73
  forward-compute:
     rank  0: 4940.00
     rank  1: 4904.05
     rank  2: 4911.31
     rank  3: 4910.68
     rank  4: 5654.83
     rank  5: 5666.09
     rank  6: 5682.92
     rank  7: 5678.29
     rank  8: 5164.40
     rank  9: 5167.98
     rank 10: 5188.99
     rank 11: 5192.14
     rank 12: 5160.33
     rank 13: 5159.31
     rank 14: 5175.49
     rank 15: 5179.46
     rank 16: 5128.23
     rank 17: 5130.44
     rank 18: 5141.13
     rank 19: 5133.37
  backward-compute:
     rank  0: 1780.05
     rank  1: 1825.10
     rank  2: 1827.69
     rank  3: 1829.10
     rank  4: 3387.32
     rank  5: 3388.45
     rank  6: 3391.95
     rank  7: 3398.69
     rank  8: 3287.91
     rank  9: 3289.21
     rank 10: 3289.55
     rank 11: 3303.17
     rank 12: 3294.42
     rank 13: 3298.65
     rank 14: 3299.47
     rank 15: 3301.69
     rank 16: 3486.06
     rank 17: 3485.02
     rank 18: 3478.94
     rank 19: 3488.37
  pure-backward-compute:
     rank  0: 1779.14
     rank  1: 1824.17
     rank  2: 1826.68
     rank  3: 1828.25
     rank  4: 3385.47
     rank  5: 3386.79
     rank  6: 3390.72
     rank  7: 3397.70
     rank  8: 3286.51
     rank  9: 3288.01
     rank 10: 3287.86
     rank 11: 3302.36
     rank 12: 3293.14
     rank 13: 3297.36
     rank 14: 3296.33
     rank 15: 3299.99
     rank 16: 3483.10
     rank 17: 3482.49
     rank 18: 3476.19
     rank 19: 3486.48
  batch-generator:
     rank  0: 1275.46
     rank  1: 1246.03
     rank  2: 1257.00
     rank  3: 1255.02
     rank  4: 1304.49
     rank  5: 1313.97
     rank  6: 1326.61
     rank  7: 1320.50
     rank  8: 1091.30
     rank  9: 1096.34
     rank 10: 1121.30
     rank 11: 1121.77
     rank 12: 1103.02
     rank 13: 1104.71
     rank 14: 1120.80
     rank 15: 1124.26
     rank 16: 1173.59
     rank 17: 1179.24
     rank 18: 1184.27
     rank 19: 1182.77
  forward-recv:
     rank  4: 3983.72
     rank  5: 3977.41
     rank  6: 3987.43
     rank  7: 3991.26
     rank  8: 6853.04
     rank  9: 6851.93
     rank 10: 6846.44
     rank 11: 6838.90
     rank 12: 9449.11
     rank 13: 9447.25
     rank 14: 9437.42
     rank 15: 9445.42
     rank 16: 12019.96
     rank 17: 12021.51
     rank 18: 12016.31
     rank 19: 12018.45
  forward-send:
     rank  0: 7992.92
     rank  1: 7984.67
     rank  2: 7976.54
     rank  3: 7980.96
     rank  4: 4842.70
     rank  5: 4841.21
     rank  6: 4821.52
     rank  7: 4823.03
     rank  8: 2436.81
     rank  9: 2436.76
     rank 10: 2421.52
     rank 11: 2430.64
     rank 12: 33.52
     rank 13: 35.32
     rank 14: 29.75
     rank 15: 31.65
  backward-recv:
     rank  0: 1390.97
     rank  1: 1390.71
     rank  2: 1390.84
     rank  3: 1389.86
     rank  4: 569.62
     rank  5: 569.80
     rank  6: 570.06
     rank  7: 570.11
     rank  8: 380.02
     rank  9: 380.60
     rank 10: 379.60
     rank 11: 381.37
     rank 12: 193.98
     rank 13: 193.64
     rank 14: 194.25
     rank 15: 193.50
  backward-send:
     rank  4: 4.75
     rank  5: 4.41
     rank  6: 4.34
     rank  7: 3.79
     rank  8: 31.34
     rank  9: 31.04
     rank 10: 31.32
     rank 11: 29.92
     rank 12: 20.75
     rank 13: 20.84
     rank 14: 20.49
     rank 15: 20.83
     rank 16: 10.60
     rank 17: 10.30
     rank 18: 10.48
     rank 19: 10.44
  forward-send-backward-recv:
     rank  0: 6086.90
     rank  1: 6087.16
     rank  2: 6085.07
     rank  3: 6081.38
     rank  4: 3034.32
     rank  5: 3032.36
     rank  6: 3030.02
     rank  7: 3025.29
     rank  8: 2820.84
     rank  9: 2821.68
     rank 10: 2820.08
     rank 11: 2808.87
     rank 12: 2644.93
     rank 13: 2641.82
     rank 14: 2642.10
     rank 15: 2640.08
  backward-send-forward-recv:
     rank  4: 652.57
     rank  5: 650.17
     rank  6: 643.86
     rank  7: 642.87
     rank  8: 945.05
     rank  9: 942.56
     rank 10: 940.51
     rank 11: 936.85
     rank 12: 908.64
     rank 13: 908.94
     rank 14: 908.62
     rank 15: 892.86
     rank 16: 847.08
     rank 17: 843.97
     rank 18: 844.30
     rank 19: 843.57
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.05
     rank  5: 0.03
     rank  6: 0.05
     rank  7: 0.03
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.07
     rank 14: 0.06
     rank 15: 0.09
     rank 16: 0.06
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.15
     rank  5: 0.09
     rank  6: 0.15
     rank  7: 0.10
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.06
     rank 16: 0.16
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 46.74
     rank  1: 43.76
     rank  2: 43.19
     rank  3: 42.98
     rank  4: 52.11
     rank  5: 46.45
     rank  6: 49.42
     rank  7: 45.90
     rank  8: 31.64
     rank  9: 41.38
     rank 10: 45.92
     rank 11: 38.96
     rank 12: 41.80
     rank 13: 49.56
     rank 14: 51.51
     rank 15: 51.60
     rank 16: 3.38
     rank 17: 2.88
     rank 18: 2.83
     rank 19: 2.61
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.03
     rank  4: 0.12
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.08
     rank  8: 0.11
     rank  9: 0.09
     rank 10: 0.10
     rank 11: 0.09
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.11
     rank 15: 0.10
     rank 16: 0.19
     rank 17: 0.06
     rank 18: 0.11
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.04
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.29
     rank  1: 1.59
     rank  2: 1.31
     rank  3: 1.28
     rank  4: 60.18
     rank  5: 60.10
     rank  6: 60.03
     rank  7: 60.03
     rank  8: 55.51
     rank  9: 55.40
     rank 10: 55.79
     rank 11: 55.63
     rank 12: 52.25
     rank 13: 52.24
     rank 14: 52.61
     rank 15: 52.58
     rank 16: 56.19
     rank 17: 56.09
     rank 18: 56.09
     rank 19: 55.54
  optimizer:
     rank  0: 2.56
     rank  1: 2.84
     rank  2: 2.57
     rank  3: 2.56
     rank  4: 61.46
     rank  5: 61.40
     rank  6: 61.32
     rank  7: 61.32
     rank  8: 56.79
     rank  9: 56.75
     rank 10: 57.09
     rank 11: 56.92
     rank 12: 53.53
     rank 13: 53.52
     rank 14: 53.89
     rank 15: 53.86
     rank 16: 57.48
     rank 17: 57.37
     rank 18: 57.38
     rank 19: 56.82
 [2024-12-02 22:11:38] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 8025.5 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7929.31
     rank  1: 7929.17
     rank  2: 7929.18
     rank  3: 7929.13
     rank  4: 7933.81
     rank  5: 7933.61
     rank  6: 7933.64
     rank  7: 7933.63
     rank  8: 7933.37
     rank  9: 7933.14
     rank 10: 7933.17
     rank 11: 7933.17
     rank 12: 7933.34
     rank 13: 7933.15
     rank 14: 7933.21
     rank 15: 7933.21
     rank 16: 7933.81
     rank 17: 7933.43
     rank 18: 7933.42
     rank 19: 7933.50
  forward-compute:
     rank  0: 919.33
     rank  1: 920.82
     rank  2: 920.97
     rank  3: 922.11
     rank  4: 2966.20
     rank  5: 2969.70
     rank  6: 2970.30
     rank  7: 2974.34
     rank  8: 2769.97
     rank  9: 2768.22
     rank 10: 2769.73
     rank 11: 2772.96
     rank 12: 2781.45
     rank 13: 2780.97
     rank 14: 2787.83
     rank 15: 2782.83
     rank 16: 2911.48
     rank 17: 2912.80
     rank 18: 2911.53
     rank 19: 2922.20
  backward-compute:
     rank  0: 939.37
     rank  1: 940.07
     rank  2: 937.74
     rank  3: 942.58
     rank  4: 3384.65
     rank  5: 3381.46
     rank  6: 3389.44
     rank  7: 3388.89
     rank  8: 3272.04
     rank  9: 3272.75
     rank 10: 3276.56
     rank 11: 3270.72
     rank 12: 3288.21
     rank 13: 3287.71
     rank 14: 3287.11
     rank 15: 3297.97
     rank 16: 3495.86
     rank 17: 3492.96
     rank 18: 3493.39
     rank 19: 3495.57
  pure-backward-compute:
     rank  0: 938.71
     rank  1: 939.43
     rank  2: 937.00
     rank  3: 941.95
     rank  4: 3383.41
     rank  5: 3380.51
     rank  6: 3388.58
     rank  7: 3388.09
     rank  8: 3270.57
     rank  9: 3271.88
     rank 10: 3275.34
     rank 11: 3269.87
     rank 12: 3287.17
     rank 13: 3286.53
     rank 14: 3286.07
     rank 15: 3296.64
     rank 16: 3493.17
     rank 17: 3490.16
     rank 18: 3490.34
     rank 19: 3494.04
  batch-generator:
     rank  0: 51.56
     rank  1: 54.67
     rank  2: 55.33
     rank  3: 57.30
     rank  4: 75.78
     rank  5: 78.96
     rank  6: 76.84
     rank  7: 81.74
     rank  8: 53.85
     rank  9: 55.22
     rank 10: 59.89
     rank 11: 63.24
     rank 12: 51.23
     rank 13: 55.52
     rank 14: 61.21
     rank 15: 54.66
     rank 16: 57.74
     rank 17: 62.73
     rank 18: 62.56
     rank 19: 70.85
  forward-recv:
     rank  4: 62.15
     rank  5: 61.48
     rank  6: 61.43
     rank  7: 61.23
     rank  8: 279.29
     rank  9: 279.00
     rank 10: 279.13
     rank 11: 278.44
     rank 12: 453.65
     rank 13: 453.42
     rank 14: 453.64
     rank 15: 453.66
     rank 16: 627.61
     rank 17: 627.50
     rank 18: 627.66
     rank 19: 627.73
  forward-send:
     rank  0: 416.06
     rank  1: 414.39
     rank  2: 414.37
     rank  3: 413.26
     rank  4: 33.72
     rank  5: 32.24
     rank  6: 32.86
     rank  7: 32.24
     rank  8: 22.57
     rank  9: 21.72
     rank 10: 22.19
     rank 11: 22.57
     rank 12: 10.53
     rank 13: 10.34
     rank 14: 10.41
     rank 15: 10.46
  backward-recv:
     rank  0: 1407.23
     rank  1: 1406.89
     rank  2: 1408.48
     rank  3: 1406.50
     rank  4: 592.44
     rank  5: 593.10
     rank  6: 591.60
     rank  7: 592.70
     rank  8: 392.50
     rank  9: 393.35
     rank 10: 392.47
     rank 11: 393.78
     rank 12: 194.16
     rank 13: 193.98
     rank 14: 194.00
     rank 15: 193.94
  backward-send:
     rank  4: 4.64
     rank  5: 3.88
     rank  6: 3.57
     rank  7: 3.64
     rank  8: 31.13
     rank  9: 31.04
     rank 10: 31.39
     rank 11: 30.42
     rank 12: 20.75
     rank 13: 20.84
     rank 14: 20.56
     rank 15: 20.90
     rank 16: 10.58
     rank 17: 10.49
     rank 18: 10.53
     rank 19: 10.22
  forward-send-backward-recv:
     rank  0: 4232.52
     rank  1: 4233.92
     rank  2: 4234.68
     rank  3: 4230.23
     rank  4: 792.14
     rank  5: 795.03
     rank  6: 791.41
     rank  7: 788.54
     rank  8: 717.84
     rank  9: 719.15
     rank 10: 715.08
     rank 11: 722.55
     rank 12: 525.54
     rank 13: 526.49
     rank 14: 527.74
     rank 15: 518.00
  backward-send-forward-recv:
     rank  4: 20.92
     rank  5: 20.37
     rank  6: 18.65
     rank  7: 17.34
     rank  8: 158.14
     rank  9: 161.83
     rank 10: 158.03
     rank 11: 155.21
     rank 12: 155.80
     rank 13: 155.61
     rank 14: 150.07
     rank 15: 154.11
     rank 16: 167.19
     rank 17: 167.98
     rank 18: 168.19
     rank 19: 158.95
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.11
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.26
     rank  1: 0.30
     rank  2: 0.34
     rank  3: 0.22
     rank  4: 2.37
     rank  5: 2.38
     rank  6: 2.39
     rank  7: 2.38
     rank  8: 2.20
     rank  9: 2.16
     rank 10: 2.17
     rank 11: 2.16
     rank 12: 2.15
     rank 13: 2.17
     rank 14: 2.40
     rank 15: 2.36
     rank 16: 2.40
     rank 17: 2.46
     rank 18: 2.48
     rank 19: 2.51
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.11
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.51
     rank  1: 0.63
     rank  2: 0.64
     rank  3: 0.52
     rank  4: 48.04
     rank  5: 48.09
     rank  6: 48.00
     rank  7: 48.04
     rank  8: 43.11
     rank  9: 42.97
     rank 10: 42.93
     rank 11: 42.92
     rank 12: 42.90
     rank 13: 42.93
     rank 14: 43.15
     rank 15: 43.11
     rank 16: 46.59
     rank 17: 46.38
     rank 18: 46.43
     rank 19: 46.31
  optimizer:
     rank  0: 1.16
     rank  1: 1.28
     rank  2: 1.30
     rank  3: 1.17
     rank  4: 48.69
     rank  5: 48.75
     rank  6: 48.66
     rank  7: 48.69
     rank  8: 43.76
     rank  9: 43.63
     rank 10: 43.58
     rank 11: 43.57
     rank 12: 43.55
     rank 13: 43.59
     rank 14: 43.80
     rank 15: 43.76
     rank 16: 47.24
     rank 17: 47.03
     rank 18: 47.08
     rank 19: 46.96
 [2024-12-02 22:11:46] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 8022.7 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 2.786489E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7939.50
     rank  1: 7939.41
     rank  2: 7939.45
     rank  3: 7939.49
     rank  4: 7943.98
     rank  5: 7943.89
     rank  6: 7943.97
     rank  7: 7944.04
     rank  8: 7943.55
     rank  9: 7943.46
     rank 10: 7943.53
     rank 11: 7943.57
     rank 12: 7943.51
     rank 13: 7943.44
     rank 14: 7943.54
     rank 15: 7943.58
     rank 16: 7943.94
     rank 17: 7943.77
     rank 18: 7943.84
     rank 19: 7943.82
  forward-compute:
     rank  0: 915.24
     rank  1: 915.35
     rank  2: 917.32
     rank  3: 916.40
     rank  4: 2951.12
     rank  5: 2952.21
     rank  6: 2955.37
     rank  7: 2954.67
     rank  8: 2767.72
     rank  9: 2769.20
     rank 10: 2767.46
     rank 11: 2771.66
     rank 12: 2779.28
     rank 13: 2778.30
     rank 14: 2782.76
     rank 15: 2784.16
     rank 16: 2922.83
     rank 17: 2925.67
     rank 18: 2925.97
     rank 19: 2931.68
  backward-compute:
     rank  0: 936.23
     rank  1: 936.48
     rank  2: 936.77
     rank  3: 937.86
     rank  4: 3378.73
     rank  5: 3379.12
     rank  6: 3378.29
     rank  7: 3383.85
     rank  8: 3275.85
     rank  9: 3276.41
     rank 10: 3278.69
     rank 11: 3276.96
     rank 12: 3283.15
     rank 13: 3284.44
     rank 14: 3284.83
     rank 15: 3289.67
     rank 16: 3492.73
     rank 17: 3494.85
     rank 18: 3494.97
     rank 19: 3497.32
  pure-backward-compute:
     rank  0: 935.55
     rank  1: 935.85
     rank  2: 936.13
     rank  3: 937.22
     rank  4: 3377.79
     rank  5: 3378.25
     rank  6: 3377.52
     rank  7: 3382.90
     rank  8: 3274.17
     rank  9: 3275.52
     rank 10: 3277.46
     rank 11: 3276.11
     rank 12: 3282.09
     rank 13: 3283.30
     rank 14: 3283.85
     rank 15: 3288.76
     rank 16: 3489.03
     rank 17: 3492.33
     rank 18: 3492.68
     rank 19: 3495.25
  batch-generator:
     rank  0: 50.95
     rank  1: 52.70
     rank  2: 54.48
     rank  3: 55.03
     rank  4: 60.52
     rank  5: 62.33
     rank  6: 63.42
     rank  7: 69.51
     rank  8: 53.91
     rank  9: 55.66
     rank 10: 57.02
     rank 11: 61.78
     rank 12: 50.79
     rank 13: 54.27
     rank 14: 57.96
     rank 15: 57.44
     rank 16: 71.72
     rank 17: 75.13
     rank 18: 74.73
     rank 19: 79.13
  forward-recv:
     rank  4: 62.97
     rank  5: 62.72
     rank  6: 62.18
     rank  7: 62.42
     rank  8: 278.44
     rank  9: 279.38
     rank 10: 278.38
     rank 11: 280.31
     rank 12: 448.67
     rank 13: 448.23
     rank 14: 448.63
     rank 15: 447.30
     rank 16: 622.57
     rank 17: 622.58
     rank 18: 622.54
     rank 19: 622.67
  forward-send:
     rank  0: 412.36
     rank  1: 412.38
     rank  2: 410.53
     rank  3: 411.25
     rank  4: 31.48
     rank  5: 31.75
     rank  6: 30.85
     rank  7: 31.32
     rank  8: 21.06
     rank  9: 20.39
     rank 10: 20.85
     rank 11: 19.58
     rank 12: 10.47
     rank 13: 10.56
     rank 14: 10.43
     rank 15: 10.54
  backward-recv:
     rank  0: 1417.22
     rank  1: 1417.21
     rank  2: 1417.51
     rank  3: 1416.79
     rank  4: 602.90
     rank  5: 603.42
     rank  6: 602.74
     rank  7: 603.23
     rank  8: 395.67
     rank  9: 396.36
     rank 10: 395.55
     rank 11: 395.85
     rank 12: 198.06
     rank 13: 197.88
     rank 14: 198.48
     rank 15: 198.52
  backward-send:
     rank  4: 4.40
     rank  5: 4.44
     rank  6: 4.53
     rank  7: 3.89
     rank  8: 30.27
     rank  9: 31.15
     rank 10: 30.01
     rank 11: 30.08
     rank 12: 21.05
     rank 13: 20.94
     rank 14: 20.56
     rank 15: 20.45
     rank 16: 10.78
     rank 17: 10.38
     rank 18: 10.32
     rank 19: 10.39
  forward-send-backward-recv:
     rank  0: 4243.72
     rank  1: 4245.28
     rank  2: 4244.82
     rank  3: 4242.90
     rank  4: 813.23
     rank  5: 812.18
     rank  6: 813.38
     rank  7: 808.52
     rank  8: 726.04
     rank  9: 726.82
     rank 10: 723.79
     rank 11: 728.28
     rank 12: 540.64
     rank 13: 539.69
     rank 14: 541.48
     rank 15: 534.96
  backward-send-forward-recv:
     rank  4: 21.24
     rank  5: 20.54
     rank  6: 20.39
     rank  7: 19.59
     rank  8: 156.24
     rank  9: 156.21
     rank 10: 156.79
     rank 11: 152.06
     rank 12: 157.69
     rank 13: 157.86
     rank 14: 153.88
     rank 15: 155.07
     rank 16: 168.69
     rank 17: 167.78
     rank 18: 166.86
     rank 19: 161.63
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.10
  all-grads-sync:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.29
     rank  3: 0.24
     rank  4: 2.38
     rank  5: 2.38
     rank  6: 2.43
     rank  7: 2.43
     rank  8: 2.22
     rank  9: 2.24
     rank 10: 2.32
     rank 11: 2.19
     rank 12: 2.14
     rank 13: 2.26
     rank 14: 2.25
     rank 15: 2.26
     rank 16: 2.67
     rank 17: 2.50
     rank 18: 2.59
     rank 19: 2.67
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.06
     rank 11: 0.05
     rank 12: 0.04
     rank 13: 0.09
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.41
     rank 17: 0.10
     rank 18: 0.08
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.50
     rank  1: 0.52
     rank  2: 0.56
     rank  3: 0.49
     rank  4: 48.04
     rank  5: 48.06
     rank  6: 48.17
     rank  7: 48.16
     rank  8: 43.11
     rank  9: 43.12
     rank 10: 43.08
     rank 11: 42.99
     rank 12: 42.99
     rank 13: 43.06
     rank 14: 43.00
     rank 15: 42.98
     rank 16: 46.68
     rank 17: 46.37
     rank 18: 46.63
     rank 19: 46.51
  optimizer:
     rank  0: 1.47
     rank  1: 1.48
     rank  2: 1.53
     rank  3: 1.46
     rank  4: 49.01
     rank  5: 49.03
     rank  6: 49.15
     rank  7: 49.13
     rank  8: 44.10
     rank  9: 44.09
     rank 10: 44.05
     rank 11: 43.96
     rank 12: 43.96
     rank 13: 44.04
     rank 14: 43.97
     rank 15: 43.95
     rank 16: 47.67
     rank 17: 47.34
     rank 18: 47.61
     rank 19: 47.54
 [2024-12-02 22:11:54] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 8016.1 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.020505E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7931.95
     rank  1: 7931.96
     rank  2: 7932.02
     rank  3: 7931.97
     rank  4: 7936.44
     rank  5: 7936.56
     rank  6: 7936.50
     rank  7: 7936.48
     rank  8: 7936.01
     rank  9: 7936.06
     rank 10: 7936.01
     rank 11: 7936.03
     rank 12: 7935.96
     rank 13: 7936.05
     rank 14: 7936.07
     rank 15: 7936.06
     rank 16: 7936.29
     rank 17: 7936.26
     rank 18: 7936.26
     rank 19: 7936.28
  forward-compute:
     rank  0: 1085.56
     rank  1: 1085.49
     rank  2: 1086.29
     rank  3: 1087.60
     rank  4: 2945.93
     rank  5: 2946.16
     rank  6: 2948.40
     rank  7: 2949.93
     rank  8: 2777.50
     rank  9: 2779.98
     rank 10: 2776.76
     rank 11: 2780.19
     rank 12: 2782.94
     rank 13: 2781.39
     rank 14: 2788.36
     rank 15: 2787.54
     rank 16: 2916.43
     rank 17: 2917.70
     rank 18: 2916.57
     rank 19: 2920.84
  backward-compute:
     rank  0: 932.45
     rank  1: 932.51
     rank  2: 932.28
     rank  3: 933.37
     rank  4: 3374.96
     rank  5: 3376.71
     rank  6: 3374.74
     rank  7: 3378.01
     rank  8: 3268.78
     rank  9: 3270.46
     rank 10: 3274.39
     rank 11: 3269.79
     rank 12: 3285.85
     rank 13: 3285.15
     rank 14: 3284.80
     rank 15: 3292.63
     rank 16: 3501.84
     rank 17: 3497.47
     rank 18: 3498.20
     rank 19: 3499.46
  pure-backward-compute:
     rank  0: 931.77
     rank  1: 931.87
     rank  2: 931.54
     rank  3: 932.71
     rank  4: 3374.10
     rank  5: 3375.79
     rank  6: 3374.00
     rank  7: 3377.06
     rank  8: 3267.19
     rank  9: 3269.56
     rank 10: 3273.24
     rank 11: 3268.82
     rank 12: 3284.78
     rank 13: 3283.89
     rank 14: 3283.95
     rank 15: 3291.50
     rank 16: 3499.69
     rank 17: 3495.16
     rank 18: 3495.39
     rank 19: 3497.20
  batch-generator:
     rank  0: 52.90
     rank  1: 54.33
     rank  2: 56.08
     rank  3: 57.67
     rank  4: 55.59
     rank  5: 57.85
     rank  6: 59.17
     rank  7: 65.73
     rank  8: 61.38
     rank  9: 61.67
     rank 10: 61.65
     rank 11: 65.98
     rank 12: 51.37
     rank 13: 54.60
     rank 14: 60.56
     rank 15: 57.61
     rank 16: 54.73
     rank 17: 60.50
     rank 18: 59.61
     rank 19: 62.84
  forward-recv:
     rank  4: 62.47
     rank  5: 62.51
     rank  6: 62.01
     rank  7: 61.59
     rank  8: 276.98
     rank  9: 277.30
     rank 10: 277.01
     rank 11: 278.30
     rank 12: 449.03
     rank 13: 449.12
     rank 14: 449.07
     rank 15: 447.50
     rank 16: 621.25
     rank 17: 620.91
     rank 18: 620.98
     rank 19: 621.10
  forward-send:
     rank  0: 406.78
     rank  1: 406.75
     rank  2: 406.04
     rank  3: 404.70
     rank  4: 31.50
     rank  5: 31.70
     rank  6: 31.41
     rank  7: 30.63
     rank  8: 20.91
     rank  9: 20.68
     rank 10: 20.97
     rank 11: 19.46
     rank 12: 10.49
     rank 13: 10.35
     rank 14: 10.48
     rank 15: 10.51
  backward-recv:
     rank  0: 1414.86
     rank  1: 1415.55
     rank  2: 1415.41
     rank  3: 1415.11
     rank  4: 601.19
     rank  5: 600.40
     rank  6: 600.97
     rank  7: 600.27
     rank  8: 392.44
     rank  9: 392.79
     rank 10: 391.72
     rank 11: 393.71
     rank 12: 193.88
     rank 13: 194.30
     rank 14: 194.62
     rank 15: 193.52
  backward-send:
     rank  4: 4.11
     rank  5: 4.65
     rank  6: 4.22
     rank  7: 4.19
     rank  8: 31.13
     rank  9: 31.08
     rank 10: 31.03
     rank 11: 30.52
     rank 12: 20.86
     rank 13: 20.57
     rank 14: 20.21
     rank 15: 20.96
     rank 16: 10.30
     rank 17: 10.53
     rank 18: 10.60
     rank 19: 10.09
  forward-send-backward-recv:
     rank  0: 4077.40
     rank  1: 4078.81
     rank  2: 4078.81
     rank  3: 4076.84
     rank  4: 819.26
     rank  5: 818.08
     rank  6: 820.90
     rank  7: 817.91
     rank  8: 732.15
     rank  9: 732.69
     rank 10: 728.05
     rank 11: 734.26
     rank 12: 533.58
     rank 13: 535.21
     rank 14: 536.39
     rank 15: 529.73
  backward-send-forward-recv:
     rank  4: 21.32
     rank  5: 20.72
     rank  6: 20.44
     rank  7: 19.71
     rank  8: 147.10
     rank  9: 146.17
     rank 10: 148.10
     rank 11: 144.67
     rank 12: 157.09
     rank 13: 156.89
     rank 14: 151.27
     rank 15: 152.93
     rank 16: 167.28
     rank 17: 168.51
     rank 18: 168.81
     rank 19: 165.25
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.23
     rank  1: 0.25
     rank  2: 0.34
     rank  3: 0.26
     rank  4: 2.37
     rank  5: 2.43
     rank  6: 2.39
     rank  7: 2.39
     rank  8: 2.22
     rank  9: 2.20
     rank 10: 2.17
     rank 11: 2.26
     rank 12: 2.14
     rank 13: 2.21
     rank 14: 2.45
     rank 15: 2.32
     rank 16: 2.51
     rank 17: 2.54
     rank 18: 2.51
     rank 19: 2.52
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.09
     rank 15: 0.06
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.52
     rank  1: 0.53
     rank  2: 0.65
     rank  3: 0.54
     rank  4: 48.09
     rank  5: 48.17
     rank  6: 48.07
     rank  7: 48.17
     rank  8: 43.09
     rank  9: 43.00
     rank 10: 42.95
     rank 11: 43.04
     rank 12: 42.96
     rank 13: 42.99
     rank 14: 43.15
     rank 15: 43.12
     rank 16: 46.49
     rank 17: 46.40
     rank 18: 46.45
     rank 19: 46.49
  optimizer:
     rank  0: 1.11
     rank  1: 1.12
     rank  2: 1.24
     rank  3: 1.13
     rank  4: 48.68
     rank  5: 48.76
     rank  6: 48.67
     rank  7: 48.76
     rank  8: 43.68
     rank  9: 43.59
     rank 10: 43.54
     rank 11: 43.63
     rank 12: 43.55
     rank 13: 43.58
     rank 14: 43.74
     rank 15: 43.71
     rank 16: 47.08
     rank 17: 46.99
     rank 18: 47.04
     rank 19: 47.08
 [2024-12-02 22:12:02] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 8027.8 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.727008E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7942.33
     rank  1: 7942.30
     rank  2: 7942.30
     rank  3: 7942.64
     rank  4: 7946.87
     rank  5: 7946.91
     rank  6: 7946.84
     rank  7: 7947.16
     rank  8: 7946.38
     rank  9: 7946.40
     rank 10: 7946.38
     rank 11: 7946.69
     rank 12: 7946.38
     rank 13: 7946.37
     rank 14: 7946.40
     rank 15: 7946.68
     rank 16: 7946.75
     rank 17: 7946.62
     rank 18: 7946.62
     rank 19: 7947.52
  forward-compute:
     rank  0: 922.25
     rank  1: 920.91
     rank  2: 922.36
     rank  3: 924.42
     rank  4: 2970.16
     rank  5: 2971.48
     rank  6: 2973.77
     rank  7: 2975.38
     rank  8: 2768.07
     rank  9: 2770.49
     rank 10: 2768.41
     rank 11: 2772.43
     rank 12: 2783.96
     rank 13: 2784.58
     rank 14: 2789.26
     rank 15: 2786.64
     rank 16: 2917.73
     rank 17: 2917.93
     rank 18: 2916.12
     rank 19: 2918.84
  backward-compute:
     rank  0: 923.78
     rank  1: 925.56
     rank  2: 924.83
     rank  3: 925.87
     rank  4: 3380.62
     rank  5: 3381.43
     rank  6: 3381.37
     rank  7: 3384.67
     rank  8: 3285.93
     rank  9: 3287.68
     rank 10: 3290.22
     rank 11: 3286.02
     rank 12: 3299.18
     rank 13: 3297.59
     rank 14: 3297.76
     rank 15: 3301.47
     rank 16: 3505.80
     rank 17: 3504.35
     rank 18: 3504.54
     rank 19: 3507.63
  pure-backward-compute:
     rank  0: 923.13
     rank  1: 924.84
     rank  2: 924.14
     rank  3: 925.21
     rank  4: 3379.58
     rank  5: 3380.32
     rank  6: 3380.66
     rank  7: 3383.84
     rank  8: 3284.75
     rank  9: 3286.84
     rank 10: 3289.03
     rank 11: 3285.23
     rank 12: 3298.10
     rank 13: 3296.55
     rank 14: 3296.96
     rank 15: 3300.61
     rank 16: 3503.93
     rank 17: 3502.14
     rank 18: 3501.76
     rank 19: 3505.58
  batch-generator:
     rank  0: 50.35
     rank  1: 51.66
     rank  2: 54.07
     rank  3: 57.33
     rank  4: 72.51
     rank  5: 76.11
     rank  6: 74.75
     rank  7: 78.70
     rank  8: 53.06
     rank  9: 56.06
     rank 10: 57.27
     rank 11: 61.96
     rank 12: 51.10
     rank 13: 56.28
     rank 14: 59.71
     rank 15: 55.17
     rank 16: 58.90
     rank 17: 62.21
     rank 18: 60.82
     rank 19: 62.19
  forward-recv:
     rank  4: 59.88
     rank  5: 59.68
     rank  6: 60.15
     rank  7: 59.56
     rank  8: 275.07
     rank  9: 276.23
     rank 10: 274.96
     rank 11: 274.25
     rank 12: 447.88
     rank 13: 447.15
     rank 14: 447.82
     rank 15: 446.96
     rank 16: 622.62
     rank 17: 622.31
     rank 18: 622.85
     rank 19: 622.91
  forward-send:
     rank  0: 411.34
     rank  1: 410.62
     rank  2: 411.39
     rank  3: 409.31
     rank  4: 31.49
     rank  5: 31.24
     rank  6: 31.46
     rank  7: 29.73
     rank  8: 21.63
     rank  9: 20.50
     rank 10: 21.98
     rank 11: 21.44
     rank 12: 10.37
     rank 13: 10.10
     rank 14: 10.58
     rank 15: 10.57
  backward-recv:
     rank  0: 1416.00
     rank  1: 1416.39
     rank  2: 1416.01
     rank  3: 1415.60
     rank  4: 591.41
     rank  5: 589.93
     rank  6: 591.60
     rank  7: 591.15
     rank  8: 388.64
     rank  9: 388.74
     rank 10: 388.09
     rank 11: 388.98
     rank 12: 193.40
     rank 13: 193.33
     rank 14: 193.96
     rank 15: 193.48
  backward-send:
     rank  4: 4.19
     rank  5: 4.87
     rank  6: 4.08
     rank  7: 3.57
     rank  8: 31.15
     rank  9: 31.20
     rank 10: 31.42
     rank 11: 30.60
     rank 12: 20.92
     rank 13: 20.81
     rank 14: 20.33
     rank 15: 20.88
     rank 16: 10.47
     rank 17: 10.49
     rank 18: 10.37
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 4253.16
     rank  1: 4254.89
     rank  2: 4253.77
     rank  3: 4251.93
     rank  4: 810.42
     rank  5: 809.61
     rank  6: 810.17
     rank  7: 807.61
     rank  8: 731.33
     rank  9: 732.07
     rank 10: 728.81
     rank 11: 735.33
     rank 12: 528.24
     rank 13: 530.04
     rank 14: 531.37
     rank 15: 527.90
  backward-send-forward-recv:
     rank  4: 21.05
     rank  5: 20.54
     rank  6: 18.93
     rank  7: 18.99
     rank  8: 152.98
     rank  9: 152.03
     rank 10: 152.37
     rank 11: 148.83
     rank 12: 156.94
     rank 13: 156.65
     rank 14: 151.77
     rank 15: 155.20
     rank 16: 168.70
     rank 17: 169.52
     rank 18: 169.70
     rank 19: 166.63
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.07
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.07
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.19
  all-grads-sync:
     rank  0: 0.23
     rank  1: 0.29
     rank  2: 0.27
     rank  3: 0.23
     rank  4: 2.38
     rank  5: 2.50
     rank  6: 2.38
     rank  7: 2.38
     rank  8: 2.21
     rank  9: 2.24
     rank 10: 2.19
     rank 11: 2.24
     rank 12: 2.17
     rank 13: 2.18
     rank 14: 2.26
     rank 15: 2.26
     rank 16: 2.76
     rank 17: 2.49
     rank 18: 2.49
     rank 19: 3.26
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.06
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.06
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.15
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.14
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.05
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-inner-step:
     rank  0: 0.50
     rank  1: 0.59
     rank  2: 0.55
     rank  3: 0.49
     rank  4: 48.09
     rank  5: 48.32
     rank  6: 48.04
     rank  7: 48.08
     rank  8: 43.09
     rank  9: 43.01
     rank 10: 42.97
     rank 11: 43.01
     rank 12: 43.00
     rank 13: 42.98
     rank 14: 43.05
     rank 15: 42.99
     rank 16: 46.99
     rank 17: 46.33
     rank 18: 46.45
     rank 19: 47.09
  optimizer:
     rank  0: 1.92
     rank  1: 2.01
     rank  2: 1.98
     rank  3: 1.91
     rank  4: 49.52
     rank  5: 49.75
     rank  6: 49.47
     rank  7: 49.50
     rank  8: 44.52
     rank  9: 44.43
     rank 10: 44.40
     rank 11: 44.44
     rank 12: 44.42
     rank 13: 44.40
     rank 14: 44.47
     rank 15: 44.41
     rank 16: 48.41
     rank 17: 47.75
     rank 18: 47.87
     rank 19: 48.48
 [2024-12-02 22:12:10] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 8015.3 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.566090E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7928.94
     rank  1: 7929.03
     rank  2: 7929.03
     rank  3: 7929.22
     rank  4: 7933.44
     rank  5: 7933.49
     rank  6: 7933.50
     rank  7: 7933.73
     rank  8: 7933.01
     rank  9: 7933.07
     rank 10: 7933.01
     rank 11: 7933.31
     rank 12: 7932.97
     rank 13: 7933.05
     rank 14: 7933.04
     rank 15: 7933.29
     rank 16: 7933.28
     rank 17: 7933.30
     rank 18: 7933.27
     rank 19: 7934.01
  forward-compute:
     rank  0: 920.59
     rank  1: 920.29
     rank  2: 921.92
     rank  3: 923.49
     rank  4: 2968.64
     rank  5: 2968.94
     rank  6: 2972.57
     rank  7: 2974.18
     rank  8: 2773.95
     rank  9: 2773.40
     rank 10: 2772.35
     rank 11: 2777.01
     rank 12: 2785.52
     rank 13: 2784.82
     rank 14: 2793.24
     rank 15: 2789.31
     rank 16: 2897.64
     rank 17: 2897.67
     rank 18: 2896.53
     rank 19: 2894.99
  backward-compute:
     rank  0: 920.88
     rank  1: 920.45
     rank  2: 920.93
     rank  3: 921.96
     rank  4: 3380.25
     rank  5: 3380.52
     rank  6: 3381.63
     rank  7: 3385.12
     rank  8: 3282.93
     rank  9: 3284.11
     rank 10: 3289.91
     rank 11: 3282.92
     rank 12: 3305.82
     rank 13: 3304.74
     rank 14: 3304.55
     rank 15: 3306.02
     rank 16: 3517.10
     rank 17: 3513.41
     rank 18: 3513.20
     rank 19: 3517.68
  pure-backward-compute:
     rank  0: 920.23
     rank  1: 919.80
     rank  2: 920.27
     rank  3: 921.34
     rank  4: 3379.41
     rank  5: 3379.41
     rank  6: 3380.82
     rank  7: 3384.31
     rank  8: 3281.61
     rank  9: 3283.28
     rank 10: 3288.82
     rank 11: 3282.14
     rank 12: 3304.78
     rank 13: 3303.55
     rank 14: 3303.76
     rank 15: 3305.08
     rank 16: 3515.16
     rank 17: 3511.08
     rank 18: 3510.69
     rank 19: 3515.37
  batch-generator:
     rank  0: 51.05
     rank  1: 51.98
     rank  2: 54.41
     rank  3: 56.28
     rank  4: 57.98
     rank  5: 64.80
     rank  6: 65.37
     rank  7: 68.17
     rank  8: 52.15
     rank  9: 53.80
     rank 10: 57.40
     rank 11: 61.39
     rank 12: 50.39
     rank 13: 54.45
     rank 14: 61.47
     rank 15: 55.63
     rank 16: 51.49
     rank 17: 54.98
     rank 18: 54.47
     rank 19: 52.06
  forward-recv:
     rank  4: 61.98
     rank  5: 61.81
     rank  6: 61.28
     rank  7: 60.59
     rank  8: 281.91
     rank  9: 282.73
     rank 10: 281.56
     rank 11: 282.50
     rank 12: 449.80
     rank 13: 449.66
     rank 14: 450.21
     rank 15: 448.79
     rank 16: 622.41
     rank 17: 622.38
     rank 18: 622.28
     rank 19: 622.47
  forward-send:
     rank  0: 413.91
     rank  1: 414.01
     rank  2: 412.61
     rank  3: 411.02
     rank  4: 31.73
     rank  5: 31.80
     rank  6: 31.13
     rank  7: 30.12
     rank  8: 20.98
     rank  9: 20.42
     rank 10: 21.00
     rank 11: 19.60
     rank 12: 10.52
     rank 13: 10.45
     rank 14: 10.24
     rank 15: 10.42
  backward-recv:
     rank  0: 1419.59
     rank  1: 1419.97
     rank  2: 1419.92
     rank  3: 1419.70
     rank  4: 597.54
     rank  5: 596.18
     rank  6: 597.48
     rank  7: 596.36
     rank  8: 395.37
     rank  9: 395.20
     rank 10: 394.47
     rank 11: 395.87
     rank 12: 194.78
     rank 13: 194.61
     rank 14: 195.75
     rank 15: 194.86
  backward-send:
     rank  4: 4.03
     rank  5: 4.77
     rank  6: 4.10
     rank  7: 4.05
     rank  8: 31.35
     rank  9: 31.36
     rank 10: 31.24
     rank 11: 30.33
     rank 12: 20.87
     rank 13: 20.77
     rank 14: 19.78
     rank 15: 20.83
     rank 16: 10.50
     rank 17: 10.25
     rank 18: 10.54
     rank 19: 10.48
  forward-send-backward-recv:
     rank  0: 4237.15
     rank  1: 4239.66
     rank  2: 4239.11
     rank  3: 4235.59
     rank  4: 793.73
     rank  5: 792.39
     rank  6: 792.97
     rank  7: 790.71
     rank  8: 700.63
     rank  9: 702.77
     rank 10: 696.92
     rank 11: 704.05
     rank 12: 506.68
     rank 13: 508.09
     rank 14: 509.32
     rank 15: 508.34
  backward-send-forward-recv:
     rank  4: 20.72
     rank  5: 20.89
     rank  6: 19.52
     rank  7: 19.10
     rank  8: 157.73
     rank  9: 157.70
     rank 10: 157.47
     rank 11: 154.69
     rank 12: 157.04
     rank 13: 157.17
     rank 14: 149.58
     rank 15: 154.70
     rank 16: 167.84
     rank 17: 169.27
     rank 18: 170.27
     rank 19: 169.19
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.08
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.06
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.11
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.17
  all-grads-sync:
     rank  0: 0.24
     rank  1: 0.25
     rank  2: 0.33
     rank  3: 0.23
     rank  4: 2.37
     rank  5: 2.38
     rank  6: 2.39
     rank  7: 2.39
     rank  8: 2.23
     rank  9: 2.27
     rank 10: 2.20
     rank 11: 2.24
     rank 12: 2.15
     rank 13: 2.15
     rank 14: 2.28
     rank 15: 2.31
     rank 16: 2.49
     rank 17: 2.60
     rank 18: 2.51
     rank 19: 3.20
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.10
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.10
     rank 18: 0.06
     rank 19: 0.15
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.05
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-inner-step:
     rank  0: 0.51
     rank  1: 0.54
     rank  2: 0.60
     rank  3: 0.52
     rank  4: 48.05
     rank  5: 48.09
     rank  6: 48.06
     rank  7: 48.21
     rank  8: 43.08
     rank  9: 43.07
     rank 10: 43.05
     rank 11: 43.01
     rank 12: 43.00
     rank 13: 43.00
     rank 14: 43.06
     rank 15: 43.03
     rank 16: 46.36
     rank 17: 46.33
     rank 18: 46.41
     rank 19: 47.19
  optimizer:
     rank  0: 2.61
     rank  1: 2.64
     rank  2: 2.71
     rank  3: 2.62
     rank  4: 49.13
     rank  5: 50.19
     rank  6: 50.16
     rank  7: 50.31
     rank  8: 45.18
     rank  9: 45.17
     rank 10: 45.15
     rank 11: 45.11
     rank 12: 45.10
     rank 13: 45.10
     rank 14: 45.17
     rank 15: 45.13
     rank 16: 48.48
     rank 17: 48.43
     rank 18: 48.51
     rank 19: 49.27
 [2024-12-02 22:12:18] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 8009.7 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.188240E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7922.82
     rank  1: 7922.48
     rank  2: 7922.52
     rank  3: 7922.48
     rank  4: 7927.33
     rank  5: 7927.00
     rank  6: 7927.15
     rank  7: 7927.00
     rank  8: 7926.85
     rank  9: 7926.54
     rank 10: 7926.67
     rank 11: 7926.54
     rank 12: 7926.84
     rank 13: 7926.55
     rank 14: 7926.66
     rank 15: 7926.55
     rank 16: 7927.53
     rank 17: 7926.80
     rank 18: 7926.81
     rank 19: 7926.84
  forward-compute:
     rank  0: 935.56
     rank  1: 935.03
     rank  2: 937.08
     rank  3: 936.51
     rank  4: 2956.26
     rank  5: 2955.17
     rank  6: 2959.99
     rank  7: 2961.41
     rank  8: 2768.31
     rank  9: 2770.49
     rank 10: 2769.52
     rank 11: 2773.31
     rank 12: 2778.29
     rank 13: 2777.85
     rank 14: 2782.46
     rank 15: 2782.97
     rank 16: 2904.42
     rank 17: 2904.25
     rank 18: 2904.45
     rank 19: 2904.75
  backward-compute:
     rank  0: 927.84
     rank  1: 927.84
     rank  2: 928.73
     rank  3: 931.15
     rank  4: 3388.98
     rank  5: 3391.61
     rank  6: 3391.22
     rank  7: 3397.36
     rank  8: 3286.98
     rank  9: 3287.64
     rank 10: 3289.48
     rank 11: 3287.38
     rank 12: 3293.49
     rank 13: 3291.46
     rank 14: 3293.16
     rank 15: 3294.41
     rank 16: 3509.33
     rank 17: 3504.82
     rank 18: 3505.98
     rank 19: 3509.21
  pure-backward-compute:
     rank  0: 927.13
     rank  1: 927.18
     rank  2: 928.04
     rank  3: 930.51
     rank  4: 3388.04
     rank  5: 3390.50
     rank  6: 3390.23
     rank  7: 3396.54
     rank  8: 3285.59
     rank  9: 3286.82
     rank 10: 3288.34
     rank 11: 3286.58
     rank 12: 3292.41
     rank 13: 3290.44
     rank 14: 3292.38
     rank 15: 3293.52
     rank 16: 3507.50
     rank 17: 3502.69
     rank 18: 3503.77
     rank 19: 3507.01
  batch-generator:
     rank  0: 54.37
     rank  1: 55.17
     rank  2: 58.96
     rank  3: 58.84
     rank  4: 59.46
     rank  5: 64.85
     rank  6: 67.62
     rank  7: 69.65
     rank  8: 52.23
     rank  9: 54.88
     rank 10: 57.27
     rank 11: 61.57
     rank 12: 50.05
     rank 13: 53.79
     rank 14: 56.91
     rank 15: 56.21
     rank 16: 52.60
     rank 17: 55.37
     rank 18: 56.56
     rank 19: 59.75
  forward-recv:
     rank  4: 62.92
     rank  5: 62.38
     rank  6: 62.62
     rank  7: 62.81
     rank  8: 276.72
     rank  9: 277.74
     rank 10: 276.86
     rank 11: 278.73
     rank 12: 447.89
     rank 13: 447.99
     rank 14: 448.49
     rank 15: 446.59
     rank 16: 619.67
     rank 17: 619.57
     rank 18: 619.43
     rank 19: 619.81
  forward-send:
     rank  0: 405.03
     rank  1: 404.98
     rank  2: 403.66
     rank  3: 404.28
     rank  4: 31.28
     rank  5: 32.21
     rank  6: 30.96
     rank  7: 31.13
     rank  8: 20.92
     rank  9: 20.64
     rank 10: 20.87
     rank 11: 19.39
     rank 12: 10.56
     rank 13: 10.43
     rank 14: 10.00
     rank 15: 10.41
  backward-recv:
     rank  0: 1410.82
     rank  1: 1411.27
     rank  2: 1411.69
     rank  3: 1410.78
     rank  4: 591.62
     rank  5: 591.20
     rank  6: 590.30
     rank  7: 591.22
     rank  8: 388.73
     rank  9: 388.79
     rank 10: 388.43
     rank 11: 389.40
     rank 12: 195.38
     rank 13: 195.53
     rank 14: 195.86
     rank 15: 195.24
  backward-send:
     rank  4: 4.03
     rank  5: 4.27
     rank  6: 4.98
     rank  7: 3.85
     rank  8: 31.37
     rank  9: 31.07
     rank 10: 31.40
     rank 11: 30.29
     rank 12: 20.96
     rank 13: 20.88
     rank 14: 20.65
     rank 15: 20.78
     rank 16: 10.43
     rank 17: 10.56
     rank 18: 10.54
     rank 19: 9.85
  forward-send-backward-recv:
     rank  0: 4227.92
     rank  1: 4230.03
     rank  2: 4228.03
     rank  3: 4224.81
     rank  4: 796.01
     rank  5: 794.83
     rank  6: 793.97
     rank  7: 789.33
     rank  8: 706.77
     rank  9: 707.14
     rank 10: 704.75
     rank 11: 709.31
     rank 12: 521.51
     rank 13: 523.34
     rank 14: 522.96
     rank 15: 521.61
  backward-send-forward-recv:
     rank  4: 21.41
     rank  5: 21.15
     rank  6: 19.17
     rank  7: 17.86
     rank  8: 159.92
     rank  9: 159.11
     rank 10: 159.05
     rank 11: 154.29
     rank 12: 157.84
     rank 13: 157.82
     rank 14: 154.22
     rank 15: 156.02
     rank 16: 167.98
     rank 17: 170.36
     rank 18: 169.20
     rank 19: 166.89
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.20
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.32
     rank  3: 0.24
     rank  4: 2.39
     rank  5: 2.37
     rank  6: 2.45
     rank  7: 2.43
     rank  8: 2.20
     rank  9: 2.24
     rank 10: 2.16
     rank 11: 2.24
     rank 12: 2.17
     rank 13: 2.17
     rank 14: 2.24
     rank 15: 2.24
     rank 16: 3.07
     rank 17: 2.49
     rank 18: 2.46
     rank 19: 2.50
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.14
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.11
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.54
     rank  1: 0.52
     rank  2: 0.62
     rank  3: 0.55
     rank  4: 48.08
     rank  5: 48.11
     rank  6: 48.22
     rank  7: 48.12
     rank  8: 43.09
     rank  9: 43.01
     rank 10: 42.97
     rank 11: 43.08
     rank 12: 42.97
     rank 13: 43.04
     rank 14: 43.05
     rank 15: 43.02
     rank 16: 47.26
     rank 17: 46.37
     rank 18: 46.36
     rank 19: 46.46
  optimizer:
     rank  0: 1.96
     rank  1: 1.94
     rank  2: 2.05
     rank  3: 1.97
     rank  4: 49.51
     rank  5: 49.54
     rank  6: 49.76
     rank  7: 49.55
     rank  8: 44.51
     rank  9: 44.43
     rank 10: 44.39
     rank 11: 44.51
     rank 12: 44.40
     rank 13: 44.47
     rank 14: 44.48
     rank 15: 44.44
     rank 16: 48.65
     rank 17: 47.80
     rank 18: 47.79
     rank 19: 47.89
 [2024-12-02 22:12:26] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 8010.0 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.399599E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7925.85
     rank  1: 7925.78
     rank  2: 7925.84
     rank  3: 7925.81
     rank  4: 7930.37
     rank  5: 7930.27
     rank  6: 7930.32
     rank  7: 7930.32
     rank  8: 7929.93
     rank  9: 7929.86
     rank 10: 7929.89
     rank 11: 7929.85
     rank 12: 7929.86
     rank 13: 7929.80
     rank 14: 7929.87
     rank 15: 7929.87
     rank 16: 7930.24
     rank 17: 7930.10
     rank 18: 7930.10
     rank 19: 7930.12
  forward-compute:
     rank  0: 929.42
     rank  1: 928.46
     rank  2: 930.45
     rank  3: 930.61
     rank  4: 2969.49
     rank  5: 2971.63
     rank  6: 2975.24
     rank  7: 2976.30
     rank  8: 2763.23
     rank  9: 2764.65
     rank 10: 2764.50
     rank 11: 2766.97
     rank 12: 2772.67
     rank 13: 2771.35
     rank 14: 2775.40
     rank 15: 2778.25
     rank 16: 2902.74
     rank 17: 2902.90
     rank 18: 2902.23
     rank 19: 2902.90
  backward-compute:
     rank  0: 928.76
     rank  1: 928.13
     rank  2: 928.87
     rank  3: 930.71
     rank  4: 3394.75
     rank  5: 3396.04
     rank  6: 3397.09
     rank  7: 3399.71
     rank  8: 3278.92
     rank  9: 3279.25
     rank 10: 3280.50
     rank 11: 3280.60
     rank 12: 3276.80
     rank 13: 3276.40
     rank 14: 3276.69
     rank 15: 3276.88
     rank 16: 3508.80
     rank 17: 3505.64
     rank 18: 3506.70
     rank 19: 3509.50
  pure-backward-compute:
     rank  0: 928.11
     rank  1: 927.49
     rank  2: 928.19
     rank  3: 930.07
     rank  4: 3393.80
     rank  5: 3395.04
     rank  6: 3396.28
     rank  7: 3398.89
     rank  8: 3277.81
     rank  9: 3278.31
     rank 10: 3279.43
     rank 11: 3279.77
     rank 12: 3275.74
     rank 13: 3275.05
     rank 14: 3275.71
     rank 15: 3275.97
     rank 16: 3506.91
     rank 17: 3503.51
     rank 18: 3504.23
     rank 19: 3507.28
  batch-generator:
     rank  0: 51.79
     rank  1: 54.08
     rank  2: 56.86
     rank  3: 58.29
     rank  4: 60.42
     rank  5: 64.62
     rank  6: 66.58
     rank  7: 68.21
     rank  8: 50.70
     rank  9: 52.76
     rank 10: 55.95
     rank 11: 58.74
     rank 12: 51.09
     rank 13: 53.49
     rank 14: 55.89
     rank 15: 57.88
     rank 16: 54.26
     rank 17: 57.82
     rank 18: 57.29
     rank 19: 58.04
  forward-recv:
     rank  4: 62.33
     rank  5: 62.34
     rank  6: 61.73
     rank  7: 61.45
     rank  8: 282.82
     rank  9: 283.50
     rank 10: 282.64
     rank 11: 283.68
     rank 12: 452.05
     rank 13: 451.77
     rank 14: 452.31
     rank 15: 451.07
     rank 16: 621.78
     rank 17: 621.80
     rank 18: 621.68
     rank 19: 621.85
  forward-send:
     rank  0: 414.09
     rank  1: 413.91
     rank  2: 412.14
     rank  3: 411.80
     rank  4: 31.82
     rank  5: 31.68
     rank  6: 30.99
     rank  7: 30.50
     rank  8: 21.07
     rank  9: 20.56
     rank 10: 21.03
     rank 11: 19.70
     rank 12: 10.54
     rank 13: 10.57
     rank 14: 10.20
     rank 15: 10.48
  backward-recv:
     rank  0: 1408.49
     rank  1: 1408.97
     rank  2: 1409.26
     rank  3: 1408.64
     rank  4: 586.39
     rank  5: 584.83
     rank  6: 585.67
     rank  7: 585.33
     rank  8: 389.70
     rank  9: 390.31
     rank 10: 389.32
     rank 11: 390.02
     rank 12: 194.43
     rank 13: 194.49
     rank 14: 195.00
     rank 15: 194.65
  backward-send:
     rank  4: 3.69
     rank  5: 4.50
     rank  6: 4.37
     rank  7: 4.30
     rank  8: 31.41
     rank  9: 30.58
     rank 10: 31.44
     rank 11: 30.83
     rank 12: 20.95
     rank 13: 20.85
     rank 14: 20.69
     rank 15: 20.60
     rank 16: 10.40
     rank 17: 10.58
     rank 18: 10.56
     rank 19: 10.06
  forward-send-backward-recv:
     rank  0: 4230.25
     rank  1: 4232.99
     rank  2: 4232.06
     rank  3: 4229.35
     rank  4: 784.68
     rank  5: 782.16
     rank  6: 782.67
     rank  7: 781.12
     rank  8: 700.16
     rank  9: 701.19
     rank 10: 699.42
     rank 11: 701.44
     rank 12: 541.23
     rank 13: 542.66
     rank 14: 543.52
     rank 15: 542.11
  backward-send-forward-recv:
     rank  4: 21.28
     rank  5: 21.29
     rank  6: 19.03
     rank  7: 18.18
     rank  8: 174.48
     rank  9: 174.06
     rank 10: 173.07
     rank 11: 170.94
     rank 12: 157.69
     rank 13: 158.21
     rank 14: 155.31
     rank 15: 155.00
     rank 16: 167.42
     rank 17: 168.90
     rank 18: 168.55
     rank 19: 166.78
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.10
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.23
     rank  1: 0.25
     rank  2: 0.27
     rank  3: 0.24
     rank  4: 2.41
     rank  5: 2.38
     rank  6: 2.43
     rank  7: 2.38
     rank  8: 2.20
     rank  9: 2.30
     rank 10: 2.19
     rank 11: 2.22
     rank 12: 2.16
     rank 13: 2.22
     rank 14: 2.30
     rank 15: 2.30
     rank 16: 2.91
     rank 17: 2.46
     rank 18: 2.45
     rank 19: 2.50
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.09
     rank  9: 0.07
     rank 10: 0.10
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.06
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.11
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.50
     rank  1: 0.51
     rank  2: 0.60
     rank  3: 0.50
     rank  4: 48.11
     rank  5: 48.10
     rank  6: 48.14
     rank  7: 48.12
     rank  8: 43.03
     rank  9: 43.11
     rank 10: 42.99
     rank 11: 43.05
     rank 12: 42.96
     rank 13: 43.07
     rank 14: 43.12
     rank 15: 43.06
     rank 16: 46.61
     rank 17: 46.34
     rank 18: 46.43
     rank 19: 46.46
  optimizer:
     rank  0: 1.13
     rank  1: 1.14
     rank  2: 1.23
     rank  3: 1.20
     rank  4: 48.74
     rank  5: 48.73
     rank  6: 48.77
     rank  7: 48.75
     rank  8: 43.65
     rank  9: 43.74
     rank 10: 43.61
     rank 11: 43.67
     rank 12: 43.59
     rank 13: 43.72
     rank 14: 43.76
     rank 15: 43.69
     rank 16: 47.33
     rank 17: 46.97
     rank 18: 47.06
     rank 19: 47.09
 [2024-12-02 22:12:34] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 8005.1 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.217433E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7922.39
     rank  1: 7922.40
     rank  2: 7922.40
     rank  3: 7922.40
     rank  4: 7926.89
     rank  5: 7926.90
     rank  6: 7926.89
     rank  7: 7926.95
     rank  8: 7926.44
     rank  9: 7926.51
     rank 10: 7926.42
     rank 11: 7926.46
     rank 12: 7926.41
     rank 13: 7926.44
     rank 14: 7926.42
     rank 15: 7926.44
     rank 16: 7926.70
     rank 17: 7926.70
     rank 18: 7926.70
     rank 19: 7926.72
  forward-compute:
     rank  0: 931.06
     rank  1: 929.22
     rank  2: 932.71
     rank  3: 932.12
     rank  4: 2986.27
     rank  5: 2986.15
     rank  6: 2990.32
     rank  7: 2990.13
     rank  8: 2772.60
     rank  9: 2772.78
     rank 10: 2773.91
     rank 11: 2775.09
     rank 12: 2770.47
     rank 13: 2770.37
     rank 14: 2772.78
     rank 15: 2782.63
     rank 16: 2905.87
     rank 17: 2906.12
     rank 18: 2906.50
     rank 19: 2910.76
  backward-compute:
     rank  0: 925.15
     rank  1: 925.63
     rank  2: 925.66
     rank  3: 928.27
     rank  4: 3400.45
     rank  5: 3399.85
     rank  6: 3405.94
     rank  7: 3408.32
     rank  8: 3287.43
     rank  9: 3287.19
     rank 10: 3288.80
     rank 11: 3298.37
     rank 12: 3289.88
     rank 13: 3288.57
     rank 14: 3291.42
     rank 15: 3294.76
     rank 16: 3490.44
     rank 17: 3488.48
     rank 18: 3489.24
     rank 19: 3490.13
  pure-backward-compute:
     rank  0: 924.51
     rank  1: 924.85
     rank  2: 924.94
     rank  3: 927.62
     rank  4: 3399.57
     rank  5: 3398.53
     rank  6: 3405.05
     rank  7: 3407.52
     rank  8: 3286.32
     rank  9: 3285.82
     rank 10: 3287.72
     rank 11: 3297.59
     rank 12: 3288.72
     rank 13: 3287.49
     rank 14: 3290.57
     rank 15: 3293.96
     rank 16: 3488.55
     rank 17: 3486.29
     rank 18: 3486.94
     rank 19: 3488.10
  batch-generator:
     rank  0: 51.32
     rank  1: 51.57
     rank  2: 57.09
     rank  3: 57.18
     rank  4: 53.83
     rank  5: 67.25
     rank  6: 67.21
     rank  7: 68.49
     rank  8: 52.09
     rank  9: 53.56
     rank 10: 60.56
     rank 11: 61.54
     rank 12: 52.11
     rank 13: 55.57
     rank 14: 56.01
     rank 15: 65.32
     rank 16: 55.08
     rank 17: 58.74
     rank 18: 59.33
     rank 19: 62.90
  forward-recv:
     rank  4: 63.32
     rank  5: 63.08
     rank  6: 62.42
     rank  7: 62.97
     rank  8: 280.43
     rank  9: 280.79
     rank 10: 280.43
     rank 11: 282.12
     rank 12: 449.72
     rank 13: 449.83
     rank 14: 449.92
     rank 15: 448.45
     rank 16: 621.72
     rank 17: 621.65
     rank 18: 621.56
     rank 19: 621.83
  forward-send:
     rank  0: 411.01
     rank  1: 410.90
     rank  2: 409.40
     rank  3: 409.88
     rank  4: 31.62
     rank  5: 31.63
     rank  6: 31.05
     rank  7: 31.26
     rank  8: 20.97
     rank  9: 20.77
     rank 10: 20.93
     rank 11: 19.55
     rank 12: 10.56
     rank 13: 10.38
     rank 14: 10.19
     rank 15: 10.45
  backward-recv:
     rank  0: 1396.51
     rank  1: 1396.42
     rank  2: 1396.66
     rank  3: 1395.66
     rank  4: 572.46
     rank  5: 572.01
     rank  6: 572.50
     rank  7: 573.45
     rank  8: 382.48
     rank  9: 382.54
     rank 10: 382.53
     rank 11: 382.95
     rank 12: 197.09
     rank 13: 197.50
     rank 14: 197.39
     rank 15: 197.63
  backward-send:
     rank  4: 4.53
     rank  5: 4.40
     rank  6: 4.59
     rank  7: 3.38
     rank  8: 31.33
     rank  9: 31.14
     rank 10: 31.30
     rank 11: 30.88
     rank 12: 20.96
     rank 13: 20.65
     rank 14: 20.84
     rank 15: 20.75
     rank 16: 10.16
     rank 17: 10.45
     rank 18: 10.35
     rank 19: 10.42
  forward-send-backward-recv:
     rank  0: 4243.99
     rank  1: 4247.26
     rank  2: 4244.82
     rank  3: 4242.15
     rank  4: 773.40
     rank  5: 772.37
     rank  6: 768.62
     rank  7: 767.29
     rank  8: 589.02
     rank  9: 589.46
     rank 10: 589.91
     rank 11: 583.05
     rank 12: 450.85
     rank 13: 451.50
     rank 14: 450.05
     rank 15: 445.91
  backward-send-forward-recv:
     rank  4: 19.90
     rank  5: 21.06
     rank  6: 18.84
     rank  7: 17.33
     rank  8: 274.14
     rank  9: 274.75
     rank 10: 271.00
     rank 11: 269.30
     rank 12: 234.84
     rank 13: 235.09
     rank 14: 234.00
     rank 15: 225.76
     rank 16: 179.82
     rank 17: 180.45
     rank 18: 179.45
     rank 19: 175.75
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.23
     rank  1: 0.29
     rank  2: 0.29
     rank  3: 0.22
     rank  4: 2.38
     rank  5: 2.37
     rank  6: 2.38
     rank  7: 2.39
     rank  8: 2.19
     rank  9: 2.27
     rank 10: 2.15
     rank 11: 2.19
     rank 12: 2.19
     rank 13: 2.17
     rank 14: 2.26
     rank 15: 2.24
     rank 16: 2.44
     rank 17: 2.45
     rank 18: 2.44
     rank 19: 2.60
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.49
     rank  1: 0.58
     rank  2: 0.65
     rank  3: 0.48
     rank  4: 48.07
     rank  5: 48.09
     rank  6: 48.02
     rank  7: 48.09
     rank  8: 43.09
     rank  9: 43.11
     rank 10: 42.98
     rank 11: 42.98
     rank 12: 43.00
     rank 13: 42.99
     rank 14: 43.04
     rank 15: 43.00
     rank 16: 46.29
     rank 17: 46.32
     rank 18: 46.38
     rank 19: 46.42
  optimizer:
     rank  0: 1.01
     rank  1: 1.10
     rank  2: 1.17
     rank  3: 1.01
     rank  4: 48.59
     rank  5: 48.62
     rank  6: 48.55
     rank  7: 48.62
     rank  8: 43.62
     rank  9: 43.65
     rank 10: 43.51
     rank 11: 43.51
     rank 12: 43.53
     rank 13: 43.52
     rank 14: 43.57
     rank 15: 43.53
     rank 16: 46.81
     rank 17: 46.85
     rank 18: 46.90
     rank 19: 46.94
 [2024-12-02 22:12:42] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 8015.5 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.284643E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7935.99
     rank  1: 7935.99
     rank  2: 7936.00
     rank  3: 7936.01
     rank  4: 7940.48
     rank  5: 7940.49
     rank  6: 7940.49
     rank  7: 7940.53
     rank  8: 7940.10
     rank  9: 7940.70
     rank 10: 7940.04
     rank 11: 7940.03
     rank 12: 7940.02
     rank 13: 7940.16
     rank 14: 7940.03
     rank 15: 7940.06
     rank 16: 7940.31
     rank 17: 7940.32
     rank 18: 7940.32
     rank 19: 7940.32
  forward-compute:
     rank  0: 923.64
     rank  1: 923.52
     rank  2: 925.34
     rank  3: 926.63
     rank  4: 2998.38
     rank  5: 3005.11
     rank  6: 3007.75
     rank  7: 3004.34
     rank  8: 2767.20
     rank  9: 2766.27
     rank 10: 2768.03
     rank 11: 2770.89
     rank 12: 2783.81
     rank 13: 2783.68
     rank 14: 2785.21
     rank 15: 2791.92
     rank 16: 2919.07
     rank 17: 2919.33
     rank 18: 2920.15
     rank 19: 2924.90
  backward-compute:
     rank  0: 917.59
     rank  1: 920.71
     rank  2: 922.81
     rank  3: 919.80
     rank  4: 3397.74
     rank  5: 3395.79
     rank  6: 3401.35
     rank  7: 3404.82
     rank  8: 3286.41
     rank  9: 3288.13
     rank 10: 3287.85
     rank 11: 3291.93
     rank 12: 3290.54
     rank 13: 3289.61
     rank 14: 3291.58
     rank 15: 3295.80
     rank 16: 3494.14
     rank 17: 3492.07
     rank 18: 3492.95
     rank 19: 3493.55
  pure-backward-compute:
     rank  0: 916.94
     rank  1: 920.09
     rank  2: 922.04
     rank  3: 919.14
     rank  4: 3396.18
     rank  5: 3394.91
     rank  6: 3400.37
     rank  7: 3403.53
     rank  8: 3285.17
     rank  9: 3286.45
     rank 10: 3286.85
     rank 11: 3291.14
     rank 12: 3289.33
     rank 13: 3288.57
     rank 14: 3290.75
     rank 15: 3295.00
     rank 16: 3492.28
     rank 17: 3489.96
     rank 18: 3490.76
     rank 19: 3491.46
  batch-generator:
     rank  0: 51.58
     rank  1: 52.81
     rank  2: 56.04
     rank  3: 57.47
     rank  4: 89.99
     rank  5: 88.00
     rank  6: 94.04
     rank  7: 96.45
     rank  8: 51.78
     rank  9: 52.64
     rank 10: 57.32
     rank 11: 60.03
     rank 12: 59.73
     rank 13: 60.64
     rank 14: 60.74
     rank 15: 66.90
     rank 16: 53.41
     rank 17: 57.43
     rank 18: 58.45
     rank 19: 62.42
  forward-recv:
     rank  4: 62.47
     rank  5: 62.18
     rank  6: 62.00
     rank  7: 61.49
     rank  8: 278.42
     rank  9: 278.55
     rank 10: 278.12
     rank 11: 279.61
     rank 12: 449.68
     rank 13: 450.01
     rank 14: 450.32
     rank 15: 448.18
     rank 16: 620.66
     rank 17: 620.60
     rank 18: 620.28
     rank 19: 620.76
  forward-send:
     rank  0: 412.29
     rank  1: 412.46
     rank  2: 410.70
     rank  3: 409.39
     rank  4: 31.72
     rank  5: 31.61
     rank  6: 30.94
     rank  7: 30.43
     rank  8: 20.83
     rank  9: 20.69
     rank 10: 20.89
     rank 11: 19.09
     rank 12: 10.55
     rank 13: 10.42
     rank 14: 10.07
     rank 15: 10.40
  backward-recv:
     rank  0: 1403.14
     rank  1: 1403.16
     rank  2: 1402.09
     rank  3: 1403.24
     rank  4: 575.58
     rank  5: 576.65
     rank  6: 578.01
     rank  7: 575.78
     rank  8: 382.07
     rank  9: 381.32
     rank 10: 381.53
     rank 11: 382.46
     rank 12: 194.79
     rank 13: 195.00
     rank 14: 195.00
     rank 15: 194.20
  backward-send:
     rank  4: 4.27
     rank  5: 3.92
     rank  6: 3.47
     rank  7: 4.50
     rank  8: 31.21
     rank  9: 31.30
     rank 10: 31.12
     rank 11: 30.51
     rank 12: 20.96
     rank 13: 20.73
     rank 14: 20.37
     rank 15: 20.89
     rank 16: 10.50
     rank 17: 10.50
     rank 18: 10.50
     rank 19: 9.96
  forward-send-backward-recv:
     rank  0: 4263.87
     rank  1: 4262.52
     rank  2: 4260.76
     rank  3: 4261.72
     rank  4: 772.09
     rank  5: 771.23
     rank  6: 768.20
     rank  7: 766.26
     rank  8: 621.75
     rank  9: 622.80
     rank 10: 622.49
     rank 11: 619.33
     rank 12: 475.62
     rank 13: 476.33
     rank 14: 476.02
     rank 15: 472.00
  backward-send-forward-recv:
     rank  4: 20.74
     rank  5: 17.62
     rank  6: 15.76
     rank  7: 18.65
     rank  8: 263.92
     rank  9: 263.86
     rank 10: 262.26
     rank 11: 260.48
     rank 12: 211.47
     rank 13: 211.83
     rank 14: 211.24
     rank 15: 206.76
     rank 16: 176.84
     rank 17: 177.37
     rank 18: 176.12
     rank 19: 171.71
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.08
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.06
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.23
     rank  1: 0.25
     rank  2: 0.28
     rank  3: 0.22
     rank  4: 2.39
     rank  5: 2.38
     rank  6: 2.39
     rank  7: 2.39
     rank  8: 2.23
     rank  9: 2.95
     rank 10: 2.22
     rank 11: 2.16
     rank 12: 2.16
     rank 13: 2.17
     rank 14: 2.25
     rank 15: 2.29
     rank 16: 2.49
     rank 17: 2.50
     rank 18: 2.45
     rank 19: 2.54
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.12
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 0.50
     rank  1: 0.53
     rank  2: 0.55
     rank  3: 0.49
     rank  4: 48.07
     rank  5: 48.07
     rank  6: 48.04
     rank  7: 48.12
     rank  8: 43.14
     rank  9: 44.05
     rank 10: 43.03
     rank 11: 42.99
     rank 12: 42.98
     rank 13: 42.95
     rank 14: 43.01
     rank 15: 42.98
     rank 16: 46.35
     rank 17: 46.30
     rank 18: 46.37
     rank 19: 46.40
  optimizer:
     rank  0: 1.95
     rank  1: 1.97
     rank  2: 2.00
     rank  3: 1.94
     rank  4: 49.52
     rank  5: 49.52
     rank  6: 49.50
     rank  7: 49.58
     rank  8: 44.59
     rank  9: 45.50
     rank 10: 44.48
     rank 11: 44.44
     rank 12: 44.44
     rank 13: 44.40
     rank 14: 44.46
     rank 15: 44.43
     rank 16: 47.81
     rank 17: 47.75
     rank 18: 47.83
     rank 19: 47.87
