examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-03 23:41:28,010] torch.distributed.run: [WARNING] 
[2024-12-03 23:41:28,010] torch.distributed.run: [WARNING] *****************************************
[2024-12-03 23:41:28,010] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-03 23:41:28,010] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 77.34
     rank  1: 112.60
     rank  2: 76.36
     rank  3: 115.08
     rank  4: 34.28
     rank  5: 42.89
     rank  6: 42.97
     rank  7: 44.45
     rank  8: 46.54
     rank  9: 50.39
     rank 10: 45.41
     rank 11: 56.67
     rank 12: 33.38
     rank 13: 26.51
     rank 14: 37.50
     rank 15: 38.22
     rank 16: 49.17
     rank 17: 51.22
     rank 18: 47.09
     rank 19: 48.75
  train/valid/test-data-iterators-setup:
     rank  0: 1039.44
     rank  1: 1039.45
     rank  2: 1039.34
     rank  3: 1039.33
     rank  4: 1051.02
     rank  5: 1051.03
     rank  6: 1055.89
     rank  7: 1051.49
     rank  8: 1134.51
     rank  9: 1134.48
     rank 10: 1134.56
     rank 11: 1134.58
     rank 12: 1212.12
     rank 13: 1212.28
     rank 14: 1212.14
     rank 15: 1212.20
     rank 16: 1212.21
     rank 17: 1212.64
     rank 18: 1212.18
     rank 19: 1212.24
 [2024-12-03 23:42:12] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21507.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4540.0 | max reserved: 4540.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4520.0 | max reserved: 4520.0[Rank 16] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4496.0 | max reserved: 4496.0

[Rank 19] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4508.0 | max reserved: 4508.0
times across ranks (ms):
  forward-backward:
     rank  0: 21429.88
     rank  1: 21429.90
     rank  2: 21429.90
     rank  3: 21429.85
     rank  4: 21429.43
     rank  5: 21429.20
     rank  6: 21428.99
     rank  7: 21429.31
     rank  8: 21477.87
     rank  9: 21475.57
     rank 10: 21456.44
     rank 11: 21470.55
     rank 12: 21472.77
     rank 13: 21472.91
     rank 14: 21477.17
     rank 15: 21477.41
     rank 16: 21428.64
     rank 17: 21428.64
     rank 18: 21428.45
     rank 19: 21428.45
  forward-compute:
     rank  0: 4929.32
     rank  1: 4932.93
     rank  2: 4987.33
     rank  3: 4987.97
     rank  4: 5371.53
     rank  5: 5388.57
     rank  6: 5393.44
     rank  7: 5393.05
     rank  8: 4965.35
     rank  9: 4973.88
     rank 10: 4973.76
     rank 11: 4971.76
     rank 12: 5163.96
     rank 13: 5182.20
     rank 14: 5166.52
     rank 15: 5168.63
     rank 16: 5072.65
     rank 17: 5073.80
     rank 18: 5077.96
     rank 19: 5077.73
  backward-compute:
     rank  0: 1991.30
     rank  1: 2003.83
     rank  2: 1951.69
     rank  3: 1949.79
     rank  4: 3046.05
     rank  5: 3043.45
     rank  6: 3042.58
     rank  7: 3041.54
     rank  8: 2996.81
     rank  9: 3001.66
     rank 10: 2996.22
     rank 11: 2999.66
     rank 12: 3003.51
     rank 13: 3005.72
     rank 14: 3007.58
     rank 15: 3011.09
     rank 16: 3178.15
     rank 17: 3186.84
     rank 18: 3181.84
     rank 19: 3183.75
  pure-backward-compute:
     rank  0: 1990.27
     rank  1: 2002.88
     rank  2: 1950.72
     rank  3: 1948.92
     rank  4: 3044.75
     rank  5: 3041.99
     rank  6: 3041.33
     rank  7: 3040.10
     rank  8: 2995.71
     rank  9: 3000.46
     rank 10: 2995.06
     rank 11: 2998.32
     rank 12: 3002.01
     rank 13: 3004.57
     rank 14: 3006.40
     rank 15: 3009.77
     rank 16: 3175.41
     rank 17: 3183.67
     rank 18: 3180.21
     rank 19: 3182.03
  batch-generator:
     rank  0: 1046.41
     rank  1: 1055.02
     rank  2: 1111.88
     rank  3: 1112.12
     rank  4: 1233.91
     rank  5: 1260.84
     rank  6: 1274.22
     rank  7: 1271.76
     rank  8: 876.27
     rank  9: 890.73
     rank 10: 890.53
     rank 11: 887.38
     rank 12: 1074.50
     rank 13: 1087.04
     rank 14: 1073.64
     rank 15: 1076.22
     rank 16: 1099.92
     rank 17: 1109.73
     rank 18: 1106.71
     rank 19: 1110.87
  forward-recv:
     rank  4: 3959.11
     rank  5: 3961.06
     rank  6: 3954.56
     rank  7: 3951.45
     rank  8: 6546.04
     rank  9: 6544.23
     rank 10: 6536.83
     rank 11: 6542.07
     rank 12: 8914.32
     rank 13: 8910.96
     rank 14: 8914.06
     rank 15: 8913.25
     rank 16: 11512.27
     rank 17: 11504.70
     rank 18: 11510.77
     rank 19: 11511.66
  forward-send:
     rank  0: 7498.62
     rank  1: 7485.84
     rank  2: 7485.02
     rank  3: 7482.72
     rank  4: 4642.97
     rank  5: 4628.95
     rank  6: 4631.98
     rank  7: 4634.39
     rank  8: 2460.99
     rank  9: 2450.38
     rank 10: 2459.18
     rank 11: 2458.27
     rank 12: 35.94
     rank 13: 28.05
     rank 14: 34.15
     rank 15: 35.27
  backward-recv:
     rank  0: 1280.41
     rank  1: 1282.40
     rank  2: 1282.35
     rank  3: 1282.68
     rank  4: 594.15
     rank  5: 591.61
     rank  6: 593.89
     rank  7: 593.53
     rank  8: 386.20
     rank  9: 387.44
     rank 10: 386.30
     rank 11: 386.51
     rank 12: 191.67
     rank 13: 191.51
     rank 14: 191.39
     rank 15: 190.95
  backward-send:
     rank  4: 3.50
     rank  5: 4.97
     rank  6: 4.03
     rank  7: 4.44
     rank  8: 31.36
     rank  9: 30.37
     rank 10: 31.28
     rank 11: 31.06
     rank 12: 20.76
     rank 13: 20.89
     rank 14: 20.86
     rank 15: 20.54
     rank 16: 10.47
     rank 17: 10.58
     rank 18: 10.33
     rank 19: 9.95
  forward-send-backward-recv:
     rank  0: 5665.35
     rank  1: 5660.10
     rank  2: 5660.39
     rank  3: 5664.87
     rank  4: 2923.03
     rank  5: 2927.00
     rank  6: 2926.41
     rank  7: 2925.56
     rank  8: 2783.44
     rank  9: 2780.92
     rank 10: 2783.80
     rank 11: 2778.76
     rank 12: 2628.76
     rank 13: 2628.38
     rank 14: 2625.97
     rank 15: 2623.56
  backward-send-forward-recv:
     rank  4: 757.95
     rank  5: 752.31
     rank  6: 749.66
     rank  7: 754.24
     rank  8: 940.24
     rank  9: 942.35
     rank 10: 942.03
     rank 11: 941.56
     rank 12: 953.20
     rank 13: 946.71
     rank 14: 952.87
     rank 15: 951.42
     rank 16: 940.85
     rank 17: 938.82
     rank 18: 937.66
     rank 19: 934.58
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.07
     rank  2: 0.05
     rank  3: 0.06
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.07
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.07
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.19
     rank  5: 0.15
     rank  6: 0.10
     rank  7: 0.15
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 49.30
     rank  1: 46.46
     rank  2: 48.03
     rank  3: 42.94
     rank  4: 45.24
     rank  5: 45.37
     rank  6: 41.49
     rank  7: 45.43
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.06
     rank 13: 0.07
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.23
     rank  2: 0.24
     rank  3: 0.23
     rank  4: 0.10
     rank  5: 0.02
     rank  6: 0.13
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.04
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 25.24
     rank  1: 25.30
     rank  2: 25.29
     rank  3: 25.53
     rank  4: 2.97
     rank  5: 2.35
     rank  6: 3.81
     rank  7: 2.45
     rank  8: 0.15
     rank  9: 0.16
     rank 10: 0.04
     rank 11: 0.12
     rank 12: 0.17
     rank 13: 0.20
     rank 14: 0.08
     rank 15: 0.18
     rank 16: 0.09
     rank 17: 0.08
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 26.63
     rank  1: 26.69
     rank  2: 26.68
     rank  3: 26.91
     rank  4: 4.37
     rank  5: 3.75
     rank  6: 5.20
     rank  7: 3.84
     rank  8: 1.53
     rank  9: 1.57
     rank 10: 1.42
     rank 11: 1.46
     rank 12: 1.55
     rank 13: 1.60
     rank 14: 1.48
     rank 15: 1.58
     rank 16: 1.48
     rank 17: 1.47
     rank 18: 1.43
     rank 19: 1.44
 [2024-12-03 23:42:19] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7625.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7579.60
     rank  1: 7579.67
     rank  2: 7579.74
     rank  3: 7579.93
     rank  4: 7579.19
     rank  5: 7579.09
     rank  6: 7579.30
     rank  7: 7579.36
     rank  8: 7578.95
     rank  9: 7579.02
     rank 10: 7578.99
     rank 11: 7579.23
     rank 12: 7578.99
     rank 13: 7579.03
     rank 14: 7579.02
     rank 15: 7579.25
     rank 16: 7578.98
     rank 17: 7578.98
     rank 18: 7578.83
     rank 19: 7579.64
  forward-compute:
     rank  0: 1028.77
     rank  1: 1027.24
     rank  2: 1029.39
     rank  3: 1030.16
     rank  4: 3003.18
     rank  5: 2997.69
     rank  6: 2998.42
     rank  7: 3006.48
     rank  8: 2786.94
     rank  9: 2789.53
     rank 10: 2786.38
     rank 11: 2790.20
     rank 12: 2760.78
     rank 13: 2762.77
     rank 14: 2761.18
     rank 15: 2769.65
     rank 16: 2901.78
     rank 17: 2901.33
     rank 18: 2904.51
     rank 19: 2908.41
  backward-compute:
     rank  0: 1069.47
     rank  1: 1070.94
     rank  2: 1066.29
     rank  3: 1071.00
     rank  4: 3020.15
     rank  5: 3023.91
     rank  6: 3021.49
     rank  7: 3022.56
     rank  8: 2983.89
     rank  9: 2984.85
     rank 10: 2983.91
     rank 11: 2992.24
     rank 12: 2994.06
     rank 13: 2996.80
     rank 14: 2996.89
     rank 15: 2997.34
     rank 16: 3192.57
     rank 17: 3192.46
     rank 18: 3195.91
     rank 19: 3194.74
  pure-backward-compute:
     rank  0: 1068.79
     rank  1: 1070.12
     rank  2: 1065.65
     rank  3: 1070.24
     rank  4: 3018.66
     rank  5: 3022.75
     rank  6: 3020.09
     rank  7: 3021.80
     rank  8: 2982.55
     rank  9: 2983.81
     rank 10: 2982.87
     rank 11: 2991.31
     rank 12: 2992.88
     rank 13: 2995.32
     rank 14: 2995.66
     rank 15: 2996.18
     rank 16: 3189.95
     rank 17: 3189.90
     rank 18: 3194.61
     rank 19: 3193.34
  batch-generator:
     rank  0: 56.15
     rank  1: 56.92
     rank  2: 60.32
     rank  3: 61.94
     rank  4: 69.88
     rank  5: 76.31
     rank  6: 82.66
     rank  7: 81.27
     rank  8: 55.20
     rank  9: 61.76
     rank 10: 58.73
     rank 11: 60.22
     rank 12: 54.22
     rank 13: 57.22
     rank 14: 57.26
     rank 15: 66.40
     rank 16: 53.60
     rank 17: 55.87
     rank 18: 57.96
     rank 19: 62.48
  forward-recv:
     rank  4: 81.71
     rank  5: 82.90
     rank  6: 82.72
     rank  7: 82.04
     rank  8: 297.05
     rank  9: 299.06
     rank 10: 297.96
     rank 11: 297.97
     rank 12: 471.24
     rank 13: 470.33
     rank 14: 471.09
     rank 15: 470.97
     rank 16: 646.30
     rank 17: 646.46
     rank 18: 646.41
     rank 19: 646.45
  forward-send:
     rank  0: 367.76
     rank  1: 370.20
     rank  2: 370.06
     rank  3: 368.29
     rank  4: 30.60
     rank  5: 31.88
     rank  6: 31.71
     rank  7: 31.05
     rank  8: 21.19
     rank  9: 20.26
     rank 10: 21.11
     rank 11: 20.75
     rank 12: 10.42
     rank 13: 10.45
     rank 14: 10.48
     rank 15: 10.56
  backward-recv:
     rank  0: 1297.39
     rank  1: 1295.13
     rank  2: 1297.64
     rank  3: 1296.72
     rank  4: 602.47
     rank  5: 604.81
     rank  6: 603.04
     rank  7: 604.38
     rank  8: 390.44
     rank  9: 390.86
     rank 10: 390.75
     rank 11: 389.75
     rank 12: 193.09
     rank 13: 193.29
     rank 14: 192.85
     rank 15: 193.64
  backward-send:
     rank  4: 5.22
     rank  5: 3.08
     rank  6: 4.66
     rank  7: 3.80
     rank  8: 31.36
     rank  9: 30.96
     rank 10: 31.02
     rank 11: 31.33
     rank 12: 20.58
     rank 13: 20.77
     rank 14: 20.68
     rank 15: 20.11
     rank 16: 10.60
     rank 17: 10.55
     rank 18: 10.17
     rank 19: 9.90
  forward-send-backward-recv:
     rank  0: 3802.36
     rank  1: 3802.03
     rank  2: 3804.41
     rank  3: 3801.32
     rank  4: 736.28
     rank  5: 736.53
     rank  6: 736.59
     rank  7: 736.06
     rank  8: 651.73
     rank  9: 653.47
     rank 10: 651.79
     rank 11: 644.63
     rank 12: 509.76
     rank 13: 509.79
     rank 14: 508.00
     rank 15: 508.03
  backward-send-forward-recv:
     rank  4: 19.39
     rank  5: 19.35
     rank  6: 20.25
     rank  7: 16.77
     rank  8: 149.88
     rank  9: 145.42
     rank 10: 149.82
     rank 11: 147.31
     rank 12: 157.23
     rank 13: 154.69
     rank 14: 156.79
     rank 15: 149.55
     rank 16: 168.43
     rank 17: 168.75
     rank 18: 166.25
     rank 19: 163.20
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.06
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.05
     rank  6: 0.07
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.04
     rank 19: 0.14
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.56
     rank  2: 0.62
     rank  3: 0.63
     rank  4: 0.42
     rank  5: 0.32
     rank  6: 0.46
     rank  7: 0.28
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.06
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.07
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.04
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.03
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.03
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.51
     rank  2: 8.40
     rank  3: 8.61
     rank  4: 2.17
     rank  5: 2.05
     rank  6: 2.18
     rank  7: 2.00
     rank  8: 0.07
     rank  9: 0.08
     rank 10: 0.03
     rank 11: 0.09
     rank 12: 0.04
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.03
     rank 19: 0.10
  optimizer:
     rank  0: 9.51
     rank  1: 9.51
     rank  2: 9.40
     rank  3: 9.59
     rank  4: 3.17
     rank  5: 3.05
     rank  6: 3.19
     rank  7: 3.00
     rank  8: 1.07
     rank  9: 1.07
     rank 10: 1.03
     rank 11: 1.08
     rank 12: 1.03
     rank 13: 1.06
     rank 14: 1.02
     rank 15: 1.06
     rank 16: 1.03
     rank 17: 1.07
     rank 18: 1.03
     rank 19: 1.14
 [2024-12-03 23:42:27] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7599.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.965661E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7560.39
     rank  1: 7560.36
     rank  2: 7560.25
     rank  3: 7560.39
     rank  4: 7559.89
     rank  5: 7559.86
     rank  6: 7559.77
     rank  7: 7559.82
     rank  8: 7559.69
     rank  9: 7559.69
     rank 10: 7559.58
     rank 11: 7559.63
     rank 12: 7559.78
     rank 13: 7559.83
     rank 14: 7559.62
     rank 15: 7559.74
     rank 16: 7559.72
     rank 17: 7559.71
     rank 18: 7559.58
     rank 19: 7559.60
  forward-compute:
     rank  0: 1114.94
     rank  1: 1117.02
     rank  2: 1116.26
     rank  3: 1116.86
     rank  4: 3004.56
     rank  5: 3009.46
     rank  6: 3006.68
     rank  7: 3012.88
     rank  8: 2793.03
     rank  9: 2795.25
     rank 10: 2792.92
     rank 11: 2794.51
     rank 12: 2787.12
     rank 13: 2793.02
     rank 14: 2789.28
     rank 15: 2794.75
     rank 16: 2906.06
     rank 17: 2906.70
     rank 18: 2911.08
     rank 19: 2911.59
  backward-compute:
     rank  0: 1070.48
     rank  1: 1074.11
     rank  2: 1070.83
     rank  3: 1073.12
     rank  4: 3029.67
     rank  5: 3036.53
     rank  6: 3033.39
     rank  7: 3032.85
     rank  8: 2996.92
     rank  9: 2998.72
     rank 10: 2996.54
     rank 11: 3003.06
     rank 12: 2996.36
     rank 13: 2999.34
     rank 14: 2999.53
     rank 15: 2998.37
     rank 16: 3184.12
     rank 17: 3184.25
     rank 18: 3187.48
     rank 19: 3186.00
  pure-backward-compute:
     rank  0: 1069.66
     rank  1: 1073.22
     rank  2: 1070.18
     rank  3: 1072.49
     rank  4: 3027.44
     rank  5: 3035.57
     rank  6: 3032.29
     rank  7: 3032.07
     rank  8: 2995.62
     rank  9: 2997.83
     rank 10: 2995.39
     rank 11: 3001.90
     rank 12: 2994.92
     rank 13: 2998.14
     rank 14: 2998.66
     rank 15: 2997.39
     rank 16: 3181.55
     rank 17: 3181.81
     rank 18: 3186.15
     rank 19: 3184.03
  batch-generator:
     rank  0: 61.43
     rank  1: 66.95
     rank  2: 63.25
     rank  3: 64.55
     rank  4: 86.23
     rank  5: 94.43
     rank  6: 92.48
     rank  7: 91.11
     rank  8: 52.78
     rank  9: 58.33
     rank 10: 56.63
     rank 11: 56.72
     rank 12: 64.42
     rank 13: 66.36
     rank 14: 64.13
     rank 15: 70.73
     rank 16: 53.52
     rank 17: 57.98
     rank 18: 60.10
     rank 19: 62.46
  forward-recv:
     rank  4: 63.06
     rank  5: 62.77
     rank  6: 63.02
     rank  7: 63.12
     rank  8: 283.09
     rank  9: 284.47
     rank 10: 283.52
     rank 11: 283.47
     rank 12: 453.16
     rank 13: 451.56
     rank 14: 453.15
     rank 15: 454.27
     rank 16: 624.56
     rank 17: 624.59
     rank 18: 624.51
     rank 19: 623.62
  forward-send:
     rank  0: 414.13
     rank  1: 413.32
     rank  2: 415.73
     rank  3: 414.71
     rank  4: 31.14
     rank  5: 30.72
     rank  6: 31.83
     rank  7: 31.44
     rank  8: 20.80
     rank  9: 19.22
     rank 10: 20.78
     rank 11: 20.67
     rank 12: 10.40
     rank 13: 10.31
     rank 14: 10.39
     rank 15: 9.44
  backward-recv:
     rank  0: 1291.70
     rank  1: 1290.76
     rank  2: 1291.53
     rank  3: 1291.67
     rank  4: 594.25
     rank  5: 595.92
     rank  6: 595.92
     rank  7: 595.53
     rank  8: 384.76
     rank  9: 385.14
     rank 10: 385.08
     rank 11: 383.51
     rank 12: 191.45
     rank 13: 191.71
     rank 14: 191.52
     rank 15: 191.90
  backward-send:
     rank  4: 5.21
     rank  5: 4.05
     rank  6: 3.65
     rank  7: 3.81
     rank  8: 31.23
     rank  9: 30.83
     rank 10: 30.93
     rank 11: 31.16
     rank 12: 20.97
     rank 13: 20.76
     rank 14: 20.78
     rank 15: 19.77
     rank 16: 10.52
     rank 17: 10.55
     rank 18: 10.42
     rank 19: 9.93
  forward-send-backward-recv:
     rank  0: 3655.76
     rank  1: 3651.12
     rank  2: 3654.22
     rank  3: 3652.31
     rank  4: 727.70
     rank  5: 722.87
     rank  6: 724.41
     rank  7: 722.81
     rank  8: 606.27
     rank  9: 607.00
     rank 10: 606.87
     rank 11: 601.75
     rank 12: 474.32
     rank 13: 475.18
     rank 14: 472.37
     rank 15: 474.27
  backward-send-forward-recv:
     rank  4: 22.41
     rank  5: 18.62
     rank  6: 20.24
     rank  7: 17.48
     rank  8: 174.15
     rank  9: 171.63
     rank 10: 173.59
     rank 11: 173.53
     rank 12: 158.75
     rank 13: 154.22
     rank 14: 157.49
     rank 15: 152.34
     rank 16: 170.00
     rank 17: 169.51
     rank 18: 165.63
     rank 19: 166.36
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.06
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.61
     rank  2: 0.61
     rank  3: 0.61
     rank  4: 0.47
     rank  5: 0.29
     rank  6: 0.33
     rank  7: 0.34
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.21
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.06
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.54
     rank  1: 8.56
     rank  2: 8.62
     rank  3: 8.41
     rank  4: 2.12
     rank  5: 2.00
     rank  6: 2.09
     rank  7: 2.07
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.08
  optimizer:
     rank  0: 9.27
     rank  1: 9.27
     rank  2: 9.35
     rank  3: 9.14
     rank  4: 2.86
     rank  5: 2.73
     rank  6: 2.82
     rank  7: 2.80
     rank  8: 0.78
     rank  9: 0.76
     rank 10: 0.76
     rank 11: 0.76
     rank 12: 0.77
     rank 13: 0.78
     rank 14: 0.76
     rank 15: 0.79
     rank 16: 0.78
     rank 17: 0.75
     rank 18: 0.76
     rank 19: 0.80
 [2024-12-03 23:42:34] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7609.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.779282E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7570.60
     rank  1: 7570.59
     rank  2: 7570.49
     rank  3: 7570.55
     rank  4: 7570.07
     rank  5: 7570.01
     rank  6: 7569.96
     rank  7: 7570.00
     rank  8: 7569.88
     rank  9: 7569.90
     rank 10: 7569.74
     rank 11: 7570.02
     rank 12: 7569.87
     rank 13: 7569.93
     rank 14: 7569.80
     rank 15: 7569.90
     rank 16: 7569.88
     rank 17: 7569.87
     rank 18: 7569.77
     rank 19: 7569.82
  forward-compute:
     rank  0: 926.60
     rank  1: 929.45
     rank  2: 929.32
     rank  3: 928.24
     rank  4: 3004.42
     rank  5: 3009.45
     rank  6: 3011.12
     rank  7: 3011.46
     rank  8: 2787.71
     rank  9: 2788.00
     rank 10: 2788.14
     rank 11: 2789.09
     rank 12: 2778.25
     rank 13: 2782.49
     rank 14: 2778.61
     rank 15: 2788.51
     rank 16: 2913.70
     rank 17: 2914.34
     rank 18: 2917.14
     rank 19: 2918.99
  backward-compute:
     rank  0: 1074.92
     rank  1: 1079.55
     rank  2: 1079.44
     rank  3: 1079.81
     rank  4: 3032.76
     rank  5: 3033.35
     rank  6: 3031.71
     rank  7: 3031.64
     rank  8: 2986.28
     rank  9: 2986.35
     rank 10: 2984.82
     rank 11: 2994.71
     rank 12: 3001.29
     rank 13: 3004.44
     rank 14: 3004.57
     rank 15: 3002.73
     rank 16: 3187.83
     rank 17: 3186.10
     rank 18: 3190.95
     rank 19: 3190.07
  pure-backward-compute:
     rank  0: 1074.00
     rank  1: 1078.65
     rank  2: 1078.82
     rank  3: 1079.14
     rank  4: 3030.73
     rank  5: 3032.42
     rank  6: 3030.71
     rank  7: 3030.66
     rank  8: 2985.09
     rank  9: 2985.47
     rank 10: 2983.82
     rank 11: 2993.64
     rank 12: 3000.11
     rank 13: 3003.07
     rank 14: 3003.70
     rank 15: 3001.74
     rank 16: 3185.11
     rank 17: 3183.77
     rank 18: 3189.52
     rank 19: 3188.18
  batch-generator:
     rank  0: 52.78
     rank  1: 61.32
     rank  2: 59.53
     rank  3: 59.93
     rank  4: 89.19
     rank  5: 96.24
     rank  6: 96.97
     rank  7: 96.18
     rank  8: 52.10
     rank  9: 55.37
     rank 10: 56.07
     rank 11: 54.62
     rank 12: 53.52
     rank 13: 57.84
     rank 14: 54.94
     rank 15: 66.82
     rank 16: 58.03
     rank 17: 59.44
     rank 18: 61.31
     rank 19: 63.66
  forward-recv:
     rank  4: 63.52
     rank  5: 63.10
     rank  6: 63.44
     rank  7: 63.04
     rank  8: 281.24
     rank  9: 282.73
     rank 10: 281.26
     rank 11: 281.72
     rank 12: 453.65
     rank 13: 452.44
     rank 14: 453.67
     rank 15: 454.61
     rank 16: 625.94
     rank 17: 625.88
     rank 18: 625.92
     rank 19: 625.11
  forward-send:
     rank  0: 420.43
     rank  1: 418.92
     rank  2: 420.38
     rank  3: 420.51
     rank  4: 31.27
     rank  5: 31.03
     rank  6: 31.38
     rank  7: 31.37
     rank  8: 20.90
     rank  9: 19.33
     rank 10: 20.90
     rank 11: 20.62
     rank 12: 10.52
     rank 13: 10.30
     rank 14: 10.46
     rank 15: 9.57
  backward-recv:
     rank  0: 1294.87
     rank  1: 1294.02
     rank  2: 1294.87
     rank  3: 1294.10
     rank  4: 603.50
     rank  5: 604.25
     rank  6: 605.01
     rank  7: 605.54
     rank  8: 393.48
     rank  9: 394.25
     rank 10: 393.43
     rank 11: 392.74
     rank 12: 195.11
     rank 13: 195.09
     rank 14: 194.84
     rank 15: 196.16
  backward-send:
     rank  4: 5.19
     rank  5: 3.95
     rank  6: 3.84
     rank  7: 3.60
     rank  8: 31.28
     rank  9: 30.52
     rank 10: 31.20
     rank 11: 31.30
     rank 12: 20.74
     rank 13: 20.76
     rank 14: 20.72
     rank 15: 20.04
     rank 16: 10.62
     rank 17: 10.44
     rank 18: 10.18
     rank 19: 10.35
  forward-send-backward-recv:
     rank  0: 3839.99
     rank  1: 3834.29
     rank  2: 3835.02
     rank  3: 3836.21
     rank  4: 728.91
     rank  5: 727.62
     rank  6: 726.45
     rank  7: 727.29
     rank  8: 648.30
     rank  9: 650.33
     rank 10: 649.82
     rank 11: 640.97
     rank 12: 489.54
     rank 13: 489.23
     rank 14: 487.39
     rank 15: 490.16
  backward-send-forward-recv:
     rank  4: 19.77
     rank  5: 18.71
     rank  6: 18.32
     rank  7: 18.81
     rank  8: 152.93
     rank  9: 152.14
     rank 10: 152.34
     rank 11: 152.51
     rank 12: 155.96
     rank 13: 152.56
     rank 14: 156.03
     rank 15: 146.63
     rank 16: 168.78
     rank 17: 169.50
     rank 18: 166.04
     rank 19: 164.75
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.57
     rank  2: 0.55
     rank  3: 0.55
     rank  4: 0.43
     rank  5: 0.33
     rank  6: 0.33
     rank  7: 0.28
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.19
     rank  2: 0.17
     rank  3: 0.15
     rank  4: 0.05
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.67
     rank  1: 8.47
     rank  2: 8.39
     rank  3: 8.48
     rank  4: 2.13
     rank  5: 2.06
     rank  6: 2.06
     rank  7: 2.01
     rank  8: 0.05
     rank  9: 0.07
     rank 10: 0.03
     rank 11: 0.10
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.08
     rank 18: 0.03
     rank 19: 0.08
  optimizer:
     rank  0: 9.46
     rank  1: 9.25
     rank  2: 9.18
     rank  3: 9.27
     rank  4: 2.93
     rank  5: 2.85
     rank  6: 2.85
     rank  7: 2.80
     rank  8: 0.83
     rank  9: 0.85
     rank 10: 0.82
     rank 11: 0.90
     rank 12: 0.82
     rank 13: 0.86
     rank 14: 0.82
     rank 15: 0.82
     rank 16: 0.82
     rank 17: 0.87
     rank 18: 0.82
     rank 19: 0.87
 [2024-12-03 23:42:42] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7609.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.516859E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7570.27
     rank  1: 7570.28
     rank  2: 7570.33
     rank  3: 7570.33
     rank  4: 7569.85
     rank  5: 7569.77
     rank  6: 7569.78
     rank  7: 7569.77
     rank  8: 7569.60
     rank  9: 7569.60
     rank 10: 7569.60
     rank 11: 7569.56
     rank 12: 7569.66
     rank 13: 7569.77
     rank 14: 7569.65
     rank 15: 7569.67
     rank 16: 7569.66
     rank 17: 7569.63
     rank 18: 7569.60
     rank 19: 7569.57
  forward-compute:
     rank  0: 936.53
     rank  1: 939.26
     rank  2: 938.73
     rank  3: 937.77
     rank  4: 3003.98
     rank  5: 3006.59
     rank  6: 3008.28
     rank  7: 3009.71
     rank  8: 2790.13
     rank  9: 2792.97
     rank 10: 2791.44
     rank 11: 2792.58
     rank 12: 2755.35
     rank 13: 2757.83
     rank 14: 2756.55
     rank 15: 2761.47
     rank 16: 2912.44
     rank 17: 2914.74
     rank 18: 2917.05
     rank 19: 2919.95
  backward-compute:
     rank  0: 1070.29
     rank  1: 1072.39
     rank  2: 1070.88
     rank  3: 1073.11
     rank  4: 3028.46
     rank  5: 3033.30
     rank  6: 3031.09
     rank  7: 3029.51
     rank  8: 2996.54
     rank  9: 2996.72
     rank 10: 2995.77
     rank 11: 3001.47
     rank 12: 2999.57
     rank 13: 3002.84
     rank 14: 3002.00
     rank 15: 3001.85
     rank 16: 3189.97
     rank 17: 3189.26
     rank 18: 3192.83
     rank 19: 3192.13
  pure-backward-compute:
     rank  0: 1069.57
     rank  1: 1071.41
     rank  2: 1070.20
     rank  3: 1072.44
     rank  4: 3026.67
     rank  5: 3032.42
     rank  6: 3029.98
     rank  7: 3028.71
     rank  8: 2995.42
     rank  9: 2995.84
     rank 10: 2994.74
     rank 11: 3000.66
     rank 12: 2998.46
     rank 13: 3001.81
     rank 14: 3001.16
     rank 15: 3000.75
     rank 16: 3187.39
     rank 17: 3187.05
     rank 18: 3191.49
     rank 19: 3190.66
  batch-generator:
     rank  0: 53.36
     rank  1: 62.62
     rank  2: 59.93
     rank  3: 64.07
     rank  4: 88.70
     rank  5: 92.17
     rank  6: 93.59
     rank  7: 92.89
     rank  8: 52.32
     rank  9: 58.28
     rank 10: 57.32
     rank 11: 56.60
     rank 12: 53.92
     rank 13: 57.33
     rank 14: 56.11
     rank 15: 62.10
     rank 16: 53.72
     rank 17: 58.82
     rank 18: 59.94
     rank 19: 63.47
  forward-recv:
     rank  4: 63.46
     rank  5: 63.17
     rank  6: 63.54
     rank  7: 63.30
     rank  8: 278.65
     rank  9: 280.60
     rank 10: 279.02
     rank 11: 279.07
     rank 12: 456.80
     rank 13: 455.71
     rank 14: 456.56
     rank 15: 457.56
     rank 16: 624.43
     rank 17: 624.05
     rank 18: 624.38
     rank 19: 624.08
  forward-send:
     rank  0: 422.98
     rank  1: 421.25
     rank  2: 422.66
     rank  3: 423.41
     rank  4: 31.29
     rank  5: 30.89
     rank  6: 31.22
     rank  7: 31.50
     rank  8: 20.76
     rank  9: 18.94
     rank 10: 20.64
     rank 11: 20.58
     rank 12: 10.69
     rank 13: 10.06
     rank 14: 10.63
     rank 15: 10.09
  backward-recv:
     rank  0: 1298.28
     rank  1: 1297.04
     rank  2: 1297.89
     rank  3: 1296.86
     rank  4: 599.64
     rank  5: 601.29
     rank  6: 601.35
     rank  7: 602.19
     rank  8: 388.34
     rank  9: 388.88
     rank 10: 388.76
     rank 11: 387.25
     rank 12: 191.78
     rank 13: 191.90
     rank 14: 191.65
     rank 15: 192.46
  backward-send:
     rank  4: 5.33
     rank  5: 3.69
     rank  6: 4.01
     rank  7: 3.48
     rank  8: 31.23
     rank  9: 30.88
     rank 10: 30.98
     rank 11: 31.39
     rank 12: 20.86
     rank 13: 20.75
     rank 14: 20.86
     rank 15: 19.81
     rank 16: 10.58
     rank 17: 10.47
     rank 18: 10.33
     rank 19: 10.17
  forward-send-backward-recv:
     rank  0: 3828.51
     rank  1: 3826.47
     rank  2: 3828.16
     rank  3: 3827.13
     rank  4: 735.16
     rank  5: 733.46
     rank  6: 733.62
     rank  7: 733.28
     rank  8: 632.75
     rank  9: 635.80
     rank 10: 634.14
     rank 11: 630.23
     rank 12: 514.16
     rank 13: 516.42
     rank 14: 513.07
     rank 15: 513.85
  backward-send-forward-recv:
     rank  4: 20.42
     rank  5: 19.10
     rank  6: 18.12
     rank  7: 17.39
     rank  8: 163.60
     rank  9: 159.97
     rank 10: 161.74
     rank 11: 162.07
     rank 12: 156.46
     rank 13: 153.86
     rank 14: 156.02
     rank 15: 151.70
     rank 16: 170.77
     rank 17: 169.72
     rank 18: 166.98
     rank 19: 164.99
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.55
     rank  2: 0.61
     rank  3: 0.60
     rank  4: 0.42
     rank  5: 0.30
     rank  6: 0.35
     rank  7: 0.29
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.06
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.28
     rank  1: 8.44
     rank  2: 8.96
     rank  3: 8.46
     rank  4: 2.12
     rank  5: 2.04
     rank  6: 2.08
     rank  7: 2.01
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.03
     rank 18: 0.07
     rank 19: 0.05
  optimizer:
     rank  0: 8.97
     rank  1: 9.13
     rank  2: 9.65
     rank  3: 9.14
     rank  4: 2.82
     rank  5: 2.72
     rank  6: 2.77
     rank  7: 2.70
     rank  8: 0.74
     rank  9: 0.71
     rank 10: 0.71
     rank 11: 0.72
     rank 12: 0.75
     rank 13: 0.75
     rank 14: 0.72
     rank 15: 0.75
     rank 16: 0.74
     rank 17: 0.71
     rank 18: 0.76
     rank 19: 0.73
 [2024-12-03 23:42:50] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7604.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.340377E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7564.11
     rank  1: 7564.15
     rank  2: 7564.21
     rank  3: 7564.18
     rank  4: 7563.68
     rank  5: 7563.62
     rank  6: 7563.70
     rank  7: 7563.62
     rank  8: 7563.53
     rank  9: 7563.46
     rank 10: 7563.51
     rank 11: 7563.52
     rank 12: 7563.56
     rank 13: 7563.52
     rank 14: 7563.56
     rank 15: 7563.52
     rank 16: 7563.53
     rank 17: 7563.52
     rank 18: 7563.61
     rank 19: 7563.40
  forward-compute:
     rank  0: 955.05
     rank  1: 956.67
     rank  2: 956.96
     rank  3: 955.43
     rank  4: 2996.46
     rank  5: 3000.91
     rank  6: 3001.57
     rank  7: 2999.61
     rank  8: 2790.12
     rank  9: 2793.20
     rank 10: 2790.39
     rank 11: 2792.17
     rank 12: 2796.25
     rank 13: 2800.56
     rank 14: 2798.68
     rank 15: 2806.21
     rank 16: 2904.01
     rank 17: 2904.19
     rank 18: 2907.35
     rank 19: 2910.57
  backward-compute:
     rank  0: 1062.92
     rank  1: 1067.49
     rank  2: 1065.94
     rank  3: 1064.77
     rank  4: 3032.49
     rank  5: 3037.13
     rank  6: 3032.85
     rank  7: 3033.21
     rank  8: 3000.35
     rank  9: 3000.97
     rank 10: 2999.28
     rank 11: 3006.82
     rank 12: 3003.06
     rank 13: 3005.75
     rank 14: 3006.46
     rank 15: 3006.45
     rank 16: 3193.58
     rank 17: 3193.29
     rank 18: 3196.35
     rank 19: 3195.23
  pure-backward-compute:
     rank  0: 1062.27
     rank  1: 1066.70
     rank  2: 1065.29
     rank  3: 1064.04
     rank  4: 3030.64
     rank  5: 3036.42
     rank  6: 3032.02
     rank  7: 3032.33
     rank  8: 2999.23
     rank  9: 3000.20
     rank 10: 2998.29
     rank 11: 3005.85
     rank 12: 3001.69
     rank 13: 3004.86
     rank 14: 3005.44
     rank 15: 3005.44
     rank 16: 3191.07
     rank 17: 3191.08
     rank 18: 3195.01
     rank 19: 3193.74
  batch-generator:
     rank  0: 56.53
     rank  1: 61.69
     rank  2: 59.21
     rank  3: 60.31
     rank  4: 78.60
     rank  5: 83.77
     rank  6: 83.69
     rank  7: 83.25
     rank  8: 50.66
     rank  9: 56.64
     rank 10: 54.45
     rank 11: 54.02
     rank 12: 66.04
     rank 13: 66.03
     rank 14: 65.81
     rank 15: 73.92
     rank 16: 51.05
     rank 17: 53.84
     rank 18: 56.13
     rank 19: 59.88
  forward-recv:
     rank  4: 63.05
     rank  5: 62.41
     rank  6: 62.99
     rank  7: 63.20
     rank  8: 283.79
     rank  9: 283.66
     rank 10: 283.73
     rank 11: 284.22
     rank 12: 455.92
     rank 13: 456.12
     rank 14: 455.82
     rank 15: 455.90
     rank 16: 624.87
     rank 17: 624.54
     rank 18: 624.87
     rank 19: 624.74
  forward-send:
     rank  0: 421.05
     rank  1: 418.94
     rank  2: 420.01
     rank  3: 421.14
     rank  4: 31.72
     rank  5: 30.73
     rank  6: 31.26
     rank  7: 31.59
     rank  8: 21.11
     rank  9: 20.36
     rank 10: 20.84
     rank 11: 20.67
     rank 12: 10.63
     rank 13: 10.13
     rank 14: 10.57
     rank 15: 10.30
  backward-recv:
     rank  0: 1295.96
     rank  1: 1294.12
     rank  2: 1295.42
     rank  3: 1294.73
     rank  4: 595.44
     rank  5: 597.12
     rank  6: 597.12
     rank  7: 597.69
     rank  8: 384.72
     rank  9: 386.89
     rank 10: 386.19
     rank 11: 385.30
     rank 12: 192.06
     rank 13: 191.27
     rank 14: 191.06
     rank 15: 191.39
  backward-send:
     rank  4: 5.37
     rank  5: 3.27
     rank  6: 4.08
     rank  7: 3.78
     rank  8: 31.39
     rank  9: 30.23
     rank 10: 30.99
     rank 11: 31.00
     rank 12: 20.32
     rank 13: 20.96
     rank 14: 20.87
     rank 15: 20.33
     rank 16: 10.63
     rank 17: 10.52
     rank 18: 10.24
     rank 19: 10.00
  forward-send-backward-recv:
     rank  0: 3815.30
     rank  1: 3812.77
     rank  2: 3814.21
     rank  3: 3816.05
     rank  4: 737.70
     rank  5: 736.37
     rank  6: 738.65
     rank  7: 737.24
     rank  8: 635.37
     rank  9: 636.75
     rank 10: 635.79
     rank 11: 630.22
     rank 12: 462.69
     rank 13: 463.68
     rank 14: 460.55
     rank 15: 461.48
  backward-send-forward-recv:
     rank  4: 20.73
     rank  5: 18.42
     rank  6: 17.86
     rank  7: 19.76
     rank  8: 149.30
     rank  9: 146.15
     rank 10: 148.94
     rank 11: 148.40
     rank 12: 157.27
     rank 13: 153.63
     rank 14: 156.15
     rank 15: 149.62
     rank 16: 168.25
     rank 17: 168.52
     rank 18: 165.72
     rank 19: 163.35
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.09
     rank  5: 0.05
     rank  6: 0.07
     rank  7: 0.05
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.08
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.59
     rank  2: 0.59
     rank  3: 0.61
     rank  4: 0.38
     rank  5: 0.29
     rank  6: 0.43
     rank  7: 0.29
     rank  8: 0.05
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.15
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.07
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.46
     rank  1: 8.62
     rank  2: 8.48
     rank  3: 8.46
     rank  4: 2.11
     rank  5: 2.01
     rank  6: 2.14
     rank  7: 2.01
     rank  8: 0.05
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.07
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.04
     rank 19: 0.08
  optimizer:
     rank  0: 9.58
     rank  1: 9.74
     rank  2: 9.59
     rank  3: 9.57
     rank  4: 3.24
     rank  5: 3.13
     rank  6: 3.26
     rank  7: 3.13
     rank  8: 1.17
     rank  9: 1.15
     rank 10: 1.16
     rank 11: 1.15
     rank 12: 1.16
     rank 13: 1.16
     rank 14: 1.18
     rank 15: 1.16
     rank 16: 1.15
     rank 17: 1.15
     rank 18: 1.18
     rank 19: 1.19
 [2024-12-03 23:42:57] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7601.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.720394E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7562.00
     rank  1: 7561.99
     rank  2: 7562.03
     rank  3: 7562.04
     rank  4: 7561.53
     rank  5: 7561.49
     rank  6: 7561.50
     rank  7: 7561.50
     rank  8: 7561.28
     rank  9: 7561.35
     rank 10: 7561.28
     rank 11: 7561.34
     rank 12: 7561.33
     rank 13: 7561.47
     rank 14: 7561.33
     rank 15: 7561.40
     rank 16: 7561.34
     rank 17: 7561.34
     rank 18: 7561.28
     rank 19: 7561.24
  forward-compute:
     rank  0: 949.42
     rank  1: 950.56
     rank  2: 952.28
     rank  3: 949.71
     rank  4: 2995.44
     rank  5: 2999.02
     rank  6: 2999.16
     rank  7: 3000.29
     rank  8: 2793.94
     rank  9: 2796.49
     rank 10: 2794.30
     rank 11: 2796.28
     rank 12: 2760.21
     rank 13: 2763.95
     rank 14: 2761.86
     rank 15: 2768.95
     rank 16: 2907.09
     rank 17: 2906.84
     rank 18: 2908.43
     rank 19: 2916.69
  backward-compute:
     rank  0: 1056.40
     rank  1: 1060.80
     rank  2: 1058.62
     rank  3: 1060.27
     rank  4: 3039.30
     rank  5: 3041.50
     rank  6: 3039.54
     rank  7: 3040.04
     rank  8: 2998.09
     rank  9: 2998.91
     rank 10: 2997.06
     rank 11: 3004.42
     rank 12: 3001.88
     rank 13: 3004.74
     rank 14: 3003.68
     rank 15: 3007.70
     rank 16: 3182.27
     rank 17: 3181.63
     rank 18: 3185.37
     rank 19: 3184.22
  pure-backward-compute:
     rank  0: 1055.63
     rank  1: 1059.80
     rank  2: 1058.00
     rank  3: 1059.66
     rank  4: 3037.52
     rank  5: 3040.64
     rank  6: 3038.62
     rank  7: 3039.21
     rank  8: 2997.03
     rank  9: 2998.04
     rank 10: 2996.05
     rank 11: 3003.59
     rank 12: 3000.85
     rank 13: 3003.83
     rank 14: 3002.75
     rank 15: 3006.73
     rank 16: 3179.72
     rank 17: 3179.02
     rank 18: 3183.25
     rank 19: 3182.73
  batch-generator:
     rank  0: 56.22
     rank  1: 60.04
     rank  2: 60.12
     rank  3: 58.36
     rank  4: 86.38
     rank  5: 91.23
     rank  6: 92.63
     rank  7: 89.60
     rank  8: 51.66
     rank  9: 57.56
     rank 10: 55.71
     rank 11: 55.29
     rank 12: 52.12
     rank 13: 55.67
     rank 14: 54.13
     rank 15: 62.35
     rank 16: 51.39
     rank 17: 55.12
     rank 18: 55.41
     rank 19: 63.48
  forward-recv:
     rank  4: 62.60
     rank  5: 62.22
     rank  6: 62.14
     rank  7: 62.45
     rank  8: 287.15
     rank  9: 289.20
     rank 10: 287.20
     rank 11: 287.69
     rank 12: 458.89
     rank 13: 456.91
     rank 14: 458.62
     rank 15: 459.85
     rank 16: 628.12
     rank 17: 628.20
     rank 18: 628.08
     rank 19: 627.30
  forward-send:
     rank  0: 421.58
     rank  1: 419.93
     rank  2: 420.13
     rank  3: 421.94
     rank  4: 31.75
     rank  5: 30.91
     rank  6: 31.09
     rank  7: 31.90
     rank  8: 20.82
     rank  9: 18.81
     rank 10: 20.70
     rank 11: 20.49
     rank 12: 10.40
     rank 13: 10.33
     rank 14: 10.37
     rank 15: 9.48
  backward-recv:
     rank  0: 1297.16
     rank  1: 1296.13
     rank  2: 1296.61
     rank  3: 1296.50
     rank  4: 599.91
     rank  5: 602.13
     rank  6: 602.53
     rank  7: 601.30
     rank  8: 387.83
     rank  9: 386.85
     rank 10: 388.16
     rank 11: 387.47
     rank 12: 191.50
     rank 13: 191.49
     rank 14: 191.31
     rank 15: 191.78
  backward-send:
     rank  4: 5.22
     rank  5: 3.98
     rank  6: 3.59
     rank  7: 4.00
     rank  8: 30.97
     rank  9: 30.83
     rank 10: 30.80
     rank 11: 30.53
     rank 12: 20.76
     rank 13: 20.81
     rank 14: 20.86
     rank 15: 19.93
     rank 16: 10.54
     rank 17: 10.48
     rank 18: 10.38
     rank 19: 9.94
  forward-send-backward-recv:
     rank  0: 3823.56
     rank  1: 3820.74
     rank  2: 3822.76
     rank  3: 3821.99
     rank  4: 726.67
     rank  5: 726.15
     rank  6: 728.61
     rank  7: 726.90
     rank  8: 624.25
     rank  9: 626.98
     rank 10: 624.95
     rank 11: 619.49
     rank 12: 492.52
     rank 13: 494.02
     rank 14: 491.15
     rank 15: 488.10
  backward-send-forward-recv:
     rank  4: 20.98
     rank  5: 19.00
     rank  6: 18.67
     rank  7: 17.55
     rank  8: 148.43
     rank  9: 145.18
     rank 10: 148.14
     rank 11: 147.23
     rank 12: 156.90
     rank 13: 154.02
     rank 14: 156.39
     rank 15: 149.89
     rank 16: 167.15
     rank 17: 167.58
     rank 18: 164.81
     rank 19: 159.84
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.55
     rank  2: 0.57
     rank  3: 0.55
     rank  4: 0.49
     rank  5: 0.32
     rank  6: 0.29
     rank  7: 0.46
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.15
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.32
     rank  1: 8.60
     rank  2: 8.35
     rank  3: 8.26
     rank  4: 2.12
     rank  5: 2.08
     rank  6: 1.99
     rank  7: 2.09
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.06
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 9.03
     rank  1: 9.32
     rank  2: 9.06
     rank  3: 8.96
     rank  4: 2.84
     rank  5: 2.79
     rank  6: 2.70
     rank  7: 2.80
     rank  8: 0.76
     rank  9: 0.74
     rank 10: 0.77
     rank 11: 0.78
     rank 12: 0.74
     rank 13: 0.77
     rank 14: 0.75
     rank 15: 0.74
     rank 16: 0.74
     rank 17: 0.74
     rank 18: 0.78
     rank 19: 0.74
 [2024-12-03 23:43:05] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7600.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.043735E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7559.89
     rank  1: 7559.94
     rank  2: 7559.61
     rank  3: 7559.81
     rank  4: 7559.06
     rank  5: 7559.12
     rank  6: 7559.07
     rank  7: 7559.03
     rank  8: 7558.89
     rank  9: 7558.96
     rank 10: 7558.86
     rank 11: 7558.88
     rank 12: 7558.92
     rank 13: 7559.00
     rank 14: 7558.89
     rank 15: 7558.87
     rank 16: 7558.93
     rank 17: 7558.94
     rank 18: 7558.88
     rank 19: 7558.87
  forward-compute:
     rank  0: 934.61
     rank  1: 937.92
     rank  2: 937.07
     rank  3: 935.17
     rank  4: 2974.32
     rank  5: 2975.97
     rank  6: 2974.17
     rank  7: 2975.10
     rank  8: 2792.76
     rank  9: 2795.29
     rank 10: 2790.56
     rank 11: 2793.80
     rank 12: 2764.23
     rank 13: 2766.61
     rank 14: 2765.53
     rank 15: 2773.38
     rank 16: 2904.79
     rank 17: 2904.84
     rank 18: 2906.56
     rank 19: 2912.39
  backward-compute:
     rank  0: 1057.79
     rank  1: 1057.76
     rank  2: 1057.11
     rank  3: 1057.64
     rank  4: 3038.94
     rank  5: 3043.06
     rank  6: 3039.76
     rank  7: 3038.17
     rank  8: 3004.55
     rank  9: 3003.60
     rank 10: 3002.86
     rank 11: 3011.88
     rank 12: 3002.89
     rank 13: 3005.78
     rank 14: 3004.60
     rank 15: 3007.27
     rank 16: 3183.66
     rank 17: 3183.81
     rank 18: 3188.07
     rank 19: 3186.17
  pure-backward-compute:
     rank  0: 1057.10
     rank  1: 1056.84
     rank  2: 1056.47
     rank  3: 1056.97
     rank  4: 3038.01
     rank  5: 3042.13
     rank  6: 3038.89
     rank  7: 3037.37
     rank  8: 3003.49
     rank  9: 3002.77
     rank 10: 3001.82
     rank 11: 3010.99
     rank 12: 3001.84
     rank 13: 3004.79
     rank 14: 3003.76
     rank 15: 3006.32
     rank 16: 3181.06
     rank 17: 3181.07
     rank 18: 3186.15
     rank 19: 3184.72
  batch-generator:
     rank  0: 52.52
     rank  1: 58.54
     rank  2: 56.43
     rank  3: 57.78
     rank  4: 69.70
     rank  5: 71.55
     rank  6: 71.96
     rank  7: 70.40
     rank  8: 51.75
     rank  9: 57.69
     rank 10: 53.96
     rank 11: 54.31
     rank 12: 51.90
     rank 13: 54.00
     rank 14: 53.97
     rank 15: 62.88
     rank 16: 59.50
     rank 17: 60.93
     rank 18: 61.41
     rank 19: 67.76
  forward-recv:
     rank  4: 63.00
     rank  5: 61.82
     rank  6: 63.03
     rank  7: 63.03
     rank  8: 280.59
     rank  9: 280.06
     rank 10: 282.76
     rank 11: 280.72
     rank 12: 452.98
     rank 13: 452.15
     rank 14: 452.64
     rank 15: 452.95
     rank 16: 627.88
     rank 17: 627.90
     rank 18: 627.80
     rank 19: 627.88
  forward-send:
     rank  0: 422.68
     rank  1: 419.69
     rank  2: 422.09
     rank  3: 422.79
     rank  4: 31.55
     rank  5: 30.21
     rank  6: 31.28
     rank  7: 31.56
     rank  8: 21.21
     rank  9: 20.65
     rank 10: 20.74
     rank 11: 21.09
     rank 12: 10.54
     rank 13: 10.55
     rank 14: 10.30
     rank 15: 10.49
  backward-recv:
     rank  0: 1302.18
     rank  1: 1301.95
     rank  2: 1302.34
     rank  3: 1302.51
     rank  4: 605.66
     rank  5: 603.61
     rank  6: 605.22
     rank  7: 605.61
     rank  8: 388.07
     rank  9: 389.74
     rank 10: 388.52
     rank 11: 387.83
     rank 12: 193.81
     rank 13: 193.37
     rank 14: 193.54
     rank 15: 193.17
  backward-send:
     rank  4: 4.05
     rank  5: 4.48
     rank  6: 4.11
     rank  7: 4.23
     rank  8: 31.17
     rank  9: 29.64
     rank 10: 31.01
     rank 11: 31.27
     rank 12: 20.63
     rank 13: 20.97
     rank 14: 20.77
     rank 15: 20.51
     rank 16: 10.60
     rank 17: 10.48
     rank 18: 10.35
     rank 19: 9.94
  forward-send-backward-recv:
     rank  0: 3828.77
     rank  1: 3828.12
     rank  2: 3829.16
     rank  3: 3829.74
     rank  4: 744.18
     rank  5: 743.23
     rank  6: 745.03
     rank  7: 745.08
     rank  8: 624.95
     rank  9: 627.67
     rank 10: 626.21
     rank 11: 619.48
     rank 12: 491.34
     rank 13: 492.37
     rank 14: 490.11
     rank 15: 488.55
  backward-send-forward-recv:
     rank  4: 20.83
     rank  5: 20.63
     rank  6: 20.19
     rank  7: 20.32
     rank  8: 149.47
     rank  9: 147.19
     rank 10: 149.68
     rank 11: 148.96
     rank 12: 157.24
     rank 13: 154.65
     rank 14: 157.01
     rank 15: 149.48
     rank 16: 167.60
     rank 17: 167.98
     rank 18: 165.24
     rank 19: 161.46
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.77
     rank  1: 0.79
     rank  2: 0.58
     rank  3: 0.58
     rank  4: 0.28
     rank  5: 0.36
     rank  6: 0.32
     rank  7: 0.28
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.36
     rank  1: 0.35
     rank  2: 0.16
     rank  3: 0.33
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.03
     rank  1: 0.04
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.01
     rank  3: 0.03
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 9.26
     rank  1: 9.87
     rank  2: 8.40
     rank  3: 9.88
     rank  4: 2.00
     rank  5: 2.01
     rank  6: 2.06
     rank  7: 2.01
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.08
  optimizer:
     rank  0: 10.58
     rank  1: 11.23
     rank  2: 10.00
     rank  3: 11.23
     rank  4: 3.36
     rank  5: 3.37
     rank  6: 3.42
     rank  7: 3.37
     rank  8: 1.39
     rank  9: 1.42
     rank 10: 1.40
     rank 11: 1.40
     rank 12: 1.39
     rank 13: 1.43
     rank 14: 1.39
     rank 15: 1.39
     rank 16: 1.40
     rank 17: 1.43
     rank 18: 1.43
     rank 19: 1.44
 [2024-12-03 23:43:12] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7585.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.793297E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7552.19
     rank  1: 7552.23
     rank  2: 7552.27
     rank  3: 7552.23
     rank  4: 7551.71
     rank  5: 7551.70
     rank  6: 7551.71
     rank  7: 7551.70
     rank  8: 7551.52
     rank  9: 7551.50
     rank 10: 7551.52
     rank 11: 7551.50
     rank 12: 7551.57
     rank 13: 7551.60
     rank 14: 7551.57
     rank 15: 7551.56
     rank 16: 7551.59
     rank 17: 7551.57
     rank 18: 7551.48
     rank 19: 7551.48
  forward-compute:
     rank  0: 982.63
     rank  1: 986.60
     rank  2: 984.93
     rank  3: 983.58
     rank  4: 2962.26
     rank  5: 2962.89
     rank  6: 2959.92
     rank  7: 2961.28
     rank  8: 2797.02
     rank  9: 2802.62
     rank 10: 2797.58
     rank 11: 2798.14
     rank 12: 2759.46
     rank 13: 2762.46
     rank 14: 2760.20
     rank 15: 2767.99
     rank 16: 2905.98
     rank 17: 2906.27
     rank 18: 2907.33
     rank 19: 2913.90
  backward-compute:
     rank  0: 1060.90
     rank  1: 1060.32
     rank  2: 1059.64
     rank  3: 1059.98
     rank  4: 3029.90
     rank  5: 3034.65
     rank  6: 3031.32
     rank  7: 3029.26
     rank  8: 3008.89
     rank  9: 3007.41
     rank 10: 3006.51
     rank 11: 3016.58
     rank 12: 2998.93
     rank 13: 3002.23
     rank 14: 3000.80
     rank 15: 3003.64
     rank 16: 3175.14
     rank 17: 3175.09
     rank 18: 3178.36
     rank 19: 3178.91
  pure-backward-compute:
     rank  0: 1060.22
     rank  1: 1059.42
     rank  2: 1058.98
     rank  3: 1059.23
     rank  4: 3029.06
     rank  5: 3033.70
     rank  6: 3030.35
     rank  7: 3028.47
     rank  8: 3007.81
     rank  9: 3006.60
     rank 10: 3005.52
     rank 11: 3015.81
     rank 12: 2997.88
     rank 13: 3001.21
     rank 14: 2999.93
     rank 15: 3002.71
     rank 16: 3172.94
     rank 17: 3172.60
     rank 18: 3176.45
     rank 19: 3177.49
  batch-generator:
     rank  0: 54.08
     rank  1: 63.60
     rank  2: 59.64
     rank  3: 61.51
     rank  4: 50.07
     rank  5: 61.47
     rank  6: 62.63
     rank  7: 60.17
     rank  8: 51.95
     rank  9: 61.37
     rank 10: 56.71
     rank 11: 54.44
     rank 12: 52.95
     rank 13: 55.52
     rank 14: 54.36
     rank 15: 63.26
     rank 16: 51.12
     rank 17: 54.31
     rank 18: 54.33
     rank 19: 61.55
  forward-recv:
     rank  4: 66.85
     rank  5: 66.03
     rank  6: 66.89
     rank  7: 66.68
     rank  8: 281.16
     rank  9: 280.71
     rank 10: 281.02
     rank 11: 281.23
     rank 12: 454.13
     rank 13: 452.82
     rank 14: 454.02
     rank 15: 455.19
     rank 16: 628.53
     rank 17: 628.44
     rank 18: 628.60
     rank 19: 627.74
  forward-send:
     rank  0: 416.61
     rank  1: 412.77
     rank  2: 416.14
     rank  3: 416.60
     rank  4: 31.75
     rank  5: 29.12
     rank  6: 31.68
     rank  7: 31.72
     rank  8: 20.85
     rank  9: 19.35
     rank 10: 21.05
     rank 11: 20.69
     rank 12: 10.47
     rank 13: 10.21
     rank 14: 10.37
     rank 15: 9.56
  backward-recv:
     rank  0: 1299.97
     rank  1: 1300.11
     rank  2: 1299.88
     rank  3: 1300.33
     rank  4: 601.16
     rank  5: 601.04
     rank  6: 602.70
     rank  7: 602.28
     rank  8: 384.51
     rank  9: 385.54
     rank 10: 385.39
     rank 11: 383.55
     rank 12: 193.04
     rank 13: 193.14
     rank 14: 192.89
     rank 15: 193.45
  backward-send:
     rank  4: 4.63
     rank  5: 4.42
     rank  6: 3.47
     rank  7: 4.34
     rank  8: 30.97
     rank  9: 30.46
     rank 10: 30.62
     rank 11: 31.37
     rank 12: 20.56
     rank 13: 20.76
     rank 14: 20.89
     rank 15: 19.90
     rank 16: 10.48
     rank 17: 10.50
     rank 18: 10.37
     rank 19: 9.88
  forward-send-backward-recv:
     rank  0: 3778.57
     rank  1: 3778.64
     rank  2: 3780.40
     rank  3: 3779.79
     rank  4: 758.67
     rank  5: 756.57
     rank  6: 758.97
     rank  7: 759.39
     rank  8: 615.19
     rank  9: 618.35
     rank 10: 616.60
     rank 11: 610.09
     rank 12: 492.72
     rank 13: 493.29
     rank 14: 491.61
     rank 15: 489.64
  backward-send-forward-recv:
     rank  4: 19.41
     rank  5: 20.31
     rank  6: 19.99
     rank  7: 20.22
     rank  8: 146.56
     rank  9: 142.77
     rank 10: 146.26
     rank 11: 146.19
     rank 12: 156.79
     rank 13: 154.17
     rank 14: 156.66
     rank 15: 149.42
     rank 16: 168.32
     rank 17: 168.56
     rank 18: 166.53
     rank 19: 161.26
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.53
     rank  1: 0.55
     rank  2: 0.57
     rank  3: 0.56
     rank  4: 0.39
     rank  5: 0.27
     rank  6: 0.34
     rank  7: 0.38
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.15
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.50
     rank  1: 8.42
     rank  2: 8.55
     rank  3: 8.34
     rank  4: 2.06
     rank  5: 1.98
     rank  6: 2.07
     rank  7: 2.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 9.12
     rank  1: 9.03
     rank  2: 9.17
     rank  3: 8.94
     rank  4: 2.67
     rank  5: 2.59
     rank  6: 2.68
     rank  7: 2.64
     rank  8: 0.64
     rank  9: 0.64
     rank 10: 0.64
     rank 11: 0.64
     rank 12: 0.64
     rank 13: 0.68
     rank 14: 0.64
     rank 15: 0.64
     rank 16: 0.68
     rank 17: 0.64
     rank 18: 0.68
     rank 19: 0.64
 [2024-12-03 23:43:20] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7584.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.410534E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7553.44
     rank  1: 7553.38
     rank  2: 7553.45
     rank  3: 7553.40
     rank  4: 7552.90
     rank  5: 7552.85
     rank  6: 7552.89
     rank  7: 7552.85
     rank  8: 7552.78
     rank  9: 7552.69
     rank 10: 7552.71
     rank 11: 7552.69
     rank 12: 7552.78
     rank 13: 7552.78
     rank 14: 7552.75
     rank 15: 7552.74
     rank 16: 7552.86
     rank 17: 7552.75
     rank 18: 7552.67
     rank 19: 7552.64
  forward-compute:
     rank  0: 937.27
     rank  1: 941.51
     rank  2: 941.21
     rank  3: 939.11
     rank  4: 2966.53
     rank  5: 2965.78
     rank  6: 2965.51
     rank  7: 2967.36
     rank  8: 2798.47
     rank  9: 2801.62
     rank 10: 2798.84
     rank 11: 2798.65
     rank 12: 2759.45
     rank 13: 2763.22
     rank 14: 2760.88
     rank 15: 2769.21
     rank 16: 2908.81
     rank 17: 2908.69
     rank 18: 2910.42
     rank 19: 2917.38
  backward-compute:
     rank  0: 1070.74
     rank  1: 1068.33
     rank  2: 1067.87
     rank  3: 1069.08
     rank  4: 3030.62
     rank  5: 3031.18
     rank  6: 3031.10
     rank  7: 3030.04
     rank  8: 3009.75
     rank  9: 3008.51
     rank 10: 3007.80
     rank 11: 3016.93
     rank 12: 2996.84
     rank 13: 2999.73
     rank 14: 2998.63
     rank 15: 3001.99
     rank 16: 3176.95
     rank 17: 3176.77
     rank 18: 3180.50
     rank 19: 3179.13
  pure-backward-compute:
     rank  0: 1069.89
     rank  1: 1067.45
     rank  2: 1067.23
     rank  3: 1068.41
     rank  4: 3029.81
     rank  5: 3030.20
     rank  6: 3030.22
     rank  7: 3029.26
     rank  8: 3008.73
     rank  9: 3007.71
     rank 10: 3006.80
     rank 11: 3016.16
     rank 12: 2995.77
     rank 13: 2998.74
     rank 14: 2997.79
     rank 15: 3001.05
     rank 16: 3174.69
     rank 17: 3174.24
     rank 18: 3178.63
     rank 19: 3177.79
  batch-generator:
     rank  0: 65.30
     rank  1: 67.42
     rank  2: 67.14
     rank  3: 67.30
     rank  4: 54.16
     rank  5: 65.10
     rank  6: 64.48
     rank  7: 62.96
     rank  8: 52.59
     rank  9: 60.08
     rank 10: 57.63
     rank 11: 54.75
     rank 12: 52.91
     rank 13: 56.49
     rank 14: 55.06
     rank 15: 64.33
     rank 16: 52.04
     rank 17: 54.52
     rank 18: 55.09
     rank 19: 62.54
  forward-recv:
     rank  4: 62.22
     rank  5: 61.49
     rank  6: 62.18
     rank  7: 62.17
     rank  8: 277.88
     rank  9: 278.89
     rank 10: 277.74
     rank 11: 277.98
     rank 12: 451.27
     rank 13: 449.44
     rank 14: 451.12
     rank 15: 451.38
     rank 16: 624.97
     rank 17: 625.01
     rank 18: 624.98
     rank 19: 624.87
  forward-send:
     rank  0: 418.54
     rank  1: 415.79
     rank  2: 417.43
     rank  3: 418.66
     rank  4: 31.62
     rank  5: 30.32
     rank  6: 31.45
     rank  7: 31.70
     rank  8: 20.75
     rank  9: 19.04
     rank 10: 20.75
     rank 11: 20.65
     rank 12: 10.52
     rank 13: 10.50
     rank 14: 10.49
     rank 15: 10.37
  backward-recv:
     rank  0: 1297.41
     rank  1: 1297.81
     rank  2: 1297.82
     rank  3: 1297.97
     rank  4: 602.01
     rank  5: 601.65
     rank  6: 602.59
     rank  7: 602.18
     rank  8: 385.64
     rank  9: 387.60
     rank 10: 386.47
     rank 11: 385.90
     rank 12: 191.70
     rank 13: 191.18
     rank 14: 191.35
     rank 15: 190.77
  backward-send:
     rank  4: 4.53
     rank  5: 4.13
     rank  6: 3.92
     rank  7: 4.60
     rank  8: 31.23
     rank  9: 30.07
     rank 10: 30.68
     rank 11: 31.26
     rank 12: 20.37
     rank 13: 21.01
     rank 14: 20.69
     rank 15: 20.84
     rank 16: 10.51
     rank 17: 10.46
     rank 18: 10.23
     rank 19: 9.63
  forward-send-backward-recv:
     rank  0: 3815.82
     rank  1: 3816.05
     rank  2: 3817.31
     rank  3: 3816.59
     rank  4: 758.81
     rank  5: 760.62
     rank  6: 759.92
     rank  7: 758.11
     rank  8: 609.52
     rank  9: 613.46
     rank 10: 611.42
     rank 11: 605.05
     rank 12: 498.88
     rank 13: 500.14
     rank 14: 497.66
     rank 15: 495.61
  backward-send-forward-recv:
     rank  4: 19.80
     rank  5: 20.96
     rank  6: 19.93
     rank  7: 19.78
     rank  8: 152.50
     rank  9: 148.94
     rank 10: 151.76
     rank 11: 152.16
     rank 12: 157.58
     rank 13: 154.53
     rank 14: 156.88
     rank 15: 149.30
     rank 16: 167.75
     rank 17: 168.13
     rank 18: 166.02
     rank 19: 160.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.53
     rank  1: 0.54
     rank  2: 0.60
     rank  3: 0.54
     rank  4: 0.31
     rank  5: 0.31
     rank  6: 0.28
     rank  7: 0.29
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.03
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.14
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.19
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.37
     rank  1: 8.46
     rank  2: 8.46
     rank  3: 8.49
     rank  4: 2.04
     rank  5: 2.05
     rank  6: 1.98
     rank  7: 2.01
     rank  8: 0.07
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.08
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.04
     rank 17: 0.03
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 9.03
     rank  1: 9.13
     rank  2: 9.13
     rank  3: 9.17
     rank  4: 2.71
     rank  5: 2.72
     rank  6: 2.66
     rank  7: 2.68
     rank  8: 0.74
     rank  9: 0.70
     rank 10: 0.71
     rank 11: 0.71
     rank 12: 0.71
     rank 13: 0.74
     rank 14: 0.70
     rank 15: 0.70
     rank 16: 0.72
     rank 17: 0.70
     rank 18: 0.74
     rank 19: 0.70
