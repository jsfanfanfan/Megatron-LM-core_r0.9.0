examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 16:39:41,569] torch.distributed.run: [WARNING] 
[2024-12-02 16:39:41,569] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 16:39:41,569] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 16:39:41,569] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3][rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.output_layer.weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 81.91
     rank  1: 114.28
     rank  2: 84.10
     rank  3: 81.76
     rank  4: 71.41
     rank  5: 58.10
     rank  6: 58.54
     rank  7: 64.90
     rank  8: 46.50
     rank  9: 31.21
     rank 10: 29.87
     rank 11: 56.87
     rank 12: 45.28
     rank 13: 30.29
     rank 14: 49.19
     rank 15: 49.17
     rank 16: 48.94
     rank 17: 54.06
     rank 18: 31.20
     rank 19: 48.74
  train/valid/test-data-iterators-setup:
     rank  0: 807.76
     rank  1: 807.64
     rank  2: 807.66
     rank  3: 807.73
     rank  4: 1223.06
     rank  5: 1222.75
     rank  6: 1222.82
     rank  7: 1222.77
     rank  8: 1222.84
     rank  9: 1223.13
     rank 10: 1222.88
     rank 11: 1223.17
     rank 12: 1222.88
     rank 13: 1222.88
     rank 14: 1222.92
     rank 15: 1222.86
     rank 16: 1222.96
     rank 17: 1222.98
     rank 18: 1223.05
     rank 19: 1223.02
 [2024-12-02 16:40:27] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22219.8 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.108734E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10086.0 | max reserved: 10086.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10126.0 | max reserved: 10126.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
times across ranks (ms):
  forward-backward:
     rank  0: 22152.32
     rank  1: 22152.06
     rank  2: 22151.64
     rank  3: 22152.11
     rank  4: 22155.38
     rank  5: 22155.29
     rank  6: 22155.27
     rank  7: 22155.15
     rank  8: 22154.90
     rank  9: 22154.89
     rank 10: 22154.90
     rank 11: 22155.37
     rank 12: 22154.92
     rank 13: 22155.01
     rank 14: 22154.98
     rank 15: 22155.07
     rank 16: 22154.77
     rank 17: 22154.72
     rank 18: 22154.69
     rank 19: 22154.81
  forward-compute:
     rank  0: 5015.49
     rank  1: 5024.99
     rank  2: 4962.53
     rank  3: 5031.98
     rank  4: 5563.81
     rank  5: 5576.75
     rank  6: 5577.48
     rank  7: 5589.95
     rank  8: 4984.89
     rank  9: 4995.85
     rank 10: 4989.01
     rank 11: 4994.10
     rank 12: 4974.19
     rank 13: 4983.41
     rank 14: 4977.45
     rank 15: 4976.99
     rank 16: 5377.88
     rank 17: 5387.95
     rank 18: 5376.73
     rank 19: 5380.27
  backward-compute:
     rank  0: 1996.93
     rank  1: 1996.63
     rank  2: 2061.21
     rank  3: 2007.31
     rank  4: 3330.16
     rank  5: 3330.00
     rank  6: 3333.18
     rank  7: 3337.33
     rank  8: 3310.12
     rank  9: 3314.37
     rank 10: 3313.24
     rank 11: 3312.95
     rank 12: 3313.13
     rank 13: 3314.88
     rank 14: 3312.98
     rank 15: 3316.44
     rank 16: 3492.82
     rank 17: 3491.36
     rank 18: 3495.94
     rank 19: 3494.22
  pure-backward-compute:
     rank  0: 1995.63
     rank  1: 1995.20
     rank  2: 2060.26
     rank  3: 2006.30
     rank  4: 3328.79
     rank  5: 3328.96
     rank  6: 3331.86
     rank  7: 3336.29
     rank  8: 3308.26
     rank  9: 3312.58
     rank 10: 3312.24
     rank 11: 3311.93
     rank 12: 3311.79
     rank 13: 3313.69
     rank 14: 3311.62
     rank 15: 3315.23
     rank 16: 3490.25
     rank 17: 3488.34
     rank 18: 3494.10
     rank 19: 3492.51
  batch-generator:
     rank  0: 1072.25
     rank  1: 1144.51
     rank  2: 1085.87
     rank  3: 1155.84
     rank  4: 1297.87
     rank  5: 1311.19
     rank  6: 1316.16
     rank  7: 1333.94
     rank  8: 1057.32
     rank  9: 1073.73
     rank 10: 1065.84
     rank 11: 1075.35
     rank 12: 1141.98
     rank 13: 1156.61
     rank 14: 1148.88
     rank 15: 1149.67
     rank 16: 1233.52
     rank 17: 1245.57
     rank 18: 1237.55
     rank 19: 1240.92
  forward-recv:
     rank  4: 3942.22
     rank  5: 3943.20
     rank  6: 3930.91
     rank  7: 3921.97
     rank  8: 6812.09
     rank  9: 6812.43
     rank 10: 6809.91
     rank 11: 6809.57
     rank 12: 9185.72
     rank 13: 9183.06
     rank 14: 9185.10
     rank 15: 9185.87
     rank 16: 11562.88
     rank 17: 11557.30
     rank 18: 11564.75
     rank 19: 11564.41
  forward-send:
     rank  0: 7559.40
     rank  1: 7553.02
     rank  2: 7548.51
     rank  3: 7538.22
     rank  4: 4431.63
     rank  5: 4423.88
     rank  6: 4430.64
     rank  7: 4430.63
     rank  8: 2237.47
     rank  9: 2229.62
     rank 10: 2239.22
     rank 11: 2239.44
     rank 12: 33.58
     rank 13: 28.08
     rank 14: 35.40
     rank 15: 35.28
  backward-recv:
     rank  0: 1373.04
     rank  1: 1374.17
     rank  2: 1373.80
     rank  3: 1371.88
     rank  4: 591.13
     rank  5: 592.38
     rank  6: 591.48
     rank  7: 593.93
     rank  8: 387.83
     rank  9: 386.61
     rank 10: 387.64
     rank 11: 387.67
     rank 12: 195.14
     rank 13: 195.52
     rank 14: 195.63
     rank 15: 194.90
  backward-send:
     rank  4: 22.65
     rank  5: 22.78
     rank  6: 22.48
     rank  7: 21.07
     rank  8: 30.79
     rank  9: 31.50
     rank 10: 30.73
     rank 11: 31.06
     rank 12: 20.87
     rank 13: 20.28
     rank 14: 20.33
     rank 15: 20.97
     rank 16: 10.53
     rank 17: 10.60
     rank 18: 10.23
     rank 19: 9.93
  forward-send-backward-recv:
     rank  0: 6143.28
     rank  1: 6141.09
     rank  2: 6143.87
     rank  3: 6139.24
     rank  4: 3329.47
     rank  5: 3329.02
     rank  6: 3327.95
     rank  7: 3324.12
     rank  8: 3140.30
     rank  9: 3136.33
     rank 10: 3138.93
     rank 11: 3138.42
     rank 12: 2931.34
     rank 13: 2929.71
     rank 14: 2931.25
     rank 15: 2929.27
  backward-send-forward-recv:
     rank  4: 811.66
     rank  5: 809.57
     rank  6: 810.79
     rank  7: 809.07
     rank  8: 906.24
     rank  9: 903.80
     rank 10: 904.65
     rank 11: 897.74
     rank 12: 941.50
     rank 13: 939.62
     rank 14: 938.10
     rank 15: 937.36
     rank 16: 934.18
     rank 17: 931.33
     rank 18: 933.55
     rank 19: 931.74
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.04
     rank  3: 0.06
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.05
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.20
     rank  1: 0.19
     rank  2: 0.09
     rank  3: 0.14
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.03
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.03
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.06
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.07
  all-grads-sync:
     rank  0: 46.25
     rank  1: 44.30
     rank  2: 41.36
     rank  3: 46.16
     rank  4: 43.36
     rank  5: 46.45
     rank  6: 44.53
     rank  7: 39.37
     rank  8: 43.30
     rank  9: 40.42
     rank 10: 46.78
     rank 11: 47.02
     rank 12: 38.18
     rank 13: 45.37
     rank 14: 46.12
     rank 15: 46.58
     rank 16: 2.89
     rank 17: 2.88
     rank 18: 2.61
     rank 19: 3.47
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.27
     rank  2: 0.28
     rank  3: 0.24
     rank  4: 0.08
     rank  5: 0.10
     rank  6: 0.08
     rank  7: 0.07
     rank  8: 0.07
     rank  9: 0.07
     rank 10: 0.10
     rank 11: 0.11
     rank 12: 0.15
     rank 13: 0.10
     rank 14: 0.12
     rank 15: 0.08
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.06
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 28.72
     rank  1: 27.90
     rank  2: 28.04
     rank  3: 28.36
     rank  4: 58.88
     rank  5: 58.70
     rank  6: 58.67
     rank  7: 58.73
     rank  8: 52.24
     rank  9: 52.25
     rank 10: 52.50
     rank 11: 52.61
     rank 12: 52.38
     rank 13: 52.79
     rank 14: 53.06
     rank 15: 53.06
     rank 16: 56.08
     rank 17: 56.07
     rank 18: 55.60
     rank 19: 56.45
  optimizer:
     rank  0: 30.50
     rank  1: 29.74
     rank  2: 29.89
     rank  3: 30.13
     rank  4: 60.75
     rank  5: 60.55
     rank  6: 60.53
     rank  7: 60.58
     rank  8: 54.10
     rank  9: 54.11
     rank 10: 54.36
     rank 11: 54.46
     rank 12: 54.23
     rank 13: 54.65
     rank 14: 54.92
     rank 15: 54.93
     rank 16: 57.94
     rank 17: 57.93
     rank 18: 57.47
     rank 19: 58.32
 [2024-12-02 16:40:35] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 8055.1 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.285778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7957.24
     rank  1: 7957.66
     rank  2: 7957.18
     rank  3: 7957.14
     rank  4: 7960.68
     rank  5: 7960.82
     rank  6: 7960.65
     rank  7: 7960.64
     rank  8: 7960.41
     rank  9: 7960.54
     rank 10: 7960.32
     rank 11: 7960.37
     rank 12: 7960.45
     rank 13: 7960.51
     rank 14: 7960.41
     rank 15: 7960.50
     rank 16: 7960.68
     rank 17: 7960.72
     rank 18: 7960.57
     rank 19: 7960.62
  forward-compute:
     rank  0: 1016.69
     rank  1: 1017.09
     rank  2: 1018.27
     rank  3: 1022.97
     rank  4: 2879.97
     rank  5: 2883.13
     rank  6: 2881.29
     rank  7: 2899.58
     rank  8: 2790.63
     rank  9: 2791.61
     rank 10: 2793.69
     rank 11: 2791.14
     rank 12: 2790.76
     rank 13: 2790.91
     rank 14: 2797.22
     rank 15: 2794.82
     rank 16: 2915.12
     rank 17: 2915.50
     rank 18: 2922.43
     rank 19: 2917.21
  backward-compute:
     rank  0: 1059.04
     rank  1: 1060.29
     rank  2: 1057.46
     rank  3: 1073.90
     rank  4: 3327.56
     rank  5: 3324.16
     rank  6: 3330.02
     rank  7: 3323.13
     rank  8: 3299.34
     rank  9: 3304.13
     rank 10: 3304.71
     rank 11: 3300.08
     rank 12: 3297.71
     rank 13: 3295.91
     rank 14: 3301.49
     rank 15: 3300.53
     rank 16: 3503.13
     rank 17: 3502.04
     rank 18: 3504.16
     rank 19: 3505.12
  pure-backward-compute:
     rank  0: 1058.37
     rank  1: 1059.62
     rank  2: 1056.77
     rank  3: 1073.13
     rank  4: 3326.61
     rank  5: 3323.34
     rank  6: 3328.92
     rank  7: 3322.18
     rank  8: 3297.98
     rank  9: 3302.88
     rank 10: 3303.89
     rank 11: 3298.88
     rank 12: 3296.40
     rank 13: 3294.78
     rank 14: 3300.35
     rank 15: 3299.46
     rank 16: 3500.49
     rank 17: 3499.45
     rank 18: 3502.56
     rank 19: 3502.65
  batch-generator:
     rank  0: 53.00
     rank  1: 55.79
     rank  2: 57.71
     rank  3: 66.35
     rank  4: 55.74
     rank  5: 63.37
     rank  6: 63.18
     rank  7: 83.39
     rank  8: 52.00
     rank  9: 53.53
     rank 10: 56.80
     rank 11: 57.66
     rank 12: 54.93
     rank 13: 59.79
     rank 14: 64.74
     rank 15: 59.31
     rank 16: 55.17
     rank 17: 58.08
     rank 18: 64.62
     rank 19: 60.14
  forward-recv:
     rank  4: 84.33
     rank  5: 84.34
     rank  6: 83.70
     rank  7: 79.48
     rank  8: 281.66
     rank  9: 281.49
     rank 10: 282.16
     rank 11: 280.19
     rank 12: 454.38
     rank 13: 454.63
     rank 14: 454.15
     rank 15: 453.53
     rank 16: 630.60
     rank 17: 630.44
     rank 18: 630.24
     rank 19: 630.63
  forward-send:
     rank  0: 396.78
     rank  1: 395.87
     rank  2: 394.91
     rank  3: 389.05
     rank  4: 31.57
     rank  5: 30.92
     rank  6: 30.57
     rank  7: 29.77
     rank  8: 20.88
     rank  9: 20.69
     rank 10: 19.89
     rank 11: 20.80
     rank 12: 10.39
     rank 13: 10.22
     rank 14: 9.99
     rank 15: 10.56
  backward-recv:
     rank  0: 1374.71
     rank  1: 1374.74
     rank  2: 1375.60
     rank  3: 1372.34
     rank  4: 592.93
     rank  5: 593.62
     rank  6: 592.23
     rank  7: 595.98
     rank  8: 392.13
     rank  9: 390.82
     rank 10: 392.28
     rank 11: 391.39
     rank 12: 193.32
     rank 13: 193.86
     rank 14: 193.39
     rank 15: 193.00
  backward-send:
     rank  4: 22.23
     rank  5: 22.25
     rank  6: 22.70
     rank  7: 20.30
     rank  8: 30.56
     rank  9: 31.22
     rank 10: 30.88
     rank 11: 31.16
     rank 12: 20.85
     rank 13: 19.77
     rank 14: 20.76
     rank 15: 20.88
     rank 16: 10.68
     rank 17: 10.53
     rank 18: 10.12
     rank 19: 9.98
  forward-send-backward-recv:
     rank  0: 4093.71
     rank  1: 4095.04
     rank  2: 4096.36
     rank  3: 4082.14
     rank  4: 860.12
     rank  5: 863.81
     rank  6: 857.43
     rank  7: 864.37
     rank  8: 699.64
     rank  9: 696.49
     rank 10: 696.61
     rank 11: 700.65
     rank 12: 518.63
     rank 13: 520.99
     rank 14: 517.05
     rank 15: 517.23
  backward-send-forward-recv:
     rank  4: 72.90
     rank  5: 72.65
     rank  6: 73.37
     rank  7: 62.43
     rank  8: 144.45
     rank  9: 144.58
     rank 10: 142.64
     rank 11: 145.00
     rank 12: 157.99
     rank 13: 157.17
     rank 14: 152.57
     rank 15: 156.02
     rank 16: 167.68
     rank 17: 168.34
     rank 18: 162.68
     rank 19: 165.32
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.05
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.13
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.64
     rank  2: 0.68
     rank  3: 0.67
     rank  4: 2.31
     rank  5: 2.29
     rank  6: 2.37
     rank  7: 2.30
     rank  8: 2.20
     rank  9: 2.17
     rank 10: 2.24
     rank 11: 2.35
     rank 12: 2.26
     rank 13: 2.16
     rank 14: 2.31
     rank 15: 2.31
     rank 16: 2.64
     rank 17: 2.58
     rank 18: 2.41
     rank 19: 2.45
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.32
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.07
     rank 12: 0.07
     rank 13: 0.05
     rank 14: 0.09
     rank 15: 0.09
     rank 16: 0.11
     rank 17: 0.08
     rank 18: 0.05
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.03
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.03
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.36
     rank  1: 11.64
     rank  2: 10.47
     rank  3: 10.37
     rank  4: 46.25
     rank  5: 46.23
     rank  6: 46.33
     rank  7: 46.26
     rank  8: 43.06
     rank  9: 43.00
     rank 10: 42.97
     rank 11: 43.08
     rank 12: 43.07
     rank 13: 42.95
     rank 14: 43.09
     rank 15: 43.08
     rank 16: 46.50
     rank 17: 46.52
     rank 18: 46.31
     rank 19: 46.38
  optimizer:
     rank  0: 11.59
     rank  1: 12.88
     rank  2: 11.70
     rank  3: 11.59
     rank  4: 47.47
     rank  5: 47.45
     rank  6: 47.56
     rank  7: 47.49
     rank  8: 44.29
     rank  9: 44.23
     rank 10: 44.20
     rank 11: 44.31
     rank 12: 44.30
     rank 13: 44.17
     rank 14: 44.31
     rank 15: 44.31
     rank 16: 47.72
     rank 17: 47.75
     rank 18: 47.54
     rank 19: 47.61
 [2024-12-02 16:40:43] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 8044.9 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 3.159381E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7959.73
     rank  1: 7959.78
     rank  2: 7959.79
     rank  3: 7959.82
     rank  4: 7963.20
     rank  5: 7963.24
     rank  6: 7963.25
     rank  7: 7963.27
     rank  8: 7962.92
     rank  9: 7962.95
     rank 10: 7962.94
     rank 11: 7963.75
     rank 12: 7962.89
     rank 13: 7962.93
     rank 14: 7962.96
     rank 15: 7963.11
     rank 16: 7963.21
     rank 17: 7963.22
     rank 18: 7963.21
     rank 19: 7963.23
  forward-compute:
     rank  0: 1180.10
     rank  1: 1180.33
     rank  2: 1182.09
     rank  3: 1187.80
     rank  4: 2888.65
     rank  5: 2892.59
     rank  6: 2889.20
     rank  7: 2907.01
     rank  8: 2787.41
     rank  9: 2787.84
     rank 10: 2790.44
     rank 11: 2788.78
     rank 12: 2789.11
     rank 13: 2788.93
     rank 14: 2795.64
     rank 15: 2792.42
     rank 16: 2919.06
     rank 17: 2919.06
     rank 18: 2924.93
     rank 19: 2920.87
  backward-compute:
     rank  0: 1051.49
     rank  1: 1053.52
     rank  2: 1050.08
     rank  3: 1063.61
     rank  4: 3325.29
     rank  5: 3326.76
     rank  6: 3331.22
     rank  7: 3324.07
     rank  8: 3301.08
     rank  9: 3304.31
     rank 10: 3306.58
     rank 11: 3302.26
     rank 12: 3302.58
     rank 13: 3300.54
     rank 14: 3304.76
     rank 15: 3304.68
     rank 16: 3504.92
     rank 17: 3504.76
     rank 18: 3506.71
     rank 19: 3505.97
  pure-backward-compute:
     rank  0: 1050.81
     rank  1: 1052.81
     rank  2: 1049.35
     rank  3: 1062.95
     rank  4: 3324.15
     rank  5: 3326.05
     rank  6: 3330.02
     rank  7: 3323.21
     rank  8: 3299.98
     rank  9: 3302.96
     rank 10: 3305.79
     rank 11: 3301.10
     rank 12: 3301.42
     rank 13: 3299.49
     rank 14: 3303.85
     rank 15: 3303.85
     rank 16: 3502.69
     rank 17: 3501.84
     rank 18: 3505.11
     rank 19: 3503.75
  batch-generator:
     rank  0: 53.11
     rank  1: 55.10
     rank  2: 57.73
     rank  3: 66.48
     rank  4: 68.08
     rank  5: 67.16
     rank  6: 71.60
     rank  7: 89.97
     rank  8: 50.69
     rank  9: 52.25
     rank 10: 55.68
     rank 11: 57.15
     rank 12: 53.27
     rank 13: 57.83
     rank 14: 62.98
     rank 15: 58.03
     rank 16: 52.90
     rank 17: 56.97
     rank 18: 61.21
     rank 19: 57.79
  forward-recv:
     rank  4: 84.41
     rank  5: 84.66
     rank  6: 84.19
     rank  7: 79.96
     rank  8: 280.89
     rank  9: 280.63
     rank 10: 281.23
     rank 11: 279.47
     rank 12: 454.38
     rank 13: 454.51
     rank 14: 454.07
     rank 15: 453.71
     rank 16: 628.90
     rank 17: 628.59
     rank 18: 628.62
     rank 19: 629.00
  forward-send:
     rank  0: 390.31
     rank  1: 388.93
     rank  2: 388.02
     rank  3: 382.08
     rank  4: 31.64
     rank  5: 30.80
     rank  6: 30.49
     rank  7: 29.34
     rank  8: 21.38
     rank  9: 20.91
     rank 10: 20.39
     rank 11: 20.76
     rank 12: 10.57
     rank 13: 10.32
     rank 14: 10.28
     rank 15: 10.52
  backward-recv:
     rank  0: 1391.18
     rank  1: 1391.16
     rank  2: 1391.17
     rank  3: 1389.04
     rank  4: 600.26
     rank  5: 600.13
     rank  6: 599.89
     rank  7: 602.24
     rank  8: 394.42
     rank  9: 394.16
     rank 10: 394.19
     rank 11: 394.63
     rank 12: 197.07
     rank 13: 197.37
     rank 14: 197.63
     rank 15: 196.72
  backward-send:
     rank  4: 22.61
     rank  5: 22.26
     rank  6: 22.42
     rank  7: 20.22
     rank  8: 31.26
     rank  9: 30.99
     rank 10: 31.07
     rank 11: 30.72
     rank 12: 20.96
     rank 13: 20.51
     rank 14: 20.30
     rank 15: 20.93
     rank 16: 10.50
     rank 17: 10.61
     rank 18: 10.27
     rank 19: 9.90
  forward-send-backward-recv:
     rank  0: 3930.21
     rank  1: 3931.35
     rank  2: 3933.51
     rank  3: 3920.50
     rank  4: 853.15
     rank  5: 855.04
     rank  6: 851.71
     rank  7: 856.70
     rank  8: 705.62
     rank  9: 702.77
     rank 10: 703.27
     rank 11: 706.87
     rank 12: 517.51
     rank 13: 519.52
     rank 14: 517.42
     rank 15: 517.49
  backward-send-forward-recv:
     rank  4: 73.14
     rank  5: 71.43
     rank  6: 71.11
     rank  7: 63.24
     rank  8: 144.85
     rank  9: 145.42
     rank 10: 142.76
     rank 11: 144.35
     rank 12: 158.14
     rank 13: 157.70
     rank 14: 152.40
     rank 15: 156.29
     rank 16: 169.49
     rank 17: 169.78
     rank 18: 165.00
     rank 19: 168.23
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.07
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.66
     rank  2: 0.68
     rank  3: 0.62
     rank  4: 2.33
     rank  5: 2.30
     rank  6: 2.32
     rank  7: 2.29
     rank  8: 2.16
     rank  9: 2.22
     rank 10: 2.23
     rank 11: 2.74
     rank 12: 2.17
     rank 13: 2.17
     rank 14: 2.20
     rank 15: 2.21
     rank 16: 2.43
     rank 17: 2.45
     rank 18: 2.41
     rank 19: 2.44
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.17
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.13
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.21
     rank  1: 10.38
     rank  2: 10.29
     rank  3: 10.15
     rank  4: 46.33
     rank  5: 46.25
     rank  6: 46.26
     rank  7: 46.23
     rank  8: 42.98
     rank  9: 43.07
     rank 10: 43.01
     rank 11: 43.64
     rank 12: 42.96
     rank 13: 43.01
     rank 14: 43.07
     rank 15: 43.10
     rank 16: 46.27
     rank 17: 46.28
     rank 18: 46.36
     rank 19: 46.45
  optimizer:
     rank  0: 11.52
     rank  1: 11.70
     rank  2: 11.61
     rank  3: 11.46
     rank  4: 47.64
     rank  5: 47.57
     rank  6: 47.57
     rank  7: 47.55
     rank  8: 44.30
     rank  9: 44.38
     rank 10: 44.32
     rank 11: 45.01
     rank 12: 44.27
     rank 13: 44.33
     rank 14: 44.39
     rank 15: 44.42
     rank 16: 47.59
     rank 17: 47.60
     rank 18: 47.67
     rank 19: 47.76
 [2024-12-02 16:40:51] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 8046.3 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 5.006929E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7963.32
     rank  1: 7963.29
     rank  2: 7963.46
     rank  3: 7963.32
     rank  4: 7966.80
     rank  5: 7966.74
     rank  6: 7966.92
     rank  7: 7966.77
     rank  8: 7966.50
     rank  9: 7966.44
     rank 10: 7966.61
     rank 11: 7966.55
     rank 12: 7966.48
     rank 13: 7966.44
     rank 14: 7966.61
     rank 15: 7966.51
     rank 16: 7966.70
     rank 17: 7966.70
     rank 18: 7967.33
     rank 19: 7966.76
  forward-compute:
     rank  0: 1099.75
     rank  1: 1099.74
     rank  2: 1103.48
     rank  3: 1105.91
     rank  4: 2875.18
     rank  5: 2875.89
     rank  6: 2876.89
     rank  7: 2891.53
     rank  8: 2793.66
     rank  9: 2793.14
     rank 10: 2796.40
     rank 11: 2795.84
     rank 12: 2800.25
     rank 13: 2800.07
     rank 14: 2807.97
     rank 15: 2802.43
     rank 16: 2921.23
     rank 17: 2921.74
     rank 18: 2926.45
     rank 19: 2923.64
  backward-compute:
     rank  0: 1049.09
     rank  1: 1048.77
     rank  2: 1047.88
     rank  3: 1061.11
     rank  4: 3331.46
     rank  5: 3332.57
     rank  6: 3333.02
     rank  7: 3330.28
     rank  8: 3308.46
     rank  9: 3312.05
     rank 10: 3314.27
     rank 11: 3308.03
     rank 12: 3305.06
     rank 13: 3303.43
     rank 14: 3306.78
     rank 15: 3308.02
     rank 16: 3511.13
     rank 17: 3509.59
     rank 18: 3512.49
     rank 19: 3512.26
  pure-backward-compute:
     rank  0: 1048.23
     rank  1: 1047.99
     rank  2: 1047.06
     rank  3: 1060.33
     rank  4: 3330.56
     rank  5: 3331.73
     rank  6: 3331.89
     rank  7: 3329.54
     rank  8: 3307.35
     rank  9: 3310.82
     rank 10: 3313.40
     rank 11: 3306.91
     rank 12: 3303.87
     rank 13: 3302.20
     rank 14: 3305.98
     rank 15: 3307.16
     rank 16: 3508.73
     rank 17: 3507.31
     rank 18: 3510.80
     rank 19: 3510.22
  batch-generator:
     rank  0: 66.08
     rank  1: 65.66
     rank  2: 69.18
     rank  3: 75.75
     rank  4: 53.96
     rank  5: 56.08
     rank  6: 65.18
     rank  7: 78.74
     rank  8: 51.98
     rank  9: 52.61
     rank 10: 56.79
     rank 11: 60.56
     rank 12: 59.15
     rank 13: 62.15
     rank 14: 68.41
     rank 15: 61.24
     rank 16: 53.46
     rank 17: 56.57
     rank 18: 61.04
     rank 19: 58.79
  forward-recv:
     rank  4: 82.77
     rank  5: 82.71
     rank  6: 81.31
     rank  7: 78.83
     rank  8: 277.93
     rank  9: 278.02
     rank 10: 278.52
     rank 11: 276.59
     rank 12: 452.39
     rank 13: 452.63
     rank 14: 451.57
     rank 15: 452.15
     rank 16: 627.51
     rank 17: 627.30
     rank 18: 627.12
     rank 19: 627.57
  forward-send:
     rank  0: 392.99
     rank  1: 392.46
     rank  2: 389.18
     rank  3: 385.52
     rank  4: 32.48
     rank  5: 32.22
     rank  6: 30.48
     rank  7: 30.36
     rank  8: 21.71
     rank  9: 21.48
     rank 10: 19.78
     rank 11: 21.12
     rank 12: 10.56
     rank 13: 10.39
     rank 14: 10.08
     rank 15: 10.43
  backward-recv:
     rank  0: 1387.25
     rank  1: 1388.08
     rank  2: 1387.81
     rank  3: 1386.26
     rank  4: 595.99
     rank  5: 595.10
     rank  6: 595.07
     rank  7: 597.57
     rank  8: 393.20
     rank  9: 392.62
     rank 10: 392.45
     rank 11: 393.53
     rank 12: 194.01
     rank 13: 194.37
     rank 14: 194.71
     rank 15: 193.86
  backward-send:
     rank  4: 21.80
     rank  5: 22.45
     rank  6: 22.67
     rank  7: 21.03
     rank  8: 31.08
     rank  9: 31.04
     rank 10: 31.32
     rank 11: 30.79
     rank 12: 20.95
     rank 13: 20.53
     rank 14: 20.26
     rank 15: 20.92
     rank 16: 10.55
     rank 17: 10.48
     rank 18: 10.33
     rank 19: 9.94
  forward-send-backward-recv:
     rank  0: 4015.16
     rank  1: 4019.30
     rank  2: 4020.21
     rank  3: 4006.93
     rank  4: 871.69
     rank  5: 873.51
     rank  6: 871.09
     rank  7: 874.91
     rank  8: 701.38
     rank  9: 698.79
     rank 10: 698.81
     rank 11: 702.10
     rank 12: 515.69
     rank 13: 517.66
     rank 14: 515.82
     rank 15: 515.47
  backward-send-forward-recv:
     rank  4: 73.20
     rank  5: 72.97
     rank  6: 74.25
     rank  7: 63.81
     rank  8: 144.68
     rank  9: 145.20
     rank 10: 143.39
     rank 11: 143.77
     rank 12: 157.84
     rank 13: 157.19
     rank 14: 151.91
     rank 15: 156.48
     rank 16: 168.56
     rank 17: 169.44
     rank 18: 165.16
     rank 19: 166.90
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.16
     rank 19: 0.09
  all-grads-sync:
     rank  0: 0.67
     rank  1: 0.68
     rank  2: 0.68
     rank  3: 0.64
     rank  4: 2.29
     rank  5: 2.32
     rank  6: 2.31
     rank  7: 2.29
     rank  8: 2.21
     rank  9: 2.17
     rank 10: 2.16
     rank 11: 2.26
     rank 12: 2.16
     rank 13: 2.20
     rank 14: 2.17
     rank 15: 2.17
     rank 16: 2.59
     rank 17: 2.50
     rank 18: 2.95
     rank 19: 2.60
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.07
     rank 12: 0.10
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.13
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.45
     rank  1: 10.31
     rank  2: 10.45
     rank  3: 10.30
     rank  4: 46.27
     rank  5: 46.31
     rank  6: 46.27
     rank  7: 46.23
     rank  8: 43.04
     rank  9: 43.04
     rank 10: 43.02
     rank 11: 43.10
     rank 12: 42.96
     rank 13: 43.03
     rank 14: 42.99
     rank 15: 42.97
     rank 16: 46.35
     rank 17: 46.36
     rank 18: 46.90
     rank 19: 46.49
  optimizer:
     rank  0: 11.72
     rank  1: 11.58
     rank  2: 11.73
     rank  3: 11.57
     rank  4: 47.54
     rank  5: 47.58
     rank  6: 47.54
     rank  7: 47.51
     rank  8: 44.33
     rank  9: 44.31
     rank 10: 44.31
     rank 11: 44.38
     rank 12: 44.23
     rank 13: 44.30
     rank 14: 44.27
     rank 15: 44.24
     rank 16: 47.62
     rank 17: 47.63
     rank 18: 48.14
     rank 19: 47.77
 [2024-12-02 16:40:59] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 8041.5 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.595369E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7958.25
     rank  1: 7958.31
     rank  2: 7958.33
     rank  3: 7958.30
     rank  4: 7961.76
     rank  5: 7961.76
     rank  6: 7961.76
     rank  7: 7961.78
     rank  8: 7961.49
     rank  9: 7961.46
     rank 10: 7961.46
     rank 11: 7961.49
     rank 12: 7961.45
     rank 13: 7961.47
     rank 14: 7961.48
     rank 15: 7961.49
     rank 16: 7961.76
     rank 17: 7961.75
     rank 18: 7961.74
     rank 19: 7961.75
  forward-compute:
     rank  0: 1013.54
     rank  1: 1013.50
     rank  2: 1016.86
     rank  3: 1020.33
     rank  4: 2877.99
     rank  5: 2879.47
     rank  6: 2880.97
     rank  7: 2894.36
     rank  8: 2793.10
     rank  9: 2792.79
     rank 10: 2796.27
     rank 11: 2794.41
     rank 12: 2796.06
     rank 13: 2794.21
     rank 14: 2802.63
     rank 15: 2799.06
     rank 16: 2921.38
     rank 17: 2922.12
     rank 18: 2923.52
     rank 19: 2926.49
  backward-compute:
     rank  0: 1056.86
     rank  1: 1057.63
     rank  2: 1056.79
     rank  3: 1067.97
     rank  4: 3335.16
     rank  5: 3334.54
     rank  6: 3337.75
     rank  7: 3332.79
     rank  8: 3302.26
     rank  9: 3304.41
     rank 10: 3307.83
     rank 11: 3303.49
     rank 12: 3306.22
     rank 13: 3305.34
     rank 14: 3306.27
     rank 15: 3309.70
     rank 16: 3507.52
     rank 17: 3507.48
     rank 18: 3509.87
     rank 19: 3509.64
  pure-backward-compute:
     rank  0: 1056.17
     rank  1: 1056.81
     rank  2: 1056.07
     rank  3: 1067.24
     rank  4: 3334.19
     rank  5: 3333.76
     rank  6: 3336.67
     rank  7: 3332.09
     rank  8: 3301.16
     rank  9: 3303.18
     rank 10: 3306.93
     rank 11: 3302.42
     rank 12: 3305.12
     rank 13: 3304.27
     rank 14: 3305.43
     rank 15: 3308.79
     rank 16: 3505.19
     rank 17: 3505.28
     rank 18: 3507.76
     rank 19: 3507.98
  batch-generator:
     rank  0: 51.67
     rank  1: 55.51
     rank  2: 58.58
     rank  3: 63.65
     rank  4: 54.68
     rank  5: 58.48
     rank  6: 63.16
     rank  7: 76.27
     rank  8: 50.47
     rank  9: 52.13
     rank 10: 56.82
     rank 11: 56.81
     rank 12: 53.54
     rank 13: 57.20
     rank 14: 63.90
     rank 15: 58.38
     rank 16: 53.90
     rank 17: 57.49
     rank 18: 58.27
     rank 19: 62.85
  forward-recv:
     rank  4: 84.23
     rank  5: 84.36
     rank  6: 82.79
     rank  7: 80.88
     rank  8: 279.85
     rank  9: 280.12
     rank 10: 280.23
     rank 11: 279.03
     rank 12: 454.59
     rank 13: 454.97
     rank 14: 454.02
     rank 15: 454.20
     rank 16: 629.84
     rank 17: 629.81
     rank 18: 629.52
     rank 19: 629.87
  forward-send:
     rank  0: 394.55
     rank  1: 393.77
     rank  2: 390.31
     rank  3: 387.23
     rank  4: 32.26
     rank  5: 31.51
     rank  6: 29.96
     rank  7: 29.66
     rank  8: 22.21
     rank  9: 21.78
     rank 10: 20.51
     rank 11: 21.24
     rank 12: 10.65
     rank 13: 10.36
     rank 14: 10.10
     rank 15: 10.36
  backward-recv:
     rank  0: 1378.84
     rank  1: 1378.18
     rank  2: 1379.05
     rank  3: 1376.72
     rank  4: 593.75
     rank  5: 596.42
     rank  6: 593.62
     rank  7: 596.17
     rank  8: 389.78
     rank  9: 389.27
     rank 10: 389.51
     rank 11: 389.93
     rank 12: 196.77
     rank 13: 196.91
     rank 14: 197.78
     rank 15: 196.88
  backward-send:
     rank  4: 22.18
     rank  5: 21.95
     rank  6: 22.55
     rank  7: 20.81
     rank  8: 31.28
     rank  9: 31.43
     rank 10: 31.03
     rank 11: 30.62
     rank 12: 20.92
     rank 13: 20.74
     rank 14: 20.30
     rank 15: 20.60
     rank 16: 10.42
     rank 17: 10.31
     rank 18: 10.43
     rank 19: 10.25
  forward-send-backward-recv:
     rank  0: 4098.24
     rank  1: 4100.91
     rank  2: 4100.74
     rank  3: 4089.43
     rank  4: 863.75
     rank  5: 865.14
     rank  6: 861.64
     rank  7: 867.52
     rank  8: 704.79
     rank  9: 702.38
     rank 10: 702.81
     rank 11: 705.05
     rank 12: 511.52
     rank 13: 514.13
     rank 14: 514.07
     rank 15: 510.44
  backward-send-forward-recv:
     rank  4: 73.95
     rank  5: 73.84
     rank  6: 74.58
     rank  7: 64.80
     rank  8: 145.75
     rank  9: 145.98
     rank 10: 143.16
     rank 11: 145.32
     rank 12: 157.78
     rank 13: 157.45
     rank 14: 151.62
     rank 15: 155.37
     rank 16: 169.52
     rank 17: 169.49
     rank 18: 168.22
     rank 19: 165.58
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.07
     rank  2: 0.06
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.67
     rank  2: 0.68
     rank  3: 0.62
     rank  4: 2.32
     rank  5: 2.30
     rank  6: 2.31
     rank  7: 2.36
     rank  8: 2.15
     rank  9: 2.19
     rank 10: 2.17
     rank 11: 2.26
     rank 12: 2.16
     rank 13: 2.16
     rank 14: 2.21
     rank 15: 2.20
     rank 16: 2.48
     rank 17: 2.46
     rank 18: 2.61
     rank 19: 2.46
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.09
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.33
     rank  1: 10.45
     rank  2: 10.27
     rank  3: 10.32
     rank  4: 46.28
     rank  5: 46.31
     rank  6: 46.27
     rank  7: 46.39
     rank  8: 42.96
     rank  9: 43.07
     rank 10: 43.03
     rank 11: 43.09
     rank 12: 42.98
     rank 13: 43.01
     rank 14: 43.09
     rank 15: 43.17
     rank 16: 46.34
     rank 17: 46.33
     rank 18: 46.46
     rank 19: 46.44
  optimizer:
     rank  0: 10.98
     rank  1: 11.10
     rank  2: 10.93
     rank  3: 10.96
     rank  4: 46.92
     rank  5: 46.95
     rank  6: 46.92
     rank  7: 47.03
     rank  8: 43.64
     rank  9: 43.71
     rank 10: 43.67
     rank 11: 43.74
     rank 12: 43.62
     rank 13: 43.65
     rank 14: 43.73
     rank 15: 43.81
     rank 16: 46.98
     rank 17: 46.97
     rank 18: 47.21
     rank 19: 47.08
 [2024-12-02 16:41:08] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 8060.0 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.382645E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7983.01
     rank  1: 7983.04
     rank  2: 7983.05
     rank  3: 7983.13
     rank  4: 7986.52
     rank  5: 7986.47
     rank  6: 7986.51
     rank  7: 7986.58
     rank  8: 7986.17
     rank  9: 7986.17
     rank 10: 7986.21
     rank 11: 7986.29
     rank 12: 7986.15
     rank 13: 7986.16
     rank 14: 7986.22
     rank 15: 7986.28
     rank 16: 7986.47
     rank 17: 7986.47
     rank 18: 7986.47
     rank 19: 7986.57
  forward-compute:
     rank  0: 1018.27
     rank  1: 1018.51
     rank  2: 1027.58
     rank  3: 1025.74
     rank  4: 2882.26
     rank  5: 2886.20
     rank  6: 2893.05
     rank  7: 2902.01
     rank  8: 2783.58
     rank  9: 2785.20
     rank 10: 2791.81
     rank 11: 2788.41
     rank 12: 2805.36
     rank 13: 2803.86
     rank 14: 2811.62
     rank 15: 2806.78
     rank 16: 2925.77
     rank 17: 2926.17
     rank 18: 2928.87
     rank 19: 2930.94
  backward-compute:
     rank  0: 1062.38
     rank  1: 1064.91
     rank  2: 1063.36
     rank  3: 1076.55
     rank  4: 3338.21
     rank  5: 3338.60
     rank  6: 3338.85
     rank  7: 3336.37
     rank  8: 3287.95
     rank  9: 3288.72
     rank 10: 3292.36
     rank 11: 3288.06
     rank 12: 3309.79
     rank 13: 3308.29
     rank 14: 3310.22
     rank 15: 3313.56
     rank 16: 3516.06
     rank 17: 3515.07
     rank 18: 3517.86
     rank 19: 3517.24
  pure-backward-compute:
     rank  0: 1061.72
     rank  1: 1064.04
     rank  2: 1062.63
     rank  3: 1075.84
     rank  4: 3337.11
     rank  5: 3337.76
     rank  6: 3337.92
     rank  7: 3335.63
     rank  8: 3286.79
     rank  9: 3287.60
     rank 10: 3291.47
     rank 11: 3287.07
     rank 12: 3308.68
     rank 13: 3307.20
     rank 14: 3309.32
     rank 15: 3312.71
     rank 16: 3513.67
     rank 17: 3512.38
     rank 18: 3515.92
     rank 19: 3515.55
  batch-generator:
     rank  0: 52.85
     rank  1: 57.94
     rank  2: 67.63
     rank  3: 67.33
     rank  4: 56.22
     rank  5: 63.57
     rank  6: 72.54
     rank  7: 84.24
     rank  8: 49.69
     rank  9: 52.88
     rank 10: 60.67
     rank 11: 59.31
     rank 12: 60.55
     rank 13: 63.52
     rank 14: 69.46
     rank 15: 62.79
     rank 16: 53.97
     rank 17: 58.50
     rank 18: 58.86
     rank 19: 61.37
  forward-recv:
     rank  4: 82.35
     rank  5: 82.21
     rank  6: 79.61
     rank  7: 77.73
     rank  8: 277.93
     rank  9: 277.88
     rank 10: 274.25
     rank 11: 276.35
     rank 12: 454.11
     rank 13: 454.29
     rank 14: 452.93
     rank 15: 453.78
     rank 16: 638.09
     rank 17: 637.94
     rank 18: 637.72
     rank 19: 638.05
  forward-send:
     rank  0: 404.75
     rank  1: 403.27
     rank  2: 395.01
     rank  3: 396.12
     rank  4: 39.05
     rank  5: 38.47
     rank  6: 32.23
     rank  7: 36.19
     rank  8: 30.01
     rank  9: 29.81
     rank 10: 27.73
     rank 11: 29.36
     rank 12: 10.60
     rank 13: 10.45
     rank 14: 9.94
     rank 15: 10.40
  backward-recv:
     rank  0: 1380.49
     rank  1: 1380.40
     rank  2: 1380.49
     rank  3: 1377.23
     rank  4: 598.86
     rank  5: 599.42
     rank  6: 599.20
     rank  7: 601.68
     rank  8: 394.39
     rank  9: 394.23
     rank 10: 393.81
     rank 11: 394.57
     rank 12: 197.58
     rank 13: 197.76
     rank 14: 197.66
     rank 15: 197.66
  backward-send:
     rank  4: 22.76
     rank  5: 22.47
     rank  6: 22.59
     rank  7: 20.34
     rank  8: 31.25
     rank  9: 31.17
     rank 10: 30.90
     rank 11: 30.95
     rank 12: 20.97
     rank 13: 20.70
     rank 14: 20.09
     rank 15: 20.81
     rank 16: 10.50
     rank 17: 10.51
     rank 18: 9.86
     rank 19: 10.42
  forward-send-backward-recv:
     rank  0: 4100.82
     rank  1: 4101.75
     rank  2: 4102.62
     rank  3: 4090.68
     rank  4: 870.76
     rank  5: 871.35
     rank  6: 869.28
     rank  7: 872.92
     rank  8: 743.11
     rank  9: 740.72
     rank 10: 741.14
     rank 11: 744.19
     rank 12: 521.56
     rank 13: 524.84
     rank 14: 524.77
     rank 15: 521.33
  backward-send-forward-recv:
     rank  4: 73.58
     rank  5: 72.90
     rank  6: 74.28
     rank  7: 64.05
     rank  8: 146.11
     rank  9: 146.55
     rank 10: 144.26
     rank 11: 143.28
     rank 12: 157.44
     rank 13: 156.84
     rank 14: 152.22
     rank 15: 155.74
     rank 16: 169.61
     rank 17: 169.81
     rank 18: 167.63
     rank 19: 165.85
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.06
     rank  3: 0.04
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.64
     rank  2: 0.66
     rank  3: 0.63
     rank  4: 2.40
     rank  5: 2.28
     rank  6: 2.31
     rank  7: 2.29
     rank  8: 2.15
     rank  9: 2.16
     rank 10: 2.16
     rank 11: 2.18
     rank 12: 2.20
     rank 13: 2.15
     rank 14: 2.17
     rank 15: 2.17
     rank 16: 2.54
     rank 17: 2.43
     rank 18: 2.46
     rank 19: 2.47
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.09
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.17
     rank  1: 10.15
     rank  2: 11.72
     rank  3: 10.19
     rank  4: 46.45
     rank  5: 46.26
     rank  6: 46.25
     rank  7: 46.27
     rank  8: 42.96
     rank  9: 43.00
     rank 10: 42.97
     rank 11: 43.02
     rank 12: 43.06
     rank 13: 42.97
     rank 14: 43.01
     rank 15: 43.00
     rank 16: 46.28
     rank 17: 46.31
     rank 18: 46.50
     rank 19: 46.50
  optimizer:
     rank  0: 10.87
     rank  1: 10.86
     rank  2: 12.49
     rank  3: 10.90
     rank  4: 47.18
     rank  5: 46.97
     rank  6: 46.97
     rank  7: 46.98
     rank  8: 43.67
     rank  9: 43.71
     rank 10: 43.68
     rank 11: 43.73
     rank 12: 43.77
     rank 13: 43.68
     rank 14: 43.72
     rank 15: 43.71
     rank 16: 47.00
     rank 17: 47.02
     rank 18: 47.21
     rank 19: 47.22
 [2024-12-02 16:41:16] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 8056.7 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.117906E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7974.79
     rank  1: 7974.81
     rank  2: 7975.07
     rank  3: 7974.82
     rank  4: 7978.24
     rank  5: 7978.27
     rank  6: 7978.35
     rank  7: 7978.26
     rank  8: 7977.94
     rank  9: 7977.99
     rank 10: 7978.03
     rank 11: 7978.04
     rank 12: 7977.97
     rank 13: 7977.97
     rank 14: 7978.02
     rank 15: 7977.99
     rank 16: 7978.25
     rank 17: 7978.24
     rank 18: 7978.24
     rank 19: 7978.25
  forward-compute:
     rank  0: 1033.40
     rank  1: 1034.68
     rank  2: 1032.98
     rank  3: 1041.04
     rank  4: 2889.38
     rank  5: 2892.66
     rank  6: 2888.67
     rank  7: 2903.63
     rank  8: 2787.21
     rank  9: 2788.63
     rank 10: 2789.27
     rank 11: 2790.92
     rank 12: 2794.68
     rank 13: 2794.16
     rank 14: 2800.88
     rank 15: 2796.96
     rank 16: 2917.96
     rank 17: 2917.85
     rank 18: 2923.92
     rank 19: 2920.10
  backward-compute:
     rank  0: 1062.85
     rank  1: 1062.98
     rank  2: 1061.58
     rank  3: 1072.92
     rank  4: 3339.76
     rank  5: 3341.29
     rank  6: 3342.29
     rank  7: 3338.76
     rank  8: 3291.51
     rank  9: 3292.72
     rank 10: 3295.66
     rank 11: 3290.31
     rank 12: 3311.61
     rank 13: 3309.27
     rank 14: 3313.83
     rank 15: 3314.42
     rank 16: 3521.75
     rank 17: 3521.30
     rank 18: 3522.40
     rank 19: 3524.04
  pure-backward-compute:
     rank  0: 1062.18
     rank  1: 1062.29
     rank  2: 1060.93
     rank  3: 1072.19
     rank  4: 3338.83
     rank  5: 3340.42
     rank  6: 3341.24
     rank  7: 3338.03
     rank  8: 3290.37
     rank  9: 3291.43
     rank 10: 3294.84
     rank 11: 3289.36
     rank 12: 3310.60
     rank 13: 3308.22
     rank 14: 3312.98
     rank 15: 3313.67
     rank 16: 3519.54
     rank 17: 3518.80
     rank 18: 3520.60
     rank 19: 3522.49
  batch-generator:
     rank  0: 51.75
     rank  1: 55.06
     rank  2: 54.82
     rank  3: 64.64
     rank  4: 54.91
     rank  5: 64.07
     rank  6: 64.31
     rank  7: 87.37
     rank  8: 50.67
     rank  9: 53.93
     rank 10: 54.92
     rank 11: 58.67
     rank 12: 51.31
     rank 13: 55.67
     rank 14: 60.69
     rank 15: 54.99
     rank 16: 51.66
     rank 17: 54.63
     rank 18: 60.04
     rank 19: 56.87
  forward-recv:
     rank  4: 83.39
     rank  5: 83.24
     rank  6: 83.50
     rank  7: 78.98
     rank  8: 282.19
     rank  9: 281.64
     rank 10: 283.67
     rank 11: 280.72
     rank 12: 453.51
     rank 13: 453.79
     rank 14: 453.02
     rank 15: 453.23
     rank 16: 628.78
     rank 17: 628.67
     rank 18: 628.46
     rank 19: 628.74
  forward-send:
     rank  0: 400.75
     rank  1: 398.90
     rank  2: 400.40
     rank  3: 391.92
     rank  4: 31.50
     rank  5: 30.64
     rank  6: 31.42
     rank  7: 28.90
     rank  8: 21.07
     rank  9: 21.05
     rank 10: 19.71
     rank 11: 20.62
     rank 12: 10.55
     rank 13: 10.44
     rank 14: 10.10
     rank 15: 10.40
  backward-recv:
     rank  0: 1383.39
     rank  1: 1383.91
     rank  2: 1384.70
     rank  3: 1380.60
     rank  4: 595.70
     rank  5: 593.38
     rank  6: 594.18
     rank  7: 595.92
     rank  8: 398.19
     rank  9: 398.04
     rank 10: 397.74
     rank 11: 398.50
     rank 12: 194.54
     rank 13: 194.75
     rank 14: 195.19
     rank 15: 194.33
  backward-send:
     rank  4: 21.36
     rank  5: 22.22
     rank  6: 22.08
     rank  7: 19.69
     rank  8: 31.39
     rank  9: 31.04
     rank 10: 31.32
     rank 11: 30.81
     rank 12: 20.91
     rank 13: 20.53
     rank 14: 20.24
     rank 15: 20.85
     rank 16: 10.51
     rank 17: 10.52
     rank 18: 10.39
     rank 19: 10.05
  forward-send-backward-recv:
     rank  0: 4078.51
     rank  1: 4080.05
     rank  2: 4081.39
     rank  3: 4071.75
     rank  4: 860.84
     rank  5: 861.67
     rank  6: 859.59
     rank  7: 863.75
     rank  8: 722.69
     rank  9: 721.14
     rank 10: 721.46
     rank 11: 724.83
     rank 12: 522.14
     rank 13: 524.00
     rank 14: 521.03
     rank 15: 521.21
  backward-send-forward-recv:
     rank  4: 73.81
     rank  5: 73.13
     rank  6: 74.41
     rank  7: 67.33
     rank  8: 146.13
     rank  9: 146.59
     rank 10: 144.69
     rank 11: 144.41
     rank 12: 157.75
     rank 13: 157.51
     rank 14: 153.00
     rank 15: 156.34
     rank 16: 168.45
     rank 17: 169.13
     rank 18: 164.32
     rank 19: 166.70
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.09
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.65
     rank  2: 0.76
     rank  3: 0.67
     rank  4: 2.32
     rank  5: 2.30
     rank  6: 2.36
     rank  7: 2.29
     rank  8: 2.16
     rank  9: 2.16
     rank 10: 2.16
     rank 11: 2.24
     rank 12: 2.14
     rank 13: 2.15
     rank 14: 2.15
     rank 15: 2.15
     rank 16: 2.42
     rank 17: 2.45
     rank 18: 2.48
     rank 19: 2.42
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.25
     rank  3: 0.18
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.13
     rank  1: 10.39
     rank  2: 10.78
     rank  3: 10.25
     rank  4: 46.28
     rank  5: 46.28
     rank  6: 46.30
     rank  7: 46.25
     rank  8: 42.98
     rank  9: 42.98
     rank 10: 42.96
     rank 11: 43.27
     rank 12: 42.97
     rank 13: 42.97
     rank 14: 42.96
     rank 15: 43.01
     rank 16: 46.36
     rank 17: 46.30
     rank 18: 46.40
     rank 19: 46.36
  optimizer:
     rank  0: 11.04
     rank  1: 11.31
     rank  2: 11.70
     rank  3: 11.16
     rank  4: 47.19
     rank  5: 47.19
     rank  6: 47.22
     rank  7: 47.16
     rank  8: 43.89
     rank  9: 43.89
     rank 10: 43.87
     rank 11: 44.18
     rank 12: 43.89
     rank 13: 43.88
     rank 14: 43.88
     rank 15: 43.92
     rank 16: 47.28
     rank 17: 47.22
     rank 18: 47.32
     rank 19: 47.28
 [2024-12-02 16:41:24] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 8054.8 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.474286E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7979.66
     rank  1: 7979.71
     rank  2: 7979.75
     rank  3: 7979.66
     rank  4: 7983.13
     rank  5: 7983.13
     rank  6: 7983.21
     rank  7: 7983.11
     rank  8: 7982.80
     rank  9: 7982.85
     rank 10: 7982.89
     rank 11: 7982.80
     rank 12: 7982.79
     rank 13: 7982.84
     rank 14: 7982.89
     rank 15: 7982.79
     rank 16: 7983.11
     rank 17: 7983.13
     rank 18: 7983.13
     rank 19: 7983.12
  forward-compute:
     rank  0: 1017.16
     rank  1: 1017.64
     rank  2: 1017.21
     rank  3: 1026.96
     rank  4: 2892.95
     rank  5: 2894.06
     rank  6: 2893.03
     rank  7: 2913.89
     rank  8: 2789.51
     rank  9: 2791.78
     rank 10: 2792.47
     rank 11: 2793.95
     rank 12: 2797.01
     rank 13: 2796.69
     rank 14: 2803.08
     rank 15: 2799.29
     rank 16: 2917.32
     rank 17: 2917.66
     rank 18: 2920.90
     rank 19: 2920.89
  backward-compute:
     rank  0: 1068.55
     rank  1: 1066.99
     rank  2: 1067.60
     rank  3: 1081.61
     rank  4: 3339.62
     rank  5: 3341.98
     rank  6: 3341.82
     rank  7: 3338.66
     rank  8: 3290.99
     rank  9: 3292.28
     rank 10: 3295.36
     rank 11: 3290.54
     rank 12: 3305.14
     rank 13: 3303.24
     rank 14: 3305.81
     rank 15: 3309.02
     rank 16: 3525.68
     rank 17: 3524.40
     rank 18: 3527.42
     rank 19: 3527.33
  pure-backward-compute:
     rank  0: 1067.85
     rank  1: 1066.32
     rank  2: 1066.79
     rank  3: 1080.89
     rank  4: 3338.69
     rank  5: 3340.92
     rank  6: 3340.86
     rank  7: 3337.92
     rank  8: 3289.78
     rank  9: 3291.11
     rank 10: 3294.57
     rank 11: 3289.61
     rank 12: 3304.07
     rank 13: 3302.05
     rank 14: 3305.01
     rank 15: 3308.20
     rank 16: 3523.34
     rank 17: 3522.17
     rank 18: 3525.69
     rank 19: 3525.66
  batch-generator:
     rank  0: 52.44
     rank  1: 54.80
     rank  2: 55.23
     rank  3: 67.54
     rank  4: 54.45
     rank  5: 60.89
     rank  6: 62.13
     rank  7: 84.86
     rank  8: 52.83
     rank  9: 56.23
     rank 10: 56.91
     rank 11: 61.89
     rank 12: 54.23
     rank 13: 59.03
     rank 14: 63.62
     rank 15: 58.98
     rank 16: 52.30
     rank 17: 55.70
     rank 18: 58.25
     rank 19: 58.87
  forward-recv:
     rank  4: 83.46
     rank  5: 82.99
     rank  6: 83.03
     rank  7: 76.73
     rank  8: 279.99
     rank  9: 279.76
     rank 10: 280.39
     rank 11: 279.13
     rank 12: 454.35
     rank 13: 454.74
     rank 14: 454.73
     rank 15: 453.03
     rank 16: 629.03
     rank 17: 629.01
     rank 18: 628.78
     rank 19: 629.01
  forward-send:
     rank  0: 396.78
     rank  1: 396.01
     rank  2: 396.03
     rank  3: 385.92
     rank  4: 31.28
     rank  5: 31.08
     rank  6: 31.08
     rank  7: 28.13
     rank  8: 20.90
     rank  9: 20.82
     rank 10: 20.42
     rank 11: 19.26
     rank 12: 10.55
     rank 13: 10.39
     rank 14: 10.12
     rank 15: 10.41
  backward-recv:
     rank  0: 1386.68
     rank  1: 1387.99
     rank  2: 1387.37
     rank  3: 1384.46
     rank  4: 602.43
     rank  5: 599.02
     rank  6: 601.55
     rank  7: 602.57
     rank  8: 396.33
     rank  9: 396.61
     rank 10: 396.44
     rank 11: 396.93
     rank 12: 196.66
     rank 13: 197.12
     rank 14: 196.96
     rank 15: 196.30
  backward-send:
     rank  4: 21.61
     rank  5: 22.71
     rank  6: 22.09
     rank  7: 20.05
     rank  8: 31.39
     rank  9: 30.83
     rank 10: 31.21
     rank 11: 30.71
     rank 12: 20.81
     rank 13: 20.67
     rank 14: 20.62
     rank 15: 20.92
     rank 16: 10.46
     rank 17: 10.57
     rank 18: 10.22
     rank 19: 9.90
  forward-send-backward-recv:
     rank  0: 4093.43
     rank  1: 4097.20
     rank  2: 4097.31
     rank  3: 4084.26
     rank  4: 855.05
     rank  5: 855.35
     rank  6: 853.87
     rank  7: 857.08
     rank  8: 729.57
     rank  9: 726.45
     rank 10: 727.66
     rank 11: 732.11
     rank 12: 526.57
     rank 13: 528.35
     rank 14: 526.76
     rank 15: 524.18
  backward-send-forward-recv:
     rank  4: 73.02
     rank  5: 73.42
     rank  6: 73.67
     rank  7: 63.95
     rank  8: 145.78
     rank  9: 145.73
     rank 10: 143.77
     rank 11: 143.36
     rank 12: 157.62
     rank 13: 157.02
     rank 14: 152.22
     rank 15: 157.18
     rank 16: 167.87
     rank 17: 169.04
     rank 18: 165.39
     rank 19: 165.55
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.64
     rank  2: 0.67
     rank  3: 0.63
     rank  4: 2.31
     rank  5: 2.28
     rank  6: 2.33
     rank  7: 2.29
     rank  8: 2.20
     rank  9: 2.17
     rank 10: 2.14
     rank 11: 2.22
     rank 12: 2.14
     rank 13: 2.15
     rank 14: 2.16
     rank 15: 2.15
     rank 16: 2.45
     rank 17: 2.47
     rank 18: 2.49
     rank 19: 2.42
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.17
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.05
     rank 11: 0.07
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.16
     rank  1: 10.19
     rank  2: 10.96
     rank  3: 10.16
     rank  4: 46.25
     rank  5: 46.27
     rank  6: 46.26
     rank  7: 46.29
     rank  8: 43.05
     rank  9: 43.01
     rank 10: 42.96
     rank 11: 42.98
     rank 12: 42.98
     rank 13: 42.95
     rank 14: 42.96
     rank 15: 42.98
     rank 16: 46.30
     rank 17: 46.38
     rank 18: 46.44
     rank 19: 46.39
  optimizer:
     rank  0: 10.75
     rank  1: 10.78
     rank  2: 11.65
     rank  3: 10.75
     rank  4: 46.84
     rank  5: 46.86
     rank  6: 46.85
     rank  7: 46.88
     rank  8: 43.64
     rank  9: 43.60
     rank 10: 43.55
     rank 11: 43.57
     rank 12: 43.58
     rank 13: 43.54
     rank 14: 43.55
     rank 15: 43.57
     rank 16: 46.89
     rank 17: 46.97
     rank 18: 47.04
     rank 19: 46.98
 [2024-12-02 16:41:32] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 8045.5 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 7.008098E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7968.91
     rank  1: 7968.90
     rank  2: 7968.95
     rank  3: 7968.93
     rank  4: 7972.37
     rank  5: 7972.36
     rank  6: 7972.43
     rank  7: 7972.38
     rank  8: 7972.05
     rank  9: 7972.09
     rank 10: 7972.11
     rank 11: 7972.08
     rank 12: 7972.10
     rank 13: 7972.06
     rank 14: 7972.14
     rank 15: 7972.09
     rank 16: 7972.34
     rank 17: 7972.35
     rank 18: 7972.34
     rank 19: 7972.35
  forward-compute:
     rank  0: 1017.55
     rank  1: 1017.94
     rank  2: 1023.49
     rank  3: 1020.66
     rank  4: 2891.36
     rank  5: 2892.26
     rank  6: 2894.86
     rank  7: 2902.92
     rank  8: 2788.53
     rank  9: 2790.46
     rank 10: 2791.84
     rank 11: 2790.78
     rank 12: 2808.35
     rank 13: 2808.53
     rank 14: 2814.73
     rank 15: 2811.01
     rank 16: 2914.97
     rank 17: 2915.19
     rank 18: 2919.48
     rank 19: 2919.27
  backward-compute:
     rank  0: 1059.65
     rank  1: 1059.03
     rank  2: 1057.72
     rank  3: 1071.66
     rank  4: 3337.95
     rank  5: 3338.71
     rank  6: 3340.35
     rank  7: 3335.95
     rank  8: 3303.68
     rank  9: 3306.59
     rank 10: 3308.76
     rank 11: 3304.96
     rank 12: 3306.42
     rank 13: 3304.34
     rank 14: 3306.60
     rank 15: 3310.15
     rank 16: 3512.18
     rank 17: 3511.70
     rank 18: 3512.52
     rank 19: 3513.04
  pure-backward-compute:
     rank  0: 1058.95
     rank  1: 1058.42
     rank  2: 1057.10
     rank  3: 1070.95
     rank  4: 3336.98
     rank  5: 3337.72
     rank  6: 3339.25
     rank  7: 3335.15
     rank  8: 3302.67
     rank  9: 3305.51
     rank 10: 3307.98
     rank 11: 3303.92
     rank 12: 3305.11
     rank 13: 3303.31
     rank 14: 3305.78
     rank 15: 3309.30
     rank 16: 3509.92
     rank 17: 3509.57
     rank 18: 3510.77
     rank 19: 3511.45
  batch-generator:
     rank  0: 55.26
     rank  1: 55.80
     rank  2: 65.19
     rank  3: 62.43
     rank  4: 54.45
     rank  5: 64.66
     rank  6: 69.44
     rank  7: 76.38
     rank  8: 51.56
     rank  9: 54.18
     rank 10: 56.47
     rank 11: 57.99
     rank 12: 67.19
     rank 13: 69.80
     rank 14: 72.90
     rank 15: 67.02
     rank 16: 52.05
     rank 17: 55.20
     rank 18: 58.06
     rank 19: 59.16
  forward-recv:
     rank  4: 83.67
     rank  5: 83.49
     rank  6: 80.97
     rank  7: 81.72
     rank  8: 280.05
     rank  9: 279.99
     rank 10: 279.36
     rank 11: 279.01
     rank 12: 455.34
     rank 13: 455.75
     rank 14: 454.84
     rank 15: 455.17
     rank 16: 630.01
     rank 17: 629.70
     rank 18: 629.70
     rank 19: 629.88
  forward-send:
     rank  0: 399.67
     rank  1: 399.30
     rank  2: 393.51
     rank  3: 394.75
     rank  4: 32.17
     rank  5: 31.84
     rank  6: 29.29
     rank  7: 30.54
     rank  8: 21.18
     rank  9: 21.12
     rank 10: 19.85
     rank 11: 20.64
     rank 12: 10.56
     rank 13: 10.30
     rank 14: 10.17
     rank 15: 10.40
  backward-recv:
     rank  0: 1390.36
     rank  1: 1391.00
     rank  2: 1391.38
     rank  3: 1386.82
     rank  4: 593.68
     rank  5: 593.73
     rank  6: 593.85
     rank  7: 595.74
     rank  8: 393.16
     rank  9: 391.74
     rank 10: 391.83
     rank 11: 392.98
     rank 12: 194.72
     rank 13: 195.16
     rank 14: 195.39
     rank 15: 194.87
  backward-send:
     rank  4: 22.07
     rank  5: 22.50
     rank  6: 22.41
     rank  7: 19.69
     rank  8: 30.63
     rank  9: 31.24
     rank 10: 31.21
     rank 11: 30.80
     rank 12: 21.05
     rank 13: 20.44
     rank 14: 20.29
     rank 15: 20.81
     rank 16: 10.51
     rank 17: 10.53
     rank 18: 10.34
     rank 19: 10.10
  forward-send-backward-recv:
     rank  0: 4085.37
     rank  1: 4087.59
     rank  2: 4089.17
     rank  3: 4078.05
     rank  4: 852.81
     rank  5: 853.46
     rank  6: 852.66
     rank  7: 856.00
     rank  8: 708.41
     rank  9: 704.60
     rank 10: 705.93
     rank 11: 710.60
     rank 12: 500.31
     rank 13: 503.33
     rank 14: 502.72
     rank 15: 499.77
  backward-send-forward-recv:
     rank  4: 72.47
     rank  5: 72.65
     rank  6: 72.86
     rank  7: 65.48
     rank  8: 145.91
     rank  9: 146.08
     rank 10: 145.37
     rank 11: 143.90
     rank 12: 157.21
     rank 13: 156.42
     rank 14: 152.40
     rank 15: 155.49
     rank 16: 168.72
     rank 17: 169.63
     rank 18: 166.30
     rank 19: 166.00
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.65
     rank  1: 0.63
     rank  2: 0.63
     rank  3: 0.63
     rank  4: 2.32
     rank  5: 2.33
     rank  6: 2.31
     rank  7: 2.30
     rank  8: 2.14
     rank  9: 2.19
     rank 10: 2.14
     rank 11: 2.15
     rank 12: 2.21
     rank 13: 2.15
     rank 14: 2.22
     rank 15: 2.19
     rank 16: 2.44
     rank 17: 2.44
     rank 18: 2.47
     rank 19: 2.45
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.17
     rank  1: 10.30
     rank  2: 10.43
     rank  3: 10.20
     rank  4: 46.26
     rank  5: 46.38
     rank  6: 46.25
     rank  7: 46.27
     rank  8: 42.99
     rank  9: 43.09
     rank 10: 42.91
     rank 11: 42.98
     rank 12: 43.06
     rank 13: 42.96
     rank 14: 43.03
     rank 15: 43.06
     rank 16: 46.37
     rank 17: 46.29
     rank 18: 46.45
     rank 19: 46.47
  optimizer:
     rank  0: 10.86
     rank  1: 10.98
     rank  2: 11.11
     rank  3: 10.89
     rank  4: 46.95
     rank  5: 47.06
     rank  6: 46.94
     rank  7: 46.95
     rank  8: 43.68
     rank  9: 43.78
     rank 10: 43.60
     rank 11: 43.67
     rank 12: 43.76
     rank 13: 43.64
     rank 14: 43.72
     rank 15: 43.75
     rank 16: 47.06
     rank 17: 46.98
     rank 18: 47.14
     rank 19: 47.16
 [2024-12-02 16:41:40] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 8030.7 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.324623E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7949.63
     rank  1: 7949.68
     rank  2: 7949.75
     rank  3: 7949.63
     rank  4: 7953.09
     rank  5: 7953.13
     rank  6: 7953.24
     rank  7: 7953.08
     rank  8: 7952.79
     rank  9: 7952.85
     rank 10: 7952.90
     rank 11: 7952.82
     rank 12: 7952.76
     rank 13: 7952.82
     rank 14: 7952.90
     rank 15: 7952.81
     rank 16: 7953.12
     rank 17: 7953.12
     rank 18: 7953.18
     rank 19: 7953.11
  forward-compute:
     rank  0: 1017.90
     rank  1: 1019.21
     rank  2: 1019.50
     rank  3: 1027.10
     rank  4: 2891.47
     rank  5: 2895.02
     rank  6: 2892.41
     rank  7: 2913.87
     rank  8: 2794.78
     rank  9: 2796.06
     rank 10: 2796.96
     rank 11: 2798.70
     rank 12: 2792.71
     rank 13: 2792.65
     rank 14: 2797.99
     rank 15: 2796.33
     rank 16: 2913.21
     rank 17: 2912.49
     rank 18: 2913.91
     rank 19: 2917.84
  backward-compute:
     rank  0: 1063.93
     rank  1: 1064.64
     rank  2: 1063.79
     rank  3: 1079.96
     rank  4: 3339.46
     rank  5: 3339.05
     rank  6: 3340.78
     rank  7: 3337.40
     rank  8: 3298.26
     rank  9: 3300.46
     rank 10: 3302.64
     rank 11: 3298.52
     rank 12: 3304.57
     rank 13: 3302.25
     rank 14: 3303.66
     rank 15: 3308.88
     rank 16: 3501.52
     rank 17: 3501.00
     rank 18: 3502.95
     rank 19: 3502.54
  pure-backward-compute:
     rank  0: 1063.27
     rank  1: 1064.03
     rank  2: 1063.16
     rank  3: 1079.21
     rank  4: 3338.57
     rank  5: 3338.21
     rank  6: 3339.74
     rank  7: 3336.76
     rank  8: 3297.16
     rank  9: 3299.29
     rank 10: 3301.87
     rank 11: 3297.57
     rank 12: 3303.45
     rank 13: 3301.24
     rank 14: 3302.85
     rank 15: 3308.16
     rank 16: 3499.33
     rank 17: 3498.85
     rank 18: 3500.94
     rank 19: 3500.86
  batch-generator:
     rank  0: 52.66
     rank  1: 60.42
     rank  2: 60.01
     rank  3: 71.28
     rank  4: 50.71
     rank  5: 60.90
     rank  6: 66.13
     rank  7: 86.35
     rank  8: 51.55
     rank  9: 53.56
     rank 10: 55.44
     rank 11: 60.27
     rank 12: 53.06
     rank 13: 57.82
     rank 14: 61.65
     rank 15: 57.88
     rank 16: 51.83
     rank 17: 54.01
     rank 18: 54.95
     rank 19: 59.45
  forward-recv:
     rank  4: 83.96
     rank  5: 82.73
     rank  6: 82.76
     rank  7: 77.37
     rank  8: 281.09
     rank  9: 280.84
     rank 10: 281.18
     rank 11: 279.81
     rank 12: 455.53
     rank 13: 455.98
     rank 14: 455.76
     rank 15: 454.35
     rank 16: 630.01
     rank 17: 629.89
     rank 18: 630.02
     rank 19: 629.99
  forward-send:
     rank  0: 396.12
     rank  1: 394.31
     rank  2: 394.09
     rank  3: 384.52
     rank  4: 31.52
     rank  5: 31.15
     rank  6: 31.01
     rank  7: 28.05
     rank  8: 20.86
     rank  9: 20.94
     rank 10: 20.51
     rank 11: 19.31
     rank 12: 10.54
     rank 13: 10.38
     rank 14: 10.26
     rank 15: 10.41
  backward-recv:
     rank  0: 1382.23
     rank  1: 1382.37
     rank  2: 1382.24
     rank  3: 1379.53
     rank  4: 591.18
     rank  5: 590.48
     rank  6: 590.65
     rank  7: 593.49
     rank  8: 392.39
     rank  9: 391.50
     rank 10: 392.24
     rank 11: 393.11
     rank 12: 197.12
     rank 13: 197.39
     rank 14: 197.91
     rank 15: 196.85
  backward-send:
     rank  4: 22.38
     rank  5: 22.64
     rank  6: 22.49
     rank  7: 19.87
     rank  8: 31.30
     rank  9: 31.59
     rank 10: 31.05
     rank 11: 30.62
     rank 12: 20.98
     rank 13: 20.55
     rank 14: 20.30
     rank 15: 20.80
     rank 16: 10.47
     rank 17: 10.51
     rank 18: 10.54
     rank 19: 9.89
  forward-send-backward-recv:
     rank  0: 4072.73
     rank  1: 4074.89
     rank  2: 4075.90
     rank  3: 4062.11
     rank  4: 835.77
     rank  5: 836.40
     rank  6: 835.03
     rank  7: 838.37
     rank  8: 690.02
     rank  9: 687.08
     rank 10: 687.98
     rank 11: 692.97
     rank 12: 501.35
     rank 13: 503.02
     rank 14: 504.08
     rank 15: 499.60
  backward-send-forward-recv:
     rank  4: 72.81
     rank  5: 73.10
     rank  6: 73.29
     rank  7: 62.67
     rank  8: 145.86
     rank  9: 146.11
     rank 10: 144.70
     rank 11: 143.71
     rank 12: 156.72
     rank 13: 155.86
     rank 14: 151.89
     rank 15: 155.00
     rank 16: 167.58
     rank 17: 168.98
     rank 18: 167.22
     rank 19: 164.32
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.11
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.67
     rank  2: 0.66
     rank  3: 0.63
     rank  4: 2.38
     rank  5: 2.31
     rank  6: 2.38
     rank  7: 2.29
     rank  8: 2.15
     rank  9: 2.22
     rank 10: 2.14
     rank 11: 2.21
     rank 12: 2.16
     rank 13: 2.16
     rank 14: 2.17
     rank 15: 2.16
     rank 16: 2.44
     rank 17: 2.45
     rank 18: 2.67
     rank 19: 2.63
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.17
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.08
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.09
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.33
     rank  1: 10.17
     rank  2: 10.27
     rank  3: 10.30
     rank  4: 46.29
     rank  5: 46.30
     rank  6: 46.28
     rank  7: 46.28
     rank  8: 42.98
     rank  9: 43.11
     rank 10: 43.01
     rank 11: 43.03
     rank 12: 42.98
     rank 13: 42.96
     rank 14: 42.96
     rank 15: 42.98
     rank 16: 46.32
     rank 17: 46.32
     rank 18: 46.53
     rank 19: 46.38
  optimizer:
     rank  0: 10.98
     rank  1: 10.82
     rank  2: 10.91
     rank  3: 10.95
     rank  4: 46.94
     rank  5: 46.94
     rank  6: 46.92
     rank  7: 46.92
     rank  8: 43.63
     rank  9: 43.76
     rank 10: 43.65
     rank 11: 43.68
     rank 12: 43.62
     rank 13: 43.60
     rank 14: 43.60
     rank 15: 43.63
     rank 16: 46.96
     rank 17: 46.97
     rank 18: 47.18
     rank 19: 47.02
