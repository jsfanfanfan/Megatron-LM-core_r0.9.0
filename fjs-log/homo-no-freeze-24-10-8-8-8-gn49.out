examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 22:14:08,091] torch.distributed.run: [WARNING] 
[2024-12-02 22:14:08,091] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 22:14:08,091] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 22:14:08,091] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0][rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.output_layer.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True


name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True


name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True


name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 81.25
     rank  1: 117.05
     rank  2: 116.86
     rank  3: 115.65
     rank  4: 65.20
     rank  5: 66.99
     rank  6: 65.17
     rank  7: 65.07
     rank  8: 33.40
     rank  9: 33.44
     rank 10: 44.98
     rank 11: 33.46
     rank 12: 33.26
     rank 13: 46.10
     rank 14: 33.30
     rank 15: 46.10
     rank 16: 47.77
     rank 17: 47.50
     rank 18: 30.37
     rank 19: 47.71
  train/valid/test-data-iterators-setup:
     rank  0: 798.53
     rank  1: 798.45
     rank  2: 798.45
     rank  3: 798.49
     rank  4: 1108.22
     rank  5: 1108.25
     rank  6: 1109.08
     rank  7: 1108.32
     rank  8: 1108.40
     rank  9: 1108.37
     rank 10: 1108.39
     rank 11: 1108.65
     rank 12: 1108.46
     rank 13: 1108.68
     rank 14: 1108.39
     rank 15: 1108.40
     rank 16: 1167.53
     rank 17: 1167.46
     rank 18: 1167.47
     rank 19: 1167.39
 [2024-12-02 22:14:52] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22124.8 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10126.0 | max reserved: 10126.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10134.0 | max reserved: 10134.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
times across ranks (ms):
  forward-backward:
     rank  0: 22056.41
     rank  1: 22056.36
     rank  2: 22056.37
     rank  3: 22056.45
     rank  4: 22060.01
     rank  5: 22059.90
     rank  6: 22059.86
     rank  7: 22059.97
     rank  8: 22059.57
     rank  9: 22059.59
     rank 10: 22059.55
     rank 11: 22059.69
     rank 12: 22059.85
     rank 13: 22059.75
     rank 14: 22059.33
     rank 15: 22059.50
     rank 16: 22059.46
     rank 17: 22059.40
     rank 18: 22059.55
     rank 19: 22059.65
  forward-compute:
     rank  0: 4937.84
     rank  1: 4953.41
     rank  2: 4960.39
     rank  3: 4971.29
     rank  4: 5253.47
     rank  5: 5260.36
     rank  6: 5264.71
     rank  7: 5273.78
     rank  8: 5112.00
     rank  9: 5118.97
     rank 10: 5120.48
     rank 11: 5136.01
     rank 12: 5295.17
     rank 13: 5304.61
     rank 14: 5298.23
     rank 15: 5311.58
     rank 16: 5167.75
     rank 17: 5178.62
     rank 18: 5173.90
     rank 19: 5178.80
  backward-compute:
     rank  0: 1974.22
     rank  1: 1960.43
     rank  2: 1960.70
     rank  3: 1960.76
     rank  4: 3409.28
     rank  5: 3410.08
     rank  6: 3413.13
     rank  7: 3410.75
     rank  8: 3294.60
     rank  9: 3302.59
     rank 10: 3298.65
     rank 11: 3297.49
     rank 12: 3292.18
     rank 13: 3298.31
     rank 14: 3298.55
     rank 15: 3293.69
     rank 16: 3513.84
     rank 17: 3512.76
     rank 18: 3513.99
     rank 19: 3512.43
  pure-backward-compute:
     rank  0: 1973.38
     rank  1: 1959.41
     rank  2: 1959.74
     rank  3: 1959.84
     rank  4: 3408.16
     rank  5: 3409.05
     rank  6: 3412.13
     rank  7: 3409.70
     rank  8: 3293.41
     rank  9: 3301.63
     rank 10: 3297.43
     rank 11: 3296.41
     rank 12: 3290.59
     rank 13: 3296.39
     rank 14: 3297.50
     rank 15: 3292.86
     rank 16: 3511.03
     rank 17: 3511.00
     rank 18: 3512.21
     rank 19: 3508.78
  batch-generator:
     rank  0: 1146.77
     rank  1: 1166.02
     rank  2: 1173.37
     rank  3: 1182.89
     rank  4: 943.50
     rank  5: 948.13
     rank  6: 956.32
     rank  7: 970.26
     rank  8: 1276.59
     rank  9: 1284.80
     rank 10: 1289.13
     rank 11: 1301.39
     rank 12: 1212.10
     rank 13: 1225.32
     rank 14: 1219.14
     rank 15: 1229.59
     rank 16: 1175.90
     rank 17: 1185.01
     rank 18: 1181.77
     rank 19: 1191.11
  forward-recv:
     rank  4: 4004.71
     rank  5: 4009.63
     rank  6: 4001.86
     rank  7: 4003.65
     rank  8: 6492.35
     rank  9: 6490.63
     rank 10: 6486.47
     rank 11: 6485.66
     rank 12: 9022.26
     rank 13: 9018.69
     rank 14: 9021.13
     rank 15: 9016.57
     rank 16: 11737.13
     rank 17: 11735.84
     rank 18: 11737.94
     rank 19: 11727.81
  forward-send:
     rank  0: 7681.44
     rank  1: 7678.17
     rank  2: 7672.54
     rank  3: 7658.99
     rank  4: 4920.24
     rank  5: 4912.08
     rank  6: 4912.85
     rank  7: 4898.21
     rank  8: 2573.12
     rank  9: 2567.26
     rank 10: 2572.82
     rank 11: 2557.96
     rank 12: 32.44
     rank 13: 31.04
     rank 14: 33.46
     rank 15: 23.41
  backward-recv:
     rank  0: 1381.61
     rank  1: 1382.00
     rank  2: 1381.63
     rank  3: 1381.17
     rank  4: 605.83
     rank  5: 604.99
     rank  6: 605.77
     rank  7: 605.95
     rank  8: 399.79
     rank  9: 400.56
     rank 10: 399.56
     rank 11: 399.97
     rank 12: 199.44
     rank 13: 198.91
     rank 14: 199.54
     rank 15: 199.54
  backward-send:
     rank  4: 4.18
     rank  5: 4.64
     rank  6: 4.38
     rank  7: 3.71
     rank  8: 31.27
     rank  9: 30.88
     rank 10: 31.35
     rank 11: 30.81
     rank 12: 20.79
     rank 13: 21.07
     rank 14: 20.40
     rank 15: 20.27
     rank 16: 10.40
     rank 17: 9.82
     rank 18: 10.06
     rank 19: 10.48
  forward-send-backward-recv:
     rank  0: 6015.75
     rank  1: 6017.00
     rank  2: 6016.20
     rank  3: 6019.08
     rank  4: 3053.65
     rank  5: 3052.52
     rank  6: 3051.03
     rank  7: 3054.63
     rank  8: 2905.66
     rank  9: 2901.23
     rank 10: 2901.50
     rank 11: 2904.45
     rank 12: 2743.78
     rank 13: 2736.06
     rank 14: 2741.47
     rank 15: 2746.56
  backward-send-forward-recv:
     rank  4: 678.63
     rank  5: 678.87
     rank  6: 678.82
     rank  7: 680.38
     rank  8: 911.29
     rank  9: 910.66
     rank 10: 908.84
     rank 11: 909.26
     rank 12: 896.89
     rank 13: 892.11
     rank 14: 893.11
     rank 15: 895.03
     rank 16: 859.11
     rank 17: 853.59
     rank 18: 855.11
     rank 19: 856.88
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.06
     rank  4: 0.13
     rank  5: 0.09
     rank  6: 0.09
     rank  7: 0.13
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.06
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.09
     rank 19: 0.09
  all-grads-sync:
     rank  0: 43.63
     rank  1: 47.09
     rank  2: 45.12
     rank  3: 48.12
     rank  4: 45.72
     rank  5: 46.65
     rank  6: 41.29
     rank  7: 44.84
     rank  8: 45.95
     rank  9: 44.41
     rank 10: 42.24
     rank 11: 50.63
     rank 12: 43.85
     rank 13: 44.89
     rank 14: 42.99
     rank 15: 42.85
     rank 16: 2.76
     rank 17: 2.61
     rank 18: 3.65
     rank 19: 3.20
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.27
     rank  2: 0.27
     rank  3: 0.28
     rank  4: 0.09
     rank  5: 0.09
     rank  6: 0.09
     rank  7: 0.08
     rank  8: 0.09
     rank  9: 0.13
     rank 10: 0.06
     rank 11: 0.10
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.06
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.09
     rank 19: 0.12
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 29.01
     rank  1: 28.33
     rank  2: 27.29
     rank  3: 29.26
     rank  4: 59.92
     rank  5: 60.13
     rank  6: 59.90
     rank  7: 60.24
     rank  8: 52.94
     rank  9: 52.99
     rank 10: 52.53
     rank 11: 53.20
     rank 12: 51.75
     rank 13: 52.09
     rank 14: 51.76
     rank 15: 51.65
     rank 16: 56.77
     rank 17: 56.35
     rank 18: 56.59
     rank 19: 57.14
  optimizer:
     rank  0: 30.13
     rank  1: 29.45
     rank  2: 28.42
     rank  3: 30.37
     rank  4: 61.06
     rank  5: 61.26
     rank  6: 61.04
     rank  7: 61.37
     rank  8: 54.06
     rank  9: 54.12
     rank 10: 53.67
     rank 11: 54.33
     rank 12: 52.88
     rank 13: 53.22
     rank 14: 52.89
     rank 15: 52.79
     rank 16: 57.95
     rank 17: 57.48
     rank 18: 57.73
     rank 19: 58.28
 [2024-12-02 22:15:00] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 8050.3 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7962.72
     rank  1: 7962.66
     rank  2: 7962.98
     rank  3: 7962.74
     rank  4: 7966.51
     rank  5: 7966.44
     rank  6: 7966.79
     rank  7: 7966.54
     rank  8: 7966.01
     rank  9: 7965.98
     rank 10: 7966.34
     rank 11: 7966.07
     rank 12: 7966.06
     rank 13: 7966.03
     rank 14: 7966.36
     rank 15: 7966.01
     rank 16: 7966.27
     rank 17: 7966.25
     rank 18: 7967.05
     rank 19: 7966.21
  forward-compute:
     rank  0: 993.59
     rank  1: 992.68
     rank  2: 993.32
     rank  3: 995.35
     rank  4: 3005.07
     rank  5: 3012.97
     rank  6: 3010.47
     rank  7: 3007.63
     rank  8: 2772.38
     rank  9: 2770.78
     rank 10: 2774.11
     rank 11: 2776.02
     rank 12: 2765.17
     rank 13: 2765.52
     rank 14: 2768.89
     rank 15: 2767.39
     rank 16: 2916.95
     rank 17: 2926.09
     rank 18: 2921.02
     rank 19: 2916.68
  backward-compute:
     rank  0: 1064.19
     rank  1: 1069.45
     rank  2: 1068.48
     rank  3: 1066.71
     rank  4: 3396.17
     rank  5: 3396.15
     rank  6: 3398.26
     rank  7: 3397.41
     rank  8: 3271.16
     rank  9: 3271.47
     rank 10: 3274.69
     rank 11: 3273.10
     rank 12: 3273.77
     rank 13: 3280.21
     rank 14: 3276.61
     rank 15: 3273.28
     rank 16: 3518.72
     rank 17: 3520.28
     rank 18: 3521.82
     rank 19: 3519.98
  pure-backward-compute:
     rank  0: 1063.36
     rank  1: 1068.75
     rank  2: 1067.74
     rank  3: 1066.03
     rank  4: 3394.51
     rank  5: 3395.38
     rank  6: 3397.29
     rank  7: 3396.29
     rank  8: 3270.03
     rank  9: 3270.59
     rank 10: 3273.61
     rank 11: 3271.95
     rank 12: 3272.33
     rank 13: 3278.64
     rank 14: 3275.77
     rank 15: 3272.50
     rank 16: 3516.13
     rank 17: 3518.73
     rank 18: 3519.49
     rank 19: 3517.51
  batch-generator:
     rank  0: 56.35
     rank  1: 57.99
     rank  2: 58.64
     rank  3: 60.96
     rank  4: 90.95
     rank  5: 88.62
     rank  6: 90.96
     rank  7: 93.78
     rank  8: 54.05
     rank  9: 54.64
     rank 10: 58.96
     rank 11: 58.86
     rank 12: 52.21
     rank 13: 56.21
     rank 14: 59.47
     rank 15: 58.32
     rank 16: 53.70
     rank 17: 64.04
     rank 18: 59.98
     rank 19: 57.48
  forward-recv:
     rank  4: 63.30
     rank  5: 63.23
     rank  6: 63.09
     rank  7: 62.49
     rank  8: 282.46
     rank  9: 284.26
     rank 10: 282.34
     rank 11: 281.89
     rank 12: 452.16
     rank 13: 450.94
     rank 14: 453.28
     rank 15: 452.76
     rank 16: 622.81
     rank 17: 622.89
     rank 18: 622.12
     rank 19: 622.35
  forward-send:
     rank  0: 399.32
     rank  1: 399.19
     rank  2: 398.49
     rank  3: 397.67
     rank  4: 31.61
     rank  5: 31.37
     rank  6: 31.20
     rank  7: 30.70
     rank  8: 20.93
     rank  9: 19.48
     rank 10: 20.98
     rank 11: 20.71
     rank 12: 10.52
     rank 13: 10.51
     rank 14: 9.69
     rank 15: 10.02
  backward-recv:
     rank  0: 1384.59
     rank  1: 1384.36
     rank  2: 1383.60
     rank  3: 1383.77
     rank  4: 593.30
     rank  5: 594.36
     rank  6: 595.12
     rank  7: 594.88
     rank  8: 400.33
     rank  9: 400.59
     rank 10: 399.84
     rank 11: 401.54
     rank 12: 196.94
     rank 13: 195.64
     rank 14: 197.25
     rank 15: 197.02
  backward-send:
     rank  4: 5.16
     rank  5: 4.22
     rank  6: 4.83
     rank  7: 3.62
     rank  8: 31.35
     rank  9: 30.98
     rank 10: 32.62
     rank 11: 30.31
     rank 12: 20.68
     rank 13: 20.80
     rank 14: 20.23
     rank 15: 20.80
     rank 16: 10.39
     rank 17: 9.36
     rank 18: 10.09
     rank 19: 10.37
  forward-send-backward-recv:
     rank  0: 4103.41
     rank  1: 4101.16
     rank  2: 4103.10
     rank  3: 4102.01
     rank  4: 765.51
     rank  5: 765.44
     rank  6: 762.73
     rank  7: 765.25
     rank  8: 664.02
     rank  9: 667.05
     rank 10: 660.78
     rank 11: 662.24
     rank 12: 577.64
     rank 13: 570.83
     rank 14: 578.45
     rank 15: 582.40
  backward-send-forward-recv:
     rank  4: 20.76
     rank  5: 17.09
     rank  6: 18.37
     rank  7: 20.12
     rank  8: 227.28
     rank  9: 227.73
     rank 10: 225.69
     rank 11: 226.22
     rank 12: 156.37
     rank 13: 157.84
     rank 14: 152.60
     rank 15: 153.76
     rank 16: 166.97
     rank 17: 159.45
     rank 18: 163.00
     rank 19: 167.19
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.07
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.18
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.74
     rank  1: 0.63
     rank  2: 0.59
     rank  3: 0.67
     rank  4: 2.48
     rank  5: 2.40
     rank  6: 2.45
     rank  7: 2.48
     rank  8: 2.17
     rank  9: 2.20
     rank 10: 2.15
     rank 11: 2.24
     rank 12: 2.35
     rank 13: 2.45
     rank 14: 2.21
     rank 15: 2.16
     rank 16: 2.50
     rank 17: 2.45
     rank 18: 3.21
     rank 19: 2.48
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.23
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.10
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.14
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.43
     rank  1: 8.42
     rank  2: 8.40
     rank  3: 8.57
     rank  4: 48.09
     rank  5: 48.14
     rank  6: 48.14
     rank  7: 48.11
     rank  8: 42.95
     rank  9: 43.02
     rank 10: 42.98
     rank 11: 43.01
     rank 12: 43.06
     rank 13: 43.14
     rank 14: 43.05
     rank 15: 42.97
     rank 16: 46.40
     rank 17: 46.30
     rank 18: 47.43
     rank 19: 46.44
  optimizer:
     rank  0: 9.74
     rank  1: 9.72
     rank  2: 9.71
     rank  3: 9.87
     rank  4: 49.40
     rank  5: 49.45
     rank  6: 49.45
     rank  7: 49.42
     rank  8: 44.26
     rank  9: 44.33
     rank 10: 44.29
     rank 11: 44.33
     rank 12: 44.37
     rank 13: 44.45
     rank 14: 44.36
     rank 15: 44.28
     rank 16: 47.71
     rank 17: 47.61
     rank 18: 48.73
     rank 19: 47.75
 [2024-12-02 22:15:08] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 8068.1 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 2.732755E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7983.46
     rank  1: 7983.73
     rank  2: 7983.49
     rank  3: 7983.49
     rank  4: 7987.28
     rank  5: 7987.51
     rank  6: 7987.29
     rank  7: 7987.33
     rank  8: 7986.80
     rank  9: 7987.06
     rank 10: 7986.81
     rank 11: 7986.90
     rank 12: 7986.95
     rank 13: 7987.11
     rank 14: 7986.82
     rank 15: 7986.88
     rank 16: 7987.05
     rank 17: 7987.79
     rank 18: 7987.07
     rank 19: 7987.05
  forward-compute:
     rank  0: 1084.10
     rank  1: 1083.35
     rank  2: 1085.04
     rank  3: 1084.84
     rank  4: 3013.09
     rank  5: 3021.96
     rank  6: 3022.16
     rank  7: 3012.94
     rank  8: 2776.35
     rank  9: 2776.45
     rank 10: 2780.10
     rank 11: 2779.23
     rank 12: 2763.96
     rank 13: 2764.97
     rank 14: 2771.09
     rank 15: 2763.09
     rank 16: 2930.87
     rank 17: 2941.08
     rank 18: 2935.60
     rank 19: 2931.09
  backward-compute:
     rank  0: 1051.27
     rank  1: 1058.21
     rank  2: 1059.11
     rank  3: 1053.24
     rank  4: 3396.27
     rank  5: 3397.53
     rank  6: 3400.41
     rank  7: 3397.73
     rank  8: 3278.25
     rank  9: 3279.84
     rank 10: 3284.96
     rank 11: 3277.29
     rank 12: 3299.08
     rank 13: 3304.62
     rank 14: 3301.55
     rank 15: 3299.98
     rank 16: 3528.81
     rank 17: 3530.13
     rank 18: 3530.56
     rank 19: 3529.43
  pure-backward-compute:
     rank  0: 1050.53
     rank  1: 1057.54
     rank  2: 1058.46
     rank  3: 1052.60
     rank  4: 3394.57
     rank  5: 3396.76
     rank  6: 3399.56
     rank  7: 3396.15
     rank  8: 3276.99
     rank  9: 3278.63
     rank 10: 3283.82
     rank 11: 3276.30
     rank 12: 3297.76
     rank 13: 3303.19
     rank 14: 3300.73
     rank 15: 3299.24
     rank 16: 3526.25
     rank 17: 3528.52
     rank 18: 3528.38
     rank 19: 3526.71
  batch-generator:
     rank  0: 54.12
     rank  1: 55.90
     rank  2: 59.42
     rank  3: 59.03
     rank  4: 92.22
     rank  5: 91.41
     rank  6: 94.05
     rank  7: 98.87
     rank  8: 55.09
     rank  9: 57.26
     rank 10: 61.63
     rank 11: 59.00
     rank 12: 57.93
     rank 13: 61.58
     rank 14: 67.36
     rank 15: 59.27
     rank 16: 58.35
     rank 17: 70.71
     rank 18: 64.69
     rank 19: 62.39
  forward-recv:
     rank  4: 60.35
     rank  5: 60.32
     rank  6: 59.91
     rank  7: 59.96
     rank  8: 280.41
     rank  9: 281.73
     rank 10: 280.10
     rank 11: 280.47
     rank 12: 451.95
     rank 13: 450.80
     rank 14: 452.74
     rank 15: 452.77
     rank 16: 618.68
     rank 17: 618.66
     rank 18: 618.25
     rank 19: 618.08
  forward-send:
     rank  0: 422.23
     rank  1: 422.23
     rank  2: 421.09
     rank  3: 421.13
     rank  4: 31.60
     rank  5: 31.39
     rank  6: 30.89
     rank  7: 31.12
     rank  8: 21.03
     rank  9: 20.08
     rank 10: 20.93
     rank 11: 20.63
     rank 12: 10.45
     rank 13: 10.50
     rank 14: 9.98
     rank 15: 9.75
  backward-recv:
     rank  0: 1391.78
     rank  1: 1391.34
     rank  2: 1391.18
     rank  3: 1391.48
     rank  4: 600.95
     rank  5: 603.77
     rank  6: 604.17
     rank  7: 601.30
     rank  8: 397.26
     rank  9: 396.59
     rank 10: 396.52
     rank 11: 398.33
     rank 12: 198.79
     rank 13: 198.89
     rank 14: 199.47
     rank 15: 199.01
  backward-send:
     rank  4: 5.31
     rank  5: 3.51
     rank  6: 3.61
     rank  7: 4.30
     rank  8: 31.27
     rank  9: 30.98
     rank 10: 31.31
     rank 11: 30.21
     rank 12: 21.00
     rank 13: 20.46
     rank 14: 20.09
     rank 15: 20.80
     rank 16: 10.61
     rank 17: 10.18
     rank 18: 10.28
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 4016.75
     rank  1: 4013.03
     rank  2: 4012.39
     rank  3: 4016.75
     rank  4: 773.15
     rank  5: 771.28
     rank  6: 769.17
     rank  7: 772.90
     rank  8: 727.43
     rank  9: 730.97
     rank 10: 721.63
     rank 11: 729.29
     rank 12: 572.15
     rank 13: 566.12
     rank 14: 573.26
     rank 15: 576.99
  backward-send-forward-recv:
     rank  4: 20.58
     rank  5: 15.67
     rank  6: 15.34
     rank  7: 20.64
     rank  8: 177.64
     rank  9: 175.91
     rank 10: 174.76
     rank 11: 177.27
     rank 12: 156.08
     rank 13: 156.74
     rank 14: 148.94
     rank 15: 155.26
     rank 16: 166.85
     rank 17: 158.52
     rank 18: 163.49
     rank 19: 166.48
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.06
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.16
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.60
     rank  2: 0.54
     rank  3: 0.57
     rank  4: 2.49
     rank  5: 2.43
     rank  6: 2.40
     rank  7: 2.49
     rank  8: 2.19
     rank  9: 2.19
     rank 10: 2.17
     rank 11: 2.17
     rank 12: 2.71
     rank 13: 2.29
     rank 14: 2.15
     rank 15: 2.20
     rank 16: 2.39
     rank 17: 3.01
     rank 18: 2.49
     rank 19: 2.44
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.17
     rank  2: 0.16
     rank  3: 0.21
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.10
     rank 14: 0.04
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.13
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.35
     rank  2: 8.30
     rank  3: 8.41
     rank  4: 48.11
     rank  5: 48.20
     rank  6: 48.07
     rank  7: 48.06
     rank  8: 43.07
     rank  9: 43.08
     rank 10: 42.96
     rank 11: 43.01
     rank 12: 43.25
     rank 13: 43.02
     rank 14: 42.94
     rank 15: 43.00
     rank 16: 46.31
     rank 17: 46.85
     rank 18: 46.47
     rank 19: 46.40
  optimizer:
     rank  0: 9.71
     rank  1: 9.55
     rank  2: 9.50
     rank  3: 9.61
     rank  4: 49.31
     rank  5: 49.41
     rank  6: 49.27
     rank  7: 49.26
     rank  8: 44.27
     rank  9: 44.28
     rank 10: 44.18
     rank 11: 44.21
     rank 12: 44.46
     rank 13: 44.23
     rank 14: 44.14
     rank 15: 44.20
     rank 16: 47.52
     rank 17: 48.04
     rank 18: 47.68
     rank 19: 47.61
 [2024-12-02 22:15:16] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 8075.9 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.380791E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7992.36
     rank  1: 7992.39
     rank  2: 7992.35
     rank  3: 7992.36
     rank  4: 7996.17
     rank  5: 7996.18
     rank  6: 7996.18
     rank  7: 7996.20
     rank  8: 7995.67
     rank  9: 7995.71
     rank 10: 7995.69
     rank 11: 7995.73
     rank 12: 7995.70
     rank 13: 7995.75
     rank 14: 7995.68
     rank 15: 7995.70
     rank 16: 7995.91
     rank 17: 7996.00
     rank 18: 7995.94
     rank 19: 7995.88
  forward-compute:
     rank  0: 962.46
     rank  1: 961.14
     rank  2: 961.25
     rank  3: 961.52
     rank  4: 3010.02
     rank  5: 3020.57
     rank  6: 3016.37
     rank  7: 3009.81
     rank  8: 2790.00
     rank  9: 2790.91
     rank 10: 2790.54
     rank 11: 2793.77
     rank 12: 2768.79
     rank 13: 2771.49
     rank 14: 2775.02
     rank 15: 2768.71
     rank 16: 2929.70
     rank 17: 2931.84
     rank 18: 2936.06
     rank 19: 2929.90
  backward-compute:
     rank  0: 1062.14
     rank  1: 1070.99
     rank  2: 1069.13
     rank  3: 1064.70
     rank  4: 3391.60
     rank  5: 3395.16
     rank  6: 3392.57
     rank  7: 3393.61
     rank  8: 3280.55
     rank  9: 3281.81
     rank 10: 3287.00
     rank 11: 3280.04
     rank 12: 3305.31
     rank 13: 3304.75
     rank 14: 3308.73
     rank 15: 3306.33
     rank 16: 3536.32
     rank 17: 3536.89
     rank 18: 3536.86
     rank 19: 3536.06
  pure-backward-compute:
     rank  0: 1061.39
     rank  1: 1070.31
     rank  2: 1068.38
     rank  3: 1064.07
     rank  4: 3389.74
     rank  5: 3394.47
     rank  6: 3391.52
     rank  7: 3392.06
     rank  8: 3279.45
     rank  9: 3281.09
     rank 10: 3285.67
     rank 11: 3279.24
     rank 12: 3304.08
     rank 13: 3303.57
     rank 14: 3307.84
     rank 15: 3305.27
     rank 16: 3533.95
     rank 17: 3534.45
     rank 18: 3535.09
     rank 19: 3533.86
  batch-generator:
     rank  0: 62.33
     rank  1: 63.30
     rank  2: 64.09
     rank  3: 64.02
     rank  4: 87.98
     rank  5: 89.99
     rank  6: 91.95
     rank  7: 94.70
     rank  8: 53.34
     rank  9: 57.01
     rank 10: 57.54
     rank 11: 58.21
     rank 12: 53.98
     rank 13: 60.57
     rank 14: 64.12
     rank 15: 56.69
     rank 16: 52.02
     rank 17: 56.06
     rank 18: 61.20
     rank 19: 56.48
  forward-recv:
     rank  4: 59.09
     rank  5: 59.51
     rank  6: 59.23
     rank  7: 58.90
     rank  8: 276.39
     rank  9: 278.82
     rank 10: 276.86
     rank 11: 276.54
     rank 12: 449.74
     rank 13: 447.98
     rank 14: 450.20
     rank 15: 450.50
     rank 16: 622.72
     rank 17: 622.67
     rank 18: 622.44
     rank 19: 622.20
  forward-send:
     rank  0: 423.52
     rank  1: 424.27
     rank  2: 423.95
     rank  3: 423.69
     rank  4: 31.06
     rank  5: 31.24
     rank  6: 31.28
     rank  7: 31.29
     rank  8: 20.71
     rank  9: 19.14
     rank 10: 20.86
     rank 11: 20.63
     rank 12: 10.58
     rank 13: 10.59
     rank 14: 10.12
     rank 15: 9.90
  backward-recv:
     rank  0: 1391.48
     rank  1: 1390.19
     rank  2: 1390.49
     rank  3: 1390.61
     rank  4: 602.86
     rank  5: 605.38
     rank  6: 604.77
     rank  7: 603.99
     rank  8: 396.91
     rank  9: 397.17
     rank 10: 396.35
     rank 11: 397.92
     rank 12: 194.93
     rank 13: 195.39
     rank 14: 195.23
     rank 15: 195.24
  backward-send:
     rank  4: 5.25
     rank  5: 3.21
     rank  6: 3.66
     rank  7: 4.21
     rank  8: 31.55
     rank  9: 30.70
     rank 10: 31.46
     rank 11: 30.37
     rank 12: 20.94
     rank 13: 20.53
     rank 14: 20.13
     rank 15: 20.75
     rank 16: 10.32
     rank 17: 10.59
     rank 18: 9.74
     rank 19: 10.48
  forward-send-backward-recv:
     rank  0: 4136.45
     rank  1: 4130.72
     rank  2: 4133.17
     rank  3: 4136.12
     rank  4: 789.45
     rank  5: 786.48
     rank  6: 788.96
     rank  7: 788.84
     rank  8: 751.91
     rank  9: 754.66
     rank 10: 746.43
     rank 11: 752.44
     rank 12: 576.87
     rank 13: 574.97
     rank 14: 576.55
     rank 15: 580.23
  backward-send-forward-recv:
     rank  4: 20.93
     rank  5: 13.79
     rank  6: 16.38
     rank  7: 20.55
     rank  8: 152.41
     rank  9: 149.67
     rank 10: 150.70
     rank 11: 150.53
     rank 12: 156.61
     rank 13: 156.51
     rank 14: 150.63
     rank 15: 155.41
     rank 16: 167.10
     rank 17: 165.89
     rank 18: 162.78
     rank 19: 167.64
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.60
     rank  1: 0.60
     rank  2: 0.55
     rank  3: 0.56
     rank  4: 2.43
     rank  5: 2.37
     rank  6: 2.42
     rank  7: 2.43
     rank  8: 2.19
     rank  9: 2.16
     rank 10: 2.21
     rank 11: 2.22
     rank 12: 2.33
     rank 13: 2.35
     rank 14: 2.16
     rank 15: 2.18
     rank 16: 2.46
     rank 17: 2.65
     rank 18: 2.55
     rank 19: 2.37
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.16
     rank  3: 0.20
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.07
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.06
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.05
     rank 17: 0.12
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.47
     rank  1: 8.62
     rank  2: 8.43
     rank  3: 8.45
     rank  4: 48.23
     rank  5: 48.07
     rank  6: 48.13
     rank  7: 48.18
     rank  8: 42.98
     rank  9: 43.01
     rank 10: 43.06
     rank 11: 43.06
     rank 12: 43.10
     rank 13: 43.09
     rank 14: 42.99
     rank 15: 43.03
     rank 16: 46.31
     rank 17: 46.47
     rank 18: 46.41
     rank 19: 46.35
  optimizer:
     rank  0: 9.12
     rank  1: 9.28
     rank  2: 9.09
     rank  3: 9.11
     rank  4: 48.89
     rank  5: 48.72
     rank  6: 48.79
     rank  7: 48.84
     rank  8: 43.64
     rank  9: 43.66
     rank 10: 43.72
     rank 11: 43.72
     rank 12: 43.75
     rank 13: 43.75
     rank 14: 43.64
     rank 15: 43.68
     rank 16: 46.97
     rank 17: 47.13
     rank 18: 47.06
     rank 19: 47.01
 [2024-12-02 22:15:24] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 8073.2 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.958224E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7993.17
     rank  1: 7993.21
     rank  2: 7993.18
     rank  3: 7993.21
     rank  4: 7997.10
     rank  5: 7996.97
     rank  6: 7997.02
     rank  7: 7997.05
     rank  8: 7996.53
     rank  9: 7996.53
     rank 10: 7997.10
     rank 11: 7996.59
     rank 12: 7996.54
     rank 13: 7996.53
     rank 14: 7996.66
     rank 15: 7996.51
     rank 16: 7996.80
     rank 17: 7996.83
     rank 18: 7996.80
     rank 19: 7996.80
  forward-compute:
     rank  0: 1005.42
     rank  1: 1004.65
     rank  2: 1006.09
     rank  3: 1006.71
     rank  4: 3010.10
     rank  5: 3021.80
     rank  6: 3018.32
     rank  7: 3013.36
     rank  8: 2779.92
     rank  9: 2782.20
     rank 10: 2784.01
     rank 11: 2783.95
     rank 12: 2768.79
     rank 13: 2771.01
     rank 14: 2774.85
     rank 15: 2768.19
     rank 16: 2934.96
     rank 17: 2937.09
     rank 18: 2941.24
     rank 19: 2934.99
  backward-compute:
     rank  0: 1057.55
     rank  1: 1068.27
     rank  2: 1065.36
     rank  3: 1062.37
     rank  4: 3396.93
     rank  5: 3397.99
     rank  6: 3400.45
     rank  7: 3396.32
     rank  8: 3282.92
     rank  9: 3285.20
     rank 10: 3289.38
     rank 11: 3282.58
     rank 12: 3305.89
     rank 13: 3307.34
     rank 14: 3310.66
     rank 15: 3308.17
     rank 16: 3531.14
     rank 17: 3532.78
     rank 18: 3532.21
     rank 19: 3531.19
  pure-backward-compute:
     rank  0: 1056.81
     rank  1: 1067.51
     rank  2: 1064.63
     rank  3: 1061.48
     rank  4: 3394.95
     rank  5: 3397.31
     rank  6: 3399.36
     rank  7: 3394.77
     rank  8: 3281.80
     rank  9: 3284.40
     rank 10: 3288.32
     rank 11: 3281.84
     rank 12: 3304.68
     rank 13: 3306.25
     rank 14: 3309.91
     rank 15: 3307.46
     rank 16: 3528.74
     rank 17: 3530.60
     rank 18: 3530.51
     rank 19: 3528.78
  batch-generator:
     rank  0: 53.73
     rank  1: 55.25
     rank  2: 56.88
     rank  3: 61.65
     rank  4: 88.49
     rank  5: 90.21
     rank  6: 92.61
     rank  7: 97.92
     rank  8: 52.73
     rank  9: 56.67
     rank 10: 60.08
     rank 11: 57.49
     rank 12: 53.73
     rank 13: 59.83
     rank 14: 63.64
     rank 15: 56.17
     rank 16: 53.21
     rank 17: 57.03
     rank 18: 61.80
     rank 19: 57.39
  forward-recv:
     rank  4: 60.69
     rank  5: 60.80
     rank  6: 60.53
     rank  7: 60.76
     rank  8: 278.29
     rank  9: 280.24
     rank 10: 278.12
     rank 11: 277.74
     rank 12: 450.44
     rank 13: 449.12
     rank 14: 451.10
     rank 15: 450.65
     rank 16: 621.15
     rank 17: 620.96
     rank 18: 620.75
     rank 19: 620.96
  forward-send:
     rank  0: 418.09
     rank  1: 417.52
     rank  2: 416.19
     rank  3: 416.44
     rank  4: 31.67
     rank  5: 31.27
     rank  6: 31.02
     rank  7: 31.03
     rank  8: 20.81
     rank  9: 19.22
     rank 10: 20.84
     rank 11: 20.74
     rank 12: 10.52
     rank 13: 10.47
     rank 14: 9.97
     rank 15: 10.37
  backward-recv:
     rank  0: 1386.63
     rank  1: 1384.51
     rank  2: 1385.58
     rank  3: 1385.50
     rank  4: 600.09
     rank  5: 602.93
     rank  6: 601.13
     rank  7: 599.58
     rank  8: 402.06
     rank  9: 401.55
     rank 10: 401.40
     rank 11: 402.79
     rank 12: 197.72
     rank 13: 197.72
     rank 14: 198.13
     rank 15: 197.87
  backward-send:
     rank  4: 5.09
     rank  5: 2.80
     rank  6: 3.84
     rank  7: 4.88
     rank  8: 31.34
     rank  9: 31.11
     rank 10: 31.49
     rank 11: 30.10
     rank 12: 21.04
     rank 13: 20.64
     rank 14: 20.27
     rank 15: 20.76
     rank 16: 10.58
     rank 17: 10.15
     rank 18: 10.12
     rank 19: 10.60
  forward-send-backward-recv:
     rank  0: 4109.16
     rank  1: 4103.13
     rank  2: 4105.13
     rank  3: 4103.80
     rank  4: 783.06
     rank  5: 780.23
     rank  6: 779.42
     rank  7: 782.21
     rank  8: 721.00
     rank  9: 722.64
     rank 10: 714.62
     rank 11: 721.78
     rank 12: 571.77
     rank 13: 569.46
     rank 14: 570.76
     rank 15: 574.98
  backward-send-forward-recv:
     rank  4: 20.35
     rank  5: 15.37
     rank  6: 16.30
     rank  7: 20.01
     rank  8: 182.06
     rank  9: 178.70
     rank 10: 178.41
     rank 11: 181.04
     rank 12: 156.20
     rank 13: 156.34
     rank 14: 150.09
     rank 15: 155.24
     rank 16: 166.83
     rank 17: 164.63
     rank 18: 162.32
     rank 19: 166.99
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.06
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.06
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.60
     rank  1: 0.60
     rank  2: 0.54
     rank  3: 0.60
     rank  4: 2.44
     rank  5: 2.38
     rank  6: 2.44
     rank  7: 2.43
     rank  8: 2.18
     rank  9: 2.15
     rank 10: 2.78
     rank 11: 2.15
     rank 12: 2.31
     rank 13: 2.25
     rank 14: 2.33
     rank 15: 2.16
     rank 16: 2.39
     rank 17: 2.51
     rank 18: 2.47
     rank 19: 2.37
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.17
     rank  2: 0.16
     rank  3: 0.20
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.13
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.09
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.04
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.04
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.57
     rank  2: 8.54
     rank  3: 8.45
     rank  4: 48.19
     rank  5: 48.09
     rank  6: 48.10
     rank  7: 48.21
     rank  8: 43.02
     rank  9: 42.96
     rank 10: 43.55
     rank 11: 42.99
     rank 12: 43.14
     rank 13: 43.03
     rank 14: 43.10
     rank 15: 43.00
     rank 16: 46.37
     rank 17: 46.42
     rank 18: 46.42
     rank 19: 46.34
  optimizer:
     rank  0: 9.95
     rank  1: 10.00
     rank  2: 9.97
     rank  3: 9.88
     rank  4: 49.65
     rank  5: 49.53
     rank  6: 49.53
     rank  7: 49.71
     rank  8: 44.46
     rank  9: 44.39
     rank 10: 44.94
     rank 11: 44.43
     rank 12: 44.57
     rank 13: 44.46
     rank 14: 44.53
     rank 15: 44.44
     rank 16: 47.81
     rank 17: 47.85
     rank 18: 47.86
     rank 19: 47.78
 [2024-12-02 22:15:32] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 8038.9 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.529998E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7955.82
     rank  1: 7955.83
     rank  2: 7955.83
     rank  3: 7955.88
     rank  4: 7959.65
     rank  5: 7959.61
     rank  6: 7959.62
     rank  7: 7959.63
     rank  8: 7959.17
     rank  9: 7959.15
     rank 10: 7959.21
     rank 11: 7959.26
     rank 12: 7959.17
     rank 13: 7959.20
     rank 14: 7959.16
     rank 15: 7959.15
     rank 16: 7959.43
     rank 17: 7959.44
     rank 18: 7959.47
     rank 19: 7959.43
  forward-compute:
     rank  0: 913.56
     rank  1: 913.80
     rank  2: 915.85
     rank  3: 913.71
     rank  4: 3016.88
     rank  5: 3025.87
     rank  6: 3027.38
     rank  7: 3017.79
     rank  8: 2802.23
     rank  9: 2802.11
     rank 10: 2807.18
     rank 11: 2805.30
     rank 12: 2768.64
     rank 13: 2774.22
     rank 14: 2769.92
     rank 15: 2769.41
     rank 16: 2917.49
     rank 17: 2919.52
     rank 18: 2920.70
     rank 19: 2918.19
  backward-compute:
     rank  0: 1052.83
     rank  1: 1059.59
     rank  2: 1058.52
     rank  3: 1055.09
     rank  4: 3398.18
     rank  5: 3400.40
     rank  6: 3403.34
     rank  7: 3399.33
     rank  8: 3288.43
     rank  9: 3295.63
     rank 10: 3292.39
     rank 11: 3289.62
     rank 12: 3306.73
     rank 13: 3306.95
     rank 14: 3308.28
     rank 15: 3308.67
     rank 16: 3507.97
     rank 17: 3509.64
     rank 18: 3509.65
     rank 19: 3506.56
  pure-backward-compute:
     rank  0: 1052.15
     rank  1: 1058.80
     rank  2: 1057.82
     rank  3: 1054.39
     rank  4: 3396.28
     rank  5: 3399.71
     rank  6: 3402.53
     rank  7: 3398.11
     rank  8: 3287.22
     rank  9: 3294.69
     rank 10: 3290.53
     rank 11: 3288.82
     rank 12: 3305.48
     rank 13: 3305.77
     rank 14: 3307.39
     rank 15: 3307.97
     rank 16: 3505.77
     rank 17: 3507.88
     rank 18: 3507.69
     rank 19: 3504.31
  batch-generator:
     rank  0: 51.67
     rank  1: 59.20
     rank  2: 60.51
     rank  3: 59.53
     rank  4: 92.66
     rank  5: 91.59
     rank  6: 95.85
     rank  7: 96.98
     rank  8: 60.01
     rank  9: 60.51
     rank 10: 69.17
     rank 11: 66.26
     rank 12: 52.64
     rank 13: 62.20
     rank 14: 57.89
     rank 15: 56.58
     rank 16: 52.54
     rank 17: 56.01
     rank 18: 57.88
     rank 19: 57.35
  forward-recv:
     rank  4: 60.11
     rank  5: 60.00
     rank  6: 58.52
     rank  7: 60.55
     rank  8: 281.04
     rank  9: 281.94
     rank 10: 280.43
     rank 11: 281.06
     rank 12: 454.31
     rank 13: 452.83
     rank 14: 454.98
     rank 15: 454.94
     rank 16: 621.43
     rank 17: 621.44
     rank 18: 621.02
     rank 19: 621.17
  forward-send:
     rank  0: 421.73
     rank  1: 420.36
     rank  2: 418.56
     rank  3: 421.12
     rank  4: 31.83
     rank  5: 31.27
     rank  6: 30.33
     rank  7: 31.54
     rank  8: 20.89
     rank  9: 20.35
     rank 10: 20.79
     rank 11: 20.68
     rank 12: 10.43
     rank 13: 10.54
     rank 14: 9.98
     rank 15: 10.07
  backward-recv:
     rank  0: 1366.28
     rank  1: 1365.04
     rank  2: 1365.36
     rank  3: 1365.50
     rank  4: 576.19
     rank  5: 577.89
     rank  6: 578.94
     rank  7: 577.97
     rank  8: 383.67
     rank  9: 384.76
     rank 10: 382.95
     rank 11: 384.28
     rank 12: 193.15
     rank 13: 193.24
     rank 14: 193.86
     rank 15: 193.30
  backward-send:
     rank  4: 5.27
     rank  5: 3.62
     rank  6: 3.96
     rank  7: 3.96
     rank  8: 31.31
     rank  9: 29.94
     rank 10: 31.53
     rank 11: 30.98
     rank 12: 20.93
     rank 13: 20.62
     rank 14: 20.07
     rank 15: 20.80
     rank 16: 10.61
     rank 17: 10.13
     rank 18: 10.31
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 4186.29
     rank  1: 4182.89
     rank  2: 4183.02
     rank  3: 4184.88
     rank  4: 764.18
     rank  5: 762.88
     rank  6: 759.64
     rank  7: 764.47
     rank  8: 612.98
     rank  9: 610.66
     rank 10: 610.58
     rank 11: 612.57
     rank 12: 494.68
     rank 13: 493.90
     rank 14: 497.03
     rank 15: 497.85
  backward-send-forward-recv:
     rank  4: 20.23
     rank  5: 16.28
     rank  6: 16.18
     rank  7: 19.12
     rank  8: 241.96
     rank  9: 240.24
     rank 10: 236.57
     rank 11: 241.04
     rank 12: 197.77
     rank 13: 194.28
     rank 14: 196.07
     rank 15: 195.62
     rank 16: 173.08
     rank 17: 171.05
     rank 18: 170.84
     rank 19: 173.62
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.56
     rank  2: 0.59
     rank  3: 0.57
     rank  4: 2.45
     rank  5: 2.37
     rank  6: 2.39
     rank  7: 2.37
     rank  8: 2.18
     rank  9: 2.21
     rank 10: 2.39
     rank 11: 2.22
     rank 12: 2.32
     rank 13: 2.26
     rank 14: 2.18
     rank 15: 2.14
     rank 16: 2.41
     rank 17: 2.63
     rank 18: 2.52
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.21
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.11
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.05
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.24
     rank  1: 8.44
     rank  2: 8.40
     rank  3: 8.43
     rank  4: 48.19
     rank  5: 48.09
     rank  6: 48.08
     rank  7: 48.06
     rank  8: 43.01
     rank  9: 43.12
     rank 10: 43.23
     rank 11: 43.07
     rank 12: 43.08
     rank 13: 43.05
     rank 14: 43.00
     rank 15: 42.99
     rank 16: 46.36
     rank 17: 46.35
     rank 18: 46.53
     rank 19: 46.40
  optimizer:
     rank  0: 9.05
     rank  1: 9.16
     rank  2: 9.10
     rank  3: 9.14
     rank  4: 48.90
     rank  5: 48.79
     rank  6: 48.78
     rank  7: 48.77
     rank  8: 43.72
     rank  9: 43.82
     rank 10: 43.94
     rank 11: 43.79
     rank 12: 43.78
     rank 13: 43.75
     rank 14: 43.71
     rank 15: 43.69
     rank 16: 47.06
     rank 17: 47.05
     rank 18: 47.23
     rank 19: 47.10
 [2024-12-02 22:15:40] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 8042.8 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.196463E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7959.21
     rank  1: 7959.22
     rank  2: 7959.20
     rank  3: 7959.21
     rank  4: 7963.13
     rank  5: 7963.09
     rank  6: 7963.03
     rank  7: 7963.02
     rank  8: 7962.65
     rank  9: 7962.62
     rank 10: 7962.64
     rank 11: 7962.55
     rank 12: 7962.67
     rank 13: 7962.65
     rank 14: 7962.53
     rank 15: 7962.52
     rank 16: 7962.82
     rank 17: 7962.84
     rank 18: 7962.83
     rank 19: 7962.82
  forward-compute:
     rank  0: 921.15
     rank  1: 920.79
     rank  2: 920.75
     rank  3: 923.77
     rank  4: 3011.82
     rank  5: 3019.56
     rank  6: 3018.12
     rank  7: 3016.58
     rank  8: 2784.90
     rank  9: 2784.77
     rank 10: 2789.12
     rank 11: 2789.96
     rank 12: 2772.87
     rank 13: 2779.89
     rank 14: 2775.90
     rank 15: 2775.16
     rank 16: 2914.86
     rank 17: 2916.36
     rank 18: 2920.73
     rank 19: 2915.33
  backward-compute:
     rank  0: 1045.31
     rank  1: 1050.66
     rank  2: 1050.62
     rank  3: 1048.21
     rank  4: 3395.02
     rank  5: 3396.74
     rank  6: 3400.77
     rank  7: 3397.29
     rank  8: 3282.22
     rank  9: 3290.16
     rank 10: 3285.49
     rank 11: 3282.57
     rank 12: 3306.93
     rank 13: 3306.89
     rank 14: 3310.38
     rank 15: 3308.28
     rank 16: 3511.16
     rank 17: 3512.18
     rank 18: 3512.29
     rank 19: 3510.88
  pure-backward-compute:
     rank  0: 1044.63
     rank  1: 1049.90
     rank  2: 1049.91
     rank  3: 1047.50
     rank  4: 3393.17
     rank  5: 3396.09
     rank  6: 3400.00
     rank  7: 3396.37
     rank  8: 3281.08
     rank  9: 3289.45
     rank 10: 3283.71
     rank 11: 3281.86
     rank 12: 3305.49
     rank 13: 3305.72
     rank 14: 3309.60
     rank 15: 3307.58
     rank 16: 3508.97
     rank 17: 3510.04
     rank 18: 3510.68
     rank 19: 3508.59
  batch-generator:
     rank  0: 53.22
     rank  1: 55.05
     rank  2: 55.35
     rank  3: 59.54
     rank  4: 91.88
     rank  5: 89.16
     rank  6: 90.53
     rank  7: 94.95
     rank  8: 53.53
     rank  9: 55.43
     rank 10: 61.63
     rank 11: 59.01
     rank 12: 61.31
     rank 13: 69.65
     rank 14: 65.70
     rank 15: 64.97
     rank 16: 51.75
     rank 17: 55.12
     rank 18: 60.07
     rank 19: 56.32
  forward-recv:
     rank  4: 59.22
     rank  5: 59.18
     rank  6: 59.09
     rank  7: 58.53
     rank  8: 277.00
     rank  9: 278.34
     rank 10: 278.16
     rank 11: 278.51
     rank 12: 447.10
     rank 13: 447.07
     rank 14: 447.67
     rank 15: 447.00
     rank 16: 627.60
     rank 17: 627.60
     rank 18: 627.35
     rank 19: 627.12
  forward-send:
     rank  0: 419.00
     rank  1: 417.57
     rank  2: 417.90
     rank  3: 416.39
     rank  4: 31.74
     rank  5: 31.07
     rank  6: 31.16
     rank  7: 30.37
     rank  8: 22.25
     rank  9: 21.06
     rank 10: 20.82
     rank 11: 19.63
     rank 12: 11.64
     rank 13: 10.66
     rank 14: 10.22
     rank 15: 9.93
  backward-recv:
     rank  0: 1368.03
     rank  1: 1367.28
     rank  2: 1366.98
     rank  3: 1366.31
     rank  4: 573.45
     rank  5: 574.67
     rank  6: 575.83
     rank  7: 574.77
     rank  8: 383.19
     rank  9: 383.89
     rank 10: 382.55
     rank 11: 384.19
     rank 12: 194.19
     rank 13: 194.12
     rank 14: 194.96
     rank 15: 194.48
  backward-send:
     rank  4: 5.02
     rank  5: 4.11
     rank  6: 3.95
     rank  7: 3.93
     rank  8: 31.42
     rank  9: 30.41
     rank 10: 31.60
     rank 11: 30.57
     rank 12: 20.89
     rank 13: 20.67
     rank 14: 20.07
     rank 15: 20.82
     rank 16: 10.53
     rank 17: 10.01
     rank 18: 10.32
     rank 19: 10.57
  forward-send-backward-recv:
     rank  0: 4190.54
     rank  1: 4188.80
     rank  2: 4188.88
     rank  3: 4187.57
     rank  4: 778.17
     rank  5: 776.32
     rank  6: 773.74
     rank  7: 777.65
     rank  8: 613.57
     rank  9: 609.31
     rank 10: 610.76
     rank 11: 613.10
     rank 12: 478.32
     rank 13: 477.61
     rank 14: 478.36
     rank 15: 482.75
  backward-send-forward-recv:
     rank  4: 20.75
     rank  5: 18.46
     rank  6: 18.50
     rank  7: 19.18
     rank  8: 271.34
     rank  9: 270.61
     rank 10: 266.26
     rank 11: 269.92
     rank 12: 218.46
     rank 13: 213.45
     rank 14: 216.58
     rank 15: 216.25
     rank 16: 170.24
     rank 17: 169.45
     rank 18: 165.78
     rank 19: 170.53
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.05
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.57
     rank  1: 0.60
     rank  2: 0.55
     rank  3: 0.56
     rank  4: 2.45
     rank  5: 2.42
     rank  6: 2.43
     rank  7: 2.39
     rank  8: 2.26
     rank  9: 2.16
     rank 10: 2.49
     rank 11: 2.17
     rank 12: 2.38
     rank 13: 2.37
     rank 14: 2.14
     rank 15: 2.15
     rank 16: 2.45
     rank 17: 2.46
     rank 18: 2.43
     rank 19: 2.44
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.16
     rank  2: 0.16
     rank  3: 0.22
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.12
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.05
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.48
     rank  1: 8.41
     rank  2: 8.36
     rank  3: 8.43
     rank  4: 48.26
     rank  5: 48.19
     rank  6: 48.13
     rank  7: 48.10
     rank  8: 43.07
     rank  9: 42.98
     rank 10: 43.27
     rank 11: 43.02
     rank 12: 43.09
     rank 13: 43.04
     rank 14: 42.99
     rank 15: 42.96
     rank 16: 46.33
     rank 17: 46.37
     rank 18: 46.36
     rank 19: 46.34
  optimizer:
     rank  0: 9.22
     rank  1: 9.15
     rank  2: 9.10
     rank  3: 9.16
     rank  4: 49.02
     rank  5: 48.93
     rank  6: 48.87
     rank  7: 48.85
     rank  8: 43.81
     rank  9: 43.72
     rank 10: 44.01
     rank 11: 43.79
     rank 12: 43.83
     rank 13: 43.78
     rank 14: 43.73
     rank 15: 43.70
     rank 16: 47.07
     rank 17: 47.11
     rank 18: 47.10
     rank 19: 47.08
 [2024-12-02 22:15:48] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 8031.8 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.432818E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7949.21
     rank  1: 7949.23
     rank  2: 7949.15
     rank  3: 7949.22
     rank  4: 7953.02
     rank  5: 7953.02
     rank  6: 7952.96
     rank  7: 7953.01
     rank  8: 7952.55
     rank  9: 7952.54
     rank 10: 7952.57
     rank 11: 7952.56
     rank 12: 7952.59
     rank 13: 7952.56
     rank 14: 7952.55
     rank 15: 7952.56
     rank 16: 7952.82
     rank 17: 7952.81
     rank 18: 7952.79
     rank 19: 7952.80
  forward-compute:
     rank  0: 951.32
     rank  1: 950.80
     rank  2: 953.28
     rank  3: 953.81
     rank  4: 2990.42
     rank  5: 2993.13
     rank  6: 2995.04
     rank  7: 2990.09
     rank  8: 2796.20
     rank  9: 2796.44
     rank 10: 2799.49
     rank 11: 2799.43
     rank 12: 2761.26
     rank 13: 2763.57
     rank 14: 2767.80
     rank 15: 2760.35
     rank 16: 2919.66
     rank 17: 2922.07
     rank 18: 2924.77
     rank 19: 2920.17
  backward-compute:
     rank  0: 1060.64
     rank  1: 1062.03
     rank  2: 1062.70
     rank  3: 1059.07
     rank  4: 3399.90
     rank  5: 3402.77
     rank  6: 3404.41
     rank  7: 3400.68
     rank  8: 3288.50
     rank  9: 3289.82
     rank 10: 3295.35
     rank 11: 3288.35
     rank 12: 3310.45
     rank 13: 3310.70
     rank 14: 3312.50
     rank 15: 3312.07
     rank 16: 3505.15
     rank 17: 3506.36
     rank 18: 3506.72
     rank 19: 3504.80
  pure-backward-compute:
     rank  0: 1059.79
     rank  1: 1061.25
     rank  2: 1062.02
     rank  3: 1058.42
     rank  4: 3399.00
     rank  5: 3401.90
     rank  6: 3403.66
     rank  7: 3399.78
     rank  8: 3287.37
     rank  9: 3289.05
     rank 10: 3293.81
     rank 11: 3287.57
     rank 12: 3309.14
     rank 13: 3309.54
     rank 14: 3311.75
     rank 15: 3311.17
     rank 16: 3502.85
     rank 17: 3504.50
     rank 18: 3505.09
     rank 19: 3502.69
  batch-generator:
     rank  0: 54.95
     rank  1: 57.19
     rank  2: 59.48
     rank  3: 61.87
     rank  4: 63.64
     rank  5: 67.67
     rank  6: 71.62
     rank  7: 73.14
     rank  8: 57.66
     rank  9: 58.92
     rank 10: 63.95
     rank 11: 60.79
     rank 12: 52.74
     rank 13: 58.77
     rank 14: 63.21
     rank 15: 55.71
     rank 16: 52.09
     rank 17: 56.24
     rank 18: 59.57
     rank 19: 56.66
  forward-recv:
     rank  4: 59.97
     rank  5: 60.43
     rank  6: 59.86
     rank  7: 60.37
     rank  8: 282.43
     rank  9: 284.07
     rank 10: 282.00
     rank 11: 282.02
     rank 12: 455.25
     rank 13: 453.94
     rank 14: 456.03
     rank 15: 455.94
     rank 16: 622.08
     rank 17: 622.05
     rank 18: 621.63
     rank 19: 621.71
  forward-send:
     rank  0: 421.30
     rank  1: 420.14
     rank  2: 418.93
     rank  3: 418.90
     rank  4: 31.77
     rank  5: 31.10
     rank  6: 30.78
     rank  7: 30.88
     rank  8: 21.00
     rank  9: 19.48
     rank 10: 20.97
     rank 11: 20.72
     rank 12: 10.57
     rank 13: 10.63
     rank 14: 9.92
     rank 15: 10.02
  backward-recv:
     rank  0: 1381.75
     rank  1: 1381.63
     rank  2: 1381.69
     rank  3: 1381.81
     rank  4: 589.63
     rank  5: 589.05
     rank  6: 590.72
     rank  7: 590.57
     rank  8: 389.31
     rank  9: 389.99
     rank 10: 388.79
     rank 11: 389.58
     rank 12: 192.62
     rank 13: 192.66
     rank 14: 193.02
     rank 15: 192.70
  backward-send:
     rank  4: 4.48
     rank  5: 4.52
     rank  6: 4.19
     rank  7: 3.93
     rank  8: 31.24
     rank  9: 30.46
     rank 10: 31.30
     rank 11: 30.86
     rank 12: 20.88
     rank 13: 20.49
     rank 14: 20.33
     rank 15: 20.81
     rank 16: 10.54
     rank 17: 10.26
     rank 18: 10.35
     rank 19: 10.52
  forward-send-backward-recv:
     rank  0: 4118.40
     rank  1: 4119.46
     rank  2: 4118.69
     rank  3: 4119.88
     rank  4: 772.64
     rank  5: 770.96
     rank  6: 769.24
     rank  7: 772.57
     rank  8: 661.87
     rank  9: 664.23
     rank 10: 655.18
     rank 11: 662.35
     rank 12: 534.20
     rank 13: 532.66
     rank 14: 535.32
     rank 15: 538.34
  backward-send-forward-recv:
     rank  4: 20.53
     rank  5: 20.39
     rank  6: 18.76
     rank  7: 21.07
     rank  8: 186.33
     rank  9: 185.71
     rank 10: 182.95
     rank 11: 186.21
     rank 12: 156.75
     rank 13: 156.71
     rank 14: 150.41
     rank 15: 155.47
     rank 16: 167.71
     rank 17: 166.63
     rank 18: 164.37
     rank 19: 168.24
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.59
     rank  2: 0.54
     rank  3: 0.59
     rank  4: 2.42
     rank  5: 2.39
     rank  6: 2.39
     rank  7: 2.38
     rank  8: 2.23
     rank  9: 2.14
     rank 10: 2.29
     rank 11: 2.21
     rank 12: 2.30
     rank 13: 2.29
     rank 14: 2.16
     rank 15: 2.14
     rank 16: 2.43
     rank 17: 2.49
     rank 18: 2.44
     rank 19: 2.42
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.16
     rank  3: 0.17
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.45
     rank  1: 8.60
     rank  2: 8.43
     rank  3: 8.54
     rank  4: 48.16
     rank  5: 48.11
     rank  6: 48.08
     rank  7: 48.08
     rank  8: 43.02
     rank  9: 43.01
     rank 10: 43.09
     rank 11: 43.10
     rank 12: 43.09
     rank 13: 43.08
     rank 14: 42.98
     rank 15: 43.01
     rank 16: 46.31
     rank 17: 46.37
     rank 18: 46.42
     rank 19: 46.35
  optimizer:
     rank  0: 9.06
     rank  1: 9.21
     rank  2: 9.04
     rank  3: 9.15
     rank  4: 48.77
     rank  5: 48.72
     rank  6: 48.69
     rank  7: 48.69
     rank  8: 43.64
     rank  9: 43.62
     rank 10: 43.71
     rank 11: 43.71
     rank 12: 43.70
     rank 13: 43.70
     rank 14: 43.59
     rank 15: 43.63
     rank 16: 46.92
     rank 17: 46.99
     rank 18: 47.03
     rank 19: 46.96
 [2024-12-02 22:15:56] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 8037.2 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.025037E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7960.16
     rank  1: 7960.11
     rank  2: 7960.10
     rank  3: 7960.13
     rank  4: 7963.96
     rank  5: 7963.92
     rank  6: 7963.95
     rank  7: 7963.94
     rank  8: 7963.52
     rank  9: 7963.47
     rank 10: 7963.52
     rank 11: 7963.48
     rank 12: 7963.56
     rank 13: 7963.48
     rank 14: 7963.47
     rank 15: 7963.44
     rank 16: 7963.85
     rank 17: 7963.73
     rank 18: 7963.72
     rank 19: 7963.75
  forward-compute:
     rank  0: 919.51
     rank  1: 918.68
     rank  2: 920.88
     rank  3: 920.60
     rank  4: 2982.48
     rank  5: 2984.69
     rank  6: 2985.38
     rank  7: 2980.67
     rank  8: 2789.78
     rank  9: 2790.17
     rank 10: 2790.41
     rank 11: 2791.65
     rank 12: 2766.91
     rank 13: 2768.93
     rank 14: 2773.67
     rank 15: 2766.95
     rank 16: 2926.83
     rank 17: 2930.18
     rank 18: 2931.53
     rank 19: 2927.55
  backward-compute:
     rank  0: 1055.86
     rank  1: 1057.53
     rank  2: 1057.18
     rank  3: 1055.49
     rank  4: 3402.90
     rank  5: 3406.78
     rank  6: 3404.15
     rank  7: 3403.31
     rank  8: 3287.40
     rank  9: 3289.04
     rank 10: 3294.40
     rank 11: 3287.47
     rank 12: 3314.82
     rank 13: 3314.74
     rank 14: 3316.14
     rank 15: 3315.89
     rank 16: 3501.80
     rank 17: 3502.95
     rank 18: 3503.35
     rank 19: 3500.76
  pure-backward-compute:
     rank  0: 1055.16
     rank  1: 1056.68
     rank  2: 1056.51
     rank  3: 1054.86
     rank  4: 3402.00
     rank  5: 3406.04
     rank  6: 3403.27
     rank  7: 3402.29
     rank  8: 3286.36
     rank  9: 3288.30
     rank 10: 3292.99
     rank 11: 3286.74
     rank 12: 3313.25
     rank 13: 3313.60
     rank 14: 3315.42
     rank 15: 3315.12
     rank 16: 3499.45
     rank 17: 3501.22
     rank 18: 3501.79
     rank 19: 3498.56
  batch-generator:
     rank  0: 53.32
     rank  1: 55.87
     rank  2: 59.23
     rank  3: 58.59
     rank  4: 53.99
     rank  5: 55.93
     rank  6: 63.18
     rank  7: 63.06
     rank  8: 53.97
     rank  9: 57.10
     rank 10: 58.57
     rank 11: 56.82
     rank 12: 51.48
     rank 13: 57.43
     rank 14: 61.99
     rank 15: 54.81
     rank 16: 58.64
     rank 17: 62.31
     rank 18: 64.16
     rank 19: 62.04
  forward-recv:
     rank  4: 62.11
     rank  5: 62.27
     rank  6: 61.11
     rank  7: 62.39
     rank  8: 282.50
     rank  9: 284.34
     rank 10: 281.88
     rank 11: 282.67
     rank 12: 454.01
     rank 13: 452.80
     rank 14: 454.93
     rank 15: 454.00
     rank 16: 623.91
     rank 17: 623.91
     rank 18: 623.40
     rank 19: 623.88
  forward-send:
     rank  0: 420.71
     rank  1: 420.00
     rank  2: 418.50
     rank  3: 419.75
     rank  4: 31.65
     rank  5: 31.18
     rank  6: 30.59
     rank  7: 31.35
     rank  8: 20.95
     rank  9: 19.48
     rank 10: 21.03
     rank 11: 20.62
     rank 12: 10.51
     rank 13: 10.51
     rank 14: 9.81
     rank 15: 10.32
  backward-recv:
     rank  0: 1388.50
     rank  1: 1388.29
     rank  2: 1388.50
     rank  3: 1388.68
     rank  4: 596.52
     rank  5: 595.42
     rank  6: 597.10
     rank  7: 595.41
     rank  8: 394.01
     rank  9: 394.53
     rank 10: 393.02
     rank 11: 394.44
     rank 12: 195.72
     rank 13: 196.22
     rank 14: 197.03
     rank 15: 196.37
  backward-send:
     rank  4: 3.75
     rank  5: 4.28
     rank  6: 3.98
     rank  7: 4.53
     rank  8: 31.13
     rank  9: 30.46
     rank 10: 31.29
     rank 11: 30.37
     rank 12: 20.94
     rank 13: 20.34
     rank 14: 19.92
     rank 15: 20.67
     rank 16: 10.47
     rank 17: 10.35
     rank 18: 10.30
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 4160.41
     rank  1: 4161.58
     rank  2: 4161.16
     rank  3: 4160.15
     rank  4: 780.43
     rank  5: 777.45
     rank  6: 779.56
     rank  7: 781.04
     rank  8: 707.24
     rank  9: 709.50
     rank 10: 702.04
     rank 11: 707.74
     rank 12: 530.83
     rank 13: 529.33
     rank 14: 531.79
     rank 15: 534.30
  backward-send-forward-recv:
     rank  4: 20.01
     rank  5: 20.35
     rank  6: 20.22
     rank  7: 20.90
     rank  8: 154.03
     rank  9: 152.81
     rank 10: 153.05
     rank 11: 154.77
     rank 12: 155.78
     rank 13: 156.05
     rank 14: 149.54
     rank 15: 154.63
     rank 16: 168.37
     rank 17: 166.44
     rank 18: 165.48
     rank 19: 168.87
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.56
     rank  2: 0.53
     rank  3: 0.54
     rank  4: 2.39
     rank  5: 2.37
     rank  6: 2.43
     rank  7: 2.38
     rank  8: 2.28
     rank  9: 2.16
     rank 10: 2.28
     rank 11: 2.16
     rank 12: 2.37
     rank 13: 2.24
     rank 14: 2.18
     rank 15: 2.14
     rank 16: 2.44
     rank 17: 2.43
     rank 18: 2.41
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.16
     rank  2: 0.15
     rank  3: 0.17
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.11
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.09
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.50
     rank  1: 8.55
     rank  2: 8.52
     rank  3: 8.29
     rank  4: 48.08
     rank  5: 48.10
     rank  6: 48.13
     rank  7: 48.08
     rank  8: 43.09
     rank  9: 43.00
     rank 10: 43.00
     rank 11: 42.98
     rank 12: 43.19
     rank 13: 43.01
     rank 14: 43.02
     rank 15: 42.99
     rank 16: 46.34
     rank 17: 46.36
     rank 18: 46.41
     rank 19: 46.35
  optimizer:
     rank  0: 9.12
     rank  1: 9.16
     rank  2: 9.13
     rank  3: 8.91
     rank  4: 48.71
     rank  5: 48.72
     rank  6: 48.75
     rank  7: 48.70
     rank  8: 43.71
     rank  9: 43.62
     rank 10: 43.62
     rank 11: 43.60
     rank 12: 43.81
     rank 13: 43.63
     rank 14: 43.64
     rank 15: 43.61
     rank 16: 46.96
     rank 17: 46.99
     rank 18: 47.03
     rank 19: 46.97
 [2024-12-02 22:16:04] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 8026.7 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.434684E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7950.04
     rank  1: 7950.14
     rank  2: 7950.13
     rank  3: 7950.01
     rank  4: 7953.90
     rank  5: 7953.96
     rank  6: 7953.95
     rank  7: 7953.89
     rank  8: 7953.46
     rank  9: 7953.47
     rank 10: 7953.48
     rank 11: 7953.40
     rank 12: 7953.47
     rank 13: 7953.48
     rank 14: 7953.46
     rank 15: 7953.42
     rank 16: 7953.70
     rank 17: 7953.76
     rank 18: 7953.75
     rank 19: 7953.71
  forward-compute:
     rank  0: 978.20
     rank  1: 977.87
     rank  2: 979.90
     rank  3: 978.76
     rank  4: 2980.35
     rank  5: 2983.20
     rank  6: 2984.46
     rank  7: 2979.33
     rank  8: 2792.89
     rank  9: 2793.31
     rank 10: 2795.26
     rank 11: 2796.56
     rank 12: 2772.53
     rank 13: 2774.61
     rank 14: 2779.34
     rank 15: 2772.46
     rank 16: 2916.28
     rank 17: 2920.68
     rank 18: 2920.27
     rank 19: 2916.87
  backward-compute:
     rank  0: 1059.72
     rank  1: 1062.34
     rank  2: 1062.23
     rank  3: 1059.96
     rank  4: 3401.04
     rank  5: 3404.00
     rank  6: 3404.57
     rank  7: 3402.74
     rank  8: 3292.32
     rank  9: 3293.90
     rank 10: 3298.16
     rank 11: 3292.72
     rank 12: 3312.44
     rank 13: 3313.45
     rank 14: 3313.71
     rank 15: 3313.52
     rank 16: 3502.39
     rank 17: 3503.15
     rank 18: 3503.81
     rank 19: 3501.47
  pure-backward-compute:
     rank  0: 1059.02
     rank  1: 1061.52
     rank  2: 1061.54
     rank  3: 1059.30
     rank  4: 3400.19
     rank  5: 3403.32
     rank  6: 3403.82
     rank  7: 3401.79
     rank  8: 3291.27
     rank  9: 3293.04
     rank 10: 3296.93
     rank 11: 3292.00
     rank 12: 3311.12
     rank 13: 3312.33
     rank 14: 3312.83
     rank 15: 3312.78
     rank 16: 3499.73
     rank 17: 3501.40
     rank 18: 3502.11
     rank 19: 3499.38
  batch-generator:
     rank  0: 59.39
     rank  1: 59.04
     rank  2: 61.80
     rank  3: 61.99
     rank  4: 51.95
     rank  5: 55.18
     rank  6: 60.54
     rank  7: 61.03
     rank  8: 52.86
     rank  9: 55.53
     rank 10: 59.08
     rank 11: 57.22
     rank 12: 52.50
     rank 13: 58.38
     rank 14: 62.89
     rank 15: 55.19
     rank 16: 54.87
     rank 17: 59.73
     rank 18: 60.10
     rank 19: 58.56
  forward-recv:
     rank  4: 60.29
     rank  5: 60.33
     rank  6: 58.96
     rank  7: 60.46
     rank  8: 280.77
     rank  9: 282.71
     rank 10: 280.09
     rank 11: 280.95
     rank 12: 451.12
     rank 13: 449.79
     rank 14: 451.99
     rank 15: 451.16
     rank 16: 624.37
     rank 17: 624.26
     rank 18: 623.71
     rank 19: 624.27
  forward-send:
     rank  0: 406.27
     rank  1: 405.56
     rank  2: 403.52
     rank  3: 405.72
     rank  4: 31.76
     rank  5: 31.29
     rank  6: 30.58
     rank  7: 31.58
     rank  8: 20.89
     rank  9: 19.31
     rank 10: 21.01
     rank 11: 20.63
     rank 12: 10.47
     rank 13: 10.52
     rank 14: 9.85
     rank 15: 10.34
  backward-recv:
     rank  0: 1379.56
     rank  1: 1378.25
     rank  2: 1379.12
     rank  3: 1379.49
     rank  4: 594.33
     rank  5: 594.83
     rank  6: 595.62
     rank  7: 594.04
     rank  8: 395.40
     rank  9: 395.93
     rank 10: 395.03
     rank 11: 395.30
     rank 12: 193.77
     rank 13: 193.77
     rank 14: 193.92
     rank 15: 194.57
  backward-send:
     rank  4: 4.59
     rank  5: 3.96
     rank  6: 3.79
     rank  7: 4.67
     rank  8: 31.36
     rank  9: 30.54
     rank 10: 31.36
     rank 11: 30.93
     rank 12: 20.93
     rank 13: 20.62
     rank 14: 20.50
     rank 15: 20.28
     rank 16: 10.58
     rank 17: 10.42
     rank 18: 10.17
     rank 19: 10.48
  forward-send-backward-recv:
     rank  0: 4110.48
     rank  1: 4111.50
     rank  2: 4110.25
     rank  3: 4110.30
     rank  4: 775.90
     rank  5: 774.35
     rank  6: 773.77
     rank  7: 774.76
     rank  8: 683.05
     rank  9: 684.84
     rank 10: 677.85
     rank 11: 683.64
     rank 12: 522.97
     rank 13: 520.34
     rank 14: 524.47
     rank 15: 526.04
  backward-send-forward-recv:
     rank  4: 21.21
     rank  5: 20.81
     rank  6: 20.59
     rank  7: 21.20
     rank  8: 160.31
     rank  9: 158.72
     rank 10: 157.96
     rank 11: 158.86
     rank 12: 156.39
     rank 13: 156.64
     rank 14: 149.79
     rank 15: 155.52
     rank 16: 169.10
     rank 17: 166.52
     rank 18: 167.12
     rank 19: 169.92
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.59
     rank  2: 0.54
     rank  3: 0.55
     rank  4: 2.37
     rank  5: 2.37
     rank  6: 2.38
     rank  7: 2.43
     rank  8: 2.30
     rank  9: 2.20
     rank 10: 2.33
     rank 11: 2.16
     rank 12: 2.26
     rank 13: 2.24
     rank 14: 2.17
     rank 15: 2.28
     rank 16: 2.42
     rank 17: 2.58
     rank 18: 2.53
     rank 19: 2.43
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.16
     rank  3: 0.17
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.09
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.35
     rank  1: 8.53
     rank  2: 8.39
     rank  3: 8.33
     rank  4: 48.09
     rank  5: 48.09
     rank  6: 48.03
     rank  7: 48.19
     rank  8: 43.06
     rank  9: 43.03
     rank 10: 43.11
     rank 11: 42.99
     rank 12: 43.01
     rank 13: 43.02
     rank 14: 43.01
     rank 15: 43.12
     rank 16: 46.27
     rank 17: 46.42
     rank 18: 46.52
     rank 19: 46.33
  optimizer:
     rank  0: 8.95
     rank  1: 9.12
     rank  2: 8.99
     rank  3: 8.93
     rank  4: 48.69
     rank  5: 48.70
     rank  6: 48.63
     rank  7: 48.79
     rank  8: 43.66
     rank  9: 43.63
     rank 10: 43.71
     rank 11: 43.59
     rank 12: 43.78
     rank 13: 43.62
     rank 14: 43.61
     rank 15: 43.72
     rank 16: 46.87
     rank 17: 47.02
     rank 18: 47.12
     rank 19: 46.93
