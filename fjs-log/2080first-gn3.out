examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
4
[2024-12-02 15:47:59,412] torch.distributed.run: [WARNING] 
[2024-12-02 15:47:59,412] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 15:47:59,412] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 15:47:59,412] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 251695104
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (49.33, 122.86)
    train/valid/test-data-iterators-setup ..........: (1011.83, 1245.98)
before training log report memory flag:True
before training log report memory flag:True
before training log report memory flag:True
before training log report memory flag:True
 [2024-12-02 15:48:45] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 19253.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.152636E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 16] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:48:56] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 10928.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.115383E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:49:07] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 10930.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.776823E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:Falseafter training log report memory flag:False
after training log report memory flag:False

after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:49:18] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 10913.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.909885E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:49:29] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 10921.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.725075E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:49:40] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 10923.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.628387E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:49:51] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 10921.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.822122E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:50:01] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 10916.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.252022E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:50:12] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10918.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.795928E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
 [2024-12-02 15:50:23] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 10916.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.678910E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
