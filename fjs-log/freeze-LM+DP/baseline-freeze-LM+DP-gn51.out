examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-05 20:26:04,595] torch.distributed.run: [WARNING] 
[2024-12-05 20:26:04,595] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 20:26:04,595] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 20:26:04,595] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
---Rank 16---Tensor Parallel Group GPUs: [0, 0]
---Rank 19---Tensor Parallel Group GPUs: [1, 1]---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Tensor Parallel Group GPUs: [0, 0]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Tensor Parallel Group GPUs: [1, 1][rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 939593728
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 939593728
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<1094>, pretrain-18.tar[9700, 9800), pretrain-18.tar[9800, 9900), pretrain-18.tar[9900, 10000)] sum(count)=110000
rank=0, worker=1: shard_range=[pretrain-19.tar[0, 100), pretrain-19.tar[100, 200), pretrain-19.tar[200, 300), ...<1094>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=110000
rank=1, worker=0: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<1094>, pretrain-38.tar[9700, 9800), pretrain-38.tar[9800, 9900), pretrain-38.tar[9900, 10000)] sum(count)=110000
rank=1, worker=1: shard_range=[pretrain-39.tar[0, 100), pretrain-39.tar[100, 200), pretrain-39.tar[200, 300), ...<1094>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=110000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 5000)] sum(count)=15000
rank=0, worker=1: shard_range=[pretrain-5.tar[5000, 10000), pretrain-50.tar[0, 10000)] sum(count)=15000
rank=1, worker=0: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 5000)] sum(count)=15000
rank=1, worker=1: shard_range=[pretrain-52.tar[5000, 10000), pretrain-53.tar[0, 10000)] sum(count)=15000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 82.87
     rank  1: 123.24
     rank  2: 81.12
     rank  3: 119.66
     rank  4: 57.93
     rank  5: 57.23
     rank  6: 54.75
     rank  7: 58.27
     rank  8: 56.92
     rank  9: 57.02
     rank 10: 58.11
     rank 11: 58.12
     rank 12: 44.76
     rank 13: 43.24
     rank 14: 35.81
     rank 15: 45.15
     rank 16: 43.78
     rank 17: 58.26
     rank 18: 55.27
     rank 19: 54.93
  train/valid/test-data-iterators-setup:
     rank  0: 841.45
     rank  1: 841.42
     rank  2: 848.50
     rank  3: 848.68
     rank  4: 1245.47
     rank  5: 1245.60
     rank  6: 1245.49
     rank  7: 1245.53
     rank  8: 1245.56
     rank  9: 1245.56
     rank 10: 1245.53
     rank 11: 1245.61
     rank 12: 1245.74
     rank 13: 1246.18
     rank 14: 1263.19
     rank 15: 1264.86
     rank 16: 1264.88
     rank 17: 1265.19
     rank 18: 1264.80
     rank 19: 1265.14
 [2024-12-05 20:26:57] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22934.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.091202E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 16] (after 1 iterations) memory (MB) | allocated: 3669.5185546875 | max allocated: 7145.1943359375 | reserved: 7872.0 | max reserved: 7872.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 3669.5185546875 | max allocated: 7145.1943359375 | reserved: 7904.0 | max reserved: 7904.0
times across ranks (ms):
  forward-backward:
     rank  0: 22827.99
     rank  1: 22827.98
     rank  2: 22827.91
     rank  3: 22827.91
     rank  4: 22904.54
     rank  5: 22905.06
     rank  6: 22904.55
     rank  7: 22905.02
     rank  8: 22897.54
     rank  9: 22907.29
     rank 10: 22897.81
     rank 11: 22907.25
     rank 12: 22900.96
     rank 13: 22901.77
     rank 14: 22900.90
     rank 15: 22901.82
     rank 16: 22825.99
     rank 17: 22825.98
     rank 18: 22825.90
     rank 19: 22825.89
  forward-compute:
     rank  0: 5042.33
     rank  1: 5047.21
     rank  2: 4796.45
     rank  3: 4787.47
     rank  4: 3848.47
     rank  5: 3850.89
     rank  6: 3807.24
     rank  7: 3795.54
     rank  8: 3833.29
     rank  9: 3843.03
     rank 10: 3838.15
     rank 11: 3828.25
     rank 12: 3675.66
     rank 13: 3686.25
     rank 14: 3641.15
     rank 15: 3634.67
     rank 16: 3897.51
     rank 17: 3902.60
     rank 18: 4234.67
     rank 19: 4234.99
  backward-compute:
     rank  0: 4645.07
     rank  1: 4645.71
     rank  2: 4637.70
     rank  3: 4633.59
     rank  4: 1744.77
     rank  5: 1746.49
     rank  6: 1749.17
     rank  7: 1749.08
     rank  8: 1738.94
     rank  9: 1740.49
     rank 10: 1744.55
     rank 11: 1741.19
     rank 12: 1746.47
     rank 13: 1745.85
     rank 14: 1742.07
     rank 15: 1743.23
     rank 16: 1880.54
     rank 17: 1879.87
     rank 18: 1863.00
     rank 19: 1865.40
  pure-backward-compute:
     rank  0: 4644.50
     rank  1: 4645.20
     rank  2: 4637.19
     rank  3: 4632.92
     rank  4: 1743.82
     rank  5: 1745.86
     rank  6: 1748.27
     rank  7: 1748.36
     rank  8: 1738.27
     rank  9: 1739.78
     rank 10: 1743.56
     rank 11: 1740.07
     rank 12: 1745.59
     rank 13: 1744.94
     rank 14: 1741.28
     rank 15: 1742.35
     rank 16: 1878.62
     rank 17: 1878.30
     rank 18: 1861.43
     rank 19: 1863.86
  batch-generator:
     rank  0: 1314.50
     rank  1: 1318.54
     rank  2: 1373.75
     rank  3: 1368.26
     rank  4: 1051.89
     rank  5: 1055.53
     rank  6: 1011.42
     rank  7: 1001.06
     rank  8: 1089.70
     rank  9: 1099.39
     rank 10: 1079.53
     rank 11: 1072.19
     rank 12: 1094.38
     rank 13: 1105.92
     rank 14: 1017.98
     rank 15: 1009.62
     rank 16: 990.92
     rank 17: 997.38
     rank 18: 891.27
     rank 19: 891.90
  forward-recv:
     rank  4: 4615.58
     rank  5: 4616.15
     rank  6: 4408.31
     rank  7: 4408.24
     rank  8: 7176.73
     rank  9: 7179.76
     rank 10: 6887.96
     rank 11: 6892.48
     rank 12: 9753.20
     rank 13: 9748.49
     rank 14: 9455.63
     rank 15: 9460.36
     rank 16: 12158.62
     rank 17: 12153.37
     rank 18: 11830.78
     rank 19: 11830.21
  forward-send:
     rank  0: 7473.35
     rank  1: 7468.57
     rank  2: 7389.96
     rank  3: 7398.37
     rank  4: 4639.21
     rank  5: 4633.15
     rank  6: 4593.65
     rank  7: 4601.56
     rank  8: 2257.17
     rank  9: 2247.69
     rank 10: 2219.58
     rank 11: 2223.30
     rank 12: 28.31
     rank 13: 23.27
     rank 14: 26.42
     rank 15: 25.90
  backward-recv:
     rank  0: 1490.10
     rank  1: 1488.06
     rank  2: 1485.86
     rank  3: 1486.24
     rank  4: 674.83
     rank  5: 677.16
     rank  6: 662.81
     rank  7: 664.22
     rank  8: 436.65
     rank  9: 436.09
     rank 10: 433.77
     rank 11: 435.30
     rank 12: 218.95
     rank 13: 218.88
     rank 14: 217.51
     rank 15: 218.55
  backward-send:
     rank  4: 18.62
     rank  5: 17.37
     rank  6: 17.92
     rank  7: 18.27
     rank  8: 27.46
     rank  9: 27.62
     rank 10: 26.85
     rank 11: 26.26
     rank 12: 16.85
     rank 13: 16.88
     rank 14: 16.11
     rank 15: 16.29
     rank 16: 8.27
     rank 17: 9.58
     rank 18: 7.40
     rank 19: 8.25
  forward-send-backward-recv:
     rank  0: 3941.72
     rank  1: 3944.39
     rank  2: 4281.65
     rank  3: 4284.74
     rank  4: 3109.82
     rank  5: 3108.64
     rank  6: 3417.82
     rank  7: 3417.40
     rank  8: 2909.25
     rank  9: 2908.34
     rank 10: 3210.47
     rank 11: 3214.16
     rank 12: 2677.83
     rank 13: 2678.17
     rank 14: 3004.91
     rank 15: 3004.02
  backward-send-forward-recv:
     rank  4: 3875.76
     rank  5: 3879.78
     rank  6: 3869.08
     rank  7: 3872.86
     rank  8: 3928.84
     rank  9: 3925.88
     rank 10: 3943.10
     rank 11: 3943.76
     rank 12: 3968.35
     rank 13: 3967.76
     rank 14: 3978.40
     rank 15: 3980.96
     rank 16: 3916.58
     rank 17: 3917.23
     rank 18: 3921.76
     rank 19: 3920.87
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.15
     rank 10: 0.05
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.11
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.07
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.11
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 222.15
     rank  1: 224.56
     rank  2: 222.17
     rank  3: 224.50
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.41
     rank  2: 0.22
     rank  3: 0.23
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.07
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 22.20
     rank  1: 23.48
     rank  2: 21.97
     rank  3: 22.14
     rank  4: 0.14
     rank  5: 0.17
     rank  6: 0.23
     rank  7: 0.12
     rank  8: 0.11
     rank  9: 0.10
     rank 10: 0.10
     rank 11: 0.11
     rank 12: 0.19
     rank 13: 0.19
     rank 14: 0.14
     rank 15: 0.12
     rank 16: 0.10
     rank 17: 0.14
     rank 18: 0.10
     rank 19: 0.09
  optimizer:
     rank  0: 23.76
     rank  1: 25.05
     rank  2: 23.52
     rank  3: 23.69
     rank  4: 1.71
     rank  5: 1.75
     rank  6: 1.81
     rank  7: 1.68
     rank  8: 1.65
     rank  9: 1.66
     rank 10: 1.63
     rank 11: 1.67
     rank 12: 1.77
     rank 13: 1.77
     rank 14: 1.71
     rank 15: 1.68
     rank 16: 1.64
     rank 17: 1.68
     rank 18: 1.66
     rank 19: 1.65
 [2024-12-05 20:27:02] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 5180.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.198039E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5128.35
     rank  1: 5128.40
     rank  2: 5128.36
     rank  3: 5128.45
     rank  4: 5126.60
     rank  5: 5126.74
     rank  6: 5126.60
     rank  7: 5126.82
     rank  8: 5126.60
     rank  9: 5126.77
     rank 10: 5126.67
     rank 11: 5126.81
     rank 12: 5126.73
     rank 13: 5126.81
     rank 14: 5126.75
     rank 15: 5126.82
     rank 16: 5126.62
     rank 17: 5126.67
     rank 18: 5126.63
     rank 19: 5126.74
  forward-compute:
     rank  0: 624.14
     rank  1: 624.97
     rank  2: 559.74
     rank  3: 558.82
     rank  4: 1501.66
     rank  5: 1507.14
     rank  6: 1537.05
     rank  7: 1536.62
     rank  8: 1463.27
     rank  9: 1463.48
     rank 10: 1467.71
     rank 11: 1467.08
     rank 12: 1480.24
     rank 13: 1480.21
     rank 14: 1451.32
     rank 15: 1458.12
     rank 16: 1598.55
     rank 17: 1598.81
     rank 18: 1633.49
     rank 19: 1631.35
  backward-compute:
     rank  0: 551.25
     rank  1: 557.13
     rank  2: 536.66
     rank  3: 537.35
     rank  4: 1728.77
     rank  5: 1725.03
     rank  6: 1740.22
     rank  7: 1741.70
     rank  8: 1724.67
     rank  9: 1724.77
     rank 10: 1729.77
     rank 11: 1735.50
     rank 12: 1728.74
     rank 13: 1729.75
     rank 14: 1727.60
     rank 15: 1722.93
     rank 16: 1881.40
     rank 17: 1880.26
     rank 18: 1866.84
     rank 19: 1868.24
  pure-backward-compute:
     rank  0: 550.88
     rank  1: 556.71
     rank  2: 536.30
     rank  3: 536.98
     rank  4: 1728.11
     rank  5: 1724.58
     rank  6: 1739.51
     rank  7: 1741.08
     rank  8: 1723.89
     rank  9: 1723.92
     rank 10: 1728.54
     rank 11: 1734.08
     rank 12: 1728.09
     rank 13: 1729.24
     rank 14: 1727.00
     rank 15: 1722.54
     rank 16: 1879.59
     rank 17: 1878.70
     rank 18: 1865.05
     rank 19: 1865.86
  batch-generator:
     rank  0: 38.15
     rank  1: 38.74
     rank  2: 29.74
     rank  3: 29.09
     rank  4: 34.37
     rank  5: 39.57
     rank  6: 36.55
     rank  7: 34.70
     rank  8: 25.54
     rank  9: 25.86
     rank 10: 31.79
     rank 11: 32.49
     rank 12: 26.82
     rank 13: 26.99
     rank 14: 28.15
     rank 15: 34.25
     rank 16: 27.29
     rank 17: 28.22
     rank 18: 37.27
     rank 19: 34.48
  forward-recv:
     rank  4: 90.28
     rank  5: 89.73
     rank  6: 75.74
     rank  7: 75.65
     rank  8: 291.90
     rank  9: 291.46
     rank 10: 284.89
     rank 11: 285.44
     rank 12: 473.80
     rank 13: 473.89
     rank 14: 465.60
     rank 15: 465.80
     rank 16: 658.52
     rank 17: 658.71
     rank 18: 637.86
     rank 19: 637.56
  forward-send:
     rank  0: 357.74
     rank  1: 356.07
     rank  2: 395.69
     rank  3: 395.59
     rank  4: 16.42
     rank  5: 15.99
     rank  6: 16.18
     rank  7: 16.29
     rank  8: 11.33
     rank  9: 11.31
     rank 10: 11.09
     rank 11: 11.04
     rank 12: 5.59
     rank 13: 5.49
     rank 14: 5.32
     rank 15: 5.04
  backward-recv:
     rank  0: 1505.78
     rank  1: 1504.43
     rank  2: 1509.83
     rank  3: 1508.42
     rank  4: 674.03
     rank  5: 676.44
     rank  6: 668.33
     rank  7: 666.95
     rank  8: 443.23
     rank  9: 441.58
     rank 10: 446.71
     rank 11: 447.02
     rank 12: 219.62
     rank 13: 219.62
     rank 14: 220.21
     rank 15: 220.81
  backward-send:
     rank  4: 17.85
     rank  5: 16.17
     rank  6: 17.52
     rank  7: 17.37
     rank  8: 23.42
     rank  9: 24.98
     rank 10: 23.33
     rank 11: 22.39
     rank 12: 18.28
     rank 13: 18.30
     rank 14: 17.54
     rank 15: 17.53
     rank 16: 10.17
     rank 17: 10.24
     rank 18: 9.81
     rank 19: 10.21
  forward-send-backward-recv:
     rank  0: 1924.09
     rank  1: 1920.87
     rank  2: 1960.14
     rank  3: 1961.81
     rank  4: 841.99
     rank  5: 843.76
     rank  6: 816.70
     rank  7: 817.19
     rank  8: 663.39
     rank  9: 663.85
     rank 10: 653.97
     rank 11: 648.88
     rank 12: 460.49
     rank 13: 459.61
     rank 14: 502.04
     rank 15: 504.25
  backward-send-forward-recv:
     rank  4: 22.98
     rank  5: 21.05
     rank  6: 22.02
     rank  7: 22.00
     rank  8: 53.28
     rank  9: 53.47
     rank 10: 51.94
     rank 11: 51.55
     rank 12: 62.18
     rank 13: 62.35
     rank 14: 58.23
     rank 15: 54.95
     rank 16: 75.28
     rank 17: 75.66
     rank 18: 72.99
     rank 19: 74.52
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 156.93
     rank  1: 157.04
     rank  2: 156.92
     rank  3: 157.04
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.19
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.58
     rank  1: 19.64
     rank  2: 19.57
     rank  3: 19.65
     rank  4: 0.04
     rank  5: 0.07
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.08
     rank 13: 0.08
     rank 14: 0.12
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.03
     rank 19: 0.08
  optimizer:
     rank  0: 20.24
     rank  1: 20.30
     rank  2: 20.24
     rank  3: 20.31
     rank  4: 0.70
     rank  5: 0.73
     rank  6: 0.70
     rank  7: 0.74
     rank  8: 0.69
     rank  9: 0.73
     rank 10: 0.73
     rank 11: 0.74
     rank 12: 0.74
     rank 13: 0.74
     rank 14: 0.78
     rank 15: 0.70
     rank 16: 0.72
     rank 17: 0.72
     rank 18: 0.69
     rank 19: 0.73
 [2024-12-05 20:27:08] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 5155.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.007344E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5113.00
     rank  1: 5112.80
     rank  2: 5112.97
     rank  3: 5112.82
     rank  4: 5111.23
     rank  5: 5111.03
     rank  6: 5111.20
     rank  7: 5111.07
     rank  8: 5111.24
     rank  9: 5111.22
     rank 10: 5111.23
     rank 11: 5111.15
     rank 12: 5111.31
     rank 13: 5111.18
     rank 14: 5111.32
     rank 15: 5111.10
     rank 16: 5111.48
     rank 17: 5111.11
     rank 18: 5111.37
     rank 19: 5111.19
  forward-compute:
     rank  0: 550.15
     rank  1: 552.21
     rank  2: 539.20
     rank  3: 538.99
     rank  4: 1509.19
     rank  5: 1512.35
     rank  6: 1538.65
     rank  7: 1538.93
     rank  8: 1469.14
     rank  9: 1469.82
     rank 10: 1467.41
     rank 11: 1467.49
     rank 12: 1484.63
     rank 13: 1483.67
     rank 14: 1453.84
     rank 15: 1454.59
     rank 16: 1599.71
     rank 17: 1600.73
     rank 18: 1603.29
     rank 19: 1602.62
  backward-compute:
     rank  0: 543.62
     rank  1: 543.62
     rank  2: 532.78
     rank  3: 533.62
     rank  4: 1731.56
     rank  5: 1730.47
     rank  6: 1736.22
     rank  7: 1737.72
     rank  8: 1726.65
     rank  9: 1727.20
     rank 10: 1734.43
     rank 11: 1734.71
     rank 12: 1729.72
     rank 13: 1729.74
     rank 14: 1725.90
     rank 15: 1727.56
     rank 16: 1877.72
     rank 17: 1877.42
     rank 18: 1871.84
     rank 19: 1873.31
  pure-backward-compute:
     rank  0: 543.28
     rank  1: 543.30
     rank  2: 532.44
     rank  3: 533.29
     rank  4: 1731.06
     rank  5: 1730.01
     rank  6: 1735.68
     rank  7: 1737.22
     rank  8: 1725.74
     rank  9: 1726.15
     rank 10: 1733.19
     rank 11: 1733.96
     rank 12: 1729.25
     rank 13: 1729.03
     rank 14: 1725.42
     rank 15: 1727.18
     rank 16: 1875.68
     rank 17: 1876.13
     rank 18: 1869.93
     rank 19: 1871.21
  batch-generator:
     rank  0: 35.51
     rank  1: 36.32
     rank  2: 27.07
     rank  3: 27.71
     rank  4: 38.03
     rank  5: 41.94
     rank  6: 32.77
     rank  7: 33.42
     rank  8: 27.08
     rank  9: 29.31
     rank 10: 29.63
     rank 11: 32.02
     rank 12: 27.09
     rank 13: 27.68
     rank 14: 28.93
     rank 15: 29.14
     rank 16: 27.84
     rank 17: 29.59
     rank 18: 27.18
     rank 19: 30.17
  forward-recv:
     rank  4: 77.59
     rank  5: 77.18
     rank  6: 79.36
     rank  7: 79.18
     rank  8: 274.76
     rank  9: 273.44
     rank 10: 288.15
     rank 11: 288.72
     rank 12: 454.99
     rank 13: 455.14
     rank 14: 468.57
     rank 15: 468.25
     rank 16: 645.17
     rank 17: 645.25
     rank 18: 647.14
     rank 19: 647.08
  forward-send:
     rank  0: 385.84
     rank  1: 384.27
     rank  2: 402.66
     rank  3: 402.20
     rank  4: 16.52
     rank  5: 16.42
     rank  6: 16.42
     rank  7: 16.21
     rank  8: 11.06
     rank  9: 11.63
     rank 10: 11.05
     rank 11: 10.67
     rank 12: 8.66
     rank 13: 8.93
     rank 14: 8.88
     rank 15: 8.84
  backward-recv:
     rank  0: 1508.25
     rank  1: 1509.58
     rank  2: 1513.88
     rank  3: 1513.87
     rank  4: 670.81
     rank  5: 670.54
     rank  6: 666.88
     rank  7: 666.95
     rank  8: 437.79
     rank  9: 437.73
     rank 10: 438.84
     rank 11: 438.88
     rank 12: 218.03
     rank 13: 217.86
     rank 14: 218.32
     rank 15: 218.21
  backward-send:
     rank  4: 20.38
     rank  5: 20.89
     rank  6: 20.28
     rank  7: 20.90
     rank  8: 29.40
     rank  9: 29.35
     rank 10: 28.40
     rank 11: 28.67
     rank 12: 19.99
     rank 13: 20.06
     rank 14: 19.07
     rank 15: 19.40
     rank 16: 10.30
     rank 17: 10.38
     rank 18: 10.07
     rank 19: 10.01
  forward-send-backward-recv:
     rank  0: 1965.80
     rank  1: 1964.95
     rank  2: 1964.91
     rank  3: 1965.41
     rank  4: 834.97
     rank  5: 836.09
     rank  6: 803.28
     rank  7: 803.16
     rank  8: 658.12
     rank  9: 657.73
     rank 10: 638.29
     rank 11: 638.43
     rank 12: 455.88
     rank 13: 455.90
     rank 14: 477.19
     rank 15: 477.40
  backward-send-forward-recv:
     rank  4: 24.32
     rank  5: 22.96
     rank  6: 23.17
     rank  7: 22.55
     rank  8: 55.68
     rank  9: 55.88
     rank 10: 54.71
     rank 11: 54.59
     rank 12: 65.27
     rank 13: 65.56
     rank 14: 63.71
     rank 15: 63.54
     rank 16: 76.88
     rank 17: 76.89
     rank 18: 77.00
     rank 19: 76.43
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.16
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.07
  all-grads-sync:
     rank  0: 150.50
     rank  1: 150.53
     rank  2: 150.52
     rank  3: 150.54
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.19
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-inner-step:
     rank  0: 19.41
     rank  1: 19.58
     rank  2: 19.58
     rank  3: 19.46
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.03
     rank  9: 0.08
     rank 10: 0.06
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.18
     rank 15: 0.03
     rank 16: 0.11
     rank 17: 0.04
     rank 18: 0.06
     rank 19: 0.06
  optimizer:
     rank  0: 20.12
     rank  1: 20.30
     rank  2: 20.30
     rank  3: 20.18
     rank  4: 0.78
     rank  5: 0.76
     rank  6: 0.75
     rank  7: 0.76
     rank  8: 0.75
     rank  9: 0.80
     rank 10: 0.78
     rank 11: 0.76
     rank 12: 0.75
     rank 13: 0.77
     rank 14: 0.90
     rank 15: 0.75
     rank 16: 0.83
     rank 17: 0.76
     rank 18: 0.78
     rank 19: 0.78
 [2024-12-05 20:27:13] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 5167.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.123513E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5117.00
     rank  1: 5116.98
     rank  2: 5116.98
     rank  3: 5116.99
     rank  4: 5115.26
     rank  5: 5115.26
     rank  6: 5115.28
     rank  7: 5115.32
     rank  8: 5115.26
     rank  9: 5115.26
     rank 10: 5115.26
     rank 11: 5115.26
     rank 12: 5115.36
     rank 13: 5115.37
     rank 14: 5115.30
     rank 15: 5115.29
     rank 16: 5115.20
     rank 17: 5115.20
     rank 18: 5115.26
     rank 19: 5115.18
  forward-compute:
     rank  0: 566.79
     rank  1: 573.74
     rank  2: 565.95
     rank  3: 564.44
     rank  4: 1505.54
     rank  5: 1515.02
     rank  6: 1536.15
     rank  7: 1533.51
     rank  8: 1469.24
     rank  9: 1474.92
     rank 10: 1467.54
     rank 11: 1465.76
     rank 12: 1484.35
     rank 13: 1485.47
     rank 14: 1454.15
     rank 15: 1455.09
     rank 16: 1599.32
     rank 17: 1600.86
     rank 18: 1610.08
     rank 19: 1608.18
  backward-compute:
     rank  0: 539.40
     rank  1: 541.43
     rank  2: 540.76
     rank  3: 540.55
     rank  4: 1734.35
     rank  5: 1732.94
     rank  6: 1740.36
     rank  7: 1741.08
     rank  8: 1726.39
     rank  9: 1727.91
     rank 10: 1736.25
     rank 11: 1737.07
     rank 12: 1730.18
     rank 13: 1730.09
     rank 14: 1722.95
     rank 15: 1722.72
     rank 16: 1879.24
     rank 17: 1878.00
     rank 18: 1868.15
     rank 19: 1871.01
  pure-backward-compute:
     rank  0: 538.81
     rank  1: 541.11
     rank  2: 540.43
     rank  3: 540.16
     rank  4: 1733.64
     rank  5: 1732.45
     rank  6: 1739.69
     rank  7: 1740.47
     rank  8: 1725.62
     rank  9: 1726.87
     rank 10: 1735.38
     rank 11: 1736.07
     rank 12: 1728.94
     rank 13: 1729.06
     rank 14: 1722.32
     rank 15: 1722.15
     rank 16: 1877.37
     rank 17: 1876.59
     rank 18: 1866.29
     rank 19: 1868.61
  batch-generator:
     rank  0: 41.22
     rank  1: 44.74
     rank  2: 30.20
     rank  3: 29.50
     rank  4: 33.81
     rank  5: 42.51
     rank  6: 35.52
     rank  7: 31.46
     rank  8: 27.27
     rank  9: 31.56
     rank 10: 28.52
     rank 11: 28.11
     rank 12: 27.90
     rank 13: 30.47
     rank 14: 28.74
     rank 15: 29.15
     rank 16: 29.21
     rank 17: 31.13
     rank 18: 31.33
     rank 19: 29.75
  forward-recv:
     rank  4: 75.74
     rank  5: 74.62
     rank  6: 78.19
     rank  7: 78.29
     rank  8: 274.76
     rank  9: 270.09
     rank 10: 281.12
     rank 11: 281.45
     rank 12: 463.70
     rank 13: 463.45
     rank 14: 467.39
     rank 15: 467.04
     rank 16: 650.13
     rank 17: 649.72
     rank 18: 651.09
     rank 19: 651.21
  forward-send:
     rank  0: 389.85
     rank  1: 383.31
     rank  2: 384.38
     rank  3: 385.21
     rank  4: 30.48
     rank  5: 24.75
     rank  6: 19.60
     rank  7: 19.76
     rank  8: 21.37
     rank  9: 20.15
     rank 10: 20.52
     rank 11: 20.65
     rank 12: 9.71
     rank 13: 9.23
     rank 14: 9.66
     rank 15: 9.65
  backward-recv:
     rank  0: 1510.55
     rank  1: 1511.64
     rank  2: 1509.79
     rank  3: 1508.41
     rank  4: 668.55
     rank  5: 670.91
     rank  6: 664.56
     rank  7: 665.72
     rank  8: 437.15
     rank  9: 437.40
     rank 10: 436.83
     rank 11: 436.43
     rank 12: 220.42
     rank 13: 220.44
     rank 14: 222.41
     rank 15: 221.97
  backward-send:
     rank  4: 18.50
     rank  5: 16.62
     rank  6: 18.09
     rank  7: 17.29
     rank  8: 28.12
     rank  9: 27.52
     rank 10: 26.40
     rank 11: 26.99
     rank 12: 16.49
     rank 13: 16.47
     rank 14: 16.11
     rank 15: 15.80
     rank 16: 9.17
     rank 17: 9.22
     rank 18: 9.26
     rank 19: 8.60
  forward-send-backward-recv:
     rank  0: 1948.31
     rank  1: 1947.59
     rank  2: 1953.20
     rank  3: 1955.73
     rank  4: 832.28
     rank  5: 833.51
     rank  6: 812.20
     rank  7: 812.53
     rank  8: 655.25
     rank  9: 654.39
     rank 10: 642.49
     rank 11: 643.49
     rank 12: 451.60
     rank 13: 451.01
     rank 14: 486.64
     rank 15: 488.38
  backward-send-forward-recv:
     rank  4: 22.32
     rank  5: 21.01
     rank  6: 19.07
     rank  7: 19.86
     rank  8: 54.27
     rank  9: 54.09
     rank 10: 51.38
     rank 11: 51.80
     rank 12: 63.67
     rank 13: 63.96
     rank 14: 59.70
     rank 15: 59.23
     rank 16: 77.55
     rank 17: 77.62
     rank 18: 73.58
     rank 19: 74.39
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.05
  all-grads-sync:
     rank  0: 151.72
     rank  1: 151.77
     rank  2: 151.74
     rank  3: 151.78
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.56
     rank  1: 19.57
     rank  2: 19.45
     rank  3: 19.51
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.08
  optimizer:
     rank  0: 20.21
     rank  1: 20.22
     rank  2: 20.09
     rank  3: 20.16
     rank  4: 0.68
     rank  5: 0.67
     rank  6: 0.70
     rank  7: 0.70
     rank  8: 0.67
     rank  9: 0.68
     rank 10: 0.68
     rank 11: 0.68
     rank 12: 0.71
     rank 13: 0.72
     rank 14: 0.68
     rank 15: 0.68
     rank 16: 0.71
     rank 17: 0.71
     rank 18: 0.69
     rank 19: 0.72
 [2024-12-05 20:27:18] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 5152.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.600870E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5102.49
     rank  1: 5102.38
     rank  2: 5102.47
     rank  3: 5102.41
     rank  4: 5100.71
     rank  5: 5100.65
     rank  6: 5100.67
     rank  7: 5100.68
     rank  8: 5100.76
     rank  9: 5100.69
     rank 10: 5100.75
     rank 11: 5100.75
     rank 12: 5100.89
     rank 13: 5100.75
     rank 14: 5100.79
     rank 15: 5100.72
     rank 16: 5100.77
     rank 17: 5100.62
     rank 18: 5100.69
     rank 19: 5100.65
  forward-compute:
     rank  0: 573.23
     rank  1: 572.97
     rank  2: 537.84
     rank  3: 537.07
     rank  4: 1509.47
     rank  5: 1512.24
     rank  6: 1535.64
     rank  7: 1532.37
     rank  8: 1473.44
     rank  9: 1473.28
     rank 10: 1470.02
     rank 11: 1469.36
     rank 12: 1480.99
     rank 13: 1479.28
     rank 14: 1454.53
     rank 15: 1457.89
     rank 16: 1592.28
     rank 17: 1592.75
     rank 18: 1615.51
     rank 19: 1616.21
  backward-compute:
     rank  0: 544.24
     rank  1: 545.71
     rank  2: 546.68
     rank  3: 541.47
     rank  4: 1731.81
     rank  5: 1730.61
     rank  6: 1740.74
     rank  7: 1743.73
     rank  8: 1729.37
     rank  9: 1730.45
     rank 10: 1735.68
     rank 11: 1737.41
     rank 12: 1720.61
     rank 13: 1722.00
     rank 14: 1721.18
     rank 15: 1721.86
     rank 16: 1882.34
     rank 17: 1881.79
     rank 18: 1862.26
     rank 19: 1862.40
  pure-backward-compute:
     rank  0: 543.59
     rank  1: 545.40
     rank  2: 546.32
     rank  3: 540.95
     rank  4: 1731.09
     rank  5: 1730.07
     rank  6: 1740.23
     rank  7: 1743.09
     rank  8: 1728.79
     rank  9: 1729.65
     rank 10: 1734.99
     rank 11: 1736.55
     rank 12: 1719.75
     rank 13: 1721.32
     rank 14: 1720.44
     rank 15: 1721.43
     rank 16: 1880.45
     rank 17: 1880.65
     rank 18: 1860.36
     rank 19: 1860.80
  batch-generator:
     rank  0: 45.06
     rank  1: 41.11
     rank  2: 28.78
     rank  3: 31.23
     rank  4: 37.33
     rank  5: 42.40
     rank  6: 32.19
     rank  7: 27.72
     rank  8: 28.39
     rank  9: 27.90
     rank 10: 29.10
     rank 11: 29.33
     rank 12: 33.49
     rank 13: 31.49
     rank 14: 28.00
     rank 15: 30.95
     rank 16: 26.68
     rank 17: 27.86
     rank 18: 32.06
     rank 19: 33.44
  forward-recv:
     rank  4: 74.55
     rank  5: 74.36
     rank  6: 75.74
     rank  7: 76.15
     rank  8: 275.17
     rank  9: 274.54
     rank 10: 284.56
     rank 11: 284.79
     rank 12: 466.87
     rank 13: 467.51
     rank 14: 472.99
     rank 15: 472.48
     rank 16: 650.54
     rank 17: 650.61
     rank 18: 647.68
     rank 19: 648.00
  forward-send:
     rank  0: 383.98
     rank  1: 384.44
     rank  2: 402.27
     rank  3: 402.54
     rank  4: 29.29
     rank  5: 30.02
     rank  6: 20.61
     rank  7: 20.50
     rank  8: 20.31
     rank  9: 21.74
     rank 10: 19.52
     rank 11: 19.55
     rank 12: 8.27
     rank 13: 8.25
     rank 14: 7.50
     rank 15: 7.81
  backward-recv:
     rank  0: 1498.03
     rank  1: 1500.30
     rank  2: 1498.21
     rank  3: 1499.66
     rank  4: 668.62
     rank  5: 667.96
     rank  6: 666.46
     rank  7: 662.67
     rank  8: 437.33
     rank  9: 436.77
     rank 10: 436.81
     rank 11: 437.34
     rank 12: 220.90
     rank 13: 221.00
     rank 14: 223.12
     rank 15: 223.36
  backward-send:
     rank  4: 16.06
     rank  5: 16.76
     rank  6: 14.62
     rank  7: 16.96
     rank  8: 23.46
     rank  9: 23.27
     rank 10: 23.11
     rank 11: 22.23
     rank 12: 14.43
     rank 13: 14.31
     rank 14: 13.69
     rank 15: 13.69
     rank 16: 8.21
     rank 17: 8.26
     rank 18: 7.89
     rank 19: 8.27
  forward-send-backward-recv:
     rank  0: 1940.37
     rank  1: 1939.02
     rank  2: 1955.88
     rank  3: 1959.49
     rank  4: 824.02
     rank  5: 824.83
     rank  6: 801.93
     rank  7: 801.30
     rank  8: 642.44
     rank  9: 641.70
     rank 10: 627.07
     rank 11: 626.58
     rank 12: 455.79
     rank 13: 455.91
     rank 14: 475.39
     rank 15: 474.67
  backward-send-forward-recv:
     rank  4: 22.94
     rank  5: 21.25
     rank  6: 20.34
     rank  7: 22.15
     rank  8: 54.26
     rank  9: 54.04
     rank 10: 51.80
     rank 11: 51.82
     rank 12: 62.70
     rank 13: 62.85
     rank 14: 60.41
     rank 15: 57.69
     rank 16: 73.93
     rank 17: 74.57
     rank 18: 70.76
     rank 19: 70.24
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.11
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 152.33
     rank  1: 152.46
     rank  2: 152.44
     rank  3: 152.47
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.08
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.58
     rank  1: 19.61
     rank  2: 19.65
     rank  3: 19.54
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.03
     rank  9: 0.08
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.09
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.03
     rank 19: 0.05
  optimizer:
     rank  0: 20.29
     rank  1: 20.33
     rank  2: 20.36
     rank  3: 20.25
     rank  4: 0.74
     rank  5: 0.74
     rank  6: 0.74
     rank  7: 0.75
     rank  8: 0.74
     rank  9: 0.79
     rank 10: 0.78
     rank 11: 0.78
     rank 12: 0.81
     rank 13: 0.78
     rank 14: 0.75
     rank 15: 0.75
     rank 16: 0.78
     rank 17: 0.75
     rank 18: 0.75
     rank 19: 0.75
 [2024-12-05 20:27:23] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 5171.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.554559E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5121.37
     rank  1: 5121.32
     rank  2: 5121.38
     rank  3: 5121.32
     rank  4: 5119.58
     rank  5: 5119.59
     rank  6: 5119.61
     rank  7: 5119.61
     rank  8: 5119.77
     rank  9: 5119.62
     rank 10: 5119.71
     rank 11: 5119.69
     rank 12: 5119.68
     rank 13: 5119.65
     rank 14: 5119.71
     rank 15: 5119.60
     rank 16: 5119.61
     rank 17: 5119.56
     rank 18: 5119.58
     rank 19: 5119.59
  forward-compute:
     rank  0: 560.08
     rank  1: 561.23
     rank  2: 554.36
     rank  3: 554.39
     rank  4: 1515.71
     rank  5: 1517.89
     rank  6: 1533.01
     rank  7: 1532.10
     rank  8: 1471.57
     rank  9: 1473.79
     rank 10: 1476.68
     rank 11: 1477.50
     rank 12: 1482.57
     rank 13: 1481.93
     rank 14: 1459.10
     rank 15: 1458.87
     rank 16: 1596.87
     rank 17: 1597.35
     rank 18: 1632.71
     rank 19: 1630.84
  backward-compute:
     rank  0: 547.00
     rank  1: 547.03
     rank  2: 546.69
     rank  3: 541.17
     rank  4: 1737.04
     rank  5: 1737.45
     rank  6: 1744.19
     rank  7: 1746.78
     rank  8: 1729.43
     rank  9: 1729.95
     rank 10: 1733.37
     rank 11: 1732.36
     rank 12: 1720.36
     rank 13: 1720.22
     rank 14: 1722.97
     rank 15: 1720.32
     rank 16: 1885.24
     rank 17: 1885.56
     rank 18: 1859.38
     rank 19: 1861.46
  pure-backward-compute:
     rank  0: 546.31
     rank  1: 546.63
     rank  2: 546.32
     rank  3: 540.77
     rank  4: 1736.59
     rank  5: 1736.88
     rank  6: 1743.41
     rank  7: 1746.20
     rank  8: 1728.81
     rank  9: 1729.35
     rank 10: 1732.73
     rank 11: 1731.77
     rank 12: 1719.54
     rank 13: 1719.38
     rank 14: 1722.45
     rank 15: 1719.88
     rank 16: 1883.10
     rank 17: 1884.14
     rank 18: 1857.38
     rank 19: 1859.44
  batch-generator:
     rank  0: 42.26
     rank  1: 40.93
     rank  2: 26.38
     rank  3: 28.03
     rank  4: 34.94
     rank  5: 36.79
     rank  6: 29.01
     rank  7: 28.41
     rank  8: 25.78
     rank  9: 28.13
     rank 10: 36.39
     rank 11: 38.26
     rank 12: 30.71
     rank 13: 30.38
     rank 14: 31.56
     rank 15: 30.40
     rank 16: 28.24
     rank 17: 29.19
     rank 18: 37.77
     rank 19: 36.72
  forward-recv:
     rank  4: 77.85
     rank  5: 77.85
     rank  6: 77.98
     rank  7: 78.42
     rank  8: 282.19
     rank  9: 279.89
     rank 10: 287.76
     rank 11: 287.91
     rank 12: 471.57
     rank 13: 471.94
     rank 14: 477.92
     rank 15: 477.63
     rank 16: 653.74
     rank 17: 653.57
     rank 18: 649.85
     rank 19: 650.00
  forward-send:
     rank  0: 398.19
     rank  1: 396.13
     rank  2: 400.77
     rank  3: 401.10
     rank  4: 31.65
     rank  5: 29.54
     rank  6: 23.65
     rank  7: 23.59
     rank  8: 20.07
     rank  9: 20.28
     rank 10: 19.91
     rank 11: 19.99
     rank 12: 7.13
     rank 13: 7.20
     rank 14: 6.64
     rank 15: 6.82
  backward-recv:
     rank  0: 1505.01
     rank  1: 1506.90
     rank  2: 1511.86
     rank  3: 1514.43
     rank  4: 675.25
     rank  5: 673.93
     rank  6: 678.65
     rank  7: 674.48
     rank  8: 444.31
     rank  9: 444.30
     rank 10: 446.27
     rank 11: 446.70
     rank 12: 223.34
     rank 13: 223.22
     rank 14: 225.86
     rank 15: 227.17
  backward-send:
     rank  4: 13.38
     rank  5: 13.96
     rank  6: 11.00
     rank  7: 13.20
     rank  8: 23.13
     rank  9: 23.04
     rank 10: 22.54
     rank 11: 22.02
     rank 12: 15.60
     rank 13: 15.77
     rank 14: 15.10
     rank 15: 14.82
     rank 16: 9.43
     rank 17: 9.46
     rank 18: 8.46
     rank 19: 9.53
  forward-send-backward-recv:
     rank  0: 1944.94
     rank  1: 1945.74
     rank  2: 1942.75
     rank  3: 1944.54
     rank  4: 820.68
     rank  5: 821.16
     rank  6: 805.23
     rank  7: 803.39
     rank  8: 648.83
     rank  9: 648.76
     rank 10: 635.09
     rank 11: 636.32
     rank 12: 462.80
     rank 13: 463.60
     rank 14: 482.62
     rank 15: 484.40
  backward-send-forward-recv:
     rank  4: 21.05
     rank  5: 21.00
     rank  6: 15.75
     rank  7: 16.56
     rank  8: 50.17
     rank  9: 50.10
     rank 10: 43.93
     rank 11: 42.87
     rank 12: 60.20
     rank 13: 60.41
     rank 14: 52.73
     rank 15: 53.24
     rank 16: 73.36
     rank 17: 73.66
     rank 18: 65.58
     rank 19: 66.70
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 156.47
     rank  1: 156.56
     rank  2: 156.55
     rank  3: 156.56
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.06
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.19
     rank  2: 0.18
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.71
     rank  1: 19.51
     rank  2: 19.61
     rank  3: 19.63
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.07
     rank 16: 0.08
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 20.50
     rank  1: 20.28
     rank  2: 20.39
     rank  3: 20.42
     rank  4: 0.81
     rank  5: 0.81
     rank  6: 0.81
     rank  7: 0.81
     rank  8: 0.82
     rank  9: 0.82
     rank 10: 0.82
     rank 11: 0.83
     rank 12: 0.85
     rank 13: 0.85
     rank 14: 0.82
     rank 15: 0.85
     rank 16: 0.86
     rank 17: 0.82
     rank 18: 0.81
     rank 19: 0.81
 [2024-12-05 20:27:28] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 5181.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.705936E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5130.01
     rank  1: 5130.00
     rank  2: 5129.99
     rank  3: 5130.00
     rank  4: 5128.23
     rank  5: 5128.25
     rank  6: 5128.20
     rank  7: 5128.28
     rank  8: 5128.34
     rank  9: 5128.33
     rank 10: 5128.31
     rank 11: 5128.28
     rank 12: 5128.44
     rank 13: 5128.33
     rank 14: 5128.55
     rank 15: 5128.30
     rank 16: 5128.32
     rank 17: 5128.24
     rank 18: 5128.26
     rank 19: 5128.29
  forward-compute:
     rank  0: 566.19
     rank  1: 568.13
     rank  2: 527.93
     rank  3: 526.80
     rank  4: 1517.15
     rank  5: 1516.53
     rank  6: 1535.98
     rank  7: 1531.84
     rank  8: 1472.17
     rank  9: 1473.14
     rank 10: 1476.07
     rank 11: 1478.64
     rank 12: 1475.42
     rank 13: 1475.04
     rank 14: 1463.04
     rank 15: 1467.59
     rank 16: 1597.67
     rank 17: 1598.68
     rank 18: 1623.80
     rank 19: 1622.00
  backward-compute:
     rank  0: 546.71
     rank  1: 545.17
     rank  2: 538.73
     rank  3: 535.43
     rank  4: 1738.14
     rank  5: 1737.50
     rank  6: 1746.13
     rank  7: 1751.14
     rank  8: 1729.96
     rank  9: 1731.29
     rank 10: 1733.90
     rank 11: 1737.86
     rank 12: 1722.10
     rank 13: 1722.26
     rank 14: 1723.56
     rank 15: 1723.03
     rank 16: 1886.77
     rank 17: 1888.30
     rank 18: 1870.51
     rank 19: 1871.59
  pure-backward-compute:
     rank  0: 545.88
     rank  1: 544.86
     rank  2: 538.38
     rank  3: 535.00
     rank  4: 1737.61
     rank  5: 1737.02
     rank  6: 1745.54
     rank  7: 1750.53
     rank  8: 1729.19
     rank  9: 1730.56
     rank 10: 1732.81
     rank 11: 1736.89
     rank 12: 1721.29
     rank 13: 1721.60
     rank 14: 1722.60
     rank 15: 1722.63
     rank 16: 1884.52
     rank 17: 1886.97
     rank 18: 1868.79
     rank 19: 1870.32
  batch-generator:
     rank  0: 43.52
     rank  1: 45.29
     rank  2: 27.34
     rank  3: 28.39
     rank  4: 37.41
     rank  5: 37.92
     rank  6: 28.39
     rank  7: 24.78
     rank  8: 26.91
     rank  9: 26.58
     rank 10: 34.26
     rank 11: 36.94
     rank 12: 28.23
     rank 13: 28.92
     rank 14: 32.54
     rank 15: 36.26
     rank 16: 27.84
     rank 17: 29.66
     rank 18: 32.88
     rank 19: 32.11
  forward-recv:
     rank  4: 78.03
     rank  5: 78.51
     rank  6: 78.70
     rank  7: 79.09
     rank  8: 282.70
     rank  9: 281.48
     rank 10: 287.88
     rank 11: 288.05
     rank 12: 468.57
     rank 13: 469.21
     rank 14: 480.62
     rank 15: 479.39
     rank 16: 656.23
     rank 17: 655.58
     rank 18: 650.39
     rank 19: 651.25
  forward-send:
     rank  0: 387.81
     rank  1: 387.00
     rank  2: 406.14
     rank  3: 406.99
     rank  4: 25.23
     rank  5: 24.75
     rank  6: 23.22
     rank  7: 23.31
     rank  8: 15.56
     rank  9: 15.62
     rank 10: 15.84
     rank 11: 16.00
     rank 12: 6.33
     rank 13: 6.37
     rank 14: 5.21
     rank 15: 6.13
  backward-recv:
     rank  0: 1506.99
     rank  1: 1509.31
     rank  2: 1514.90
     rank  3: 1515.08
     rank  4: 671.19
     rank  5: 671.48
     rank  6: 667.82
     rank  7: 666.27
     rank  8: 440.82
     rank  9: 440.84
     rank 10: 443.13
     rank 11: 442.62
     rank 12: 220.44
     rank 13: 220.50
     rank 14: 219.46
     rank 15: 221.06
  backward-send:
     rank  4: 20.59
     rank  5: 20.15
     rank  6: 20.53
     rank  7: 19.99
     rank  8: 27.76
     rank  9: 27.70
     rank 10: 27.10
     rank 11: 26.56
     rank 12: 19.62
     rank 13: 19.62
     rank 14: 19.49
     rank 15: 18.17
     rank 16: 9.64
     rank 17: 9.66
     rank 18: 9.02
     rank 19: 9.64
  forward-send-backward-recv:
     rank  0: 1958.77
     rank  1: 1959.60
     rank  2: 1979.46
     rank  3: 1982.94
     rank  4: 828.05
     rank  5: 829.85
     rank  6: 810.15
     rank  7: 807.27
     rank  8: 657.14
     rank  9: 656.27
     rank 10: 637.01
     rank 11: 633.85
     rank 12: 478.66
     rank 13: 477.96
     rank 14: 479.26
     rank 15: 481.20
  backward-send-forward-recv:
     rank  4: 21.85
     rank  5: 23.16
     rank  6: 20.10
     rank  7: 22.51
     rank  8: 52.97
     rank  9: 52.70
     rank 10: 53.76
     rank 11: 50.86
     rank 12: 61.19
     rank 13: 62.07
     rank 14: 59.48
     rank 15: 55.61
     rank 16: 74.65
     rank 17: 75.28
     rank 18: 70.84
     rank 19: 71.34
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 152.43
     rank  1: 152.59
     rank  2: 152.57
     rank  3: 152.60
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.01
     rank 16: 0.22
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.17
     rank  2: 0.22
     rank  3: 0.17
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.03
     rank 13: 0.01
     rank 14: 0.04
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.05
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.75
     rank  1: 19.62
     rank  2: 19.75
     rank  3: 19.60
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.08
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.10
     rank 13: 0.04
     rank 14: 0.28
     rank 15: 0.04
     rank 16: 0.09
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.08
  optimizer:
     rank  0: 20.77
     rank  1: 20.66
     rank  2: 20.78
     rank  3: 20.63
     rank  4: 1.06
     rank  5: 1.07
     rank  6: 1.06
     rank  7: 1.09
     rank  8: 1.10
     rank  9: 1.06
     rank 10: 1.06
     rank 11: 1.06
     rank 12: 1.13
     rank 13: 1.07
     rank 14: 1.32
     rank 15: 1.07
     rank 16: 1.12
     rank 17: 1.10
     rank 18: 1.10
     rank 19: 1.11
 [2024-12-05 20:27:33] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 5174.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.211859E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5123.20
     rank  1: 5123.08
     rank  2: 5123.17
     rank  3: 5123.16
     rank  4: 5121.42
     rank  5: 5121.38
     rank  6: 5121.42
     rank  7: 5121.43
     rank  8: 5121.48
     rank  9: 5121.47
     rank 10: 5121.54
     rank 11: 5121.48
     rank 12: 5121.56
     rank 13: 5121.50
     rank 14: 5121.58
     rank 15: 5121.46
     rank 16: 5121.46
     rank 17: 5121.36
     rank 18: 5121.40
     rank 19: 5121.41
  forward-compute:
     rank  0: 589.36
     rank  1: 589.37
     rank  2: 532.14
     rank  3: 530.85
     rank  4: 1512.45
     rank  5: 1514.44
     rank  6: 1538.76
     rank  7: 1536.77
     rank  8: 1486.44
     rank  9: 1487.64
     rank 10: 1473.81
     rank 11: 1472.24
     rank 12: 1478.70
     rank 13: 1479.12
     rank 14: 1458.80
     rank 15: 1462.13
     rank 16: 1599.56
     rank 17: 1600.53
     rank 18: 1640.11
     rank 19: 1637.93
  backward-compute:
     rank  0: 536.42
     rank  1: 535.17
     rank  2: 532.99
     rank  3: 532.37
     rank  4: 1738.15
     rank  5: 1737.66
     rank  6: 1746.63
     rank  7: 1747.76
     rank  8: 1726.95
     rank  9: 1727.36
     rank 10: 1730.97
     rank 11: 1732.33
     rank 12: 1721.54
     rank 13: 1721.87
     rank 14: 1726.05
     rank 15: 1723.41
     rank 16: 1887.81
     rank 17: 1887.59
     rank 18: 1867.61
     rank 19: 1869.00
  pure-backward-compute:
     rank  0: 535.77
     rank  1: 534.79
     rank  2: 532.53
     rank  3: 532.04
     rank  4: 1737.67
     rank  5: 1737.20
     rank  6: 1745.98
     rank  7: 1747.21
     rank  8: 1725.97
     rank  9: 1726.64
     rank 10: 1730.28
     rank 11: 1731.54
     rank 12: 1720.93
     rank 13: 1721.20
     rank 14: 1725.33
     rank 15: 1723.00
     rank 16: 1885.94
     rank 17: 1886.22
     rank 18: 1865.99
     rank 19: 1867.45
  batch-generator:
     rank  0: 36.77
     rank  1: 35.30
     rank  2: 26.39
     rank  3: 25.44
     rank  4: 30.23
     rank  5: 30.53
     rank  6: 26.90
     rank  7: 27.74
     rank  8: 38.76
     rank  9: 37.77
     rank 10: 32.56
     rank 11: 31.61
     rank 12: 29.97
     rank 13: 30.54
     rank 14: 28.96
     rank 15: 30.88
     rank 16: 27.92
     rank 17: 29.81
     rank 18: 33.77
     rank 19: 32.22
  forward-recv:
     rank  4: 85.18
     rank  5: 84.87
     rank  6: 74.26
     rank  7: 74.25
     rank  8: 285.62
     rank  9: 285.40
     rank 10: 288.03
     rank 11: 289.41
     rank 12: 471.23
     rank 13: 471.90
     rank 14: 470.25
     rank 15: 470.04
     rank 16: 657.31
     rank 17: 656.72
     rank 18: 640.19
     rank 19: 640.07
  forward-send:
     rank  0: 380.83
     rank  1: 380.55
     rank  2: 401.76
     rank  3: 403.10
     rank  4: 22.90
     rank  5: 22.75
     rank  6: 18.70
     rank  7: 19.56
     rank  8: 11.01
     rank  9: 11.14
     rank 10: 10.95
     rank 11: 10.66
     rank 12: 5.22
     rank 13: 4.91
     rank 14: 5.41
     rank 15: 5.26
  backward-recv:
     rank  0: 1510.19
     rank  1: 1514.59
     rank  2: 1509.72
     rank  3: 1509.92
     rank  4: 669.16
     rank  5: 666.64
     rank  6: 660.20
     rank  7: 659.42
     rank  8: 442.90
     rank  9: 442.94
     rank 10: 444.89
     rank 11: 445.05
     rank 12: 219.90
     rank 13: 219.73
     rank 14: 218.59
     rank 15: 219.30
  backward-send:
     rank  4: 15.78
     rank  5: 17.56
     rank  6: 17.05
     rank  7: 17.47
     rank  8: 22.53
     rank  9: 22.45
     rank 10: 21.95
     rank 11: 21.03
     rank 12: 16.35
     rank 13: 16.41
     rank 14: 16.11
     rank 15: 15.42
     rank 16: 7.85
     rank 17: 7.93
     rank 18: 7.70
     rank 19: 7.81
  forward-send-backward-recv:
     rank  0: 1944.16
     rank  1: 1943.61
     rank  2: 1984.86
     rank  3: 1985.98
     rank  4: 830.20
     rank  5: 831.66
     rank  6: 818.20
     rank  7: 817.74
     rank  8: 645.21
     rank  9: 645.30
     rank 10: 648.11
     rank 11: 647.43
     rank 12: 477.08
     rank 13: 476.33
     rank 14: 494.25
     rank 15: 496.80
  backward-send-forward-recv:
     rank  4: 22.30
     rank  5: 21.77
     rank  6: 20.76
     rank  7: 21.45
     rank  8: 54.56
     rank  9: 54.01
     rank 10: 50.72
     rank 11: 51.12
     rank 12: 62.19
     rank 13: 62.41
     rank 14: 59.07
     rank 15: 57.31
     rank 16: 74.98
     rank 17: 75.40
     rank 18: 68.76
     rank 19: 70.21
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 151.67
     rank  1: 151.84
     rank  2: 151.80
     rank  3: 151.84
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.01
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.41
     rank  1: 19.61
     rank  2: 19.57
     rank  3: 19.56
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.08
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.03
     rank 19: 0.05
  optimizer:
     rank  0: 20.03
     rank  1: 20.25
     rank  2: 20.20
     rank  3: 20.20
     rank  4: 0.66
     rank  5: 0.66
     rank  6: 0.66
     rank  7: 0.67
     rank  8: 0.69
     rank  9: 0.66
     rank 10: 0.69
     rank 11: 0.66
     rank 12: 0.70
     rank 13: 0.66
     rank 14: 0.67
     rank 15: 0.66
     rank 16: 0.69
     rank 17: 0.67
     rank 18: 0.66
     rank 19: 0.67
 [2024-12-05 20:27:39] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 5185.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.834378E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5143.45
     rank  1: 5143.40
     rank  2: 5143.42
     rank  3: 5143.44
     rank  4: 5141.67
     rank  5: 5141.68
     rank  6: 5141.66
     rank  7: 5141.73
     rank  8: 5141.73
     rank  9: 5141.72
     rank 10: 5141.70
     rank 11: 5141.73
     rank 12: 5141.80
     rank 13: 5141.74
     rank 14: 5141.77
     rank 15: 5141.77
     rank 16: 5141.63
     rank 17: 5141.62
     rank 18: 5141.60
     rank 19: 5141.61
  forward-compute:
     rank  0: 729.09
     rank  1: 731.59
     rank  2: 784.84
     rank  3: 785.00
     rank  4: 1519.25
     rank  5: 1520.87
     rank  6: 1541.65
     rank  7: 1538.98
     rank  8: 1475.18
     rank  9: 1475.76
     rank 10: 1471.19
     rank 11: 1472.86
     rank 12: 1477.66
     rank 13: 1478.55
     rank 14: 1464.93
     rank 15: 1467.43
     rank 16: 1632.14
     rank 17: 1632.51
     rank 18: 1624.55
     rank 19: 1622.62
  backward-compute:
     rank  0: 550.44
     rank  1: 548.08
     rank  2: 548.05
     rank  3: 542.28
     rank  4: 1739.97
     rank  5: 1739.72
     rank  6: 1748.13
     rank  7: 1751.98
     rank  8: 1724.87
     rank  9: 1725.39
     rank 10: 1727.48
     rank 11: 1729.03
     rank 12: 1726.18
     rank 13: 1726.11
     rank 14: 1727.39
     rank 15: 1725.45
     rank 16: 1883.88
     rank 17: 1886.47
     rank 18: 1867.85
     rank 19: 1869.15
  pure-backward-compute:
     rank  0: 549.89
     rank  1: 547.77
     rank  2: 547.71
     rank  3: 541.84
     rank  4: 1739.46
     rank  5: 1739.25
     rank  6: 1747.41
     rank  7: 1751.19
     rank  8: 1723.76
     rank  9: 1724.52
     rank 10: 1726.70
     rank 11: 1728.32
     rank 12: 1725.64
     rank 13: 1725.61
     rank 14: 1726.67
     rank 15: 1724.94
     rank 16: 1881.99
     rank 17: 1884.69
     rank 18: 1866.45
     rank 19: 1867.46
  batch-generator:
     rank  0: 34.18
     rank  1: 37.13
     rank  2: 37.10
     rank  3: 36.76
     rank  4: 38.81
     rank  5: 38.15
     rank  6: 30.67
     rank  7: 32.05
     rank  8: 32.63
     rank  9: 33.47
     rank 10: 33.43
     rank 11: 36.13
     rank 12: 27.02
     rank 13: 28.09
     rank 14: 31.71
     rank 15: 33.45
     rank 16: 29.27
     rank 17: 30.73
     rank 18: 35.61
     rank 19: 34.49
  forward-recv:
     rank  4: 73.58
     rank  5: 73.35
     rank  6: 99.13
     rank  7: 99.15
     rank  8: 273.68
     rank  9: 273.83
     rank 10: 310.85
     rank 11: 311.20
     rank 12: 456.61
     rank 13: 456.14
     rank 14: 490.52
     rank 15: 490.27
     rank 16: 635.10
     rank 17: 635.04
     rank 18: 666.41
     rank 19: 666.44
  forward-send:
     rank  0: 369.58
     rank  1: 368.82
     rank  2: 354.36
     rank  3: 354.61
     rank  4: 16.42
     rank  5: 16.19
     rank  6: 16.42
     rank  7: 16.31
     rank  8: 11.04
     rank  9: 10.84
     rank 10: 10.95
     rank 11: 10.79
     rank 12: 5.35
     rank 13: 5.57
     rank 14: 5.27
     rank 15: 5.34
  backward-recv:
     rank  0: 1516.47
     rank  1: 1517.88
     rank  2: 1513.62
     rank  3: 1516.19
     rank  4: 677.98
     rank  5: 676.19
     rank  6: 686.16
     rank  7: 681.25
     rank  8: 446.62
     rank  9: 446.40
     rank 10: 448.78
     rank 11: 449.11
     rank 12: 222.59
     rank 13: 222.62
     rank 14: 224.25
     rank 15: 225.18
  backward-send:
     rank  4: 14.85
     rank  5: 15.71
     rank  6: 11.95
     rank  7: 14.75
     rank  8: 23.46
     rank  9: 23.33
     rank 10: 22.89
     rank 11: 22.49
     rank 12: 16.01
     rank 13: 16.08
     rank 14: 15.68
     rank 15: 15.21
     rank 16: 8.97
     rank 17: 9.00
     rank 18: 8.37
     rank 19: 9.00
  forward-send-backward-recv:
     rank  0: 1814.77
     rank  1: 1815.05
     rank  2: 1777.27
     rank  3: 1780.13
     rank  4: 846.49
     rank  5: 847.05
     rank  6: 788.07
     rank  7: 786.24
     rank  8: 680.89
     rank  9: 680.78
     rank 10: 641.95
     rank 11: 640.74
     rank 12: 495.83
     rank 13: 495.40
     rank 14: 475.10
     rank 15: 476.82
  backward-send-forward-recv:
     rank  4: 21.90
     rank  5: 22.00
     rank  6: 18.74
     rank  7: 20.83
     rank  8: 52.10
     rank  9: 52.29
     rank 10: 49.34
     rank 11: 47.57
     rank 12: 62.56
     rank 13: 62.46
     rank 14: 57.80
     rank 15: 55.97
     rank 16: 74.51
     rank 17: 74.28
     rank 18: 67.09
     rank 19: 68.11
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 153.97
     rank  1: 153.97
     rank  2: 153.98
     rank  3: 153.95
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.07
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.16
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.06
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.64
     rank  1: 19.69
     rank  2: 19.74
     rank  3: 19.48
     rank  4: 0.04
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.07
     rank  9: 0.07
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 20.29
     rank  1: 20.34
     rank  2: 20.39
     rank  3: 20.12
     rank  4: 0.68
     rank  5: 0.71
     rank  6: 0.71
     rank  7: 0.72
     rank  8: 0.71
     rank  9: 0.71
     rank 10: 0.68
     rank 11: 0.68
     rank 12: 0.71
     rank 13: 0.71
     rank 14: 0.71
     rank 15: 0.68
     rank 16: 0.71
     rank 17: 0.68
     rank 18: 0.67
     rank 19: 0.67
 [2024-12-05 20:27:44] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 5166.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.629078E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 5124.57
     rank  1: 5124.64
     rank  2: 5124.55
     rank  3: 5124.63
     rank  4: 5122.87
     rank  5: 5122.88
     rank  6: 5122.81
     rank  7: 5122.86
     rank  8: 5122.96
     rank  9: 5122.92
     rank 10: 5122.88
     rank 11: 5122.88
     rank 12: 5122.94
     rank 13: 5123.14
     rank 14: 5122.98
     rank 15: 5122.97
     rank 16: 5122.75
     rank 17: 5122.96
     rank 18: 5122.74
     rank 19: 5122.88
  forward-compute:
     rank  0: 538.04
     rank  1: 539.07
     rank  2: 563.04
     rank  3: 560.00
     rank  4: 1537.68
     rank  5: 1541.17
     rank  6: 1542.54
     rank  7: 1538.80
     rank  8: 1480.16
     rank  9: 1481.63
     rank 10: 1474.21
     rank 11: 1470.87
     rank 12: 1481.43
     rank 13: 1480.71
     rank 14: 1465.47
     rank 15: 1467.74
     rank 16: 1599.84
     rank 17: 1599.86
     rank 18: 1629.13
     rank 19: 1627.53
  backward-compute:
     rank  0: 534.99
     rank  1: 538.06
     rank  2: 555.16
     rank  3: 547.29
     rank  4: 1734.15
     rank  5: 1732.81
     rank  6: 1745.25
     rank  7: 1746.02
     rank  8: 1726.33
     rank  9: 1726.24
     rank 10: 1730.23
     rank 11: 1732.66
     rank 12: 1726.54
     rank 13: 1726.51
     rank 14: 1728.44
     rank 15: 1728.05
     rank 16: 1889.74
     rank 17: 1890.95
     rank 18: 1866.53
     rank 19: 1867.42
  pure-backward-compute:
     rank  0: 534.65
     rank  1: 537.71
     rank  2: 554.82
     rank  3: 546.91
     rank  4: 1733.24
     rank  5: 1732.42
     rank  6: 1744.66
     rank  7: 1745.42
     rank  8: 1725.05
     rank  9: 1725.44
     rank 10: 1729.31
     rank 11: 1731.50
     rank 12: 1725.86
     rank 13: 1725.30
     rank 14: 1727.81
     rank 15: 1727.63
     rank 16: 1888.23
     rank 17: 1889.46
     rank 18: 1864.87
     rank 19: 1865.67
  batch-generator:
     rank  0: 32.19
     rank  1: 32.75
     rank  2: 31.64
     rank  3: 28.27
     rank  4: 54.29
     rank  5: 51.17
     rank  6: 35.91
     rank  7: 29.87
     rank  8: 35.93
     rank  9: 35.49
     rank 10: 35.46
     rank 11: 32.72
     rank 12: 27.14
     rank 13: 27.47
     rank 14: 31.59
     rank 15: 33.36
     rank 16: 26.78
     rank 17: 28.08
     rank 18: 34.81
     rank 19: 33.64
  forward-recv:
     rank  4: 78.96
     rank  5: 78.73
     rank  6: 76.67
     rank  7: 77.44
     rank  8: 291.24
     rank  9: 290.30
     rank 10: 291.41
     rank 11: 293.42
     rank 12: 473.31
     rank 13: 473.44
     rank 14: 473.80
     rank 15: 473.38
     rank 16: 654.07
     rank 17: 654.00
     rank 18: 651.02
     rank 19: 651.15
  forward-send:
     rank  0: 401.01
     rank  1: 399.42
     rank  2: 411.92
     rank  3: 414.79
     rank  4: 25.35
     rank  5: 24.17
     rank  6: 23.21
     rank  7: 24.99
     rank  8: 16.97
     rank  9: 16.96
     rank 10: 16.54
     rank 11: 16.40
     rank 12: 7.94
     rank 13: 7.96
     rank 14: 7.44
     rank 15: 7.56
  backward-recv:
     rank  0: 1515.62
     rank  1: 1514.87
     rank  2: 1509.48
     rank  3: 1515.13
     rank  4: 672.05
     rank  5: 673.36
     rank  6: 676.73
     rank  7: 671.84
     rank  8: 441.20
     rank  9: 441.42
     rank 10: 441.95
     rank 11: 441.50
     rank 12: 221.38
     rank 13: 221.38
     rank 14: 221.03
     rank 15: 221.53
  backward-send:
     rank  4: 16.57
     rank  5: 16.24
     rank  6: 11.11
     rank  7: 15.98
     rank  8: 25.16
     rank  9: 25.10
     rank 10: 24.20
     rank 11: 24.53
     rank 12: 15.86
     rank 13: 15.89
     rank 14: 15.58
     rank 15: 14.75
     rank 16: 7.53
     rank 17: 7.57
     rank 18: 7.30
     rank 19: 7.29
  forward-send-backward-recv:
     rank  0: 1974.20
     rank  1: 1974.06
     rank  2: 1924.45
     rank  3: 1927.22
     rank  4: 808.58
     rank  5: 810.63
     rank  6: 802.49
     rank  7: 803.82
     rank  8: 641.75
     rank  9: 642.50
     rank 10: 646.27
     rank 11: 645.07
     rank 12: 463.28
     rank 13: 462.63
     rank 14: 479.70
     rank 15: 481.28
  backward-send-forward-recv:
     rank  4: 20.09
     rank  5: 19.89
     rank  6: 16.57
     rank  7: 17.17
     rank  8: 51.67
     rank  9: 51.59
     rank 10: 45.80
     rank 11: 46.66
     rank 12: 60.94
     rank 13: 61.30
     rank 14: 55.76
     rank 15: 54.02
     rank 16: 73.82
     rank 17: 74.21
     rank 18: 67.86
     rank 19: 68.97
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.08
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 151.41
     rank  1: 151.45
     rank  2: 151.43
     rank  3: 151.46
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 19.54
     rank  1: 19.58
     rank  2: 19.57
     rank  3: 19.48
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.09
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.09
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 20.32
     rank  1: 20.35
     rank  2: 20.34
     rank  3: 20.24
     rank  4: 0.82
     rank  5: 0.81
     rank  6: 0.81
     rank  7: 0.81
     rank  8: 0.87
     rank  9: 0.81
     rank 10: 0.81
     rank 11: 0.85
     rank 12: 0.80
     rank 13: 0.84
     rank 14: 0.84
     rank 15: 0.81
     rank 16: 0.83
     rank 17: 0.86
     rank 18: 0.80
     rank 19: 0.80
