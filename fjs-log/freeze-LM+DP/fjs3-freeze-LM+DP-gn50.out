examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
4
[2024-12-05 23:36:58,647] torch.distributed.run: [WARNING] 
[2024-12-05 23:36:58,647] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 23:36:58,647] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 23:36:58,647] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [0, 0]
---Rank 19---Tensor Parallel Group GPUs: [1, 1]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Tensor Parallel Group GPUs: [1, 1]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Tensor Parallel Group GPUs: [0, 0]
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 1593954304
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:Falsename:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:Falsename:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False

name:module.language_model.decoder.layers.12.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:Falsename:module.language_model.decoder.layers.12.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.12.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:Falsename:module.language_model.decoder.layers.13.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False

name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:Falsename:module.language_model.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.13.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.13.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False

name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1593954304
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.12.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_proj.weight param:torch.Size([4096, 2048]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.self_attention.linear_qkv.weight param:torch.Size([3072, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc1.weight param:torch.Size([14336, 4096]) require_grad:False
name:module.language_model.decoder.layers.13.mlp.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([16384, 4096]) require_grad:False
rank=1, worker=0: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<1094>, pretrain-38.tar[9700, 9800), pretrain-38.tar[9800, 9900), pretrain-38.tar[9900, 10000)] sum(count)=110000
rank=1, worker=1: shard_range=[pretrain-39.tar[0, 100), pretrain-39.tar[100, 200), pretrain-39.tar[200, 300), ...<1094>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=110000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<1094>, pretrain-18.tar[9700, 9800), pretrain-18.tar[9800, 9900), pretrain-18.tar[9900, 10000)] sum(count)=110000
rank=0, worker=1: shard_range=[pretrain-19.tar[0, 100), pretrain-19.tar[100, 200), pretrain-19.tar[200, 300), ...<1094>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=110000
rank=1, worker=0: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 5000)] sum(count)=15000
rank=1, worker=1: shard_range=[pretrain-52.tar[5000, 10000), pretrain-53.tar[0, 10000)] sum(count)=15000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 5000)] sum(count)=15000
rank=0, worker=1: shard_range=[pretrain-5.tar[5000, 10000), pretrain-50.tar[0, 10000)] sum(count)=15000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 73.32
     rank  1: 73.66
     rank  2: 71.00
     rank  3: 73.41
     rank  4: 39.81
     rank  5: 33.19
     rank  6: 33.45
     rank  7: 38.95
     rank  8: 60.94
     rank  9: 60.98
     rank 10: 59.08
     rank 11: 59.38
     rank 12: 51.32
     rank 13: 69.39
     rank 14: 68.52
     rank 15: 50.47
     rank 16: 72.06
     rank 17: 55.54
     rank 18: 57.60
     rank 19: 57.49
  train/valid/test-data-iterators-setup:
     rank  0: 1065.78
     rank  1: 1065.69
     rank  2: 1404.97
     rank  3: 1065.69
     rank  4: 1404.81
     rank  5: 1406.06
     rank  6: 1270.59
     rank  7: 1405.04
     rank  8: 1404.52
     rank  9: 1405.89
     rank 10: 1404.68
     rank 11: 1405.42
     rank 12: 1404.77
     rank 13: 1404.67
     rank 14: 1404.99
     rank 15: 1404.89
     rank 16: 1405.10
     rank 17: 1404.67
     rank 18: 1404.73
     rank 19: 1404.96
 [2024-12-05 23:37:43] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 19738.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.091660E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 6165.7060546875 | max allocated: 12101.4755859375 | reserved: 13404.0 | max reserved: 13404.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 6165.7060546875 | max allocated: 12101.4755859375 | reserved: 13308.0 | max reserved: 13308.0
times across ranks (ms):
  forward-backward:
     rank  0: 19627.38
     rank  1: 19627.15
     rank  2: 19627.28
     rank  3: 19627.17
     rank  4: 19626.28
     rank  5: 19626.04
     rank  6: 19627.25
     rank  7: 19626.81
     rank  8: 19683.35
     rank  9: 19687.96
     rank 10: 19683.50
     rank 11: 19687.91
     rank 12: 19691.72
     rank 13: 19688.93
     rank 14: 19691.82
     rank 15: 19688.90
     rank 16: 19621.09
     rank 17: 19621.08
     rank 18: 19621.07
     rank 19: 19621.09
  forward-compute:
     rank  0: 2691.57
     rank  1: 2698.01
     rank  2: 2754.89
     rank  3: 2751.41
     rank  4: 2784.03
     rank  5: 2788.26
     rank  6: 2739.66
     rank  7: 2738.40
     rank  8: 4278.51
     rank  9: 4287.66
     rank 10: 3436.86
     rank 11: 3450.91
     rank 12: 4672.74
     rank 13: 4680.49
     rank 14: 4373.08
     rank 15: 4385.20
     rank 16: 5065.09
     rank 17: 5068.68
     rank 18: 6217.42
     rank 19: 6217.60
  backward-compute:
     rank  0: 1473.91
     rank  1: 1471.93
     rank  2: 1501.58
     rank  3: 1502.92
     rank  4: 1417.52
     rank  5: 1421.07
     rank  6: 1398.34
     rank  7: 1402.79
     rank  8: 2289.23
     rank  9: 2283.71
     rank 10: 2164.29
     rank 11: 2172.59
     rank 12: 2820.16
     rank 13: 2822.95
     rank 14: 2833.06
     rank 15: 2830.18
     rank 16: 3155.89
     rank 17: 3155.18
     rank 18: 3143.61
     rank 19: 3143.71
  pure-backward-compute:
     rank  0: 1473.11
     rank  1: 1471.36
     rank  2: 1500.99
     rank  3: 1502.35
     rank  4: 1416.71
     rank  5: 1420.19
     rank  6: 1397.36
     rank  7: 1401.93
     rank  8: 2287.80
     rank  9: 2282.78
     rank 10: 2162.79
     rank 11: 2171.78
     rank 12: 2819.30
     rank 13: 2822.18
     rank 14: 2831.89
     rank 15: 2829.57
     rank 16: 3154.29
     rank 17: 3153.87
     rank 18: 3142.43
     rank 19: 3141.98
  batch-generator:
     rank  0: 1081.31
     rank  1: 1084.08
     rank  2: 1146.61
     rank  3: 1131.36
     rank  4: 1308.91
     rank  5: 1313.01
     rank  6: 1263.68
     rank  7: 1259.49
     rank  8: 1998.99
     rank  9: 2012.95
     rank 10: 1311.44
     rank 11: 1324.61
     rank 12: 1120.91
     rank 13: 1130.64
     rank 14: 884.46
     rank 15: 897.70
     rank 16: 1134.67
     rank 17: 1135.66
     rank 18: 988.26
     rank 19: 987.35
  forward-recv:
     rank  4: 2261.63
     rank  5: 2262.54
     rank  6: 2299.93
     rank  7: 2313.13
     rank  8: 4732.05
     rank  9: 4730.39
     rank 10: 4744.33
     rank 11: 4739.37
     rank 12: 7144.17
     rank 13: 7139.09
     rank 14: 6443.28
     rank 15: 6439.54
     rank 16: 9773.67
     rank 17: 9774.78
     rank 18: 8765.66
     rank 19: 8765.21
  forward-send:
     rank  0: 7431.58
     rank  1: 7426.37
     rank  2: 6357.43
     rank  3: 6359.51
     rank  4: 4983.45
     rank  5: 4977.77
     rank  6: 3956.94
     rank  7: 3945.56
     rank  8: 2400.05
     rank  9: 2396.01
     rank 10: 2112.98
     rank 11: 2107.61
     rank 12: 27.20
     rank 13: 28.38
     rank 14: 30.84
     rank 15: 30.48
  backward-recv:
     rank  0: 2527.76
     rank  1: 2527.37
     rank  2: 2571.96
     rank  3: 2571.97
     rank  4: 2059.18
     rank  5: 2058.72
     rank  6: 2130.41
     rank  7: 2129.18
     rank  8: 903.26
     rank  9: 906.32
     rank 10: 930.05
     rank 11: 928.70
     rank 12: 374.08
     rank 13: 373.78
     rank 14: 370.56
     rank 15: 373.82
  backward-send:
     rank  4: 4.85
     rank  5: 3.96
     rank  6: 2.91
     rank  7: 2.78
     rank  8: 14.41
     rank  9: 13.90
     rank 10: 8.98
     rank 11: 8.29
     rank 12: 21.28
     rank 13: 23.30
     rank 14: 20.24
     rank 15: 18.46
     rank 16: 7.13
     rank 17: 7.03
     rank 18: 5.85
     rank 19: 7.01
  forward-send-backward-recv:
     rank  0: 5238.17
     rank  1: 5239.84
     rank  2: 6154.71
     rank  3: 6154.39
     rank  4: 4834.41
     rank  5: 4831.03
     rank  6: 5751.45
     rank  7: 5747.79
     rank  8: 3671.37
     rank  9: 3671.64
     rank 10: 4840.95
     rank 11: 4835.08
     rank 12: 3153.75
     rank 13: 3152.46
     rank 14: 4205.69
     rank 15: 4209.89
  backward-send-forward-recv:
     rank  4: 934.16
     rank  5: 934.94
     rank  6: 974.23
     rank  7: 974.50
     rank  8: 937.94
     rank  9: 939.90
     rank 10: 969.14
     rank 11: 967.02
     rank 12: 728.92
     rank 13: 725.29
     rank 14: 661.02
     rank 15: 653.31
     rank 16: 581.86
     rank 17: 579.66
     rank 18: 446.94
     rank 19: 447.78
  layernorm-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.03
     rank  2: 0.04
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.10
     rank  9: 0.03
     rank 10: 0.05
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.10
     rank  5: 0.14
     rank  6: 0.13
     rank  7: 0.12
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 249.87
     rank  1: 243.41
     rank  2: 249.79
     rank  3: 243.61
     rank  4: 230.19
     rank  5: 229.24
     rank  6: 230.28
     rank  7: 229.34
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.21
     rank  2: 0.21
     rank  3: 0.21
     rank  4: 0.14
     rank  5: 0.15
     rank  6: 0.19
     rank  7: 0.15
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 40.72
     rank  1: 40.89
     rank  2: 40.77
     rank  3: 40.96
     rank  4: 37.55
     rank  5: 37.29
     rank  6: 37.60
     rank  7: 41.65
     rank  8: 0.23
     rank  9: 0.17
     rank 10: 0.28
     rank 11: 0.25
     rank 12: 0.22
     rank 13: 0.12
     rank 14: 0.21
     rank 15: 0.10
     rank 16: 0.09
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.07
  optimizer:
     rank  0: 42.38
     rank  1: 42.54
     rank  2: 42.42
     rank  3: 42.62
     rank  4: 39.20
     rank  5: 38.94
     rank  6: 39.25
     rank  7: 43.30
     rank  8: 1.85
     rank  9: 1.79
     rank 10: 1.92
     rank 11: 1.90
     rank 12: 1.88
     rank 13: 1.74
     rank 14: 1.86
     rank 15: 1.71
     rank 16: 1.74
     rank 17: 1.72
     rank 18: 1.71
     rank 19: 1.72
 [2024-12-05 23:37:51] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7616.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.141831E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7558.39
     rank  1: 7558.43
     rank  2: 7558.38
     rank  3: 7558.47
     rank  4: 7557.94
     rank  5: 7557.93
     rank  6: 7558.04
     rank  7: 7557.88
     rank  8: 7556.76
     rank  9: 7556.63
     rank 10: 7556.89
     rank 11: 7556.72
     rank 12: 7556.64
     rank 13: 7556.59
     rank 14: 7556.72
     rank 15: 7556.56
     rank 16: 7556.54
     rank 17: 7556.46
     rank 18: 7556.61
     rank 19: 7556.53
  forward-compute:
     rank  0: 633.68
     rank  1: 638.21
     rank  2: 639.09
     rank  3: 641.09
     rank  4: 360.02
     rank  5: 364.62
     rank  6: 342.54
     rank  7: 344.14
     rank  8: 2174.79
     rank  9: 2180.62
     rank 10: 2039.85
     rank 11: 2048.03
     rank 12: 2361.74
     rank 13: 2367.66
     rank 14: 2366.19
     rank 15: 2374.44
     rank 16: 2652.99
     rank 17: 2653.40
     rank 18: 2718.72
     rank 19: 2714.22
  backward-compute:
     rank  0: 693.77
     rank  1: 694.30
     rank  2: 700.35
     rank  3: 699.73
     rank  4: 369.39
     rank  5: 371.34
     rank  6: 353.24
     rank  7: 361.30
     rank  8: 2240.45
     rank  9: 2241.31
     rank 10: 2133.34
     rank 11: 2136.18
     rank 12: 2805.34
     rank 13: 2806.78
     rank 14: 2809.53
     rank 15: 2803.19
     rank 16: 3167.06
     rank 17: 3168.62
     rank 18: 3133.23
     rank 19: 3136.11
  pure-backward-compute:
     rank  0: 693.28
     rank  1: 693.87
     rank  2: 699.90
     rank  3: 699.29
     rank  4: 368.78
     rank  5: 370.70
     rank  6: 352.59
     rank  7: 360.51
     rank  8: 2239.80
     rank  9: 2240.70
     rank 10: 2131.89
     rank 11: 2135.54
     rank 12: 2804.76
     rank 13: 2805.99
     rank 14: 2808.82
     rank 15: 2802.71
     rank 16: 3165.53
     rank 17: 3167.49
     rank 18: 3132.02
     rank 19: 3134.62
  batch-generator:
     rank  0: 51.98
     rank  1: 56.26
     rank  2: 39.47
     rank  3: 43.40
     rank  4: 39.80
     rank  5: 45.04
     rank  6: 41.31
     rank  7: 44.04
     rank  8: 44.71
     rank  9: 51.97
     rank 10: 62.40
     rank 11: 67.03
     rank 12: 30.32
     rank 13: 37.12
     rank 14: 35.99
     rank 15: 43.03
     rank 16: 32.08
     rank 17: 29.91
     rank 18: 30.84
     rank 19: 28.12
  forward-recv:
     rank  4: 109.14
     rank  5: 109.19
     rank  6: 111.97
     rank  7: 111.68
     rank  8: 141.66
     rank  9: 140.76
     rank 10: 139.78
     rank 11: 139.78
     rank 12: 421.35
     rank 13: 419.21
     rank 14: 396.26
     rank 15: 395.87
     rank 16: 716.18
     rank 17: 716.37
     rank 18: 693.22
     rank 19: 692.70
  forward-send:
     rank  0: 412.87
     rank  1: 408.37
     rank  2: 379.30
     rank  3: 377.09
     rank  4: 479.16
     rank  5: 475.12
     rank  6: 454.52
     rank  7: 452.54
     rank  8: 40.51
     rank  9: 38.99
     rank 10: 58.58
     rank 11: 57.39
     rank 12: 5.47
     rank 13: 5.47
     rank 14: 5.18
     rank 15: 4.85
  backward-recv:
     rank  0: 2588.74
     rank  1: 2588.61
     rank  2: 2586.34
     rank  3: 2586.41
     rank  4: 2052.38
     rank  5: 2053.05
     rank  6: 2071.05
     rank  7: 2068.85
     rank  8: 897.27
     rank  9: 898.32
     rank 10: 919.22
     rank 11: 922.13
     rank 12: 372.66
     rank 13: 373.07
     rank 14: 374.67
     rank 15: 376.31
  backward-send:
     rank  4: 4.73
     rank  5: 3.95
     rank  6: 3.19
     rank  7: 2.96
     rank  8: 14.80
     rank  9: 14.06
     rank 10: 8.99
     rank 11: 7.87
     rank 12: 17.45
     rank 13: 17.41
     rank 14: 13.00
     rank 15: 14.52
     rank 16: 8.41
     rank 17: 8.37
     rank 18: 7.26
     rank 19: 8.34
  forward-send-backward-recv:
     rank  0: 3059.65
     rank  1: 3059.16
     rank  2: 3061.37
     rank  3: 3062.12
     rank  4: 3919.87
     rank  5: 3916.97
     rank  6: 3937.30
     rank  7: 3931.52
     rank  8: 1705.62
     rank  9: 1704.56
     rank 10: 1911.36
     rank 11: 1906.56
     rank 12: 904.21
     rank 13: 902.39
     rank 14: 937.60
     rank 15: 944.46
  backward-send-forward-recv:
     rank  4: 9.81
     rank  5: 9.80
     rank  6: 7.15
     rank  7: 7.63
     rank  8: 41.33
     rank  9: 38.39
     rank 10: 18.84
     rank 11: 16.18
     rank 12: 83.24
     rank 13: 79.33
     rank 14: 59.47
     rank 15: 52.66
     rank 16: 66.10
     rank 17: 66.05
     rank 18: 54.89
     rank 19: 58.95
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.09
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.07
     rank  5: 0.11
     rank  6: 0.13
     rank  7: 0.09
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.08
     rank 19: 0.07
  all-grads-sync:
     rank  0: 160.85
     rank  1: 153.82
     rank  2: 160.98
     rank  3: 153.84
     rank  4: 138.15
     rank  5: 134.52
     rank  6: 138.24
     rank  7: 134.54
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.14
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.08
     rank 19: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.09
     rank  5: 0.11
     rank  6: 0.09
     rank  7: 0.08
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.52
     rank  1: 20.32
     rank  2: 20.34
     rank  3: 20.39
     rank  4: 14.12
     rank  5: 14.45
     rank  6: 14.54
     rank  7: 14.20
     rank  8: 0.04
     rank  9: 0.08
     rank 10: 0.13
     rank 11: 0.12
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.09
     rank 19: 0.08
  optimizer:
     rank  0: 21.42
     rank  1: 21.22
     rank  2: 21.24
     rank  3: 21.28
     rank  4: 15.01
     rank  5: 15.35
     rank  6: 15.44
     rank  7: 15.09
     rank  8: 0.94
     rank  9: 0.97
     rank 10: 1.02
     rank 11: 1.01
     rank 12: 0.92
     rank 13: 0.95
     rank 14: 0.92
     rank 15: 0.92
     rank 16: 0.96
     rank 17: 0.96
     rank 18: 0.98
     rank 19: 0.97
 [2024-12-05 23:37:59] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7612.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.847889E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7561.73
     rank  1: 7561.58
     rank  2: 7561.67
     rank  3: 7561.58
     rank  4: 7561.09
     rank  5: 7561.06
     rank  6: 7561.18
     rank  7: 7561.17
     rank  8: 7560.04
     rank  9: 7559.85
     rank 10: 7560.05
     rank 11: 7559.86
     rank 12: 7559.86
     rank 13: 7559.79
     rank 14: 7559.80
     rank 15: 7559.83
     rank 16: 7559.83
     rank 17: 7559.64
     rank 18: 7559.82
     rank 19: 7559.65
  forward-compute:
     rank  0: 623.87
     rank  1: 629.65
     rank  2: 636.71
     rank  3: 638.37
     rank  4: 354.00
     rank  5: 358.90
     rank  6: 348.09
     rank  7: 349.76
     rank  8: 2196.77
     rank  9: 2206.21
     rank 10: 2054.88
     rank 11: 2060.92
     rank 12: 2372.23
     rank 13: 2377.09
     rank 14: 2352.55
     rank 15: 2362.22
     rank 16: 2650.86
     rank 17: 2651.62
     rank 18: 2692.72
     rank 19: 2688.87
  backward-compute:
     rank  0: 685.15
     rank  1: 685.54
     rank  2: 700.56
     rank  3: 699.15
     rank  4: 365.09
     rank  5: 372.64
     rank  6: 357.11
     rank  7: 361.19
     rank  8: 2253.78
     rank  9: 2249.99
     rank 10: 2134.28
     rank 11: 2143.86
     rank 12: 2809.00
     rank 13: 2810.51
     rank 14: 2799.41
     rank 15: 2794.68
     rank 16: 3160.98
     rank 17: 3162.37
     rank 18: 3147.45
     rank 19: 3151.02
  pure-backward-compute:
     rank  0: 684.65
     rank  1: 685.06
     rank  2: 700.12
     rank  3: 698.74
     rank  4: 364.47
     rank  5: 372.01
     rank  6: 356.42
     rank  7: 360.36
     rank  8: 2252.70
     rank  9: 2249.33
     rank 10: 2133.10
     rank 11: 2143.22
     rank 12: 2808.12
     rank 13: 2809.85
     rank 14: 2798.54
     rank 15: 2794.14
     rank 16: 3159.44
     rank 17: 3161.27
     rank 18: 3145.95
     rank 19: 3149.43
  batch-generator:
     rank  0: 44.70
     rank  1: 50.91
     rank  2: 36.89
     rank  3: 40.83
     rank  4: 41.40
     rank  5: 46.26
     rank  6: 46.22
     rank  7: 47.74
     rank  8: 54.74
     rank  9: 62.95
     rank 10: 68.25
     rank 11: 71.23
     rank 12: 33.38
     rank 13: 39.16
     rank 14: 29.98
     rank 15: 41.09
     rank 16: 25.80
     rank 17: 26.63
     rank 18: 31.71
     rank 19: 29.36
  forward-recv:
     rank  4: 111.23
     rank  5: 111.50
     rank  6: 114.39
     rank  7: 114.49
     rank  8: 134.38
     rank  9: 134.22
     rank 10: 137.83
     rank 11: 137.84
     rank 12: 418.90
     rank 13: 414.58
     rank 14: 396.73
     rank 15: 395.97
     rank 16: 714.96
     rank 17: 715.17
     rank 18: 692.35
     rank 19: 691.59
  forward-send:
     rank  0: 414.74
     rank  1: 409.34
     rank  2: 377.06
     rank  3: 375.14
     rank  4: 478.91
     rank  5: 474.28
     rank  6: 447.84
     rank  7: 445.52
     rank  8: 40.76
     rank  9: 38.28
     rank 10: 50.23
     rank 11: 49.11
     rank 12: 5.33
     rank 13: 5.53
     rank 14: 5.18
     rank 15: 4.82
  backward-recv:
     rank  0: 2601.04
     rank  1: 2600.58
     rank  2: 2596.49
     rank  3: 2597.38
     rank  4: 2065.83
     rank  5: 2065.50
     rank  6: 2073.36
     rank  7: 2072.64
     rank  8: 903.22
     rank  9: 905.34
     rank 10: 933.12
     rank 11: 929.83
     rank 12: 376.07
     rank 13: 375.38
     rank 14: 379.75
     rank 15: 382.47
  backward-send:
     rank  4: 4.87
     rank  5: 3.51
     rank  6: 2.55
     rank  7: 3.38
     rank  8: 14.30
     rank  9: 13.00
     rank 10: 9.26
     rank 11: 9.18
     rank 12: 19.14
     rank 13: 19.24
     rank 14: 17.50
     rank 15: 14.91
     rank 16: 10.00
     rank 17: 9.40
     rank 18: 10.09
     rank 19: 10.11
  forward-send-backward-recv:
     rank  0: 3062.25
     rank  1: 3062.60
     rank  2: 3059.87
     rank  3: 3060.15
     rank  4: 3914.85
     rank  5: 3908.39
     rank  6: 3931.80
     rank  7: 3928.35
     rank  8: 1670.15
     rank  9: 1672.55
     rank 10: 1895.06
     rank 11: 1889.97
     rank 12: 888.23
     rank 13: 887.27
     rank 14: 947.76
     rank 15: 953.93
  backward-send-forward-recv:
     rank  4: 9.72
     rank  5: 9.40
     rank  6: 6.98
     rank  7: 7.30
     rank  8: 41.25
     rank  9: 36.67
     rank 10: 19.84
     rank 11: 17.60
     rank 12: 81.95
     rank 13: 81.85
     rank 14: 66.17
     rank 15: 58.44
     rank 16: 71.67
     rank 17: 71.08
     rank 18: 66.23
     rank 19: 69.39
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.02
     rank 10: 0.09
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.06
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.10
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.11
     rank 19: 0.05
  all-grads-sync:
     rank  0: 165.30
     rank  1: 157.76
     rank  2: 165.26
     rank  3: 157.54
     rank  4: 137.23
     rank  5: 133.19
     rank  6: 137.31
     rank  7: 133.20
     rank  8: 0.08
     rank  9: 0.02
     rank 10: 0.13
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.05
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.09
     rank  5: 0.09
     rank  6: 0.09
     rank  7: 0.09
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.52
     rank  1: 20.17
     rank  2: 20.37
     rank  3: 20.35
     rank  4: 13.96
     rank  5: 14.17
     rank  6: 14.53
     rank  7: 14.22
     rank  8: 0.24
     rank  9: 0.10
     rank 10: 0.12
     rank 11: 0.08
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 0.05
     rank 17: 0.07
     rank 18: 0.08
     rank 19: 0.07
  optimizer:
     rank  0: 21.32
     rank  1: 20.98
     rank  2: 21.18
     rank  3: 21.16
     rank  4: 14.77
     rank  5: 14.98
     rank  6: 15.33
     rank  7: 15.02
     rank  8: 1.05
     rank  9: 0.91
     rank 10: 0.93
     rank 11: 0.89
     rank 12: 0.86
     rank 13: 0.84
     rank 14: 0.87
     rank 15: 0.87
     rank 16: 0.85
     rank 17: 0.87
     rank 18: 0.88
     rank 19: 0.88
 [2024-12-05 23:38:06] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7635.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.146363E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7577.69
     rank  1: 7577.76
     rank  2: 7577.75
     rank  3: 7577.86
     rank  4: 7577.17
     rank  5: 7577.20
     rank  6: 7577.18
     rank  7: 7577.16
     rank  8: 7575.98
     rank  9: 7575.96
     rank 10: 7576.07
     rank 11: 7576.02
     rank 12: 7575.86
     rank 13: 7575.92
     rank 14: 7575.93
     rank 15: 7575.87
     rank 16: 7575.97
     rank 17: 7575.77
     rank 18: 7576.03
     rank 19: 7575.82
  forward-compute:
     rank  0: 625.56
     rank  1: 626.88
     rank  2: 634.22
     rank  3: 636.84
     rank  4: 349.81
     rank  5: 351.15
     rank  6: 333.73
     rank  7: 336.61
     rank  8: 2184.11
     rank  9: 2187.96
     rank 10: 2079.87
     rank 11: 2086.94
     rank 12: 2366.35
     rank 13: 2368.44
     rank 14: 2350.93
     rank 15: 2363.64
     rank 16: 2671.09
     rank 17: 2675.07
     rank 18: 2701.95
     rank 19: 2699.91
  backward-compute:
     rank  0: 686.69
     rank  1: 686.82
     rank  2: 705.89
     rank  3: 704.83
     rank  4: 371.93
     rank  5: 377.51
     rank  6: 350.94
     rank  7: 358.88
     rank  8: 2261.99
     rank  9: 2263.00
     rank 10: 2146.66
     rank 11: 2156.05
     rank 12: 2806.21
     rank 13: 2810.39
     rank 14: 2798.75
     rank 15: 2796.26
     rank 16: 3151.79
     rank 17: 3153.02
     rank 18: 3152.71
     rank 19: 3155.26
  pure-backward-compute:
     rank  0: 686.18
     rank  1: 686.37
     rank  2: 705.43
     rank  3: 704.33
     rank  4: 371.21
     rank  5: 376.76
     rank  6: 350.20
     rank  7: 358.16
     rank  8: 2260.85
     rank  9: 2262.49
     rank 10: 2145.18
     rank 11: 2155.40
     rank 12: 2805.45
     rank 13: 2809.69
     rank 14: 2797.78
     rank 15: 2795.82
     rank 16: 3149.80
     rank 17: 3152.02
     rank 18: 3151.48
     rank 19: 3153.80
  batch-generator:
     rank  0: 48.72
     rank  1: 50.28
     rank  2: 35.83
     rank  3: 40.67
     rank  4: 38.37
     rank  5: 40.63
     rank  6: 41.60
     rank  7: 46.02
     rank  8: 40.20
     rank  9: 44.87
     rank 10: 72.81
     rank 11: 70.93
     rank 12: 28.95
     rank 13: 31.92
     rank 14: 27.72
     rank 15: 41.07
     rank 16: 31.37
     rank 17: 35.58
     rank 18: 29.38
     rank 19: 27.64
  forward-recv:
     rank  4: 111.88
     rank  5: 111.65
     rank  6: 118.66
     rank  7: 118.04
     rank  8: 136.80
     rank  9: 136.78
     rank 10: 131.59
     rank 11: 131.51
     rank 12: 413.28
     rank 13: 412.32
     rank 14: 390.91
     rank 15: 390.58
     rank 16: 710.82
     rank 17: 710.59
     rank 18: 685.21
     rank 19: 684.51
  forward-send:
     rank  0: 405.16
     rank  1: 403.96
     rank  2: 372.20
     rank  3: 369.34
     rank  4: 479.54
     rank  5: 478.61
     rank  6: 454.22
     rank  7: 451.62
     rank  8: 36.81
     rank  9: 36.78
     rank 10: 40.63
     rank 11: 38.99
     rank 12: 4.93
     rank 13: 5.26
     rank 14: 5.23
     rank 15: 4.78
  backward-recv:
     rank  0: 2611.44
     rank  1: 2611.38
     rank  2: 2606.77
     rank  3: 2606.94
     rank  4: 2067.90
     rank  5: 2067.00
     rank  6: 2086.12
     rank  7: 2083.26
     rank  8: 903.65
     rank  9: 905.05
     rank 10: 934.69
     rank 11: 936.04
     rank 12: 375.06
     rank 13: 374.27
     rank 14: 378.10
     rank 15: 378.75
  backward-send:
     rank  4: 4.83
     rank  5: 3.64
     rank  6: 2.95
     rank  7: 3.27
     rank  8: 14.44
     rank  9: 12.76
     rank 10: 8.20
     rank 11: 6.81
     rank 12: 21.75
     rank 13: 20.39
     rank 14: 20.74
     rank 15: 20.53
     rank 16: 9.59
     rank 17: 8.64
     rank 18: 9.61
     rank 19: 9.50
  forward-send-backward-recv:
     rank  0: 3074.46
     rank  1: 3074.42
     rank  2: 3067.48
     rank  3: 3067.94
     rank  4: 3921.61
     rank  5: 3917.66
     rank  6: 3946.11
     rank  7: 3940.98
     rank  8: 1691.37
     rank  9: 1690.78
     rank 10: 1889.88
     rank 11: 1880.34
     rank 12: 910.50
     rank 13: 906.98
     rank 14: 954.53
     rank 15: 962.09
  backward-send-forward-recv:
     rank  4: 10.46
     rank  5: 10.41
     rank  6: 7.51
     rank  7: 7.58
     rank  8: 40.30
     rank  9: 37.63
     rank 10: 20.54
     rank 11: 18.35
     rank 12: 85.23
     rank 13: 83.47
     rank 14: 78.20
     rank 15: 67.45
     rank 16: 75.69
     rank 17: 73.19
     rank 18: 71.43
     rank 19: 73.05
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.06
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.05
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.08
  all-grads-sync:
     rank  0: 165.97
     rank  1: 158.31
     rank  2: 165.88
     rank  3: 158.23
     rank  4: 133.59
     rank  5: 131.51
     rank  6: 133.68
     rank  7: 131.54
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.18
     rank  4: 0.09
     rank  5: 0.09
     rank  6: 0.10
     rank  7: 0.09
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.32
     rank  1: 20.29
     rank  2: 20.36
     rank  3: 20.35
     rank  4: 14.18
     rank  5: 14.38
     rank  6: 14.28
     rank  7: 14.02
     rank  8: 0.12
     rank  9: 0.06
     rank 10: 0.18
     rank 11: 0.11
     rank 12: 0.03
     rank 13: 0.08
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.04
     rank 19: 0.07
  optimizer:
     rank  0: 21.16
     rank  1: 21.13
     rank  2: 21.20
     rank  3: 21.42
     rank  4: 15.01
     rank  5: 15.22
     rank  6: 15.12
     rank  7: 14.86
     rank  8: 0.95
     rank  9: 0.89
     rank 10: 1.01
     rank 11: 0.94
     rank 12: 0.86
     rank 13: 0.91
     rank 14: 0.87
     rank 15: 0.87
     rank 16: 0.89
     rank 17: 0.86
     rank 18: 0.87
     rank 19: 0.90
 [2024-12-05 23:38:14] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7631.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.233142E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7580.37
     rank  1: 7580.32
     rank  2: 7580.32
     rank  3: 7580.29
     rank  4: 7579.89
     rank  5: 7579.88
     rank  6: 7579.92
     rank  7: 7579.84
     rank  8: 7578.90
     rank  9: 7578.60
     rank 10: 7578.73
     rank 11: 7578.57
     rank 12: 7578.57
     rank 13: 7578.56
     rank 14: 7578.56
     rank 15: 7578.53
     rank 16: 7578.60
     rank 17: 7578.35
     rank 18: 7578.51
     rank 19: 7578.36
  forward-compute:
     rank  0: 628.09
     rank  1: 630.98
     rank  2: 638.70
     rank  3: 641.04
     rank  4: 367.57
     rank  5: 371.65
     rank  6: 334.95
     rank  7: 336.19
     rank  8: 2184.09
     rank  9: 2188.53
     rank 10: 2072.06
     rank 11: 2075.52
     rank 12: 2366.00
     rank 13: 2369.70
     rank 14: 2357.60
     rank 15: 2362.73
     rank 16: 2669.65
     rank 17: 2672.83
     rank 18: 2708.77
     rank 19: 2707.17
  backward-compute:
     rank  0: 687.77
     rank  1: 689.40
     rank  2: 698.99
     rank  3: 697.17
     rank  4: 371.34
     rank  5: 376.42
     rank  6: 354.94
     rank  7: 355.40
     rank  8: 2262.88
     rank  9: 2261.05
     rank 10: 2146.52
     rank 11: 2153.60
     rank 12: 2801.25
     rank 13: 2804.58
     rank 14: 2804.59
     rank 15: 2801.89
     rank 16: 3158.55
     rank 17: 3159.53
     rank 18: 3153.91
     rank 19: 3156.58
  pure-backward-compute:
     rank  0: 687.19
     rank  1: 688.90
     rank  2: 698.53
     rank  3: 696.62
     rank  4: 370.27
     rank  5: 375.72
     rank  6: 354.29
     rank  7: 354.45
     rank  8: 2262.06
     rank  9: 2260.46
     rank 10: 2145.81
     rank 11: 2152.99
     rank 12: 2800.39
     rank 13: 2804.00
     rank 14: 2803.66
     rank 15: 2801.47
     rank 16: 3156.56
     rank 17: 3158.44
     rank 18: 3152.75
     rank 19: 3155.48
  batch-generator:
     rank  0: 45.88
     rank  1: 49.61
     rank  2: 36.72
     rank  3: 42.28
     rank  4: 46.76
     rank  5: 49.72
     rank  6: 35.30
     rank  7: 40.30
     rank  8: 38.75
     rank  9: 45.45
     rank 10: 67.78
     rank 11: 67.76
     rank 12: 28.22
     rank 13: 31.76
     rank 14: 30.45
     rank 15: 36.44
     rank 16: 30.95
     rank 17: 33.05
     rank 18: 27.39
     rank 19: 26.07
  forward-recv:
     rank  4: 104.76
     rank  5: 104.99
     rank  6: 124.47
     rank  7: 124.09
     rank  8: 142.02
     rank  9: 141.51
     rank 10: 127.77
     rank 11: 127.27
     rank 12: 417.92
     rank 13: 416.93
     rank 14: 389.26
     rank 15: 388.61
     rank 16: 714.26
     rank 17: 714.25
     rank 18: 684.16
     rank 19: 683.75
  forward-send:
     rank  0: 409.48
     rank  1: 406.47
     rank  2: 367.99
     rank  3: 365.27
     rank  4: 474.77
     rank  5: 472.07
     rank  6: 442.45
     rank  7: 439.90
     rank  8: 36.30
     rank  9: 35.58
     rank 10: 49.85
     rank 11: 48.42
     rank 12: 5.37
     rank 13: 5.46
     rank 14: 5.47
     rank 15: 5.21
  backward-recv:
     rank  0: 2608.91
     rank  1: 2608.51
     rank  2: 2608.22
     rank  3: 2608.27
     rank  4: 2071.83
     rank  5: 2071.21
     rank  6: 2083.62
     rank  7: 2082.78
     rank  8: 906.55
     rank  9: 907.87
     rank 10: 935.32
     rank 11: 933.44
     rank 12: 378.87
     rank 13: 378.79
     rank 14: 378.89
     rank 15: 380.22
  backward-send:
     rank  4: 5.06
     rank  5: 3.48
     rank  6: 3.00
     rank  7: 3.43
     rank  8: 13.07
     rank  9: 11.88
     rank 10: 9.36
     rank 11: 9.25
     rank 12: 21.62
     rank 13: 21.57
     rank 14: 21.36
     rank 15: 19.31
     rank 16: 10.50
     rank 17: 10.37
     rank 18: 10.00
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 3073.74
     rank  1: 3073.47
     rank  2: 3078.92
     rank  3: 3079.72
     rank  4: 3919.90
     rank  5: 3916.18
     rank  6: 3958.74
     rank  7: 3959.03
     rank  8: 1691.25
     rank  9: 1690.82
     rank 10: 1898.20
     rank 11: 1894.46
     rank 12: 913.68
     rank 13: 910.53
     rank 14: 951.85
     rank 15: 955.96
  backward-send-forward-recv:
     rank  4: 9.75
     rank  5: 9.09
     rank  6: 6.28
     rank  7: 6.94
     rank  8: 41.71
     rank  9: 38.62
     rank 10: 20.96
     rank 11: 20.35
     rank 12: 85.55
     rank 13: 83.23
     rank 14: 78.20
     rank 15: 74.62
     rank 16: 74.17
     rank 17: 72.18
     rank 18: 70.42
     rank 19: 71.58
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.05
     rank  7: 0.03
     rank  8: 0.06
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.08
     rank  5: 0.09
     rank  6: 0.11
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 162.98
     rank  1: 155.94
     rank  2: 162.87
     rank  3: 155.82
     rank  4: 140.37
     rank  5: 136.24
     rank  6: 140.47
     rank  7: 136.36
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.10
     rank  5: 0.10
     rank  6: 0.09
     rank  7: 0.09
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.32
     rank  1: 20.69
     rank  2: 20.36
     rank  3: 20.28
     rank  4: 14.18
     rank  5: 14.31
     rank  6: 14.39
     rank  7: 14.14
     rank  8: 0.12
     rank  9: 0.04
     rank 10: 0.10
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.03
     rank 16: 0.10
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.04
  optimizer:
     rank  0: 21.12
     rank  1: 21.47
     rank  2: 21.16
     rank  3: 21.07
     rank  4: 14.98
     rank  5: 15.11
     rank  6: 15.18
     rank  7: 14.94
     rank  8: 0.92
     rank  9: 0.83
     rank 10: 0.89
     rank 11: 0.83
     rank 12: 0.82
     rank 13: 0.83
     rank 14: 0.86
     rank 15: 0.82
     rank 16: 0.89
     rank 17: 0.86
     rank 18: 0.85
     rank 19: 0.83
 [2024-12-05 23:38:22] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7644.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.140999E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7586.92
     rank  1: 7586.76
     rank  2: 7586.88
     rank  3: 7586.81
     rank  4: 7586.33
     rank  5: 7586.28
     rank  6: 7586.28
     rank  7: 7586.42
     rank  8: 7585.34
     rank  9: 7585.01
     rank 10: 7585.17
     rank 11: 7585.07
     rank 12: 7585.00
     rank 13: 7584.95
     rank 14: 7585.00
     rank 15: 7584.98
     rank 16: 7585.00
     rank 17: 7584.83
     rank 18: 7584.97
     rank 19: 7584.83
  forward-compute:
     rank  0: 623.21
     rank  1: 626.92
     rank  2: 636.84
     rank  3: 637.80
     rank  4: 355.60
     rank  5: 358.98
     rank  6: 329.56
     rank  7: 330.41
     rank  8: 2205.12
     rank  9: 2209.93
     rank 10: 2088.81
     rank 11: 2091.65
     rank 12: 2360.45
     rank 13: 2363.47
     rank 14: 2355.85
     rank 15: 2357.48
     rank 16: 2670.47
     rank 17: 2675.26
     rank 18: 2698.64
     rank 19: 2697.02
  backward-compute:
     rank  0: 687.57
     rank  1: 687.81
     rank  2: 695.50
     rank  3: 694.21
     rank  4: 364.51
     rank  5: 370.09
     rank  6: 352.27
     rank  7: 354.07
     rank  8: 2274.00
     rank  9: 2274.25
     rank 10: 2159.70
     rank 11: 2161.47
     rank 12: 2799.71
     rank 13: 2804.81
     rank 14: 2804.24
     rank 15: 2802.23
     rank 16: 3155.43
     rank 17: 3156.27
     rank 18: 3157.82
     rank 19: 3160.48
  pure-backward-compute:
     rank  0: 687.03
     rank  1: 687.39
     rank  2: 695.07
     rank  3: 693.78
     rank  4: 363.85
     rank  5: 369.51
     rank  6: 351.63
     rank  7: 353.28
     rank  8: 2272.52
     rank  9: 2273.48
     rank 10: 2158.89
     rank 11: 2160.85
     rank 12: 2798.98
     rank 13: 2804.02
     rank 14: 2803.66
     rank 15: 2801.67
     rank 16: 3153.48
     rank 17: 3155.45
     rank 18: 3156.72
     rank 19: 3159.43
  batch-generator:
     rank  0: 44.80
     rank  1: 49.00
     rank  2: 37.21
     rank  3: 41.22
     rank  4: 40.57
     rank  5: 43.95
     rank  6: 39.10
     rank  7: 40.20
     rank  8: 48.12
     rank  9: 52.50
     rank 10: 71.06
     rank 11: 69.90
     rank 12: 25.71
     rank 13: 29.52
     rank 14: 26.33
     rank 15: 29.01
     rank 16: 29.51
     rank 17: 34.34
     rank 18: 25.91
     rank 19: 24.92
  forward-recv:
     rank  4: 109.45
     rank  5: 109.75
     rank  6: 116.81
     rank  7: 116.42
     rank  8: 134.91
     rank  9: 134.05
     rank 10: 134.54
     rank 11: 134.57
     rank 12: 419.18
     rank 13: 417.17
     rank 14: 392.86
     rank 15: 392.75
     rank 16: 713.14
     rank 17: 712.93
     rank 18: 687.97
     rank 19: 687.49
  forward-send:
     rank  0: 411.61
     rank  1: 408.26
     rank  2: 375.64
     rank  3: 374.57
     rank  4: 482.02
     rank  5: 478.64
     rank  6: 452.73
     rank  7: 451.77
     rank  8: 30.47
     rank  9: 30.55
     rank 10: 52.54
     rank 11: 51.56
     rank 12: 5.44
     rank 13: 5.42
     rank 14: 5.34
     rank 15: 4.95
  backward-recv:
     rank  0: 2614.95
     rank  1: 2614.60
     rank  2: 2607.77
     rank  3: 2608.33
     rank  4: 2075.99
     rank  5: 2073.70
     rank  6: 2082.17
     rank  7: 2081.45
     rank  8: 899.51
     rank  9: 901.43
     rank 10: 930.84
     rank 11: 932.24
     rank 12: 377.13
     rank 13: 376.17
     rank 14: 377.14
     rank 15: 377.23
  backward-send:
     rank  4: 4.88
     rank  5: 3.48
     rank  6: 2.81
     rank  7: 3.13
     rank  8: 13.60
     rank  9: 11.75
     rank 10: 9.16
     rank 11: 8.32
     rank 12: 22.48
     rank 13: 23.06
     rank 14: 23.36
     rank 15: 23.29
     rank 16: 10.07
     rank 17: 9.23
     rank 18: 10.12
     rank 19: 10.13
  forward-send-backward-recv:
     rank  0: 3074.03
     rank  1: 3074.06
     rank  2: 3075.72
     rank  3: 3076.22
     rank  4: 3923.60
     rank  5: 3920.91
     rank  6: 3962.55
     rank  7: 3961.44
     rank  8: 1675.36
     rank  9: 1676.97
     rank 10: 1863.41
     rank 11: 1861.23
     rank 12: 916.59
     rank 13: 912.74
     rank 14: 947.23
     rank 15: 951.16
  backward-send-forward-recv:
     rank  4: 9.87
     rank  5: 9.63
     rank  6: 7.56
     rank  7: 7.71
     rank  8: 40.81
     rank  9: 37.97
     rank 10: 21.17
     rank 11: 20.25
     rank 12: 85.34
     rank 13: 84.59
     rank 14: 79.79
     rank 15: 78.73
     rank 16: 74.79
     rank 17: 71.78
     rank 18: 70.50
     rank 19: 71.78
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.09
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.04
     rank 18: 0.06
     rank 19: 0.04
  all-grads-sync:
     rank  0: 166.84
     rank  1: 160.47
     rank  2: 166.75
     rank  3: 160.29
     rank  4: 137.21
     rank  5: 132.95
     rank  6: 137.31
     rank  7: 133.00
     rank  8: 0.07
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.10
     rank  5: 0.09
     rank  6: 0.08
     rank  7: 0.08
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.29
     rank  1: 20.24
     rank  2: 20.31
     rank  3: 20.27
     rank  4: 14.02
     rank  5: 14.25
     rank  6: 14.15
     rank  7: 14.15
     rank  8: 0.16
     rank  9: 0.10
     rank 10: 0.12
     rank 11: 0.09
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.03
  optimizer:
     rank  0: 21.12
     rank  1: 21.08
     rank  2: 21.16
     rank  3: 21.12
     rank  4: 14.87
     rank  5: 15.10
     rank  6: 15.01
     rank  7: 15.00
     rank  8: 1.06
     rank  9: 0.94
     rank 10: 0.96
     rank 11: 0.93
     rank 12: 0.87
     rank 13: 0.87
     rank 14: 0.87
     rank 15: 0.87
     rank 16: 0.91
     rank 17: 0.87
     rank 18: 0.87
     rank 19: 0.87
 [2024-12-05 23:38:29] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7647.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.666079E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7597.14
     rank  1: 7597.14
     rank  2: 7597.19
     rank  3: 7597.17
     rank  4: 7596.67
     rank  5: 7596.70
     rank  6: 7596.73
     rank  7: 7596.67
     rank  8: 7595.73
     rank  9: 7595.43
     rank 10: 7595.62
     rank 11: 7595.47
     rank 12: 7595.34
     rank 13: 7595.38
     rank 14: 7595.37
     rank 15: 7595.38
     rank 16: 7595.21
     rank 17: 7595.21
     rank 18: 7595.23
     rank 19: 7595.21
  forward-compute:
     rank  0: 624.17
     rank  1: 628.40
     rank  2: 634.55
     rank  3: 635.49
     rank  4: 350.52
     rank  5: 355.90
     rank  6: 319.99
     rank  7: 320.67
     rank  8: 2210.91
     rank  9: 2221.11
     rank 10: 2073.85
     rank 11: 2076.61
     rank 12: 2366.62
     rank 13: 2371.10
     rank 14: 2360.93
     rank 15: 2367.15
     rank 16: 2677.52
     rank 17: 2680.61
     rank 18: 2710.45
     rank 19: 2708.59
  backward-compute:
     rank  0: 691.74
     rank  1: 691.98
     rank  2: 703.61
     rank  3: 702.59
     rank  4: 367.70
     rank  5: 372.75
     rank  6: 350.03
     rank  7: 351.99
     rank  8: 2279.42
     rank  9: 2282.50
     rank 10: 2170.75
     rank 11: 2177.65
     rank 12: 2804.86
     rank 13: 2808.01
     rank 14: 2804.97
     rank 15: 2802.24
     rank 16: 3155.50
     rank 17: 3156.45
     rank 18: 3157.77
     rank 19: 3160.09
  pure-backward-compute:
     rank  0: 691.27
     rank  1: 691.55
     rank  2: 703.17
     rank  3: 702.04
     rank  4: 367.04
     rank  5: 372.04
     rank  6: 349.34
     rank  7: 351.24
     rank  8: 2278.20
     rank  9: 2281.91
     rank 10: 2169.96
     rank 11: 2177.09
     rank 12: 2804.03
     rank 13: 2807.32
     rank 14: 2804.05
     rank 15: 2801.68
     rank 16: 3154.31
     rank 17: 3155.66
     rank 18: 3156.40
     rank 19: 3158.81
  batch-generator:
     rank  0: 44.07
     rank  1: 49.02
     rank  2: 34.13
     rank  3: 37.64
     rank  4: 40.05
     rank  5: 46.22
     rank  6: 35.70
     rank  7: 37.50
     rank  8: 48.70
     rank  9: 58.52
     rank 10: 54.07
     rank 11: 54.41
     rank 12: 31.18
     rank 13: 35.98
     rank 14: 29.60
     rank 15: 36.29
     rank 16: 31.79
     rank 17: 34.51
     rank 18: 29.12
     rank 19: 27.71
  forward-recv:
     rank  4: 109.53
     rank  5: 109.62
     rank  6: 120.92
     rank  7: 120.65
     rank  8: 137.17
     rank  9: 136.56
     rank 10: 127.58
     rank 11: 127.51
     rank 12: 419.95
     rank 13: 417.74
     rank 14: 390.27
     rank 15: 390.22
     rank 16: 716.71
     rank 17: 716.40
     rank 18: 685.64
     rank 19: 685.37
  forward-send:
     rank  0: 412.21
     rank  1: 408.06
     rank  2: 373.53
     rank  3: 372.64
     rank  4: 482.72
     rank  5: 479.00
     rank  6: 455.92
     rank  7: 455.05
     rank  8: 35.27
     rank  9: 33.03
     rank 10: 51.10
     rank 11: 50.48
     rank 12: 5.60
     rank 13: 5.19
     rank 14: 5.48
     rank 15: 5.22
  backward-recv:
     rank  0: 2612.28
     rank  1: 2612.04
     rank  2: 2607.95
     rank  3: 2608.09
     rank  4: 2072.91
     rank  5: 2074.43
     rank  6: 2088.09
     rank  7: 2086.77
     rank  8: 902.83
     rank  9: 901.68
     rank 10: 929.08
     rank 11: 928.54
     rank 12: 378.60
     rank 13: 378.77
     rank 14: 379.49
     rank 15: 378.73
  backward-send:
     rank  4: 5.00
     rank  5: 3.86
     rank  6: 3.15
     rank  7: 3.11
     rank  8: 14.20
     rank  9: 14.23
     rank 10: 8.69
     rank 11: 8.53
     rank 12: 20.80
     rank 13: 20.11
     rank 14: 20.46
     rank 15: 19.36
     rank 16: 9.96
     rank 17: 9.14
     rank 18: 10.03
     rank 19: 10.03
  forward-send-backward-recv:
     rank  0: 3080.66
     rank  1: 3080.79
     rank  2: 3085.29
     rank  3: 3085.90
     rank  4: 3933.32
     rank  5: 3927.42
     rank  6: 3971.95
     rank  7: 3970.72
     rank  8: 1665.81
     rank  9: 1662.95
     rank 10: 1889.71
     rank 11: 1884.36
     rank 12: 914.81
     rank 13: 912.37
     rank 14: 948.42
     rank 15: 955.21
  backward-send-forward-recv:
     rank  4: 10.04
     rank  5: 10.00
     rank  6: 7.55
     rank  7: 7.83
     rank  8: 38.77
     rank  9: 33.96
     rank 10: 20.81
     rank 11: 19.81
     rank 12: 85.37
     rank 13: 83.72
     rank 14: 81.98
     rank 15: 76.23
     rank 16: 74.72
     rank 17: 72.80
     rank 18: 70.65
     rank 19: 72.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.08
     rank  5: 0.09
     rank  6: 0.11
     rank  7: 0.08
     rank  8: 0.05
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 166.74
     rank  1: 159.95
     rank  2: 166.69
     rank  3: 159.79
     rank  4: 137.55
     rank  5: 134.07
     rank  6: 137.64
     rank  7: 134.12
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.10
     rank  5: 0.10
     rank  6: 0.08
     rank  7: 0.07
     rank  8: 0.06
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.05
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.28
     rank  1: 20.28
     rank  2: 20.28
     rank  3: 20.45
     rank  4: 14.18
     rank  5: 14.34
     rank  6: 14.35
     rank  7: 14.01
     rank  8: 0.53
     rank  9: 0.09
     rank 10: 0.17
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.06
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.03
     rank 18: 0.06
     rank 19: 0.03
  optimizer:
     rank  0: 21.18
     rank  1: 21.20
     rank  2: 21.20
     rank  3: 21.37
     rank  4: 15.09
     rank  5: 15.27
     rank  6: 15.27
     rank  7: 14.93
     rank  8: 1.45
     rank  9: 1.01
     rank 10: 1.08
     rank 11: 0.96
     rank 12: 0.95
     rank 13: 0.97
     rank 14: 0.94
     rank 15: 0.94
     rank 16: 0.97
     rank 17: 0.94
     rank 18: 0.97
     rank 19: 0.94
 [2024-12-05 23:38:37] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7643.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.031901E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7592.98
     rank  1: 7592.95
     rank  2: 7592.98
     rank  3: 7592.97
     rank  4: 7592.50
     rank  5: 7592.41
     rank  6: 7592.43
     rank  7: 7592.55
     rank  8: 7591.75
     rank  9: 7591.26
     rank 10: 7592.04
     rank 11: 7591.24
     rank 12: 7591.20
     rank 13: 7591.15
     rank 14: 7591.25
     rank 15: 7591.15
     rank 16: 7591.82
     rank 17: 7590.99
     rank 18: 7591.53
     rank 19: 7591.00
  forward-compute:
     rank  0: 622.43
     rank  1: 625.40
     rank  2: 635.59
     rank  3: 636.55
     rank  4: 351.12
     rank  5: 353.82
     rank  6: 325.25
     rank  7: 326.07
     rank  8: 2203.68
     rank  9: 2207.17
     rank 10: 2130.77
     rank 11: 2133.76
     rank 12: 2364.86
     rank 13: 2365.89
     rank 14: 2359.53
     rank 15: 2364.96
     rank 16: 2676.26
     rank 17: 2678.43
     rank 18: 2681.67
     rank 19: 2680.27
  backward-compute:
     rank  0: 686.81
     rank  1: 687.15
     rank  2: 700.47
     rank  3: 699.01
     rank  4: 367.24
     rank  5: 370.32
     rank  6: 351.21
     rank  7: 352.43
     rank  8: 2278.37
     rank  9: 2276.84
     rank 10: 2179.39
     rank 11: 2188.80
     rank 12: 2805.66
     rank 13: 2808.29
     rank 14: 2804.86
     rank 15: 2802.98
     rank 16: 3161.85
     rank 17: 3163.02
     rank 18: 3163.20
     rank 19: 3165.84
  pure-backward-compute:
     rank  0: 686.37
     rank  1: 686.71
     rank  2: 700.03
     rank  3: 698.57
     rank  4: 366.56
     rank  5: 369.67
     rank  6: 350.62
     rank  7: 351.56
     rank  8: 2277.70
     rank  9: 2276.11
     rank 10: 2178.41
     rank 11: 2188.13
     rank 12: 2805.08
     rank 13: 2807.78
     rank 14: 2803.92
     rank 15: 2802.49
     rank 16: 3160.49
     rank 17: 3162.19
     rank 18: 3161.87
     rank 19: 3164.69
  batch-generator:
     rank  0: 44.95
     rank  1: 48.28
     rank  2: 34.87
     rank  3: 38.13
     rank  4: 38.31
     rank  5: 41.85
     rank  6: 37.51
     rank  7: 38.49
     rank  8: 39.91
     rank  9: 45.19
     rank 10: 89.65
     rank 11: 85.36
     rank 12: 27.66
     rank 13: 28.74
     rank 14: 27.28
     rank 15: 33.65
     rank 16: 27.47
     rank 17: 29.47
     rank 18: 28.17
     rank 19: 27.07
  forward-recv:
     rank  4: 111.72
     rank  5: 111.52
     rank  6: 115.67
     rank  7: 115.30
     rank  8: 138.40
     rank  9: 137.58
     rank 10: 134.54
     rank 11: 134.23
     rank 12: 415.50
     rank 13: 415.07
     rank 14: 408.71
     rank 15: 408.42
     rank 16: 709.74
     rank 17: 709.70
     rank 18: 704.66
     rank 19: 704.34
  forward-send:
     rank  0: 410.76
     rank  1: 407.76
     rank  2: 391.52
     rank  3: 390.24
     rank  4: 479.06
     rank  5: 476.75
     rank  6: 475.77
     rank  7: 474.59
     rank  8: 32.11
     rank  9: 32.11
     rank 10: 49.13
     rank 11: 48.50
     rank 12: 5.74
     rank 13: 5.70
     rank 14: 5.56
     rank 15: 5.38
  backward-recv:
     rank  0: 2618.08
     rank  1: 2617.66
     rank  2: 2614.06
     rank  3: 2615.05
     rank  4: 2077.16
     rank  5: 2078.40
     rank  6: 2089.24
     rank  7: 2087.37
     rank  8: 905.40
     rank  9: 905.91
     rank 10: 930.96
     rank 11: 929.74
     rank 12: 379.85
     rank 13: 379.21
     rank 14: 380.59
     rank 15: 382.15
  backward-send:
     rank  4: 4.92
     rank  5: 3.51
     rank  6: 2.74
     rank  7: 3.41
     rank  8: 14.48
     rank  9: 14.37
     rank 10: 9.25
     rank 11: 8.07
     rank 12: 20.95
     rank 13: 20.82
     rank 14: 20.41
     rank 15: 18.67
     rank 16: 10.01
     rank 17: 9.39
     rank 18: 10.11
     rank 19: 10.11
  forward-send-backward-recv:
     rank  0: 3078.82
     rank  1: 3079.09
     rank  2: 3063.47
     rank  3: 3063.35
     rank  4: 3932.16
     rank  5: 3928.84
     rank  6: 3952.74
     rank  7: 3952.45
     rank  8: 1671.44
     rank  9: 1671.89
     rank 10: 1817.35
     rank 11: 1811.51
     rank 12: 920.27
     rank 13: 918.24
     rank 14: 930.50
     rank 15: 933.84
  backward-send-forward-recv:
     rank  4: 10.10
     rank  5: 10.07
     rank  6: 7.12
     rank  7: 7.34
     rank  8: 41.52
     rank  9: 39.77
     rank 10: 20.21
     rank 11: 19.54
     rank 12: 84.57
     rank 13: 84.40
     rank 14: 84.04
     rank 15: 79.40
     rank 16: 76.70
     rank 17: 75.27
     rank 18: 75.39
     rank 19: 76.34
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.10
     rank  5: 0.06
     rank  6: 0.06
     rank  7: 0.09
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 167.46
     rank  1: 160.91
     rank  2: 167.37
     rank  3: 160.91
     rank  4: 133.76
     rank  5: 131.79
     rank  6: 133.86
     rank  7: 131.79
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.05
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.11
     rank  5: 0.09
     rank  6: 0.08
     rank  7: 0.08
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.05
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.06
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.27
     rank  1: 20.32
     rank  2: 20.06
     rank  3: 20.30
     rank  4: 14.21
     rank  5: 14.13
     rank  6: 14.12
     rank  7: 14.29
     rank  8: 0.10
     rank  9: 0.04
     rank 10: 0.54
     rank 11: 0.08
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.07
     rank 16: 0.09
     rank 17: 0.03
     rank 18: 0.07
     rank 19: 0.06
  optimizer:
     rank  0: 21.39
     rank  1: 21.44
     rank  2: 21.16
     rank  3: 21.41
     rank  4: 15.33
     rank  5: 15.25
     rank  6: 15.24
     rank  7: 15.40
     rank  8: 1.21
     rank  9: 1.14
     rank 10: 1.64
     rank 11: 1.19
     rank 12: 1.17
     rank 13: 1.14
     rank 14: 1.14
     rank 15: 1.17
     rank 16: 1.24
     rank 17: 1.14
     rank 18: 1.17
     rank 19: 1.17
 [2024-12-05 23:38:44] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7659.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.619649E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7601.75
     rank  1: 7601.72
     rank  2: 7601.71
     rank  3: 7601.72
     rank  4: 7601.14
     rank  5: 7601.19
     rank  6: 7601.15
     rank  7: 7601.16
     rank  8: 7599.97
     rank  9: 7599.98
     rank 10: 7599.96
     rank 11: 7599.90
     rank 12: 7599.80
     rank 13: 7599.85
     rank 14: 7599.81
     rank 15: 7599.80
     rank 16: 7599.76
     rank 17: 7599.75
     rank 18: 7599.78
     rank 19: 7599.74
  forward-compute:
     rank  0: 626.55
     rank  1: 628.82
     rank  2: 639.68
     rank  3: 646.93
     rank  4: 350.57
     rank  5: 352.26
     rank  6: 334.35
     rank  7: 341.20
     rank  8: 2209.83
     rank  9: 2212.68
     rank 10: 2128.04
     rank 11: 2141.55
     rank 12: 2364.29
     rank 13: 2365.99
     rank 14: 2361.09
     rank 15: 2378.02
     rank 16: 2675.04
     rank 17: 2676.49
     rank 18: 2679.25
     rank 19: 2679.43
  backward-compute:
     rank  0: 691.00
     rank  1: 691.24
     rank  2: 701.11
     rank  3: 699.62
     rank  4: 370.10
     rank  5: 371.22
     rank  6: 348.54
     rank  7: 353.13
     rank  8: 2283.14
     rank  9: 2282.48
     rank 10: 2183.16
     rank 11: 2193.69
     rank 12: 2806.56
     rank 13: 2808.22
     rank 14: 2808.29
     rank 15: 2805.72
     rank 16: 3163.61
     rank 17: 3164.40
     rank 18: 3164.77
     rank 19: 3166.85
  pure-backward-compute:
     rank  0: 690.48
     rank  1: 690.76
     rank  2: 700.65
     rank  3: 699.13
     rank  4: 369.37
     rank  5: 370.49
     rank  6: 347.96
     rank  7: 352.43
     rank  8: 2282.47
     rank  9: 2281.76
     rank 10: 2182.10
     rank 11: 2193.20
     rank 12: 2805.58
     rank 13: 2807.65
     rank 14: 2807.38
     rank 15: 2805.26
     rank 16: 3162.35
     rank 17: 3163.57
     rank 18: 3163.68
     rank 19: 3165.95
  batch-generator:
     rank  0: 47.03
     rank  1: 49.42
     rank  2: 37.76
     rank  3: 47.60
     rank  4: 37.38
     rank  5: 40.22
     rank  6: 42.59
     rank  7: 50.61
     rank  8: 40.15
     rank  9: 46.43
     rank 10: 86.27
     rank 11: 93.87
     rank 12: 25.33
     rank 13: 27.45
     rank 14: 27.57
     rank 15: 45.58
     rank 16: 28.91
     rank 17: 30.13
     rank 18: 26.82
     rank 19: 27.25
  forward-recv:
     rank  4: 110.25
     rank  5: 110.31
     rank  6: 117.65
     rank  7: 117.26
     rank  8: 137.78
     rank  9: 136.89
     rank 10: 132.69
     rank 11: 131.85
     rank 12: 416.82
     rank 13: 416.58
     rank 14: 413.45
     rank 15: 409.16
     rank 16: 713.51
     rank 17: 713.54
     rank 18: 712.03
     rank 19: 709.65
  forward-send:
     rank  0: 414.30
     rank  1: 412.32
     rank  2: 395.90
     rank  3: 388.54
     rank  4: 485.44
     rank  5: 483.98
     rank  6: 476.53
     rank  7: 469.30
     rank  8: 36.45
     rank  9: 36.56
     rank 10: 57.48
     rank 11: 50.76
     rank 12: 8.68
     rank 13: 8.57
     rank 14: 8.71
     rank 15: 6.62
  backward-recv:
     rank  0: 2611.70
     rank  1: 2611.61
     rank  2: 2610.25
     rank  3: 2610.72
     rank  4: 2071.96
     rank  5: 2073.67
     rank  6: 2086.08
     rank  7: 2085.02
     rank  8: 899.84
     rank  9: 897.97
     rank 10: 926.89
     rank 11: 927.66
     rank 12: 377.32
     rank 13: 377.63
     rank 14: 378.32
     rank 15: 378.46
  backward-send:
     rank  4: 4.93
     rank  5: 3.59
     rank  6: 2.84
     rank  7: 3.42
     rank  8: 14.38
     rank  9: 14.86
     rank 10: 9.42
     rank 11: 8.84
     rank 12: 20.63
     rank 13: 19.54
     rank 14: 20.43
     rank 15: 20.29
     rank 16: 9.10
     rank 17: 8.36
     rank 18: 9.10
     rank 19: 9.16
  forward-send-backward-recv:
     rank  0: 3079.72
     rank  1: 3079.82
     rank  2: 3061.65
     rank  3: 3062.58
     rank  4: 3933.34
     rank  5: 3931.09
     rank  6: 3950.02
     rank  7: 3946.13
     rank  8: 1667.08
     rank  9: 1666.38
     rank 10: 1818.69
     rank 11: 1806.87
     rank 12: 920.67
     rank 13: 919.68
     rank 14: 923.96
     rank 15: 929.09
  backward-send-forward-recv:
     rank  4: 10.49
     rank  5: 10.50
     rank  6: 7.17
     rank  7: 7.38
     rank  8: 41.75
     rank  9: 40.18
     rank 10: 19.85
     rank 11: 17.29
     rank 12: 84.93
     rank 13: 83.98
     rank 14: 84.06
     rank 15: 74.49
     rank 16: 76.63
     rank 17: 75.68
     rank 18: 74.63
     rank 19: 76.27
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.07
     rank  6: 0.06
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 169.83
     rank  1: 163.18
     rank  2: 169.76
     rank  3: 163.00
     rank  4: 138.71
     rank  5: 135.27
     rank  6: 138.80
     rank  7: 135.32
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.10
     rank  5: 0.09
     rank  6: 0.09
     rank  7: 0.09
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.49
     rank  1: 20.29
     rank  2: 20.33
     rank  3: 20.27
     rank  4: 13.95
     rank  5: 14.35
     rank  6: 14.10
     rank  7: 14.01
     rank  8: 0.14
     rank  9: 0.09
     rank 10: 0.10
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.07
     rank 15: 0.04
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 21.29
     rank  1: 21.10
     rank  2: 21.14
     rank  3: 21.09
     rank  4: 14.76
     rank  5: 15.17
     rank  6: 14.92
     rank  7: 14.83
     rank  8: 0.95
     rank  9: 0.90
     rank 10: 0.91
     rank 11: 0.85
     rank 12: 0.83
     rank 13: 0.84
     rank 14: 0.87
     rank 15: 0.84
     rank 16: 0.87
     rank 17: 0.87
     rank 18: 0.84
     rank 19: 0.85
 [2024-12-05 23:38:52] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7648.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.361645E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7598.05
     rank  1: 7597.92
     rank  2: 7598.01
     rank  3: 7597.94
     rank  4: 7597.46
     rank  5: 7597.43
     rank  6: 7597.47
     rank  7: 7597.41
     rank  8: 7596.27
     rank  9: 7596.16
     rank 10: 7596.41
     rank 11: 7596.22
     rank 12: 7596.20
     rank 13: 7596.06
     rank 14: 7596.24
     rank 15: 7596.10
     rank 16: 7596.02
     rank 17: 7596.00
     rank 18: 7596.07
     rank 19: 7596.00
  forward-compute:
     rank  0: 623.66
     rank  1: 628.17
     rank  2: 634.71
     rank  3: 635.35
     rank  4: 350.67
     rank  5: 354.23
     rank  6: 325.14
     rank  7: 325.34
     rank  8: 2221.13
     rank  9: 2221.89
     rank 10: 2159.86
     rank 11: 2163.94
     rank 12: 2367.99
     rank 13: 2373.96
     rank 14: 2365.78
     rank 15: 2367.69
     rank 16: 2676.45
     rank 17: 2682.17
     rank 18: 2702.95
     rank 19: 2701.22
  backward-compute:
     rank  0: 684.20
     rank  1: 684.66
     rank  2: 698.99
     rank  3: 697.50
     rank  4: 369.31
     rank  5: 368.64
     rank  6: 353.46
     rank  7: 359.55
     rank  8: 2286.55
     rank  9: 2289.18
     rank 10: 2209.62
     rank 11: 2211.27
     rank 12: 2807.48
     rank 13: 2813.12
     rank 14: 2815.88
     rank 15: 2813.77
     rank 16: 3163.43
     rank 17: 3164.52
     rank 18: 3164.47
     rank 19: 3167.33
  pure-backward-compute:
     rank  0: 683.68
     rank  1: 684.21
     rank  2: 698.56
     rank  3: 697.06
     rank  4: 368.60
     rank  5: 367.98
     rank  6: 352.77
     rank  7: 358.61
     rank  8: 2285.97
     rank  9: 2288.22
     rank 10: 2208.08
     rank 11: 2210.50
     rank 12: 2806.62
     rank 13: 2812.50
     rank 14: 2815.29
     rank 15: 2813.34
     rank 16: 3161.45
     rank 17: 3163.65
     rank 18: 3163.27
     rank 19: 3166.44
  batch-generator:
     rank  0: 43.94
     rank  1: 48.59
     rank  2: 34.29
     rank  3: 37.28
     rank  4: 38.96
     rank  5: 43.23
     rank  6: 37.90
     rank  7: 38.92
     rank  8: 36.99
     rank  9: 44.12
     rank 10: 70.11
     rank 11: 72.07
     rank 12: 28.44
     rank 13: 34.94
     rank 14: 26.76
     rank 15: 29.56
     rank 16: 32.13
     rank 17: 37.37
     rank 18: 27.05
     rank 19: 25.84
  forward-recv:
     rank  4: 110.04
     rank  5: 109.83
     rank  6: 120.15
     rank  7: 119.63
     rank  8: 135.11
     rank  9: 135.33
     rank 10: 131.41
     rank 11: 131.56
     rank 12: 418.06
     rank 13: 415.87
     rank 14: 394.90
     rank 15: 394.86
     rank 16: 714.35
     rank 17: 714.00
     rank 18: 691.53
     rank 19: 691.36
  forward-send:
     rank  0: 412.22
     rank  1: 408.40
     rank  2: 379.54
     rank  3: 378.84
     rank  4: 484.23
     rank  5: 481.24
     rank  6: 462.18
     rank  7: 461.72
     rank  8: 34.30
     rank  9: 31.99
     rank 10: 45.07
     rank 11: 44.35
     rank 12: 5.63
     rank 13: 5.29
     rank 14: 5.47
     rank 15: 5.22
  backward-recv:
     rank  0: 2620.24
     rank  1: 2619.90
     rank  2: 2620.14
     rank  3: 2620.06
     rank  4: 2079.39
     rank  5: 2080.96
     rank  6: 2095.15
     rank  7: 2090.33
     rank  8: 903.59
     rank  9: 904.13
     rank 10: 919.16
     rank 11: 922.39
     rank 12: 378.83
     rank 13: 377.43
     rank 14: 379.45
     rank 15: 380.39
  backward-send:
     rank  4: 4.92
     rank  5: 3.54
     rank  6: 2.74
     rank  7: 3.41
     rank  8: 14.34
     rank  9: 14.37
     rank 10: 8.32
     rank 11: 6.88
     rank 12: 21.45
     rank 13: 21.92
     rank 14: 21.82
     rank 15: 21.16
     rank 16: 9.84
     rank 17: 8.69
     rank 18: 9.75
     rank 19: 9.80
  forward-send-backward-recv:
     rank  0: 3086.75
     rank  1: 3086.81
     rank  2: 3084.33
     rank  3: 3084.71
     rank  4: 3933.45
     rank  5: 3933.34
     rank  6: 3965.54
     rank  7: 3963.41
     rank  8: 1656.92
     rank  9: 1655.19
     rank 10: 1788.94
     rank 11: 1786.29
     rank 12: 916.42
     rank 13: 913.07
     rank 14: 938.76
     rank 15: 943.01
  backward-send-forward-recv:
     rank  4: 9.88
     rank  5: 9.81
     rank  6: 7.32
     rank  7: 7.88
     rank  8: 42.92
     rank  9: 41.87
     rank 10: 21.73
     rank 11: 19.62
     rank 12: 85.94
     rank 13: 83.39
     rank 14: 80.19
     rank 15: 78.74
     rank 16: 75.22
     rank 17: 71.28
     rank 18: 71.70
     rank 19: 73.00
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.10
     rank  5: 0.08
     rank  6: 0.09
     rank  7: 0.09
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 161.98
     rank  1: 154.93
     rank  2: 161.89
     rank  3: 154.96
     rank  4: 135.42
     rank  5: 131.92
     rank  6: 135.51
     rank  7: 131.96
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.15
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.17
     rank  4: 0.10
     rank  5: 0.10
     rank  6: 0.08
     rank  7: 0.09
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 20.32
     rank  1: 20.49
     rank  2: 20.38
     rank  3: 20.17
     rank  4: 14.16
     rank  5: 14.18
     rank  6: 14.32
     rank  7: 14.13
     rank  8: 0.04
     rank  9: 0.09
     rank 10: 0.11
     rank 11: 0.10
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 21.09
     rank  1: 21.26
     rank  2: 21.14
     rank  3: 20.93
     rank  4: 15.00
     rank  5: 14.95
     rank  6: 15.08
     rank  7: 14.89
     rank  8: 0.80
     rank  9: 0.85
     rank 10: 0.87
     rank 11: 0.86
     rank 12: 0.79
     rank 13: 0.79
     rank 14: 0.79
     rank 15: 0.78
     rank 16: 0.82
     rank 17: 0.79
     rank 18: 0.82
     rank 19: 0.79
