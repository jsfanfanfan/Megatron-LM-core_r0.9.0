examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
1
[2024-12-05 23:36:57,382] torch.distributed.run: [WARNING] 
[2024-12-05 23:36:57,382] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 23:36:57,382] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 23:36:57,382] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:10------
------num_pipeline_model_parallel_groups:4------
---Rank 6---Tensor Parallel Group GPUs: [0, 0]---Rank 5---Tensor Parallel Group GPUs: [1, 1]

---Rank 6---Pipeline Parallel Group GPUs: [1, 1, 1, 1, 1]---Rank 5---Pipeline Parallel Group GPUs: [1, 1, 1, 1, 1]

---Rank 4---Tensor Parallel Group GPUs: [0, 0]
---Rank 7---Tensor Parallel Group GPUs: [1, 1]
---Rank 4---Pipeline Parallel Group GPUs: [1, 1, 1, 1, 1]
---Rank 7---Pipeline Parallel Group GPUs: [1, 1, 1, 1, 1]
[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 87109632
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 87109632
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:Truename:module.vision_model.decoder.layers.3.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.0.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:Truename:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:Truename:module.vision_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True

name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:Truename:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:Truename:module.vision_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True

name:module.vision_model.decoder.layers.0.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:Truename:module.vision_model.decoder.layers.3.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.0.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.1.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True

name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True

name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True

name:module.vision_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True

name:module.vision_model.decoder.layers.1.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.1.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:Truename:module.vision_model.decoder.layers.5.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.2.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True

name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:Truename:module.vision_model.decoder.layers.5.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True

name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True

name:module.vision_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:Truename:module.vision_model.decoder.layers.5.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True

name:module.vision_model.decoder.layers.2.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:Truename:module.vision_model.decoder.layers.5.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.2.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:Truename:module.vision_model.decoder.layers.6.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.3.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:Truename:module.vision_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True

name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:Truename:module.vision_model.decoder.layers.3.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True

name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:Truename:module.vision_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True

name:module.vision_model.decoder.layers.6.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:Truename:module.vision_model.decoder.layers.3.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True

name:module.vision_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True

name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:Truename:module.vision_model.decoder.layers.4.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:Truename:module.vision_model.decoder.layers.4.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True

name:module.vision_projection.encoder.linear_fc1.weight param:torch.Size([7168, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:Truename:module.vision_projection.encoder.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:True

name:module.vision_model.decoder.layers.5.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_projection.encoder.linear_fc1.weight param:torch.Size([7168, 1024]) require_grad:True
name:module.vision_projection.encoder.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:True
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (87109632 elements):
	vision_model.decoder.layers.5.self_attention.linear_qkv.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.4.self_attention.linear_proj.bias
	vision_model.decoder.layers.2.mlp.linear_fc1.bias
	vision_projection.encoder.linear_fc2.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.bias
	vision_model.decoder.layers.5.mlp.linear_fc2.bias
	vision_model.decoder.layers.4.mlp.linear_fc2.weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.2.self_attention.linear_proj.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.6.mlp.linear_fc1.bias
	vision_model.decoder.layers.6.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.self_attention.linear_proj.weight
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.1.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.5.self_attention.linear_proj.bias
	vision_model.decoder.layers.7.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.weight
	vision_projection.encoder.linear_fc1.weight
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.0.self_attention.linear_proj.bias
	vision_model.decoder.layers.4.mlp.linear_fc1.weight
	vision_model.decoder.layers.2.mlp.linear_fc2.bias
	vision_model.decoder.layers.2.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.4.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.weight
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.bias
	vision_model.decoder.layers.0.mlp.linear_fc1.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.3.mlp.linear_fc1.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.0.self_attention.linear_qkv.weight
	vision_model.decoder.layers.7.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.0.mlp.linear_fc1.bias
	vision_model.decoder.layers.3.mlp.linear_fc2.weight
	vision_model.decoder.layers.1.self_attention.linear_proj.weight
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.6.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.mlp.linear_fc2.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.bias
	vision_model.decoder.layers.7.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.0.mlp.linear_fc2.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.bias
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.7.mlp.linear_fc1.bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.0.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.1.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.bias
	vision_model.decoder.layers.3.self_attention.linear_proj.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.3.mlp.linear_fc2.bias
	vision_model.decoder.layers.1.mlp.linear_fc2.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.weight
	vision_model.decoder.layers.2.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.mlp.linear_fc2.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.bias
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.1.self_attention.linear_proj.bias
	vision_model.decoder.layers.0.self_attention.linear_proj.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.weight
	vision_model.decoder.layers.5.mlp.linear_fc1.weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.weight
	vision_model.decoder.layers.4.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.self_attention.linear_proj.bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.weight
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:Truename:module.vision_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True

name:module.vision_model.decoder.layers.7.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:Truename:module.vision_projection.encoder.linear_fc1.weight param:torch.Size([7168, 1024]) require_grad:True

name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_projection.encoder.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.0.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.1.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.2.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.3.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.3.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.4.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.4.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.5.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.6.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([1024, 512]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_proj.bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.self_attention.linear_qkv.bias param:torch.Size([1536]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias param:torch.Size([1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([2048, 1024]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc1.bias param:torch.Size([2048]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([1024, 2048]) require_grad:True
name:module.vision_model.decoder.layers.7.mlp.linear_fc2.bias param:torch.Size([1024]) require_grad:True
name:module.vision_projection.encoder.linear_fc1.weight param:torch.Size([7168, 1024]) require_grad:True
name:module.vision_projection.encoder.linear_fc2.weight param:torch.Size([4096, 7168]) require_grad:True
rank=1, worker=0: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<1094>, pretrain-38.tar[9700, 9800), pretrain-38.tar[9800, 9900), pretrain-38.tar[9900, 10000)] sum(count)=110000
rank=1, worker=1: shard_range=[pretrain-39.tar[0, 100), pretrain-39.tar[100, 200), pretrain-39.tar[200, 300), ...<1094>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=110000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<1094>, pretrain-18.tar[9700, 9800), pretrain-18.tar[9800, 9900), pretrain-18.tar[9900, 10000)] sum(count)=110000
rank=0, worker=1: shard_range=[pretrain-19.tar[0, 100), pretrain-19.tar[100, 200), pretrain-19.tar[200, 300), ...<1094>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=110000
rank=1, worker=0: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 5000)] sum(count)=15000
rank=1, worker=1: shard_range=[pretrain-52.tar[5000, 10000), pretrain-53.tar[0, 10000)] sum(count)=15000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 5000)] sum(count)=15000
rank=0, worker=1: shard_range=[pretrain-5.tar[5000, 10000), pretrain-50.tar[0, 10000)] sum(count)=15000
[Rank 4] (after 1 iterations) memory (MB) | allocated: 1359.31396484375 | max allocated: 3287.408203125 | reserved: 3780.0 | max reserved: 3780.0[Rank 5] (after 1 iterations) memory (MB) | allocated: 1359.31396484375 | max allocated: 3287.658203125 | reserved: 3802.0 | max reserved: 3802.0

