examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 22:27:22,585] torch.distributed.run: [WARNING] 
[2024-12-02 22:27:22,585] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 22:27:22,585] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 22:27:22,585] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])

111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])


111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])

111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])

111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])

111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])

111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])

111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])

111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])


111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])

111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])


111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])

111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (33558528 elements):
	language_model.output_layer.weight
	language_model.decoder.final_layernorm.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False


name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False


name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False


name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False


name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 117.23
     rank  1: 75.66
     rank  2: 116.65
     rank  3: 81.08
     rank  4: 47.27
     rank  5: 49.58
     rank  6: 33.35
     rank  7: 47.12
     rank  8: 40.02
     rank  9: 28.43
     rank 10: 38.90
     rank 11: 37.44
     rank 12: 40.96
     rank 13: 41.09
     rank 14: 38.75
     rank 15: 30.58
     rank 16: 44.40
     rank 17: 44.28
     rank 18: 44.35
     rank 19: 30.55
  train/valid/test-data-iterators-setup:
     rank  0: 1105.03
     rank  1: 1105.00
     rank  2: 1105.16
     rank  3: 1104.99
     rank  4: 1158.01
     rank  5: 1157.66
     rank  6: 1158.31
     rank  7: 1158.35
     rank  8: 1157.69
     rank  9: 1157.95
     rank 10: 1157.83
     rank 11: 1157.84
     rank 12: 1157.89
     rank 13: 1157.80
     rank 14: 1157.96
     rank 15: 1158.44
     rank 16: 1158.07
     rank 17: 1158.36
     rank 18: 1158.06
     rank 19: 1158.23
 [2024-12-02 22:28:14] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21398.4 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4926.0 | max reserved: 4926.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4882.0 | max reserved: 4882.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4874.0 | max reserved: 4874.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4850.0 | max reserved: 4850.0
times across ranks (ms):
  forward-backward:
     rank  0: 21321.72
     rank  1: 21321.55
     rank  2: 21321.69
     rank  3: 21321.77
     rank  4: 21321.08
     rank  5: 21321.05
     rank  6: 21321.64
     rank  7: 21321.32
     rank  8: 21363.20
     rank  9: 21363.33
     rank 10: 21357.82
     rank 11: 21362.06
     rank 12: 21359.49
     rank 13: 21362.43
     rank 14: 21359.77
     rank 15: 21364.89
     rank 16: 21320.57
     rank 17: 21320.51
     rank 18: 21320.49
     rank 19: 21320.35
  forward-compute:
     rank  0: 4875.69
     rank  1: 4903.83
     rank  2: 4929.80
     rank  3: 4885.47
     rank  4: 5190.98
     rank  5: 5226.85
     rank  6: 5198.18
     rank  7: 5189.59
     rank  8: 4848.49
     rank  9: 4881.45
     rank 10: 4858.77
     rank 11: 4851.94
     rank 12: 5118.14
     rank 13: 5140.15
     rank 14: 5128.92
     rank 15: 5121.66
     rank 16: 5366.42
     rank 17: 5377.56
     rank 18: 5374.83
     rank 19: 5365.74
  backward-compute:
     rank  0: 1893.03
     rank  1: 1898.40
     rank  2: 1844.71
     rank  3: 1880.65
     rank  4: 3079.35
     rank  5: 3079.58
     rank  6: 3080.49
     rank  7: 3079.00
     rank  8: 2988.02
     rank  9: 2990.53
     rank 10: 2996.29
     rank 11: 2991.09
     rank 12: 2973.00
     rank 13: 2975.14
     rank 14: 2979.53
     rank 15: 2977.26
     rank 16: 3196.54
     rank 17: 3196.86
     rank 18: 3202.80
     rank 19: 3206.92
  pure-backward-compute:
     rank  0: 1891.95
     rank  1: 1897.54
     rank  2: 1843.85
     rank  3: 1879.76
     rank  4: 3078.24
     rank  5: 3078.49
     rank  6: 3079.35
     rank  7: 3077.37
     rank  8: 2986.45
     rank  9: 2988.85
     rank 10: 2995.49
     rank 11: 2990.24
     rank 12: 2971.12
     rank 13: 2974.24
     rank 14: 2978.69
     rank 15: 2975.22
     rank 16: 3193.23
     rank 17: 3193.79
     rank 18: 3201.29
     rank 19: 3205.14
  batch-generator:
     rank  0: 1099.93
     rank  1: 1128.77
     rank  2: 1154.84
     rank  3: 1114.75
     rank  4: 1099.31
     rank  5: 1136.24
     rank  6: 1110.47
     rank  7: 1109.97
     rank  8: 1039.38
     rank  9: 1075.81
     rank 10: 1050.48
     rank 11: 1045.63
     rank 12: 1276.92
     rank 13: 1298.35
     rank 14: 1288.56
     rank 15: 1283.52
     rank 16: 1209.64
     rank 17: 1222.44
     rank 18: 1218.61
     rank 19: 1216.10
  forward-recv:
     rank  4: 3945.26
     rank  5: 3944.84
     rank  6: 3945.64
     rank  7: 3945.66
     rank  8: 6374.15
     rank  9: 6362.35
     rank 10: 6371.24
     rank 11: 6370.41
     rank 12: 8634.90
     rank 13: 8622.71
     rank 14: 8632.57
     rank 15: 8635.73
     rank 16: 11161.36
     rank 17: 11153.06
     rank 18: 11159.20
     rank 19: 11160.86
  forward-send:
     rank  0: 7160.67
     rank  1: 7126.78
     rank  2: 7153.45
     rank  3: 7157.08
     rank  4: 4462.92
     rank  5: 4429.83
     rank  6: 4454.97
     rank  7: 4458.41
     rank  8: 2389.97
     rank  9: 2370.30
     rank 10: 2384.96
     rank 11: 2390.56
     rank 12: 36.35
     rank 13: 28.48
     rank 14: 34.22
     rank 15: 35.96
  backward-recv:
     rank  0: 1312.15
     rank  1: 1313.20
     rank  2: 1313.17
     rank  3: 1313.36
     rank  4: 596.41
     rank  5: 595.61
     rank  6: 595.78
     rank  7: 594.94
     rank  8: 390.20
     rank  9: 390.04
     rank 10: 390.69
     rank 11: 391.36
     rank 12: 196.47
     rank 13: 197.14
     rank 14: 197.02
     rank 15: 196.35
  backward-send:
     rank  4: 4.08
     rank  5: 4.41
     rank  6: 4.14
     rank  7: 4.64
     rank  8: 31.50
     rank  9: 31.22
     rank 10: 30.98
     rank 11: 30.31
     rank 12: 21.00
     rank 13: 19.96
     rank 14: 20.48
     rank 15: 21.02
     rank 16: 10.53
     rank 17: 10.64
     rank 18: 10.47
     rank 19: 10.23
  forward-send-backward-recv:
     rank  0: 6013.80
     rank  1: 6013.20
     rank  2: 6016.13
     rank  3: 6019.98
     rank  4: 3215.15
     rank  5: 3216.44
     rank  6: 3214.08
     rank  7: 3217.64
     rank  8: 3084.74
     rank  9: 3081.81
     rank 10: 3078.94
     rank 11: 3083.69
     rank 12: 2911.16
     rank 13: 2913.62
     rank 14: 2910.25
     rank 15: 2906.76
  backward-send-forward-recv:
     rank  4: 702.90
     rank  5: 703.33
     rank  6: 703.63
     rank  7: 706.27
     rank  8: 897.10
     rank  9: 896.25
     rank 10: 896.26
     rank 11: 898.05
     rank 12: 916.17
     rank 13: 915.20
     rank 14: 910.15
     rank 15: 911.40
     rank 16: 877.74
     rank 17: 875.63
     rank 18: 871.37
     rank 19: 873.75
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.09
     rank  7: 0.05
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.10
     rank  5: 0.11
     rank  6: 0.31
     rank  7: 0.17
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 49.47
     rank  1: 51.39
     rank  2: 44.35
     rank  3: 44.36
     rank  4: 43.92
     rank  5: 41.65
     rank  6: 50.79
     rank  7: 50.86
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.20
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.85
     rank 17: 0.69
     rank 18: 0.63
     rank 19: 0.63
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.26
     rank  2: 0.23
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.09
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.05
     rank 16: 0.04
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.06
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 25.94
     rank  1: 29.78
     rank  2: 27.54
     rank  3: 27.47
     rank  4: 5.85
     rank  5: 6.12
     rank  6: 6.56
     rank  7: 6.63
     rank  8: 0.22
     rank  9: 0.18
     rank 10: 0.10
     rank 11: 0.08
     rank 12: 0.13
     rank 13: 0.11
     rank 14: 0.11
     rank 15: 0.18
     rank 16: 4.68
     rank 17: 4.37
     rank 18: 4.28
     rank 19: 4.30
  optimizer:
     rank  0: 27.96
     rank  1: 31.77
     rank  2: 29.57
     rank  3: 29.49
     rank  4: 7.87
     rank  5: 8.15
     rank  6: 8.49
     rank  7: 8.66
     rank  8: 2.27
     rank  9: 2.22
     rank 10: 2.12
     rank 11: 2.12
     rank 12: 2.15
     rank 13: 2.13
     rank 14: 2.12
     rank 15: 2.20
     rank 16: 6.71
     rank 17: 6.40
     rank 18: 6.31
     rank 19: 6.33
 [2024-12-02 22:28:21] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7608.6 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7562.42
     rank  1: 7562.40
     rank  2: 7562.40
     rank  3: 7562.40
     rank  4: 7562.17
     rank  5: 7562.13
     rank  6: 7562.32
     rank  7: 7562.15
     rank  8: 7561.73
     rank  9: 7561.75
     rank 10: 7561.82
     rank 11: 7561.71
     rank 12: 7561.75
     rank 13: 7561.72
     rank 14: 7561.73
     rank 15: 7561.75
     rank 16: 7561.95
     rank 17: 7561.36
     rank 18: 7561.89
     rank 19: 7561.91
  forward-compute:
     rank  0: 1010.40
     rank  1: 1012.68
     rank  2: 1012.60
     rank  3: 1012.89
     rank  4: 2956.65
     rank  5: 2959.45
     rank  6: 2961.26
     rank  7: 2954.29
     rank  8: 2761.29
     rank  9: 2760.13
     rank 10: 2758.64
     rank 11: 2764.73
     rank 12: 2771.82
     rank 13: 2776.90
     rank 14: 2774.99
     rank 15: 2771.57
     rank 16: 2900.21
     rank 17: 2900.48
     rank 18: 2903.92
     rank 19: 2905.66
  backward-compute:
     rank  0: 1035.37
     rank  1: 1033.57
     rank  2: 1033.35
     rank  3: 1031.22
     rank  4: 3057.44
     rank  5: 3056.88
     rank  6: 3056.29
     rank  7: 3062.63
     rank  8: 2970.60
     rank  9: 2976.75
     rank 10: 2975.34
     rank 11: 2969.76
     rank 12: 2951.03
     rank 13: 2949.80
     rank 14: 2952.69
     rank 15: 2954.37
     rank 16: 3194.99
     rank 17: 3194.37
     rank 18: 3196.52
     rank 19: 3195.28
  pure-backward-compute:
     rank  0: 1034.70
     rank  1: 1032.89
     rank  2: 1032.68
     rank  3: 1030.59
     rank  4: 3056.53
     rank  5: 3056.15
     rank  6: 3055.49
     rank  7: 3060.99
     rank  8: 2969.30
     rank  9: 2975.00
     rank 10: 2973.93
     rank 11: 2968.79
     rank 12: 2949.62
     rank 13: 2949.08
     rank 14: 2951.87
     rank 15: 2952.71
     rank 16: 3192.21
     rank 17: 3191.66
     rank 18: 3195.08
     rank 19: 3193.67
  batch-generator:
     rank  0: 58.80
     rank  1: 60.65
     rank  2: 59.83
     rank  3: 61.57
     rank  4: 51.18
     rank  5: 54.12
     rank  6: 55.48
     rank  7: 61.37
     rank  8: 52.02
     rank  9: 54.10
     rank 10: 53.34
     rank 11: 59.95
     rank 12: 53.17
     rank 13: 60.33
     rank 14: 59.23
     rank 15: 57.23
     rank 16: 56.39
     rank 17: 59.20
     rank 18: 62.33
     rank 19: 64.68
  forward-recv:
     rank  4: 77.89
     rank  5: 77.62
     rank  6: 77.57
     rank  7: 77.83
     rank  8: 297.04
     rank  9: 296.67
     rank 10: 298.08
     rank 11: 297.55
     rank 12: 464.78
     rank 13: 465.59
     rank 14: 464.86
     rank 15: 464.41
     rank 16: 635.73
     rank 17: 634.79
     rank 18: 635.30
     rank 19: 635.73
  forward-send:
     rank  0: 371.67
     rank  1: 371.05
     rank  2: 371.64
     rank  3: 370.87
     rank  4: 31.64
     rank  5: 31.32
     rank  6: 32.03
     rank  7: 31.46
     rank  8: 21.01
     rank  9: 21.28
     rank 10: 20.09
     rank 11: 20.54
     rank 12: 10.59
     rank 13: 10.03
     rank 14: 9.97
     rank 15: 10.52
  backward-recv:
     rank  0: 1310.80
     rank  1: 1310.79
     rank  2: 1311.27
     rank  3: 1311.94
     rank  4: 603.17
     rank  5: 602.90
     rank  6: 602.09
     rank  7: 599.39
     rank  8: 389.76
     rank  9: 389.47
     rank 10: 389.42
     rank 11: 390.28
     rank 12: 194.85
     rank 13: 195.45
     rank 14: 195.39
     rank 15: 194.19
  backward-send:
     rank  4: 3.77
     rank  5: 3.44
     rank  6: 3.79
     rank  7: 5.42
     rank  8: 31.15
     rank  9: 31.33
     rank 10: 31.32
     rank 11: 30.48
     rank 12: 20.91
     rank 13: 20.69
     rank 14: 20.33
     rank 15: 20.76
     rank 16: 10.57
     rank 17: 10.51
     rank 18: 10.26
     rank 19: 9.97
  forward-send-backward-recv:
     rank  0: 3820.58
     rank  1: 3820.43
     rank  2: 3821.75
     rank  3: 3823.68
     rank  4: 741.38
     rank  5: 743.48
     rank  6: 740.06
     rank  7: 738.07
     rank  8: 685.12
     rank  9: 680.66
     rank 10: 683.52
     rank 11: 689.29
     rank 12: 532.53
     rank 13: 536.93
     rank 14: 535.73
     rank 15: 530.75
  backward-send-forward-recv:
     rank  4: 19.70
     rank  5: 19.40
     rank  6: 19.11
     rank  7: 20.90
     rank  8: 144.15
     rank  9: 143.45
     rank 10: 143.48
     rank 11: 139.93
     rank 12: 156.68
     rank 13: 151.97
     rank 14: 153.28
     rank 15: 156.38
     rank 16: 167.35
     rank 17: 168.53
     rank 18: 166.11
     rank 19: 164.43
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.08
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.59
     rank  2: 0.60
     rank  3: 0.55
     rank  4: 0.49
     rank  5: 0.42
     rank  6: 0.77
     rank  7: 0.59
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.67
     rank 17: 0.63
     rank 18: 0.48
     rank 19: 0.52
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.08
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.03
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.37
     rank  1: 8.59
     rank  2: 8.60
     rank  3: 8.54
     rank  4: 5.33
     rank  5: 5.25
     rank  6: 5.86
     rank  7: 5.40
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.07
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 3.78
     rank 17: 3.83
     rank 18: 3.67
     rank 19: 3.75
  optimizer:
     rank  0: 9.12
     rank  1: 9.34
     rank  2: 9.36
     rank  3: 9.29
     rank  4: 6.08
     rank  5: 6.00
     rank  6: 6.61
     rank  7: 6.16
     rank  8: 0.79
     rank  9: 0.82
     rank 10: 0.82
     rank 11: 0.82
     rank 12: 0.82
     rank 13: 0.78
     rank 14: 0.78
     rank 15: 0.78
     rank 16: 4.54
     rank 17: 4.57
     rank 18: 4.43
     rank 19: 4.50
 [2024-12-02 22:28:29] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7587.6 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 6.965115E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7556.30
     rank  1: 7556.29
     rank  2: 7556.33
     rank  3: 7556.28
     rank  4: 7556.05
     rank  5: 7556.00
     rank  6: 7556.06
     rank  7: 7556.00
     rank  8: 7555.63
     rank  9: 7555.59
     rank 10: 7555.67
     rank 11: 7555.58
     rank 12: 7555.69
     rank 13: 7555.58
     rank 14: 7555.63
     rank 15: 7555.58
     rank 16: 7555.82
     rank 17: 7555.82
     rank 18: 7555.79
     rank 19: 7555.78
  forward-compute:
     rank  0: 1099.46
     rank  1: 1102.27
     rank  2: 1103.87
     rank  3: 1102.02
     rank  4: 2959.58
     rank  5: 2964.02
     rank  6: 2963.13
     rank  7: 2956.88
     rank  8: 2772.23
     rank  9: 2773.12
     rank 10: 2772.93
     rank 11: 2775.51
     rank 12: 2781.81
     rank 13: 2784.97
     rank 14: 2784.47
     rank 15: 2783.98
     rank 16: 2900.63
     rank 17: 2901.61
     rank 18: 2904.47
     rank 19: 2907.34
  backward-compute:
     rank  0: 1023.98
     rank  1: 1023.27
     rank  2: 1022.16
     rank  3: 1020.83
     rank  4: 3067.13
     rank  5: 3068.63
     rank  6: 3068.09
     rank  7: 3071.60
     rank  8: 2981.53
     rank  9: 2986.83
     rank 10: 2986.49
     rank 11: 2980.51
     rank 12: 2960.75
     rank 13: 2960.11
     rank 14: 2961.51
     rank 15: 2964.87
     rank 16: 3198.01
     rank 17: 3197.15
     rank 18: 3199.54
     rank 19: 3199.12
  pure-backward-compute:
     rank  0: 1023.25
     rank  1: 1022.62
     rank  2: 1021.49
     rank  3: 1020.10
     rank  4: 3066.31
     rank  5: 3067.96
     rank  6: 3067.25
     rank  7: 3070.37
     rank  8: 2980.24
     rank  9: 2985.23
     rank 10: 2985.00
     rank 11: 2979.70
     rank 12: 2959.51
     rank 13: 2959.23
     rank 14: 2960.71
     rank 15: 2963.08
     rank 16: 3195.70
     rank 17: 3194.77
     rank 18: 3197.99
     rank 19: 3197.57
  batch-generator:
     rank  0: 56.45
     rank  1: 59.73
     rank  2: 62.20
     rank  3: 61.22
     rank  4: 51.75
     rank  5: 54.50
     rank  6: 59.62
     rank  7: 64.25
     rank  8: 57.79
     rank  9: 59.91
     rank 10: 60.71
     rank 11: 64.14
     rank 12: 56.20
     rank 13: 60.12
     rank 14: 60.63
     rank 15: 60.45
     rank 16: 54.73
     rank 17: 58.64
     rank 18: 60.53
     rank 19: 63.94
  forward-recv:
     rank  4: 62.10
     rank  5: 61.80
     rank  6: 60.97
     rank  7: 61.94
     rank  8: 276.05
     rank  9: 275.83
     rank 10: 276.16
     rank 11: 278.62
     rank 12: 449.20
     rank 13: 449.46
     rank 14: 449.84
     rank 15: 447.03
     rank 16: 622.18
     rank 17: 621.57
     rank 18: 620.84
     rank 19: 621.71
  forward-send:
     rank  0: 416.43
     rank  1: 415.51
     rank  2: 414.23
     rank  3: 415.54
     rank  4: 31.62
     rank  5: 31.27
     rank  6: 30.51
     rank  7: 31.40
     rank  8: 20.71
     rank  9: 20.72
     rank 10: 20.17
     rank 11: 18.53
     rank 12: 10.38
     rank 13: 10.32
     rank 14: 9.51
     rank 15: 10.45
  backward-recv:
     rank  0: 1317.02
     rank  1: 1317.24
     rank  2: 1317.62
     rank  3: 1317.63
     rank  4: 595.67
     rank  5: 595.20
     rank  6: 595.19
     rank  7: 595.06
     rank  8: 391.96
     rank  9: 391.61
     rank 10: 392.00
     rank 11: 393.16
     rank 12: 197.63
     rank 13: 198.47
     rank 14: 198.31
     rank 15: 196.71
  backward-send:
     rank  4: 4.42
     rank  5: 4.11
     rank  6: 4.30
     rank  7: 4.37
     rank  8: 31.30
     rank  9: 30.55
     rank 10: 31.28
     rank 11: 30.69
     rank 12: 20.81
     rank 13: 19.74
     rank 14: 20.27
     rank 15: 20.26
     rank 16: 10.53
     rank 17: 10.50
     rank 18: 10.38
     rank 19: 9.92
  forward-send-backward-recv:
     rank  0: 3685.77
     rank  1: 3684.33
     rank  2: 3686.62
     rank  3: 3688.24
     rank  4: 741.31
     rank  5: 741.06
     rank  6: 740.96
     rank  7: 738.94
     rank  8: 658.67
     rank  9: 654.93
     rank 10: 657.62
     rank 11: 662.70
     rank 12: 517.93
     rank 13: 522.44
     rank 14: 521.98
     rank 15: 515.42
  backward-send-forward-recv:
     rank  4: 20.17
     rank  5: 18.70
     rank  6: 19.65
     rank  7: 20.91
     rank  8: 157.70
     rank  9: 156.38
     rank 10: 156.19
     rank 11: 153.82
     rank 12: 156.81
     rank 13: 153.21
     rank 14: 153.32
     rank 15: 156.63
     rank 16: 169.18
     rank 17: 169.46
     rank 18: 167.83
     rank 19: 164.60
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.54
     rank  2: 0.54
     rank  3: 0.56
     rank  4: 0.48
     rank  5: 0.40
     rank  6: 0.46
     rank  7: 0.44
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.04
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.48
     rank 17: 0.45
     rank 18: 0.46
     rank 19: 0.37
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.32
     rank  1: 8.44
     rank  2: 8.30
     rank  3: 8.51
     rank  4: 5.33
     rank  5: 5.24
     rank  6: 5.27
     rank  7: 5.26
     rank  8: 0.07
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.08
     rank 16: 3.73
     rank 17: 3.72
     rank 18: 3.72
     rank 19: 3.69
  optimizer:
     rank  0: 8.98
     rank  1: 9.11
     rank  2: 8.97
     rank  3: 9.17
     rank  4: 5.99
     rank  5: 5.91
     rank  6: 5.93
     rank  7: 5.93
     rank  8: 0.73
     rank  9: 0.70
     rank 10: 0.73
     rank 11: 0.70
     rank 12: 0.71
     rank 13: 0.74
     rank 14: 0.73
     rank 15: 0.74
     rank 16: 4.40
     rank 17: 4.38
     rank 18: 4.38
     rank 19: 4.36
 [2024-12-02 22:28:36] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7603.5 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 5.775488E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7565.95
     rank  1: 7565.96
     rank  2: 7565.91
     rank  3: 7565.90
     rank  4: 7565.68
     rank  5: 7565.69
     rank  6: 7565.72
     rank  7: 7565.67
     rank  8: 7565.22
     rank  9: 7565.28
     rank 10: 7565.25
     rank 11: 7565.22
     rank 12: 7565.23
     rank 13: 7565.26
     rank 14: 7565.22
     rank 15: 7565.21
     rank 16: 7565.44
     rank 17: 7565.45
     rank 18: 7565.42
     rank 19: 7565.42
  forward-compute:
     rank  0: 940.99
     rank  1: 943.55
     rank  2: 944.88
     rank  3: 944.42
     rank  4: 2957.45
     rank  5: 2959.44
     rank  6: 2959.34
     rank  7: 2959.32
     rank  8: 2774.65
     rank  9: 2775.61
     rank 10: 2775.96
     rank 11: 2777.41
     rank 12: 2778.11
     rank 13: 2784.63
     rank 14: 2780.82
     rank 15: 2778.99
     rank 16: 2903.57
     rank 17: 2904.01
     rank 18: 2908.38
     rank 19: 2907.80
  backward-compute:
     rank  0: 1042.03
     rank  1: 1039.85
     rank  2: 1040.23
     rank  3: 1038.93
     rank  4: 3066.26
     rank  5: 3066.89
     rank  6: 3067.56
     rank  7: 3068.07
     rank  8: 2980.66
     rank  9: 2987.73
     rank 10: 2985.53
     rank 11: 2980.17
     rank 12: 2964.32
     rank 13: 2962.95
     rank 14: 2966.54
     rank 15: 2965.65
     rank 16: 3197.77
     rank 17: 3197.50
     rank 18: 3200.19
     rank 19: 3198.94
  pure-backward-compute:
     rank  0: 1041.33
     rank  1: 1039.20
     rank  2: 1039.53
     rank  3: 1038.26
     rank  4: 3065.44
     rank  5: 3065.97
     rank  6: 3066.72
     rank  7: 3066.89
     rank  8: 2979.31
     rank  9: 2986.17
     rank 10: 2984.55
     rank 11: 2979.36
     rank 12: 2962.87
     rank 13: 2962.28
     rank 14: 2965.75
     rank 15: 2963.99
     rank 16: 3195.45
     rank 17: 3195.03
     rank 18: 3198.72
     rank 19: 3197.34
  batch-generator:
     rank  0: 55.64
     rank  1: 59.33
     rank  2: 61.82
     rank  3: 62.32
     rank  4: 51.53
     rank  5: 57.17
     rank  6: 57.09
     rank  7: 60.80
     rank  8: 56.01
     rank  9: 56.38
     rank 10: 58.13
     rank 11: 61.58
     rank 12: 61.74
     rank 13: 67.43
     rank 14: 64.84
     rank 15: 64.68
     rank 16: 53.63
     rank 17: 56.68
     rank 18: 60.63
     rank 19: 60.49
  forward-recv:
     rank  4: 63.72
     rank  5: 63.76
     rank  6: 63.21
     rank  7: 63.22
     rank  8: 281.72
     rank  9: 281.52
     rank 10: 281.81
     rank 11: 282.94
     rank 12: 451.49
     rank 13: 452.14
     rank 14: 452.35
     rank 15: 450.21
     rank 16: 623.46
     rank 17: 623.18
     rank 18: 622.83
     rank 19: 623.46
  forward-send:
     rank  0: 414.17
     rank  1: 413.77
     rank  2: 412.48
     rank  3: 412.61
     rank  4: 31.62
     rank  5: 31.29
     rank  6: 31.04
     rank  7: 31.04
     rank  8: 20.93
     rank  9: 20.94
     rank 10: 20.71
     rank 11: 19.52
     rank 12: 10.52
     rank 13: 10.18
     rank 14: 9.80
     rank 15: 10.49
  backward-recv:
     rank  0: 1309.14
     rank  1: 1310.11
     rank  2: 1309.58
     rank  3: 1310.63
     rank  4: 597.41
     rank  5: 596.43
     rank  6: 596.96
     rank  7: 595.61
     rank  8: 390.24
     rank  9: 389.43
     rank 10: 390.36
     rank 11: 391.56
     rank 12: 194.87
     rank 13: 195.62
     rank 14: 195.20
     rank 15: 194.76
  backward-send:
     rank  4: 3.50
     rank  5: 4.25
     rank  6: 3.50
     rank  7: 5.44
     rank  8: 31.21
     rank  9: 31.33
     rank 10: 31.15
     rank 11: 30.30
     rank 12: 21.07
     rank 13: 20.29
     rank 14: 20.41
     rank 15: 21.05
     rank 16: 10.54
     rank 17: 10.56
     rank 18: 10.26
     rank 19: 10.28
  forward-send-backward-recv:
     rank  0: 3845.81
     rank  1: 3845.17
     rank  2: 3846.72
     rank  3: 3847.68
     rank  4: 750.46
     rank  5: 750.33
     rank  6: 749.47
     rank  7: 745.88
     rank  8: 663.58
     rank  9: 655.03
     rank 10: 660.00
     rank 11: 667.14
     rank 12: 522.47
     rank 13: 527.30
     rank 14: 525.81
     rank 15: 522.16
  backward-send-forward-recv:
     rank  4: 20.50
     rank  5: 20.36
     rank  6: 20.26
     rank  7: 20.85
     rank  8: 153.11
     rank  9: 152.51
     rank 10: 152.42
     rank 11: 150.52
     rank 12: 156.57
     rank 13: 150.83
     rank 14: 153.05
     rank 15: 156.81
     rank 16: 168.61
     rank 17: 169.09
     rank 18: 165.73
     rank 19: 166.06
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.60
     rank  1: 0.56
     rank  2: 0.55
     rank  3: 0.55
     rank  4: 0.43
     rank  5: 0.43
     rank  6: 0.50
     rank  7: 0.49
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.53
     rank 17: 0.58
     rank 18: 0.47
     rank 19: 0.50
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.56
     rank  1: 8.51
     rank  2: 8.42
     rank  3: 8.54
     rank  4: 5.28
     rank  5: 5.28
     rank  6: 5.35
     rank  7: 5.34
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 3.74
     rank 17: 3.73
     rank 18: 3.66
     rank 19: 3.73
  optimizer:
     rank  0: 9.21
     rank  1: 9.16
     rank  2: 9.08
     rank  3: 9.20
     rank  4: 5.93
     rank  5: 5.93
     rank  6: 6.00
     rank  7: 6.00
     rank  8: 0.69
     rank  9: 0.73
     rank 10: 0.72
     rank 11: 0.69
     rank 12: 0.72
     rank 13: 0.68
     rank 14: 0.69
     rank 15: 0.69
     rank 16: 4.39
     rank 17: 4.38
     rank 18: 4.31
     rank 19: 4.38
 [2024-12-02 22:28:44] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7600.9 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 4.508252E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7569.22
     rank  1: 7569.23
     rank  2: 7569.24
     rank  3: 7569.26
     rank  4: 7569.00
     rank  5: 7568.94
     rank  6: 7569.02
     rank  7: 7569.09
     rank  8: 7568.60
     rank  9: 7568.49
     rank 10: 7568.63
     rank 11: 7568.53
     rank 12: 7568.61
     rank 13: 7568.50
     rank 14: 7568.58
     rank 15: 7568.60
     rank 16: 7568.76
     rank 17: 7568.75
     rank 18: 7568.75
     rank 19: 7568.76
  forward-compute:
     rank  0: 908.13
     rank  1: 910.92
     rank  2: 912.79
     rank  3: 909.57
     rank  4: 2985.18
     rank  5: 2990.53
     rank  6: 2991.20
     rank  7: 2982.23
     rank  8: 2769.80
     rank  9: 2770.43
     rank 10: 2770.42
     rank 11: 2770.62
     rank 12: 2775.50
     rank 13: 2779.11
     rank 14: 2778.20
     rank 15: 2776.19
     rank 16: 2908.96
     rank 17: 2909.21
     rank 18: 2911.92
     rank 19: 2914.09
  backward-compute:
     rank  0: 1036.98
     rank  1: 1037.12
     rank  2: 1038.12
     rank  3: 1034.74
     rank  4: 3069.73
     rank  5: 3070.58
     rank  6: 3070.75
     rank  7: 3069.15
     rank  8: 2985.24
     rank  9: 2990.26
     rank 10: 2989.79
     rank 11: 2984.45
     rank 12: 2971.00
     rank 13: 2969.77
     rank 14: 2971.81
     rank 15: 2973.84
     rank 16: 3200.08
     rank 17: 3199.27
     rank 18: 3202.12
     rank 19: 3201.21
  pure-backward-compute:
     rank  0: 1036.26
     rank  1: 1036.46
     rank  2: 1037.39
     rank  3: 1034.07
     rank  4: 3068.79
     rank  5: 3069.82
     rank  6: 3069.91
     rank  7: 3067.57
     rank  8: 2984.08
     rank  9: 2988.88
     rank 10: 2988.65
     rank 11: 2983.63
     rank 12: 2969.86
     rank 13: 2969.11
     rank 14: 2971.11
     rank 15: 2972.12
     rank 16: 3197.81
     rank 17: 3196.87
     rank 18: 3200.63
     rank 19: 3199.72
  batch-generator:
     rank  0: 52.00
     rank  1: 55.99
     rank  2: 59.52
     rank  3: 56.97
     rank  4: 59.62
     rank  5: 61.60
     rank  6: 65.40
     rank  7: 75.82
     rank  8: 51.05
     rank  9: 54.60
     rank 10: 55.39
     rank 11: 56.22
     rank 12: 50.26
     rank 13: 56.21
     rank 14: 56.31
     rank 15: 55.28
     rank 16: 54.22
     rank 17: 57.33
     rank 18: 59.00
     rank 19: 61.76
  forward-recv:
     rank  4: 61.30
     rank  5: 60.89
     rank  6: 59.58
     rank  7: 61.00
     rank  8: 278.13
     rank  9: 277.99
     rank 10: 278.00
     rank 11: 280.01
     rank 12: 451.88
     rank 13: 452.62
     rank 14: 452.38
     rank 15: 451.12
     rank 16: 619.39
     rank 17: 619.17
     rank 18: 618.93
     rank 19: 619.31
  forward-send:
     rank  0: 414.04
     rank  1: 412.98
     rank  2: 411.08
     rank  3: 413.93
     rank  4: 31.54
     rank  5: 31.19
     rank  6: 29.91
     rank  7: 31.70
     rank  8: 21.01
     rank  9: 20.97
     rank 10: 20.37
     rank 11: 19.59
     rank 12: 10.48
     rank 13: 10.15
     rank 14: 9.93
     rank 15: 10.48
  backward-recv:
     rank  0: 1306.39
     rank  1: 1306.60
     rank  2: 1305.43
     rank  3: 1306.62
     rank  4: 592.84
     rank  5: 591.58
     rank  6: 592.18
     rank  7: 591.31
     rank  8: 388.51
     rank  9: 387.59
     rank 10: 388.60
     rank 11: 388.93
     rank 12: 199.16
     rank 13: 199.69
     rank 14: 199.66
     rank 15: 198.78
  backward-send:
     rank  4: 3.89
     rank  5: 4.54
     rank  6: 3.88
     rank  7: 4.77
     rank  8: 31.26
     rank  9: 30.99
     rank 10: 31.11
     rank 11: 30.89
     rank 12: 21.00
     rank 13: 20.33
     rank 14: 20.55
     rank 15: 21.04
     rank 16: 10.56
     rank 17: 10.52
     rank 18: 10.36
     rank 19: 10.09
  forward-send-backward-recv:
     rank  0: 3890.09
     rank  1: 3888.05
     rank  2: 3890.06
     rank  3: 3892.51
     rank  4: 727.33
     rank  5: 726.81
     rank  6: 726.02
     rank  7: 728.15
     rank  8: 663.30
     rank  9: 659.24
     rank 10: 661.71
     rank 11: 667.14
     rank 12: 516.38
     rank 13: 520.88
     rank 14: 520.36
     rank 15: 514.08
  backward-send-forward-recv:
     rank  4: 19.71
     rank  5: 17.79
     rank  6: 18.46
     rank  7: 20.38
     rank  8: 161.59
     rank  9: 160.94
     rank 10: 160.65
     rank 11: 160.02
     rank 12: 156.83
     rank 13: 153.24
     rank 14: 153.33
     rank 15: 156.57
     rank 16: 168.34
     rank 17: 168.83
     rank 18: 166.74
     rank 19: 164.77
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.08
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.56
     rank  2: 0.57
     rank  3: 0.57
     rank  4: 0.47
     rank  5: 0.46
     rank  6: 0.54
     rank  7: 0.62
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.50
     rank 17: 0.50
     rank 18: 0.49
     rank 19: 0.49
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.19
     rank  3: 0.19
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.39
     rank  1: 8.41
     rank  2: 8.44
     rank  3: 8.65
     rank  4: 5.33
     rank  5: 5.33
     rank  6: 5.39
     rank  7: 5.45
     rank  8: 0.07
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.08
     rank 16: 3.66
     rank 17: 3.67
     rank 18: 3.69
     rank 19: 3.70
  optimizer:
     rank  0: 9.12
     rank  1: 9.13
     rank  2: 9.18
     rank  3: 9.38
     rank  4: 6.06
     rank  5: 6.06
     rank  6: 6.12
     rank  7: 6.19
     rank  8: 0.80
     rank  9: 0.76
     rank 10: 0.79
     rank 11: 0.76
     rank 12: 0.77
     rank 13: 0.76
     rank 14: 0.76
     rank 15: 0.80
     rank 16: 4.39
     rank 17: 4.39
     rank 18: 4.42
     rank 19: 4.43
 [2024-12-02 22:28:52] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7625.5 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 3.330508E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7588.19
     rank  1: 7588.22
     rank  2: 7588.19
     rank  3: 7588.19
     rank  4: 7587.96
     rank  5: 7587.95
     rank  6: 7588.02
     rank  7: 7588.06
     rank  8: 7587.47
     rank  9: 7587.51
     rank 10: 7587.60
     rank 11: 7587.54
     rank 12: 7587.49
     rank 13: 7587.49
     rank 14: 7587.56
     rank 15: 7587.56
     rank 16: 7587.77
     rank 17: 7587.77
     rank 18: 7587.75
     rank 19: 7587.76
  forward-compute:
     rank  0: 903.52
     rank  1: 904.90
     rank  2: 904.54
     rank  3: 903.63
     rank  4: 2982.48
     rank  5: 2986.80
     rank  6: 2987.06
     rank  7: 2977.68
     rank  8: 2780.08
     rank  9: 2781.82
     rank 10: 2781.91
     rank 11: 2782.08
     rank 12: 2793.88
     rank 13: 2799.23
     rank 14: 2797.50
     rank 15: 2795.04
     rank 16: 2921.53
     rank 17: 2922.85
     rank 18: 2925.81
     rank 19: 2926.02
  backward-compute:
     rank  0: 1050.69
     rank  1: 1051.59
     rank  2: 1054.04
     rank  3: 1048.05
     rank  4: 3069.36
     rank  5: 3070.60
     rank  6: 3070.19
     rank  7: 3072.44
     rank  8: 2977.84
     rank  9: 2984.33
     rank 10: 2983.23
     rank 11: 2976.85
     rank 12: 2986.13
     rank 13: 2985.02
     rank 14: 2987.30
     rank 15: 2989.24
     rank 16: 3205.82
     rank 17: 3205.14
     rank 18: 3208.24
     rank 19: 3207.15
  pure-backward-compute:
     rank  0: 1050.05
     rank  1: 1050.95
     rank  2: 1053.39
     rank  3: 1047.37
     rank  4: 3068.48
     rank  5: 3069.81
     rank  6: 3069.31
     rank  7: 3070.52
     rank  8: 2976.72
     rank  9: 2982.81
     rank 10: 2982.12
     rank 11: 2976.07
     rank 12: 2984.77
     rank 13: 2984.34
     rank 14: 2986.49
     rank 15: 2987.59
     rank 16: 3203.38
     rank 17: 3202.81
     rank 18: 3206.68
     rank 19: 3205.49
  batch-generator:
     rank  0: 51.77
     rank  1: 54.25
     rank  2: 55.22
     rank  3: 54.50
     rank  4: 54.38
     rank  5: 58.63
     rank  6: 63.16
     rank  7: 69.35
     rank  8: 51.67
     rank  9: 55.45
     rank 10: 55.57
     rank 11: 57.58
     rank 12: 60.46
     rank 13: 65.26
     rank 14: 64.45
     rank 15: 62.54
     rank 16: 60.93
     rank 17: 62.77
     rank 18: 64.80
     rank 19: 65.54
  forward-recv:
     rank  4: 61.39
     rank  5: 61.26
     rank  6: 59.92
     rank  7: 61.13
     rank  8: 281.61
     rank  9: 281.30
     rank 10: 281.66
     rank 11: 283.20
     rank 12: 452.09
     rank 13: 452.75
     rank 14: 452.57
     rank 15: 451.07
     rank 16: 621.76
     rank 17: 621.50
     rank 18: 621.27
     rank 19: 621.80
  forward-send:
     rank  0: 423.42
     rank  1: 422.37
     rank  2: 420.74
     rank  3: 423.30
     rank  4: 31.67
     rank  5: 31.01
     rank  6: 30.50
     rank  7: 31.84
     rank  8: 21.05
     rank  9: 21.12
     rank 10: 20.52
     rank 11: 19.89
     rank 12: 10.52
     rank 13: 10.13
     rank 14: 9.87
     rank 15: 10.50
  backward-recv:
     rank  0: 1304.14
     rank  1: 1304.34
     rank  2: 1303.72
     rank  3: 1305.15
     rank  4: 601.30
     rank  5: 600.36
     rank  6: 600.75
     rank  7: 598.39
     rank  8: 392.38
     rank  9: 392.02
     rank 10: 392.35
     rank 11: 393.44
     rank 12: 196.04
     rank 13: 196.74
     rank 14: 196.32
     rank 15: 195.73
  backward-send:
     rank  4: 3.72
     rank  5: 3.83
     rank  6: 3.57
     rank  7: 5.39
     rank  8: 31.32
     rank  9: 30.74
     rank 10: 31.24
     rank 11: 30.40
     rank 12: 21.01
     rank 13: 20.16
     rank 14: 20.53
     rank 15: 21.13
     rank 16: 10.60
     rank 17: 10.47
     rank 18: 10.24
     rank 19: 10.12
  forward-send-backward-recv:
     rank  0: 3893.04
     rank  1: 3891.72
     rank  2: 3893.64
     rank  3: 3896.61
     rank  4: 743.35
     rank  5: 743.10
     rank  6: 742.61
     rank  7: 741.40
     rank  8: 680.35
     rank  9: 673.78
     rank 10: 676.43
     rank 11: 683.79
     rank 12: 506.32
     rank 13: 511.22
     rank 14: 510.45
     rank 15: 504.66
  backward-send-forward-recv:
     rank  4: 18.97
     rank  5: 17.80
     rank  6: 18.33
     rank  7: 20.81
     rank  8: 154.17
     rank  9: 153.42
     rank 10: 153.19
     rank 11: 151.46
     rank 12: 156.56
     rank 13: 151.69
     rank 14: 152.72
     rank 15: 155.97
     rank 16: 168.23
     rank 17: 168.11
     rank 18: 165.50
     rank 19: 165.11
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.05
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.08
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.54
     rank  2: 0.54
     rank  3: 0.54
     rank  4: 0.43
     rank  5: 0.42
     rank  6: 0.51
     rank  7: 0.57
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.49
     rank 17: 0.50
     rank 18: 0.48
     rank 19: 0.49
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.16
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.36
     rank  1: 8.46
     rank  2: 8.50
     rank  3: 8.46
     rank  4: 5.27
     rank  5: 5.26
     rank  6: 5.44
     rank  7: 5.43
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 3.68
     rank 17: 3.70
     rank 18: 3.68
     rank 19: 3.74
  optimizer:
     rank  0: 9.10
     rank  1: 9.20
     rank  2: 9.25
     rank  3: 9.21
     rank  4: 6.02
     rank  5: 6.01
     rank  6: 6.19
     rank  7: 6.18
     rank  8: 0.78
     rank  9: 0.77
     rank 10: 0.81
     rank 11: 0.78
     rank 12: 0.78
     rank 13: 0.78
     rank 14: 0.78
     rank 15: 0.78
     rank 16: 4.43
     rank 17: 4.44
     rank 18: 4.43
     rank 19: 4.49
 [2024-12-02 22:28:59] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7633.0 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 2.708300E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7594.74
     rank  1: 7594.68
     rank  2: 7594.70
     rank  3: 7594.96
     rank  4: 7594.49
     rank  5: 7594.45
     rank  6: 7594.46
     rank  7: 7594.77
     rank  8: 7594.03
     rank  9: 7594.07
     rank 10: 7594.02
     rank 11: 7594.23
     rank 12: 7594.14
     rank 13: 7594.03
     rank 14: 7594.01
     rank 15: 7594.27
     rank 16: 7594.30
     rank 17: 7594.27
     rank 18: 7594.25
     rank 19: 7594.96
  forward-compute:
     rank  0: 910.75
     rank  1: 912.26
     rank  2: 912.18
     rank  3: 911.19
     rank  4: 2983.86
     rank  5: 2988.72
     rank  6: 2987.54
     rank  7: 2981.18
     rank  8: 2779.54
     rank  9: 2779.74
     rank 10: 2780.90
     rank 11: 2780.53
     rank 12: 2787.84
     rank 13: 2792.59
     rank 14: 2789.55
     rank 15: 2790.58
     rank 16: 2911.66
     rank 17: 2912.95
     rank 18: 2915.92
     rank 19: 2916.53
  backward-compute:
     rank  0: 1053.11
     rank  1: 1054.88
     rank  2: 1056.28
     rank  3: 1051.51
     rank  4: 3077.93
     rank  5: 3076.63
     rank  6: 3079.12
     rank  7: 3077.47
     rank  8: 2977.51
     rank  9: 2982.90
     rank 10: 2981.70
     rank 11: 2976.57
     rank 12: 2973.65
     rank 13: 2973.21
     rank 14: 2975.50
     rank 15: 2976.05
     rank 16: 3216.00
     rank 17: 3215.12
     rank 18: 3217.83
     rank 19: 3217.78
  pure-backward-compute:
     rank  0: 1052.47
     rank  1: 1054.22
     rank  2: 1055.66
     rank  3: 1050.87
     rank  4: 3076.99
     rank  5: 3075.92
     rank  6: 3078.22
     rank  7: 3075.79
     rank  8: 2976.41
     rank  9: 2981.50
     rank 10: 2980.61
     rank 11: 2975.79
     rank 12: 2972.43
     rank 13: 2972.37
     rank 14: 2974.84
     rank 15: 2974.56
     rank 16: 3213.73
     rank 17: 3212.82
     rank 18: 3216.36
     rank 19: 3216.27
  batch-generator:
     rank  0: 51.59
     rank  1: 54.47
     rank  2: 55.49
     rank  3: 54.90
     rank  4: 55.71
     rank  5: 60.85
     rank  6: 66.61
     rank  7: 74.41
     rank  8: 50.51
     rank  9: 52.41
     rank 10: 54.37
     rank 11: 55.97
     rank 12: 56.49
     rank 13: 61.38
     rank 14: 59.49
     rank 15: 60.34
     rank 16: 54.79
     rank 17: 59.39
     rank 18: 61.24
     rank 19: 62.10
  forward-recv:
     rank  4: 61.73
     rank  5: 61.36
     rank  6: 60.14
     rank  7: 61.49
     rank  8: 278.61
     rank  9: 278.42
     rank 10: 278.27
     rank 11: 280.85
     rank 12: 450.57
     rank 13: 451.20
     rank 14: 451.29
     rank 15: 448.74
     rank 16: 622.22
     rank 17: 621.99
     rank 18: 621.76
     rank 19: 622.22
  forward-send:
     rank  0: 417.96
     rank  1: 417.01
     rank  2: 415.31
     rank  3: 417.71
     rank  4: 31.52
     rank  5: 31.32
     rank  6: 30.61
     rank  7: 31.69
     rank  8: 20.76
     rank  9: 20.74
     rank 10: 20.55
     rank 11: 18.95
     rank 12: 10.51
     rank 13: 10.16
     rank 14: 9.92
     rank 15: 10.51
  backward-recv:
     rank  0: 1307.20
     rank  1: 1307.54
     rank  2: 1307.28
     rank  3: 1308.65
     rank  4: 598.98
     rank  5: 598.58
     rank  6: 597.83
     rank  7: 595.78
     rank  8: 399.14
     rank  9: 398.32
     rank 10: 399.30
     rank 11: 399.74
     rank 12: 193.67
     rank 13: 194.55
     rank 14: 194.27
     rank 15: 193.32
  backward-send:
     rank  4: 3.41
     rank  5: 3.63
     rank  6: 3.67
     rank  7: 5.45
     rank  8: 31.26
     rank  9: 31.05
     rank 10: 30.78
     rank 11: 30.85
     rank 12: 20.88
     rank 13: 19.95
     rank 14: 20.26
     rank 15: 20.89
     rank 16: 10.57
     rank 17: 10.49
     rank 18: 10.32
     rank 19: 10.12
  forward-send-backward-recv:
     rank  0: 3891.12
     rank  1: 3888.71
     rank  2: 3891.42
     rank  3: 3893.22
     rank  4: 738.66
     rank  5: 740.47
     rank  6: 738.13
     rank  7: 738.72
     rank  8: 685.21
     rank  9: 678.98
     rank 10: 683.37
     rank 11: 689.00
     rank 12: 533.03
     rank 13: 537.03
     rank 14: 535.92
     rank 15: 530.49
  backward-send-forward-recv:
     rank  4: 19.96
     rank  5: 18.03
     rank  6: 19.62
     rank  7: 20.77
     rank  8: 150.31
     rank  9: 151.35
     rank 10: 149.44
     rank 11: 148.42
     rank 12: 156.63
     rank 13: 151.86
     rank 14: 153.67
     rank 15: 156.93
     rank 16: 168.56
     rank 17: 168.42
     rank 18: 166.11
     rank 19: 165.01
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.06
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.10
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.14
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.54
     rank  2: 0.54
     rank  3: 0.54
     rank  4: 0.47
     rank  5: 0.42
     rank  6: 0.45
     rank  7: 0.58
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.59
     rank 17: 0.51
     rank 18: 0.51
     rank 19: 0.92
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.04
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-inner-step:
     rank  0: 8.26
     rank  1: 8.49
     rank  2: 8.52
     rank  3: 8.44
     rank  4: 5.34
     rank  5: 5.25
     rank  6: 5.27
     rank  7: 5.49
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 3.75
     rank 17: 3.67
     rank 18: 3.66
     rank 19: 4.37
  optimizer:
     rank  0: 9.42
     rank  1: 9.65
     rank  2: 9.67
     rank  3: 9.59
     rank  4: 6.49
     rank  5: 6.40
     rank  6: 6.42
     rank  7: 6.65
     rank  8: 1.18
     rank  9: 1.22
     rank 10: 1.21
     rank 11: 1.18
     rank 12: 1.20
     rank 13: 1.18
     rank 14: 1.19
     rank 15: 1.18
     rank 16: 4.90
     rank 17: 4.82
     rank 18: 4.81
     rank 19: 5.53
 [2024-12-02 22:29:07] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7629.9 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 2.032036E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7591.95
     rank  1: 7591.94
     rank  2: 7591.95
     rank  3: 7592.02
     rank  4: 7591.68
     rank  5: 7591.67
     rank  6: 7591.68
     rank  7: 7591.78
     rank  8: 7591.23
     rank  9: 7591.23
     rank 10: 7591.32
     rank 11: 7591.32
     rank 12: 7591.33
     rank 13: 7591.22
     rank 14: 7591.27
     rank 15: 7591.29
     rank 16: 7591.50
     rank 17: 7591.52
     rank 18: 7591.49
     rank 19: 7591.53
  forward-compute:
     rank  0: 912.67
     rank  1: 915.54
     rank  2: 916.86
     rank  3: 915.90
     rank  4: 2988.80
     rank  5: 2996.17
     rank  6: 2994.13
     rank  7: 2983.55
     rank  8: 2776.46
     rank  9: 2780.29
     rank 10: 2780.39
     rank 11: 2777.57
     rank 12: 2786.30
     rank 13: 2791.98
     rank 14: 2789.08
     rank 15: 2786.90
     rank 16: 2916.85
     rank 17: 2916.83
     rank 18: 2920.93
     rank 19: 2919.08
  backward-compute:
     rank  0: 1040.79
     rank  1: 1042.88
     rank  2: 1041.69
     rank  3: 1039.36
     rank  4: 3074.52
     rank  5: 3076.83
     rank  6: 3078.41
     rank  7: 3076.61
     rank  8: 2966.91
     rank  9: 2972.89
     rank 10: 2971.72
     rank 11: 2966.43
     rank 12: 2970.24
     rank 13: 2968.51
     rank 14: 2971.82
     rank 15: 2970.75
     rank 16: 3212.99
     rank 17: 3212.13
     rank 18: 3214.29
     rank 19: 3213.05
  pure-backward-compute:
     rank  0: 1040.08
     rank  1: 1042.25
     rank  2: 1041.03
     rank  3: 1038.56
     rank  4: 3073.53
     rank  5: 3076.16
     rank  6: 3077.46
     rank  7: 3074.91
     rank  8: 2965.73
     rank  9: 2971.43
     rank 10: 2970.78
     rank 11: 2965.67
     rank 12: 2968.61
     rank 13: 2967.83
     rank 14: 2970.99
     rank 15: 2969.12
     rank 16: 3210.84
     rank 17: 3209.72
     rank 18: 3212.67
     rank 19: 3210.60
  batch-generator:
     rank  0: 51.51
     rank  1: 55.86
     rank  2: 59.61
     rank  3: 56.37
     rank  4: 63.16
     rank  5: 65.65
     rank  6: 75.99
     rank  7: 77.83
     rank  8: 56.88
     rank  9: 60.71
     rank 10: 62.11
     rank 11: 59.92
     rank 12: 64.72
     rank 13: 68.69
     rank 14: 66.98
     rank 15: 66.26
     rank 16: 52.96
     rank 17: 55.51
     rank 18: 58.72
     rank 19: 58.25
  forward-recv:
     rank  4: 61.45
     rank  5: 60.93
     rank  6: 59.78
     rank  7: 61.42
     rank  8: 278.79
     rank  9: 278.35
     rank 10: 277.82
     rank 11: 280.07
     rank 12: 449.55
     rank 13: 450.20
     rank 14: 450.43
     rank 15: 448.71
     rank 16: 623.48
     rank 17: 623.37
     rank 18: 623.00
     rank 19: 623.71
  forward-send:
     rank  0: 417.61
     rank  1: 416.04
     rank  2: 414.05
     rank  3: 417.14
     rank  4: 31.62
     rank  5: 31.21
     rank  6: 29.79
     rank  7: 31.68
     rank  8: 20.91
     rank  9: 21.06
     rank 10: 20.63
     rank 11: 19.73
     rank 12: 10.49
     rank 13: 10.24
     rank 14: 9.87
     rank 15: 10.50
  backward-recv:
     rank  0: 1304.70
     rank  1: 1305.68
     rank  2: 1305.47
     rank  3: 1303.88
     rank  4: 597.33
     rank  5: 597.24
     rank  6: 597.65
     rank  7: 596.26
     rank  8: 395.75
     rank  9: 395.09
     rank 10: 395.75
     rank 11: 396.71
     rank 12: 198.83
     rank 13: 199.16
     rank 14: 199.10
     rank 15: 199.08
  backward-send:
     rank  4: 4.26
     rank  5: 4.17
     rank  6: 3.87
     rank  7: 4.84
     rank  8: 31.25
     rank  9: 30.99
     rank 10: 31.25
     rank 11: 30.49
     rank 12: 20.99
     rank 13: 20.18
     rank 14: 20.63
     rank 15: 20.92
     rank 16: 10.53
     rank 17: 10.36
     rank 18: 10.26
     rank 19: 10.55
  forward-send-backward-recv:
     rank  0: 3902.07
     rank  1: 3898.27
     rank  2: 3902.24
     rank  3: 3902.73
     rank  4: 737.78
     rank  5: 735.93
     rank  6: 735.09
     rank  7: 734.58
     rank  8: 685.88
     rank  9: 680.11
     rank 10: 683.37
     rank 11: 689.12
     rank 12: 533.47
     rank 13: 539.28
     rank 14: 537.84
     rank 15: 534.35
  backward-send-forward-recv:
     rank  4: 19.83
     rank  5: 16.15
     rank  6: 18.85
     rank  7: 21.06
     rank  8: 165.96
     rank  9: 163.27
     rank 10: 163.30
     rank 11: 164.76
     rank 12: 156.21
     rank 13: 151.39
     rank 14: 152.79
     rank 15: 156.43
     rank 16: 168.40
     rank 17: 169.12
     rank 18: 165.59
     rank 19: 166.80
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.54
     rank  2: 0.59
     rank  3: 0.60
     rank  4: 0.42
     rank  5: 0.42
     rank  6: 0.46
     rank  7: 0.50
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.46
     rank 17: 0.54
     rank 18: 0.43
     rank 19: 0.62
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.52
     rank  1: 8.55
     rank  2: 8.48
     rank  3: 8.49
     rank  4: 5.26
     rank  5: 5.25
     rank  6: 5.31
     rank  7: 5.33
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.03
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 3.68
     rank 17: 3.72
     rank 18: 3.71
     rank 19: 3.80
  optimizer:
     rank  0: 9.25
     rank  1: 9.27
     rank  2: 9.20
     rank  3: 9.20
     rank  4: 5.98
     rank  5: 5.97
     rank  6: 6.03
     rank  7: 6.06
     rank  8: 0.75
     rank  9: 0.76
     rank 10: 0.79
     rank 11: 0.75
     rank 12: 0.81
     rank 13: 0.75
     rank 14: 0.79
     rank 15: 0.75
     rank 16: 4.40
     rank 17: 4.44
     rank 18: 4.43
     rank 19: 4.52
 [2024-12-02 22:29:15] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7635.9 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.780727E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7597.28
     rank  1: 7597.24
     rank  2: 7597.46
     rank  3: 7597.28
     rank  4: 7597.03
     rank  5: 7596.97
     rank  6: 7597.18
     rank  7: 7597.01
     rank  8: 7596.66
     rank  9: 7596.53
     rank 10: 7596.76
     rank 11: 7596.57
     rank 12: 7596.60
     rank 13: 7596.54
     rank 14: 7596.73
     rank 15: 7596.63
     rank 16: 7596.88
     rank 17: 7596.77
     rank 18: 7597.48
     rank 19: 7596.82
  forward-compute:
     rank  0: 948.68
     rank  1: 951.39
     rank  2: 953.25
     rank  3: 949.81
     rank  4: 2978.55
     rank  5: 2984.73
     rank  6: 2984.95
     rank  7: 2976.94
     rank  8: 2766.96
     rank  9: 2768.13
     rank 10: 2769.14
     rank 11: 2768.78
     rank 12: 2758.60
     rank 13: 2763.33
     rank 14: 2761.41
     rank 15: 2757.94
     rank 16: 2916.87
     rank 17: 2918.83
     rank 18: 2923.77
     rank 19: 2918.32
  backward-compute:
     rank  0: 1014.97
     rank  1: 1016.12
     rank  2: 1015.62
     rank  3: 1013.86
     rank  4: 3070.02
     rank  5: 3070.83
     rank  6: 3071.55
     rank  7: 3072.80
     rank  8: 2963.94
     rank  9: 2967.58
     rank 10: 2967.76
     rank 11: 2962.90
     rank 12: 2961.42
     rank 13: 2961.53
     rank 14: 2965.34
     rank 15: 2962.09
     rank 16: 3219.22
     rank 17: 3218.90
     rank 18: 3221.57
     rank 19: 3221.35
  pure-backward-compute:
     rank  0: 1014.25
     rank  1: 1015.47
     rank  2: 1014.98
     rank  3: 1013.12
     rank  4: 3069.06
     rank  5: 3070.18
     rank  6: 3070.74
     rank  7: 3071.70
     rank  8: 2962.77
     rank  9: 2966.21
     rank 10: 2966.74
     rank 11: 2962.11
     rank 12: 2960.31
     rank 13: 2960.82
     rank 14: 2964.60
     rank 15: 2960.49
     rank 16: 3216.74
     rank 17: 3216.70
     rank 18: 3219.97
     rank 19: 3219.19
  batch-generator:
     rank  0: 51.86
     rank  1: 56.22
     rank  2: 58.55
     rank  3: 57.83
     rank  4: 60.61
     rank  5: 62.82
     rank  6: 70.65
     rank  7: 71.67
     rank  8: 54.87
     rank  9: 57.48
     rank 10: 58.89
     rank 11: 60.03
     rank 12: 51.51
     rank 13: 57.37
     rank 14: 56.90
     rank 15: 55.68
     rank 16: 60.21
     rank 17: 62.80
     rank 18: 67.22
     rank 19: 62.53
  forward-recv:
     rank  4: 64.07
     rank  5: 63.48
     rank  6: 62.14
     rank  7: 64.04
     rank  8: 284.80
     rank  9: 284.62
     rank 10: 284.88
     rank 11: 286.35
     rank 12: 453.79
     rank 13: 454.17
     rank 14: 454.43
     rank 15: 452.94
     rank 16: 622.32
     rank 17: 622.22
     rank 18: 621.56
     rank 19: 622.35
  forward-send:
     rank  0: 421.04
     rank  1: 419.69
     rank  2: 418.10
     rank  3: 420.94
     rank  4: 31.73
     rank  5: 31.38
     rank  6: 30.43
     rank  7: 31.89
     rank  8: 21.00
     rank  9: 20.98
     rank 10: 20.30
     rank 11: 19.78
     rank 12: 10.43
     rank 13: 10.27
     rank 14: 9.62
     rank 15: 10.51
  backward-recv:
     rank  0: 1322.01
     rank  1: 1322.58
     rank  2: 1322.26
     rank  3: 1322.60
     rank  4: 602.71
     rank  5: 602.19
     rank  6: 602.88
     rank  7: 602.00
     rank  8: 398.17
     rank  9: 398.29
     rank 10: 398.67
     rank 11: 399.30
     rank 12: 197.34
     rank 13: 197.85
     rank 14: 197.43
     rank 15: 196.84
  backward-send:
     rank  4: 4.33
     rank  5: 4.48
     rank  6: 4.05
     rank  7: 4.44
     rank  8: 31.50
     rank  9: 30.79
     rank 10: 31.19
     rank 11: 30.65
     rank 12: 20.84
     rank 13: 20.41
     rank 14: 20.71
     rank 15: 20.99
     rank 16: 10.57
     rank 17: 10.52
     rank 18: 10.25
     rank 19: 10.90
  forward-send-backward-recv:
     rank  0: 3875.76
     rank  1: 3872.64
     rank  2: 3875.57
     rank  3: 3877.19
     rank  4: 747.53
     rank  5: 747.35
     rank  6: 747.05
     rank  7: 745.70
     rank  8: 708.84
     rank  9: 704.98
     rank 10: 707.44
     rank 11: 712.84
     rank 12: 573.36
     rank 13: 575.61
     rank 14: 573.74
     rank 15: 572.90
  backward-send-forward-recv:
     rank  4: 20.41
     rank  5: 18.04
     rank  6: 18.62
     rank  7: 20.58
     rank  8: 152.57
     rank  9: 152.28
     rank 10: 150.87
     rank 11: 149.97
     rank 12: 157.25
     rank 13: 153.29
     rank 14: 153.96
     rank 15: 157.72
     rank 16: 168.71
     rank 17: 167.87
     rank 18: 163.67
     rank 19: 167.29
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.16
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.59
     rank  1: 0.60
     rank  2: 0.58
     rank  3: 0.59
     rank  4: 0.46
     rank  5: 0.42
     rank  6: 0.43
     rank  7: 0.47
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.54
     rank 17: 0.52
     rank 18: 1.13
     rank 19: 0.57
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.22
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.45
     rank  1: 8.59
     rank  2: 8.61
     rank  3: 8.41
     rank  4: 5.32
     rank  5: 5.26
     rank  6: 5.27
     rank  7: 5.34
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 3.77
     rank 17: 3.68
     rank 18: 4.19
     rank 19: 3.84
  optimizer:
     rank  0: 9.66
     rank  1: 9.80
     rank  2: 9.81
     rank  3: 9.62
     rank  4: 6.53
     rank  5: 6.46
     rank  6: 6.47
     rank  7: 6.54
     rank  8: 1.25
     rank  9: 1.24
     rank 10: 1.27
     rank 11: 1.24
     rank 12: 1.24
     rank 13: 1.24
     rank 14: 1.23
     rank 15: 1.27
     rank 16: 4.98
     rank 17: 4.88
     rank 18: 5.39
     rank 19: 5.04
 [2024-12-02 22:29:22] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7616.7 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.398871E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7577.26
     rank  1: 7577.25
     rank  2: 7577.18
     rank  3: 7577.20
     rank  4: 7577.01
     rank  5: 7576.97
     rank  6: 7576.91
     rank  7: 7576.94
     rank  8: 7576.55
     rank  9: 7576.54
     rank 10: 7576.58
     rank 11: 7577.64
     rank 12: 7576.65
     rank 13: 7576.54
     rank 14: 7576.51
     rank 15: 7576.82
     rank 16: 7576.86
     rank 17: 7576.85
     rank 18: 7576.79
     rank 19: 7576.79
  forward-compute:
     rank  0: 907.15
     rank  1: 907.87
     rank  2: 909.59
     rank  3: 909.59
     rank  4: 2980.12
     rank  5: 2983.15
     rank  6: 2982.84
     rank  7: 2979.70
     rank  8: 2758.92
     rank  9: 2759.36
     rank 10: 2758.18
     rank 11: 2759.03
     rank 12: 2767.16
     rank 13: 2769.98
     rank 14: 2770.53
     rank 15: 2766.46
     rank 16: 2915.50
     rank 17: 2916.39
     rank 18: 2919.54
     rank 19: 2919.56
  backward-compute:
     rank  0: 990.47
     rank  1: 991.03
     rank  2: 991.29
     rank  3: 990.35
     rank  4: 3080.78
     rank  5: 3082.85
     rank  6: 3083.64
     rank  7: 3081.95
     rank  8: 2968.94
     rank  9: 2972.30
     rank 10: 2973.82
     rank 11: 2967.14
     rank 12: 2972.25
     rank 13: 2971.20
     rank 14: 2973.40
     rank 15: 2975.04
     rank 16: 3197.74
     rank 17: 3197.19
     rank 18: 3200.10
     rank 19: 3198.35
  pure-backward-compute:
     rank  0: 989.80
     rank  1: 990.26
     rank  2: 990.65
     rank  3: 989.73
     rank  4: 3079.93
     rank  5: 3082.07
     rank  6: 3082.82
     rank  7: 3080.92
     rank  8: 2967.73
     rank  9: 2970.95
     rank 10: 2972.83
     rank 11: 2966.35
     rank 12: 2971.03
     rank 13: 2970.47
     rank 14: 2972.75
     rank 15: 2973.31
     rank 16: 3195.22
     rank 17: 3194.84
     rank 18: 3198.28
     rank 19: 3196.19
  batch-generator:
     rank  0: 53.11
     rank  1: 55.59
     rank  2: 57.81
     rank  3: 58.54
     rank  4: 58.56
     rank  5: 60.80
     rank  6: 67.27
     rank  7: 69.99
     rank  8: 51.26
     rank  9: 55.30
     rank 10: 55.39
     rank 11: 55.68
     rank 12: 49.02
     rank 13: 54.03
     rank 14: 55.78
     rank 15: 52.94
     rank 16: 58.62
     rank 17: 60.62
     rank 18: 63.10
     rank 19: 64.74
  forward-recv:
     rank  4: 62.16
     rank  5: 62.27
     rank  6: 61.78
     rank  7: 61.50
     rank  8: 282.10
     rank  9: 282.12
     rank 10: 282.86
     rank 11: 282.60
     rank 12: 451.75
     rank 13: 451.96
     rank 14: 452.12
     rank 15: 451.05
     rank 16: 620.02
     rank 17: 619.94
     rank 18: 619.40
     rank 19: 620.11
  forward-send:
     rank  0: 419.37
     rank  1: 418.81
     rank  2: 417.51
     rank  3: 417.14
     rank  4: 31.62
     rank  5: 31.32
     rank  6: 30.96
     rank  7: 30.80
     rank  8: 21.10
     rank  9: 21.02
     rank 10: 20.39
     rank 11: 20.24
     rank 12: 10.44
     rank 13: 10.36
     rank 14: 9.79
     rank 15: 10.59
  backward-recv:
     rank  0: 1333.11
     rank  1: 1333.49
     rank  2: 1333.44
     rank  3: 1333.16
     rank  4: 603.16
     rank  5: 601.66
     rank  6: 602.33
     rank  7: 602.05
     rank  8: 396.29
     rank  9: 396.03
     rank 10: 396.14
     rank 11: 396.61
     rank 12: 198.83
     rank 13: 199.71
     rank 14: 199.41
     rank 15: 198.47
  backward-send:
     rank  4: 3.93
     rank  5: 4.61
     rank  6: 4.33
     rank  7: 4.27
     rank  8: 31.34
     rank  9: 30.80
     rank 10: 31.48
     rank 11: 30.97
     rank 12: 20.94
     rank 13: 19.94
     rank 14: 20.53
     rank 15: 20.95
     rank 16: 10.66
     rank 17: 10.52
     rank 18: 10.39
     rank 19: 10.16
  forward-send-backward-recv:
     rank  0: 3910.58
     rank  1: 3909.28
     rank  2: 3911.24
     rank  3: 3912.53
     rank  4: 716.04
     rank  5: 715.25
     rank  6: 714.71
     rank  7: 716.03
     rank  8: 662.40
     rank  9: 659.64
     rank 10: 660.88
     rank 11: 666.33
     rank 12: 529.78
     rank 13: 533.74
     rank 14: 533.01
     rank 15: 528.51
  backward-send-forward-recv:
     rank  4: 20.19
     rank  5: 19.32
     rank  6: 19.24
     rank  7: 20.82
     rank  8: 182.96
     rank  9: 182.33
     rank 10: 182.26
     rank 11: 183.22
     rank 12: 156.32
     rank 13: 153.88
     rank 14: 152.68
     rank 15: 156.25
     rank 16: 168.68
     rank 17: 168.67
     rank 18: 165.48
     rank 19: 165.99
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.05
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.56
     rank  2: 0.54
     rank  3: 0.54
     rank  4: 0.43
     rank  5: 0.42
     rank  6: 0.42
     rank  7: 0.44
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.30
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.47
     rank 17: 0.47
     rank 18: 0.48
     rank 19: 0.50
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.08
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.07
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.30
     rank  1: 8.56
     rank  2: 8.33
     rank  3: 8.29
     rank  4: 5.28
     rank  5: 5.25
     rank  6: 5.24
     rank  7: 5.28
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.58
     rank 12: 0.04
     rank 13: 0.06
     rank 14: 0.03
     rank 15: 0.05
     rank 16: 3.76
     rank 17: 3.78
     rank 18: 3.78
     rank 19: 3.79
  optimizer:
     rank  0: 9.64
     rank  1: 9.90
     rank  2: 9.67
     rank  3: 9.63
     rank  4: 6.62
     rank  5: 6.59
     rank  6: 6.58
     rank  7: 6.62
     rank  8: 1.40
     rank  9: 1.37
     rank 10: 1.37
     rank 11: 1.92
     rank 12: 1.38
     rank 13: 1.40
     rank 14: 1.37
     rank 15: 1.38
     rank 16: 5.10
     rank 17: 5.12
     rank 18: 5.12
     rank 19: 5.13
