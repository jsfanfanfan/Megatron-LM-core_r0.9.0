examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
4
[2024-12-05 19:47:10,429] torch.distributed.run: [WARNING] 
[2024-12-05 19:47:10,429] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 19:47:10,429] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 19:47:10,429] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2][rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 306229248
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 306229248
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 306229248
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 306229248
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (306229248 elements):
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.output_layer.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 136.31
     rank  1: 133.72
     rank  2: 137.66
     rank  3: 137.27
     rank  4: 52.63
     rank  5: 54.53
     rank  6: 47.14
     rank  7: 52.62
     rank  8: 47.23
     rank  9: 62.58
     rank 10: 44.31
     rank 11: 65.74
     rank 12: 58.01
     rank 13: 61.15
     rank 14: 60.02
     rank 15: 57.97
     rank 16: 60.37
     rank 17: 61.45
     rank 18: 60.09
     rank 19: 59.56
  train/valid/test-data-iterators-setup:
     rank  0: 1011.56
     rank  1: 1012.37
     rank  2: 1311.86
     rank  3: 1012.53
     rank  4: 1012.40
     rank  5: 1012.77
     rank  6: 1012.40
     rank  7: 1012.45
     rank  8: 1012.41
     rank  9: 1012.44
     rank 10: 1012.39
     rank 11: 1013.87
     rank 12: 1312.02
     rank 13: 1313.33
     rank 14: 1013.47
     rank 15: 1312.64
     rank 16: 1311.77
     rank 17: 1311.87
     rank 18: 1312.04
     rank 19: 1312.84
 [2024-12-05 19:47:56] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 20618.8 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.086742E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6664.0 | max reserved: 6664.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
times across ranks (ms):
  forward-backward:
     rank  0: 20539.89
     rank  1: 20540.17
     rank  2: 20539.89
     rank  3: 20540.11
     rank  4: 20541.12
     rank  5: 20541.02
     rank  6: 20540.96
     rank  7: 20541.04
     rank  8: 20542.40
     rank  9: 20542.27
     rank 10: 20542.16
     rank 11: 20542.16
     rank 12: 20542.03
     rank 13: 20541.87
     rank 14: 20541.85
     rank 15: 20541.80
     rank 16: 20541.63
     rank 17: 20541.61
     rank 18: 20541.46
     rank 19: 20541.39
  forward-compute:
     rank  0: 5858.26
     rank  1: 5844.71
     rank  2: 5852.39
     rank  3: 5856.64
     rank  4: 5359.44
     rank  5: 5350.88
     rank  6: 5356.73
     rank  7: 5361.96
     rank  8: 6381.84
     rank  9: 6373.91
     rank 10: 6377.39
     rank 11: 6384.29
     rank 12: 4868.46
     rank 13: 4859.23
     rank 14: 4863.13
     rank 15: 4872.08
     rank 16: 5006.69
     rank 17: 5014.04
     rank 18: 5008.34
     rank 19: 5024.44
  backward-compute:
     rank  0: 2131.27
     rank  1: 2134.87
     rank  2: 2130.99
     rank  3: 2131.81
     rank  4: 3703.94
     rank  5: 3703.74
     rank  6: 3708.55
     rank  7: 3702.57
     rank  8: 4513.12
     rank  9: 4511.63
     rank 10: 4524.07
     rank 11: 4515.98
     rank 12: 3918.55
     rank 13: 3923.75
     rank 14: 3913.53
     rank 15: 3927.87
     rank 16: 4333.72
     rank 17: 4331.64
     rank 18: 4326.14
     rank 19: 4329.00
  pure-backward-compute:
     rank  0: 2130.31
     rank  1: 2133.96
     rank  2: 2130.01
     rank  3: 2130.92
     rank  4: 3702.99
     rank  5: 3702.72
     rank  6: 3707.50
     rank  7: 3701.55
     rank  8: 4511.80
     rank  9: 4510.50
     rank 10: 4523.33
     rank 11: 4514.16
     rank 12: 3916.09
     rank 13: 3920.42
     rank 14: 3911.01
     rank 15: 3926.56
     rank 16: 4330.28
     rank 17: 4329.21
     rank 18: 4323.56
     rank 19: 4326.35
  batch-generator:
     rank  0: 1052.36
     rank  1: 1044.77
     rank  2: 1055.50
     rank  3: 1061.37
     rank  4: 1206.21
     rank  5: 1199.53
     rank  6: 1203.20
     rank  7: 1211.49
     rank  8: 1275.74
     rank  9: 1269.38
     rank 10: 1276.20
     rank 11: 1282.31
     rank 12: 1464.82
     rank 13: 1459.84
     rank 14: 1476.71
     rank 15: 1477.22
     rank 16: 1353.33
     rank 17: 1363.78
     rank 18: 1348.81
     rank 19: 1377.98
  forward-recv:
     rank  4: 2391.86
     rank  5: 2395.43
     rank  6: 2388.42
     rank  7: 2394.28
     rank  8: 4835.31
     rank  9: 4838.50
     rank 10: 4836.73
     rank 11: 4835.60
     rank 12: 7688.65
     rank 13: 7689.84
     rank 14: 7690.92
     rank 15: 7680.66
     rank 16: 9410.10
     rank 17: 9410.90
     rank 18: 9413.50
     rank 19: 9412.28
  forward-send:
     rank  0: 6475.12
     rank  1: 6487.48
     rank  2: 6479.27
     rank  3: 6475.21
     rank  4: 4220.69
     rank  5: 4228.13
     rank  6: 4227.29
     rank  7: 4217.11
     rank  8: 1520.10
     rank  9: 1524.18
     rank 10: 1527.09
     rank 11: 1515.69
     rank 12: 30.27
     rank 13: 32.19
     rank 14: 34.70
     rank 15: 33.20
  backward-recv:
     rank  0: 1551.27
     rank  1: 1550.96
     rank  2: 1550.88
     rank  3: 1550.95
     rank  4: 793.08
     rank  5: 795.28
     rank  6: 792.58
     rank  7: 794.89
     rank  8: 395.26
     rank  9: 394.95
     rank 10: 394.61
     rank 11: 393.83
     rank 12: 245.77
     rank 13: 245.96
     rank 14: 245.42
     rank 15: 246.28
  backward-send:
     rank  4: 51.65
     rank  5: 49.85
     rank  6: 51.83
     rank  7: 50.55
     rank  8: 31.18
     rank  9: 31.08
     rank 10: 30.63
     rank 11: 31.41
     rank 12: 21.17
     rank 13: 21.21
     rank 14: 20.27
     rank 15: 20.82
     rank 16: 10.41
     rank 17: 10.30
     rank 18: 9.40
     rank 19: 10.31
  forward-send-backward-recv:
     rank  0: 4450.73
     rank  1: 4449.32
     rank  2: 4452.59
     rank  3: 4452.30
     rank  4: 3553.05
     rank  5: 3554.78
     rank  6: 3548.46
     rank  7: 3554.25
     rank  8: 2259.09
     rank  9: 2262.49
     rank 10: 2254.04
     rank 11: 2259.12
     rank 12: 1962.11
     rank 13: 1959.82
     rank 14: 1964.86
     rank 15: 1953.50
  backward-send-forward-recv:
     rank  4: 260.44
     rank  5: 257.39
     rank  6: 260.78
     rank  7: 259.76
     rank  8: 153.76
     rank  9: 152.61
     rank 10: 149.63
     rank 11: 154.14
     rank 12: 1064.99
     rank 13: 1068.05
     rank 14: 1062.28
     rank 15: 1066.77
     rank 16: 784.44
     rank 17: 779.33
     rank 18: 787.71
     rank 19: 768.70
  layernorm-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.03
     rank  3: 0.05
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 0.03
     rank 17: 0.05
     rank 18: 0.03
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.14
     rank  2: 0.08
     rank  3: 0.11
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.07
     rank 13: 0.06
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.11
     rank 17: 0.11
     rank 18: 0.07
     rank 19: 0.07
  all-grads-sync:
     rank  0: 51.08
     rank  1: 60.02
     rank  2: 59.77
     rank  3: 57.87
     rank  4: 44.89
     rank  5: 36.92
     rank  6: 42.18
     rank  7: 43.61
     rank  8: 45.77
     rank  9: 38.24
     rank 10: 45.57
     rank 11: 41.15
     rank 12: 56.16
     rank 13: 57.68
     rank 14: 60.78
     rank 15: 60.35
     rank 16: 23.01
     rank 17: 25.69
     rank 18: 25.21
     rank 19: 23.68
  optimizer-copy-to-main-grad:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.06
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.06
     rank  7: 0.07
     rank  8: 0.07
     rank  9: 0.11
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.09
     rank 13: 0.09
     rank 14: 0.11
     rank 15: 0.09
     rank 16: 0.08
     rank 17: 0.14
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 48.20
     rank  1: 49.38
     rank  2: 49.38
     rank  3: 49.32
     rank  4: 57.42
     rank  5: 57.77
     rank  6: 57.46
     rank  7: 57.90
     rank  8: 70.81
     rank  9: 70.84
     rank 10: 70.94
     rank 11: 70.79
     rank 12: 63.00
     rank 13: 64.27
     rank 14: 68.62
     rank 15: 63.79
     rank 16: 67.72
     rank 17: 68.59
     rank 18: 68.43
     rank 19: 67.33
  optimizer:
     rank  0: 49.59
     rank  1: 50.76
     rank  2: 50.77
     rank  3: 50.70
     rank  4: 58.80
     rank  5: 59.15
     rank  6: 58.85
     rank  7: 59.28
     rank  8: 72.21
     rank  9: 72.22
     rank 10: 72.32
     rank 11: 72.18
     rank 12: 64.39
     rank 13: 65.64
     rank 14: 70.01
     rank 15: 65.17
     rank 16: 69.11
     rank 17: 69.96
     rank 18: 69.83
     rank 19: 68.71
 [2024-12-05 19:48:07] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 10558.9 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.071754E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10460.65
     rank  1: 10460.64
     rank  2: 10460.66
     rank  3: 10460.70
     rank  4: 10462.03
     rank  5: 10462.06
     rank  6: 10462.06
     rank  7: 10462.15
     rank  8: 10463.23
     rank  9: 10463.18
     rank 10: 10463.25
     rank 11: 10463.31
     rank 12: 10461.70
     rank 13: 10461.66
     rank 14: 10461.73
     rank 15: 10461.77
     rank 16: 10462.17
     rank 17: 10462.13
     rank 18: 10462.14
     rank 19: 10462.34
  forward-compute:
     rank  0: 3880.48
     rank  1: 3881.35
     rank  2: 3878.19
     rank  3: 3881.58
     rank  4: 3118.80
     rank  5: 3118.83
     rank  6: 3121.62
     rank  7: 3120.80
     rank  8: 3756.31
     rank  9: 3756.52
     rank 10: 3761.01
     rank 11: 3756.70
     rank 12: 3370.51
     rank 13: 3369.47
     rank 14: 3371.82
     rank 15: 3370.21
     rank 16: 3506.61
     rank 17: 3511.65
     rank 18: 3519.36
     rank 19: 3523.25
  backward-compute:
     rank  0: 2077.32
     rank  1: 2078.50
     rank  2: 2081.64
     rank  3: 2075.18
     rank  4: 3712.04
     rank  5: 3711.86
     rank  6: 3713.13
     rank  7: 3713.26
     rank  8: 4480.10
     rank  9: 4480.71
     rank 10: 4490.08
     rank 11: 4482.89
     rank 12: 3907.97
     rank 13: 3908.11
     rank 14: 3908.52
     rank 15: 3913.92
     rank 16: 4357.80
     rank 17: 4356.30
     rank 18: 4349.34
     rank 19: 4352.08
  pure-backward-compute:
     rank  0: 2076.43
     rank  1: 2077.66
     rank  2: 2080.64
     rank  3: 2074.33
     rank  4: 3711.18
     rank  5: 3710.70
     rank  6: 3712.26
     rank  7: 3712.23
     rank  8: 4479.00
     rank  9: 4479.49
     rank 10: 4489.35
     rank 11: 4481.57
     rank 12: 3906.33
     rank 13: 3906.83
     rank 14: 3906.71
     rank 15: 3912.60
     rank 16: 4354.83
     rank 17: 4354.18
     rank 18: 4346.67
     rank 19: 4349.60
  batch-generator:
     rank  0: 76.03
     rank  1: 82.20
     rank  2: 81.14
     rank  3: 86.56
     rank  4: 54.16
     rank  5: 56.61
     rank  6: 58.59
     rank  7: 57.34
     rank  8: 49.61
     rank  9: 53.60
     rank 10: 57.78
     rank 11: 53.55
     rank 12: 71.62
     rank 13: 74.33
     rank 14: 85.59
     rank 15: 80.13
     rank 16: 72.14
     rank 17: 78.47
     rank 18: 87.41
     rank 19: 93.63
  forward-recv:
     rank  4: 401.71
     rank  5: 402.08
     rank  6: 405.87
     rank  7: 400.22
     rank  8: 478.34
     rank  9: 477.79
     rank 10: 477.02
     rank 11: 478.54
     rank 12: 739.74
     rank 13: 740.18
     rank 14: 738.03
     rank 15: 740.55
     rank 16: 931.39
     rank 17: 931.16
     rank 18: 931.52
     rank 19: 930.44
  forward-send:
     rank  0: 42.87
     rank  1: 41.87
     rank  2: 42.89
     rank  3: 40.03
     rank  4: 36.18
     rank  5: 35.35
     rank  6: 31.79
     rank  7: 34.88
     rank  8: 22.96
     rank  9: 22.79
     rank 10: 21.07
     rank 11: 22.34
     rank 12: 10.64
     rank 13: 10.26
     rank 14: 10.91
     rank 15: 9.73
  backward-recv:
     rank  0: 1552.66
     rank  1: 1552.20
     rank  2: 1551.64
     rank  3: 1553.25
     rank  4: 790.86
     rank  5: 792.20
     rank  6: 791.99
     rank  7: 790.56
     rank  8: 406.34
     rank  9: 406.17
     rank 10: 404.98
     rank 11: 406.26
     rank 12: 248.05
     rank 13: 248.31
     rank 14: 247.82
     rank 15: 247.90
  backward-send:
     rank  4: 53.47
     rank  5: 51.79
     rank  6: 52.08
     rank  7: 53.46
     rank  8: 31.22
     rank  9: 31.24
     rank 10: 30.68
     rank 11: 31.12
     rank 12: 21.24
     rank 13: 21.04
     rank 14: 19.87
     rank 15: 21.14
     rank 16: 10.63
     rank 17: 10.51
     rank 18: 9.62
     rank 19: 10.55
  forward-send-backward-recv:
     rank  0: 2893.48
     rank  1: 2894.66
     rank  2: 2892.21
     rank  3: 2898.37
     rank  4: 1946.18
     rank  5: 1948.61
     rank  6: 1944.67
     rank  7: 1946.94
     rank  8: 747.84
     rank  9: 748.12
     rank 10: 743.34
     rank 11: 747.21
     rank 12: 481.56
     rank 13: 481.72
     rank 14: 479.57
     rank 15: 476.68
  backward-send-forward-recv:
     rank  4: 254.40
     rank  5: 254.36
     rank  6: 252.64
     rank  7: 255.54
     rank  8: 144.87
     rank  9: 145.09
     rank 10: 143.09
     rank 11: 144.83
     rank 12: 999.63
     rank 13: 1000.98
     rank 14: 998.38
     rank 15: 1000.23
     rank 16: 716.26
     rank 17: 714.72
     rank 18: 712.05
     rank 19: 705.96
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.08
     rank  2: 0.10
     rank  3: 0.09
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.08
  all-grads-sync:
     rank  0: 1.68
     rank  1: 1.68
     rank  2: 1.69
     rank  3: 1.74
     rank  4: 2.41
     rank  5: 2.56
     rank  6: 2.39
     rank  7: 2.56
     rank  8: 2.96
     rank  9: 2.98
     rank 10: 2.88
     rank 11: 2.97
     rank 12: 2.18
     rank 13: 2.23
     rank 14: 2.37
     rank 15: 2.19
     rank 16: 2.41
     rank 17: 2.40
     rank 18: 2.49
     rank 19: 2.50
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.07
     rank  6: 0.04
     rank  7: 0.06
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.10
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.09
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.77
     rank  1: 30.77
     rank  2: 30.81
     rank  3: 30.91
     rank  4: 48.32
     rank  5: 48.41
     rank  6: 48.37
     rank  7: 48.50
     rank  8: 59.10
     rank  9: 59.11
     rank 10: 59.00
     rank 11: 59.10
     rank 12: 42.69
     rank 13: 43.47
     rank 14: 43.38
     rank 15: 42.94
     rank 16: 48.61
     rank 17: 48.56
     rank 18: 48.66
     rank 19: 48.69
  optimizer:
     rank  0: 31.47
     rank  1: 31.47
     rank  2: 31.50
     rank  3: 31.69
     rank  4: 49.01
     rank  5: 49.11
     rank  6: 49.07
     rank  7: 49.22
     rank  8: 59.79
     rank  9: 59.81
     rank 10: 59.69
     rank 11: 59.80
     rank 12: 43.39
     rank 13: 44.16
     rank 14: 44.08
     rank 15: 43.63
     rank 16: 49.31
     rank 17: 49.25
     rank 18: 49.36
     rank 19: 49.39
 [2024-12-05 19:48:17] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 10562.9 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 3.144663E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10469.32
     rank  1: 10469.19
     rank  2: 10469.25
     rank  3: 10469.33
     rank  4: 10470.74
     rank  5: 10470.68
     rank  6: 10470.65
     rank  7: 10470.66
     rank  8: 10471.94
     rank  9: 10471.81
     rank 10: 10471.76
     rank 11: 10471.81
     rank 12: 10470.39
     rank 13: 10470.29
     rank 14: 10470.27
     rank 15: 10470.25
     rank 16: 10470.98
     rank 17: 10470.74
     rank 18: 10470.84
     rank 19: 10470.71
  forward-compute:
     rank  0: 3890.36
     rank  1: 3892.10
     rank  2: 3888.44
     rank  3: 3891.13
     rank  4: 3116.30
     rank  5: 3120.89
     rank  6: 3118.26
     rank  7: 3119.43
     rank  8: 3754.62
     rank  9: 3758.47
     rank 10: 3756.42
     rank 11: 3755.12
     rank 12: 3383.36
     rank 13: 3385.34
     rank 14: 3386.24
     rank 15: 3386.91
     rank 16: 3534.20
     rank 17: 3541.71
     rank 18: 3546.01
     rank 19: 3554.83
  backward-compute:
     rank  0: 2079.91
     rank  1: 2082.37
     rank  2: 2084.07
     rank  3: 2079.30
     rank  4: 3691.68
     rank  5: 3692.91
     rank  6: 3693.38
     rank  7: 3692.56
     rank  8: 4485.68
     rank  9: 4488.30
     rank 10: 4497.62
     rank 11: 4492.46
     rank 12: 3925.58
     rank 13: 3927.49
     rank 14: 3924.95
     rank 15: 3933.72
     rank 16: 4379.98
     rank 17: 4382.92
     rank 18: 4376.01
     rank 19: 4377.81
  pure-backward-compute:
     rank  0: 2079.01
     rank  1: 2081.56
     rank  2: 2083.09
     rank  3: 2078.45
     rank  4: 3690.82
     rank  5: 3691.98
     rank  6: 3692.42
     rank  7: 3691.42
     rank  8: 4484.50
     rank  9: 4487.31
     rank 10: 4496.92
     rank 11: 4491.16
     rank 12: 3923.96
     rank 13: 3925.92
     rank 14: 3922.93
     rank 15: 3932.35
     rank 16: 4376.53
     rank 17: 4380.39
     rank 18: 4373.43
     rank 19: 4375.22
  batch-generator:
     rank  0: 75.68
     rank  1: 81.94
     rank  2: 82.49
     rank  3: 88.71
     rank  4: 53.32
     rank  5: 61.39
     rank  6: 57.97
     rank  7: 58.37
     rank  8: 54.15
     rank  9: 60.52
     rank 10: 58.40
     rank 11: 57.16
     rank 12: 71.76
     rank 13: 77.51
     rank 14: 86.82
     rank 15: 84.35
     rank 16: 83.07
     rank 17: 89.53
     rank 18: 94.91
     rank 19: 105.89
  forward-recv:
     rank  4: 403.47
     rank  5: 405.33
     rank  6: 403.30
     rank  7: 401.70
     rank  8: 485.39
     rank  9: 483.46
     rank 10: 486.33
     rank 11: 486.74
     rank 12: 741.81
     rank 13: 742.20
     rank 14: 741.22
     rank 15: 742.25
     rank 16: 929.37
     rank 17: 929.33
     rank 18: 929.06
     rank 19: 928.15
  forward-send:
     rank  0: 42.90
     rank  1: 41.55
     rank  2: 42.66
     rank  3: 40.67
     rank  4: 31.34
     rank  5: 28.83
     rank  6: 31.26
     rank  7: 30.91
     rank  8: 22.24
     rank  9: 21.99
     rank 10: 21.50
     rank 11: 21.12
     rank 12: 10.79
     rank 13: 10.41
     rank 14: 10.85
     rank 15: 9.88
  backward-recv:
     rank  0: 1560.58
     rank  1: 1560.31
     rank  2: 1559.93
     rank  3: 1560.25
     rank  4: 807.70
     rank  5: 809.18
     rank  6: 807.16
     rank  7: 809.12
     rank  8: 409.38
     rank  9: 408.99
     rank 10: 408.54
     rank 11: 409.67
     rank 12: 252.28
     rank 13: 252.44
     rank 14: 251.84
     rank 15: 252.60
  backward-send:
     rank  4: 48.96
     rank  5: 47.34
     rank  6: 48.27
     rank  7: 47.81
     rank  8: 31.36
     rank  9: 30.74
     rank 10: 30.93
     rank 11: 31.00
     rank 12: 21.51
     rank 13: 21.25
     rank 14: 20.38
     rank 15: 21.16
     rank 16: 10.70
     rank 17: 10.39
     rank 18: 9.42
     rank 19: 10.45
  forward-send-backward-recv:
     rank  0: 2881.57
     rank  1: 2880.52
     rank  2: 2879.99
     rank  3: 2885.31
     rank  4: 1967.62
     rank  5: 1969.21
     rank  6: 1966.03
     rank  7: 1968.25
     rank  8: 738.78
     rank  9: 736.49
     rank 10: 732.20
     rank 11: 735.49
     rank 12: 479.75
     rank 13: 480.26
     rank 14: 479.58
     rank 15: 474.11
  backward-send-forward-recv:
     rank  4: 254.36
     rank  5: 249.97
     rank  6: 252.77
     rank  7: 253.92
     rank  8: 145.45
     rank  9: 144.31
     rank 10: 143.45
     rank 11: 144.31
     rank 12: 969.95
     rank 13: 967.85
     rank 14: 966.17
     rank 15: 966.64
     rank 16: 670.86
     rank 17: 666.02
     rank 18: 667.91
     rank 19: 656.66
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.07
     rank  2: 0.08
     rank  3: 0.10
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.72
     rank  1: 1.68
     rank  2: 1.71
     rank  3: 1.77
     rank  4: 2.44
     rank  5: 2.40
     rank  6: 2.40
     rank  7: 2.43
     rank  8: 3.05
     rank  9: 3.06
     rank 10: 2.90
     rank 11: 2.97
     rank 12: 2.21
     rank 13: 2.19
     rank 14: 2.24
     rank 15: 2.14
     rank 16: 2.63
     rank 17: 2.42
     rank 18: 2.60
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.07
     rank  9: 0.08
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.10
     rank 17: 0.05
     rank 18: 0.10
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.81
     rank  1: 30.79
     rank  2: 30.81
     rank  3: 30.87
     rank  4: 48.40
     rank  5: 48.33
     rank  6: 48.38
     rank  7: 48.37
     rank  8: 59.13
     rank  9: 59.16
     rank 10: 59.02
     rank 11: 59.13
     rank 12: 42.83
     rank 13: 43.32
     rank 14: 43.19
     rank 15: 42.69
     rank 16: 48.79
     rank 17: 48.54
     rank 18: 48.60
     rank 19: 48.56
  optimizer:
     rank  0: 31.54
     rank  1: 31.51
     rank  2: 31.54
     rank  3: 31.60
     rank  4: 49.12
     rank  5: 49.05
     rank  6: 49.10
     rank  7: 49.10
     rank  8: 59.91
     rank  9: 59.88
     rank 10: 59.74
     rank 11: 59.86
     rank 12: 43.55
     rank 13: 44.04
     rank 14: 43.92
     rank 15: 43.41
     rank 16: 49.52
     rank 17: 49.26
     rank 18: 49.33
     rank 19: 49.29
 [2024-12-05 19:48:28] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 10586.7 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.727484E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10485.06
     rank  1: 10485.00
     rank  2: 10485.16
     rank  3: 10485.12
     rank  4: 10486.47
     rank  5: 10486.46
     rank  6: 10486.52
     rank  7: 10486.55
     rank  8: 10487.59
     rank  9: 10487.60
     rank 10: 10487.57
     rank 11: 10487.68
     rank 12: 10486.12
     rank 13: 10486.04
     rank 14: 10486.08
     rank 15: 10486.15
     rank 16: 10486.53
     rank 17: 10486.48
     rank 18: 10486.54
     rank 19: 10486.71
  forward-compute:
     rank  0: 3887.88
     rank  1: 3889.24
     rank  2: 3886.05
     rank  3: 3887.99
     rank  4: 3101.37
     rank  5: 3106.36
     rank  6: 3103.81
     rank  7: 3106.92
     rank  8: 3761.33
     rank  9: 3763.37
     rank 10: 3763.23
     rank 11: 3762.03
     rank 12: 3395.93
     rank 13: 3394.68
     rank 14: 3395.69
     rank 15: 3397.12
     rank 16: 3542.15
     rank 17: 3550.56
     rank 18: 3554.55
     rank 19: 3562.01
  backward-compute:
     rank  0: 2083.29
     rank  1: 2084.89
     rank  2: 2086.49
     rank  3: 2085.52
     rank  4: 3687.82
     rank  5: 3688.85
     rank  6: 3689.92
     rank  7: 3687.33
     rank  8: 4507.07
     rank  9: 4506.95
     rank 10: 4515.65
     rank 11: 4512.94
     rank 12: 3930.83
     rank 13: 3933.72
     rank 14: 3930.11
     rank 15: 3938.47
     rank 16: 4425.66
     rank 17: 4428.60
     rank 18: 4421.54
     rank 19: 4423.35
  pure-backward-compute:
     rank  0: 2082.40
     rank  1: 2084.06
     rank  2: 2085.47
     rank  3: 2084.55
     rank  4: 3686.96
     rank  5: 3688.05
     rank  6: 3688.78
     rank  7: 3686.42
     rank  8: 4506.07
     rank  9: 4506.03
     rank 10: 4514.92
     rank 11: 4511.77
     rank 12: 3929.20
     rank 13: 3932.09
     rank 14: 3928.35
     rank 15: 3937.35
     rank 16: 4422.78
     rank 17: 4426.40
     rank 18: 4419.43
     rank 19: 4420.69
  batch-generator:
     rank  0: 75.22
     rank  1: 81.57
     rank  2: 81.96
     rank  3: 85.04
     rank  4: 51.91
     rank  5: 58.64
     rank  6: 55.69
     rank  7: 59.74
     rank  8: 54.31
     rank  9: 59.09
     rank 10: 58.66
     rank 11: 57.55
     rank 12: 71.84
     rank 13: 74.39
     rank 14: 83.81
     rank 15: 82.13
     rank 16: 78.89
     rank 17: 85.85
     rank 18: 91.74
     rank 19: 101.34
  forward-recv:
     rank  4: 409.96
     rank  5: 410.45
     rank  6: 410.04
     rank  7: 408.33
     rank  8: 478.40
     rank  9: 477.82
     rank 10: 479.77
     rank 11: 479.51
     rank 12: 735.57
     rank 13: 735.63
     rank 14: 735.31
     rank 15: 734.87
     rank 16: 925.87
     rank 17: 925.48
     rank 18: 924.97
     rank 19: 925.31
  forward-send:
     rank  0: 43.01
     rank  1: 41.79
     rank  2: 42.80
     rank  3: 40.23
     rank  4: 31.88
     rank  5: 30.61
     rank  6: 31.28
     rank  7: 30.67
     rank  8: 21.72
     rank  9: 21.66
     rank 10: 20.71
     rank 11: 20.56
     rank 12: 10.84
     rank 13: 10.60
     rank 14: 10.36
     rank 15: 10.60
  backward-recv:
     rank  0: 1557.48
     rank  1: 1557.14
     rank  2: 1557.33
     rank  3: 1557.50
     rank  4: 813.57
     rank  5: 814.74
     rank  6: 813.40
     rank  7: 814.16
     rank  8: 409.42
     rank  9: 409.83
     rank 10: 408.60
     rank 11: 409.32
     rank 12: 254.44
     rank 13: 254.43
     rank 14: 253.95
     rank 15: 254.69
  backward-send:
     rank  4: 45.48
     rank  5: 43.75
     rank  6: 45.49
     rank  7: 44.44
     rank  8: 31.36
     rank  9: 30.81
     rank 10: 31.00
     rank 11: 31.00
     rank 12: 21.46
     rank 13: 21.31
     rank 14: 20.30
     rank 15: 21.04
     rank 16: 10.53
     rank 17: 10.44
     rank 18: 9.56
     rank 19: 10.48
  forward-send-backward-recv:
     rank  0: 2899.81
     rank  1: 2899.76
     rank  2: 2898.35
     rank  3: 2901.38
     rank  4: 1996.12
     rank  5: 1996.58
     rank  6: 1993.41
     rank  7: 1997.44
     rank  8: 740.02
     rank  9: 740.53
     rank 10: 736.30
     rank 11: 737.69
     rank 12: 485.95
     rank 13: 485.68
     rank 14: 485.73
     rank 15: 480.62
  backward-send-forward-recv:
     rank  4: 251.80
     rank  5: 248.17
     rank  6: 251.29
     rank  7: 250.12
     rank  8: 145.51
     rank  9: 144.43
     rank 10: 143.67
     rank 11: 144.46
     rank 12: 968.54
     rank 13: 969.67
     rank 14: 968.68
     rank 15: 968.39
     rank 16: 639.82
     rank 17: 634.16
     rank 18: 637.05
     rank 19: 625.03
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.04
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.05
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.08
     rank  2: 0.09
     rank  3: 0.08
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.12
  all-grads-sync:
     rank  0: 1.69
     rank  1: 1.69
     rank  2: 1.78
     rank  3: 1.75
     rank  4: 2.43
     rank  5: 2.55
     rank  6: 2.54
     rank  7: 2.56
     rank  8: 2.95
     rank  9: 2.91
     rank 10: 2.89
     rank 11: 2.97
     rank 12: 2.22
     rank 13: 2.15
     rank 14: 2.14
     rank 15: 2.17
     rank 16: 2.41
     rank 17: 2.37
     rank 18: 2.47
     rank 19: 2.69
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.09
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.76
     rank  1: 30.78
     rank  2: 30.79
     rank  3: 30.89
     rank  4: 48.37
     rank  5: 48.52
     rank  6: 48.59
     rank  7: 48.52
     rank  8: 59.11
     rank  9: 59.08
     rank 10: 59.03
     rank 11: 59.13
     rank 12: 42.85
     rank 13: 43.25
     rank 14: 43.10
     rank 15: 42.91
     rank 16: 48.67
     rank 17: 48.48
     rank 18: 48.54
     rank 19: 48.73
  optimizer:
     rank  0: 31.44
     rank  1: 31.45
     rank  2: 31.47
     rank  3: 31.57
     rank  4: 49.04
     rank  5: 49.19
     rank  6: 49.31
     rank  7: 49.20
     rank  8: 59.78
     rank  9: 59.75
     rank 10: 59.70
     rank 11: 59.80
     rank 12: 43.52
     rank 13: 43.93
     rank 14: 43.77
     rank 15: 43.58
     rank 16: 49.34
     rank 17: 49.16
     rank 18: 49.21
     rank 19: 49.41
 [2024-12-05 19:48:38] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 10583.5 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.880755E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10491.32
     rank  1: 10491.13
     rank  2: 10491.03
     rank  3: 10491.17
     rank  4: 10492.74
     rank  5: 10492.59
     rank  6: 10492.51
     rank  7: 10492.54
     rank  8: 10493.88
     rank  9: 10493.74
     rank 10: 10493.66
     rank 11: 10493.71
     rank 12: 10492.41
     rank 13: 10492.20
     rank 14: 10492.17
     rank 15: 10492.19
     rank 16: 10492.91
     rank 17: 10492.68
     rank 18: 10492.67
     rank 19: 10492.62
  forward-compute:
     rank  0: 3888.55
     rank  1: 3890.35
     rank  2: 3886.33
     rank  3: 3889.62
     rank  4: 3100.25
     rank  5: 3103.59
     rank  6: 3104.11
     rank  7: 3104.77
     rank  8: 3791.62
     rank  9: 3792.25
     rank 10: 3797.66
     rank 11: 3793.35
     rank 12: 3405.63
     rank 13: 3404.70
     rank 14: 3406.78
     rank 15: 3405.06
     rank 16: 3544.03
     rank 17: 3549.88
     rank 18: 3554.10
     rank 19: 3562.53
  backward-compute:
     rank  0: 2085.22
     rank  1: 2087.82
     rank  2: 2089.33
     rank  3: 2087.66
     rank  4: 3672.66
     rank  5: 3673.24
     rank  6: 3673.77
     rank  7: 3672.82
     rank  8: 4495.66
     rank  9: 4497.70
     rank 10: 4505.56
     rank 11: 4501.52
     rank 12: 3944.85
     rank 13: 3945.24
     rank 14: 3942.29
     rank 15: 3950.63
     rank 16: 4452.59
     rank 17: 4454.65
     rank 18: 4447.39
     rank 19: 4449.64
  pure-backward-compute:
     rank  0: 2084.36
     rank  1: 2087.00
     rank  2: 2088.34
     rank  3: 2086.81
     rank  4: 3671.70
     rank  5: 3672.45
     rank  6: 3672.92
     rank  7: 3671.95
     rank  8: 4494.79
     rank  9: 4496.82
     rank 10: 4504.83
     rank 11: 4500.35
     rank 12: 3943.32
     rank 13: 3943.78
     rank 14: 3940.51
     rank 15: 3949.56
     rank 16: 4449.35
     rank 17: 4452.71
     rank 18: 4445.26
     rank 19: 4446.98
  batch-generator:
     rank  0: 75.88
     rank  1: 82.62
     rank  2: 81.79
     rank  3: 86.26
     rank  4: 57.60
     rank  5: 62.88
     rank  6: 62.85
     rank  7: 64.25
     rank  8: 54.53
     rank  9: 57.13
     rank 10: 62.83
     rank 11: 56.78
     rank 12: 71.87
     rank 13: 75.09
     rank 14: 85.90
     rank 15: 80.38
     rank 16: 74.22
     rank 17: 80.54
     rank 18: 86.01
     rank 19: 97.06
  forward-recv:
     rank  4: 390.32
     rank  5: 390.39
     rank  6: 396.26
     rank  7: 388.62
     rank  8: 478.03
     rank  9: 477.75
     rank 10: 474.50
     rank 11: 477.84
     rank 12: 751.98
     rank 13: 752.09
     rank 14: 751.19
     rank 15: 752.95
     rank 16: 937.06
     rank 17: 937.26
     rank 18: 936.61
     rank 19: 936.53
  forward-send:
     rank  0: 42.87
     rank  1: 41.71
     rank  2: 43.09
     rank  3: 40.17
     rank  4: 52.15
     rank  5: 51.21
     rank  6: 45.65
     rank  7: 51.34
     rank  8: 22.66
     rank  9: 22.56
     rank 10: 20.78
     rank 11: 22.36
     rank 12: 10.84
     rank 13: 10.68
     rank 14: 10.46
     rank 15: 10.45
  backward-recv:
     rank  0: 1554.45
     rank  1: 1554.49
     rank  2: 1554.12
     rank  3: 1553.95
     rank  4: 812.03
     rank  5: 813.77
     rank  6: 812.38
     rank  7: 813.29
     rank  8: 411.03
     rank  9: 410.01
     rank 10: 408.64
     rank 11: 410.39
     rank 12: 250.22
     rank 13: 252.06
     rank 14: 251.57
     rank 15: 251.87
  backward-send:
     rank  4: 47.84
     rank  5: 46.18
     rank  6: 47.48
     rank  7: 45.98
     rank  8: 31.28
     rank  9: 31.11
     rank 10: 31.26
     rank 11: 30.31
     rank 12: 22.66
     rank 13: 21.21
     rank 14: 20.05
     rank 15: 20.96
     rank 16: 10.59
     rank 17: 10.49
     rank 18: 9.81
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 2905.92
     rank  1: 2904.11
     rank  2: 2903.72
     rank  3: 2907.19
     rank  4: 2021.94
     rank  5: 2023.27
     rank  6: 2020.59
     rank  7: 2023.35
     rank  8: 725.18
     rank  9: 724.33
     rank 10: 721.47
     rank 11: 722.91
     rank 12: 491.42
     rank 13: 492.49
     rank 14: 493.11
     rank 15: 487.51
  backward-send-forward-recv:
     rank  4: 245.51
     rank  5: 243.29
     rank  6: 242.98
     rank  7: 244.54
     rank  8: 146.09
     rank  9: 145.68
     rank 10: 144.68
     rank 11: 144.95
     rank 12: 934.11
     rank 13: 934.89
     rank 14: 933.29
     rank 15: 934.17
     rank 16: 609.32
     rank 17: 606.27
     rank 18: 608.70
     rank 19: 596.21
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.07
     rank  2: 0.06
     rank  3: 0.08
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.15
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.66
     rank  1: 1.67
     rank  2: 1.69
     rank  3: 1.72
     rank  4: 2.39
     rank  5: 2.40
     rank  6: 2.38
     rank  7: 2.39
     rank  8: 2.90
     rank  9: 2.89
     rank 10: 2.88
     rank 11: 2.96
     rank 12: 2.26
     rank 13: 2.22
     rank 14: 2.22
     rank 15: 2.21
     rank 16: 2.95
     rank 17: 2.38
     rank 18: 2.43
     rank 19: 2.36
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.13
     rank 17: 0.04
     rank 18: 0.06
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.73
     rank  1: 30.83
     rank  2: 30.73
     rank  3: 30.85
     rank  4: 48.37
     rank  5: 48.34
     rank  6: 48.39
     rank  7: 48.39
     rank  8: 59.06
     rank  9: 59.06
     rank 10: 59.01
     rank 11: 59.13
     rank 12: 42.78
     rank 13: 43.37
     rank 14: 43.19
     rank 15: 42.74
     rank 16: 48.84
     rank 17: 48.39
     rank 18: 48.58
     rank 19: 48.53
  optimizer:
     rank  0: 31.47
     rank  1: 31.57
     rank  2: 31.46
     rank  3: 31.58
     rank  4: 49.10
     rank  5: 49.07
     rank  6: 49.12
     rank  7: 49.12
     rank  8: 59.80
     rank  9: 59.79
     rank 10: 59.74
     rank 11: 59.86
     rank 12: 43.52
     rank 13: 44.10
     rank 14: 43.93
     rank 15: 43.47
     rank 16: 49.57
     rank 17: 49.14
     rank 18: 49.40
     rank 19: 49.26
 [2024-12-05 19:48:49] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 10563.5 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.597890E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10471.46
     rank  1: 10471.47
     rank  2: 10471.45
     rank  3: 10471.44
     rank  4: 10472.84
     rank  5: 10472.88
     rank  6: 10472.82
     rank  7: 10472.86
     rank  8: 10474.01
     rank  9: 10474.00
     rank 10: 10473.98
     rank 11: 10474.03
     rank 12: 10472.50
     rank 13: 10472.51
     rank 14: 10472.43
     rank 15: 10472.49
     rank 16: 10472.97
     rank 17: 10472.96
     rank 18: 10472.99
     rank 19: 10472.96
  forward-compute:
     rank  0: 3886.86
     rank  1: 3889.22
     rank  2: 3885.38
     rank  3: 3889.53
     rank  4: 3092.67
     rank  5: 3094.94
     rank  6: 3094.43
     rank  7: 3098.35
     rank  8: 3769.15
     rank  9: 3769.51
     rank 10: 3771.38
     rank 11: 3771.93
     rank 12: 3406.37
     rank 13: 3407.83
     rank 14: 3410.60
     rank 15: 3409.80
     rank 16: 3547.24
     rank 17: 3553.59
     rank 18: 3553.33
     rank 19: 3566.49
  backward-compute:
     rank  0: 2091.78
     rank  1: 2092.54
     rank  2: 2093.01
     rank  3: 2095.26
     rank  4: 3681.77
     rank  5: 3682.64
     rank  6: 3683.11
     rank  7: 3682.10
     rank  8: 4486.59
     rank  9: 4488.49
     rank 10: 4496.74
     rank 11: 4492.11
     rank 12: 3960.20
     rank 13: 3960.53
     rank 14: 3954.11
     rank 15: 3965.60
     rank 16: 4431.11
     rank 17: 4434.51
     rank 18: 4428.31
     rank 19: 4429.34
  pure-backward-compute:
     rank  0: 2090.94
     rank  1: 2091.73
     rank  2: 2091.95
     rank  3: 2094.46
     rank  4: 3680.87
     rank  5: 3681.84
     rank  6: 3682.19
     rank  7: 3681.07
     rank  8: 4485.31
     rank  9: 4487.57
     rank 10: 4496.04
     rank 11: 4490.90
     rank 12: 3958.62
     rank 13: 3959.22
     rank 14: 3952.24
     rank 15: 3964.49
     rank 16: 4427.54
     rank 17: 4432.51
     rank 18: 4426.07
     rank 19: 4426.73
  batch-generator:
     rank  0: 75.62
     rank  1: 82.76
     rank  2: 83.73
     rank  3: 87.61
     rank  4: 50.16
     rank  5: 55.46
     rank  6: 54.47
     rank  7: 59.31
     rank  8: 57.85
     rank  9: 60.23
     rank 10: 61.81
     rank 11: 61.61
     rank 12: 71.16
     rank 13: 76.30
     rank 14: 87.68
     rank 15: 83.62
     rank 16: 71.50
     rank 17: 79.08
     rank 18: 81.16
     rank 19: 96.84
  forward-recv:
     rank  4: 407.81
     rank  5: 407.50
     rank  6: 409.24
     rank  7: 406.98
     rank  8: 476.31
     rank  9: 476.50
     rank 10: 476.65
     rank 11: 476.96
     rank 12: 735.51
     rank 13: 735.78
     rank 14: 735.48
     rank 15: 735.34
     rank 16: 923.96
     rank 17: 924.02
     rank 18: 923.45
     rank 19: 923.70
  forward-send:
     rank  0: 42.94
     rank  1: 41.54
     rank  2: 43.01
     rank  3: 39.93
     rank  4: 33.06
     rank  5: 32.36
     rank  6: 31.58
     rank  7: 31.42
     rank  8: 23.46
     rank  9: 23.10
     rank 10: 22.41
     rank 11: 22.10
     rank 12: 10.87
     rank 13: 10.62
     rank 14: 10.61
     rank 15: 10.36
  backward-recv:
     rank  0: 1557.74
     rank  1: 1557.89
     rank  2: 1557.29
     rank  3: 1557.37
     rank  4: 810.82
     rank  5: 811.24
     rank  6: 810.79
     rank  7: 811.66
     rank  8: 413.43
     rank  9: 414.02
     rank 10: 412.10
     rank 11: 413.15
     rank 12: 252.87
     rank 13: 253.42
     rank 14: 252.66
     rank 15: 253.52
  backward-send:
     rank  4: 47.81
     rank  5: 46.62
     rank  6: 47.58
     rank  7: 46.48
     rank  8: 31.49
     rank  9: 30.58
     rank 10: 31.01
     rank 11: 30.91
     rank 12: 21.20
     rank 13: 21.17
     rank 14: 19.78
     rank 15: 20.93
     rank 16: 10.58
     rank 17: 10.38
     rank 18: 9.42
     rank 19: 10.36
  forward-send-backward-recv:
     rank  0: 2878.75
     rank  1: 2878.02
     rank  2: 2878.48
     rank  3: 2877.23
     rank  4: 2002.34
     rank  5: 2003.78
     rank  6: 2000.68
     rank  7: 2002.92
     rank  8: 730.77
     rank  9: 730.42
     rank 10: 728.14
     rank 11: 730.04
     rank 12: 487.39
     rank 13: 488.54
     rank 14: 493.00
     rank 15: 483.43
  backward-send-forward-recv:
     rank  4: 247.68
     rank  5: 246.12
     rank  6: 246.64
     rank  7: 245.27
     rank  8: 145.60
     rank  9: 145.16
     rank 10: 143.69
     rank 11: 144.08
     rank 12: 914.99
     rank 13: 913.73
     rank 14: 910.12
     rank 15: 912.41
     rank 16: 614.88
     rank 17: 611.71
     rank 18: 618.15
     rank 19: 601.18
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.08
     rank  2: 0.07
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.66
     rank  1: 1.68
     rank  2: 1.65
     rank  3: 1.67
     rank  4: 2.42
     rank  5: 2.42
     rank  6: 2.42
     rank  7: 2.40
     rank  8: 2.96
     rank  9: 2.91
     rank 10: 2.97
     rank 11: 2.97
     rank 12: 2.19
     rank 13: 2.14
     rank 14: 2.13
     rank 15: 2.21
     rank 16: 2.35
     rank 17: 2.41
     rank 18: 2.50
     rank 19: 2.35
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.09
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.73
     rank  1: 30.78
     rank  2: 30.77
     rank  3: 30.76
     rank  4: 48.38
     rank  5: 48.36
     rank  6: 48.39
     rank  7: 48.41
     rank  8: 59.17
     rank  9: 59.07
     rank 10: 59.08
     rank 11: 59.18
     rank 12: 42.74
     rank 13: 43.25
     rank 14: 43.06
     rank 15: 42.83
     rank 16: 48.58
     rank 17: 48.54
     rank 18: 48.69
     rank 19: 48.49
  optimizer:
     rank  0: 31.40
     rank  1: 31.46
     rank  2: 31.45
     rank  3: 31.44
     rank  4: 49.05
     rank  5: 49.03
     rank  6: 49.07
     rank  7: 49.08
     rank  8: 59.84
     rank  9: 59.74
     rank 10: 59.75
     rank 11: 59.85
     rank 12: 43.41
     rank 13: 43.93
     rank 14: 43.73
     rank 15: 43.50
     rank 16: 49.25
     rank 17: 49.21
     rank 18: 49.36
     rank 19: 49.17
 [2024-12-05 19:49:00] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 10574.2 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.043219E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10481.68
     rank  1: 10481.67
     rank  2: 10481.66
     rank  3: 10481.75
     rank  4: 10483.07
     rank  5: 10483.04
     rank  6: 10483.12
     rank  7: 10483.08
     rank  8: 10484.24
     rank  9: 10484.17
     rank 10: 10484.25
     rank 11: 10484.26
     rank 12: 10482.71
     rank 13: 10482.66
     rank 14: 10482.74
     rank 15: 10482.75
     rank 16: 10483.17
     rank 17: 10483.15
     rank 18: 10483.27
     rank 19: 10483.19
  forward-compute:
     rank  0: 3889.51
     rank  1: 3891.13
     rank  2: 3887.37
     rank  3: 3891.69
     rank  4: 3108.31
     rank  5: 3110.51
     rank  6: 3109.44
     rank  7: 3115.79
     rank  8: 3761.85
     rank  9: 3762.86
     rank 10: 3762.90
     rank 11: 3763.46
     rank 12: 3411.01
     rank 13: 3408.94
     rank 14: 3410.48
     rank 15: 3412.29
     rank 16: 3557.09
     rank 17: 3563.86
     rank 18: 3567.67
     rank 19: 3576.52
  backward-compute:
     rank  0: 2088.79
     rank  1: 2090.24
     rank  2: 2091.22
     rank  3: 2090.31
     rank  4: 3685.09
     rank  5: 3685.49
     rank  6: 3685.58
     rank  7: 3685.49
     rank  8: 4487.08
     rank  9: 4486.83
     rank 10: 4495.56
     rank 11: 4491.32
     rank 12: 3965.94
     rank 13: 3967.14
     rank 14: 3964.61
     rank 15: 3973.20
     rank 16: 4441.65
     rank 17: 4447.82
     rank 18: 4441.46
     rank 19: 4442.50
  pure-backward-compute:
     rank  0: 2087.92
     rank  1: 2089.43
     rank  2: 2090.25
     rank  3: 2089.47
     rank  4: 3684.16
     rank  5: 3684.66
     rank  6: 3684.69
     rank  7: 3684.63
     rank  8: 4486.24
     rank  9: 4485.95
     rank 10: 4494.73
     rank 11: 4490.39
     rank 12: 3964.36
     rank 13: 3965.80
     rank 14: 3962.81
     rank 15: 3972.11
     rank 16: 4438.11
     rank 17: 4445.82
     rank 18: 4439.33
     rank 19: 4439.82
  batch-generator:
     rank  0: 75.48
     rank  1: 82.23
     rank  2: 82.26
     rank  3: 87.76
     rank  4: 51.20
     rank  5: 56.54
     rank  6: 54.93
     rank  7: 60.68
     rank  8: 48.86
     rank  9: 53.67
     rank 10: 53.60
     rank 11: 53.91
     rank 12: 71.69
     rank 13: 73.20
     rank 14: 83.00
     rank 15: 81.79
     rank 16: 68.73
     rank 17: 76.78
     rank 18: 82.03
     rank 19: 93.53
  forward-recv:
     rank  4: 403.83
     rank  5: 404.00
     rank  6: 404.22
     rank  7: 401.74
     rank  8: 478.06
     rank  9: 477.29
     rank 10: 478.53
     rank 11: 478.81
     rank 12: 734.83
     rank 13: 735.28
     rank 14: 735.02
     rank 15: 734.47
     rank 16: 927.00
     rank 17: 926.89
     rank 18: 926.70
     rank 19: 926.42
  forward-send:
     rank  0: 42.70
     rank  1: 41.84
     rank  2: 42.63
     rank  3: 39.37
     rank  4: 33.19
     rank  5: 32.45
     rank  6: 32.48
     rank  7: 31.78
     rank  8: 22.82
     rank  9: 22.82
     rank 10: 22.07
     rank 11: 21.54
     rank 12: 10.84
     rank 13: 10.74
     rank 14: 10.48
     rank 15: 10.41
  backward-recv:
     rank  0: 1561.65
     rank  1: 1561.49
     rank  2: 1560.77
     rank  3: 1561.72
     rank  4: 815.47
     rank  5: 817.06
     rank  6: 815.91
     rank  7: 815.94
     rank  8: 421.02
     rank  9: 421.25
     rank 10: 419.46
     rank 11: 421.29
     rank 12: 256.85
     rank 13: 257.01
     rank 14: 256.61
     rank 15: 257.61
  backward-send:
     rank  4: 50.82
     rank  5: 49.66
     rank  6: 50.01
     rank  7: 50.38
     rank  8: 31.30
     rank  9: 30.69
     rank 10: 31.35
     rank 11: 30.70
     rank 12: 21.26
     rank 13: 21.18
     rank 14: 20.01
     rank 15: 20.93
     rank 16: 10.53
     rank 17: 10.44
     rank 18: 9.53
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 2885.43
     rank  1: 2884.36
     rank  2: 2885.18
     rank  3: 2886.38
     rank  4: 1987.86
     rank  5: 1989.28
     rank  6: 1987.12
     rank  7: 1986.81
     rank  8: 743.06
     rank  9: 743.30
     rank 10: 739.67
     rank 11: 741.59
     rank 12: 488.38
     rank 13: 489.55
     rank 14: 489.26
     rank 15: 483.30
  backward-send-forward-recv:
     rank  4: 249.14
     rank  5: 247.33
     rank  6: 248.50
     rank  7: 247.20
     rank  8: 145.26
     rank  9: 144.92
     rank 10: 143.85
     rank 11: 143.86
     rank 12: 911.37
     rank 13: 913.19
     rank 14: 911.20
     rank 15: 911.02
     rank 16: 601.28
     rank 17: 596.94
     rank 18: 599.08
     rank 19: 586.55
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.07
     rank  2: 0.08
     rank  3: 0.08
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.72
     rank  1: 1.68
     rank  2: 1.70
     rank  3: 1.74
     rank  4: 2.39
     rank  5: 2.44
     rank  6: 2.39
     rank  7: 2.44
     rank  8: 2.90
     rank  9: 2.94
     rank 10: 3.01
     rank 11: 2.98
     rank 12: 2.22
     rank 13: 2.20
     rank 14: 2.25
     rank 15: 2.17
     rank 16: 2.38
     rank 17: 2.34
     rank 18: 2.58
     rank 19: 2.39
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.04
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.04
     rank 18: 0.09
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.77
     rank  1: 30.76
     rank  2: 30.74
     rank  3: 30.85
     rank  4: 48.36
     rank  5: 48.45
     rank  6: 48.38
     rank  7: 48.49
     rank  8: 59.00
     rank  9: 59.11
     rank 10: 59.12
     rank 11: 59.09
     rank 12: 42.70
     rank 13: 43.31
     rank 14: 43.16
     rank 15: 42.67
     rank 16: 48.46
     rank 17: 48.38
     rank 18: 48.55
     rank 19: 48.67
  optimizer:
     rank  0: 31.46
     rank  1: 31.45
     rank  2: 31.44
     rank  3: 31.54
     rank  4: 49.06
     rank  5: 49.14
     rank  6: 49.07
     rank  7: 49.18
     rank  8: 59.69
     rank  9: 59.80
     rank 10: 59.81
     rank 11: 59.78
     rank 12: 43.39
     rank 13: 44.00
     rank 14: 43.86
     rank 15: 43.36
     rank 16: 49.15
     rank 17: 49.07
     rank 18: 49.25
     rank 19: 49.36
 [2024-12-05 19:49:10] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 10581.7 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.437702E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10488.64
     rank  1: 10488.65
     rank  2: 10488.67
     rank  3: 10488.71
     rank  4: 10486.32
     rank  5: 10486.30
     rank  6: 10486.33
     rank  7: 10486.30
     rank  8: 10487.44
     rank  9: 10487.42
     rank 10: 10487.46
     rank 11: 10487.47
     rank 12: 10485.91
     rank 13: 10485.91
     rank 14: 10485.92
     rank 15: 10485.91
     rank 16: 10486.40
     rank 17: 10486.38
     rank 18: 10486.39
     rank 19: 10486.38
  forward-compute:
     rank  0: 3882.02
     rank  1: 3883.68
     rank  2: 3879.21
     rank  3: 3883.67
     rank  4: 3115.24
     rank  5: 3118.25
     rank  6: 3117.92
     rank  7: 3121.25
     rank  8: 3763.82
     rank  9: 3764.37
     rank 10: 3764.48
     rank 11: 3766.78
     rank 12: 3418.65
     rank 13: 3416.68
     rank 14: 3418.36
     rank 15: 3418.55
     rank 16: 3565.98
     rank 17: 3571.55
     rank 18: 3584.16
     rank 19: 3586.64
  backward-compute:
     rank  0: 2087.25
     rank  1: 2088.20
     rank  2: 2090.51
     rank  3: 2089.38
     rank  4: 3686.29
     rank  5: 3687.31
     rank  6: 3686.86
     rank  7: 3686.89
     rank  8: 4489.53
     rank  9: 4489.67
     rank 10: 4497.37
     rank 11: 4493.91
     rank 12: 3987.12
     rank 13: 3986.99
     rank 14: 3987.67
     rank 15: 3994.97
     rank 16: 4456.06
     rank 17: 4461.28
     rank 18: 4452.31
     rank 19: 4456.63
  pure-backward-compute:
     rank  0: 2086.38
     rank  1: 2087.39
     rank  2: 2089.57
     rank  3: 2088.54
     rank  4: 3685.32
     rank  5: 3686.38
     rank  6: 3686.00
     rank  7: 3685.96
     rank  8: 4488.55
     rank  9: 4488.83
     rank 10: 4496.65
     rank 11: 4492.93
     rank 12: 3985.51
     rank 13: 3985.60
     rank 14: 3985.92
     rank 15: 3993.85
     rank 16: 4453.24
     rank 17: 4459.33
     rank 18: 4450.28
     rank 19: 4454.10
  batch-generator:
     rank  0: 74.91
     rank  1: 81.85
     rank  2: 80.35
     rank  3: 86.90
     rank  4: 57.65
     rank  5: 62.11
     rank  6: 61.76
     rank  7: 64.49
     rank  8: 51.87
     rank  9: 55.26
     rank 10: 55.82
     rank 11: 56.88
     rank 12: 71.68
     rank 13: 73.00
     rank 14: 82.91
     rank 15: 79.76
     rank 16: 67.48
     rank 17: 74.32
     rank 18: 85.98
     rank 19: 92.83
  forward-recv:
     rank  4: 401.05
     rank  5: 400.85
     rank  6: 402.31
     rank  7: 399.60
     rank  8: 474.67
     rank  9: 474.09
     rank 10: 474.68
     rank 11: 474.56
     rank 12: 730.93
     rank 13: 731.18
     rank 14: 730.76
     rank 15: 730.72
     rank 16: 926.14
     rank 17: 925.96
     rank 18: 925.35
     rank 19: 925.69
  forward-send:
     rank  0: 42.81
     rank  1: 41.69
     rank  2: 43.01
     rank  3: 39.68
     rank  4: 32.14
     rank  5: 31.46
     rank  6: 30.57
     rank  7: 30.24
     rank  8: 21.69
     rank  9: 21.64
     rank 10: 20.58
     rank 11: 20.63
     rank 12: 10.78
     rank 13: 10.65
     rank 14: 10.29
     rank 15: 10.59
  backward-recv:
     rank  0: 1567.18
     rank  1: 1568.00
     rank  2: 1566.14
     rank  3: 1567.37
     rank  4: 816.39
     rank  5: 815.43
     rank  6: 817.44
     rank  7: 815.74
     rank  8: 423.05
     rank  9: 424.03
     rank 10: 422.49
     rank 11: 423.43
     rank 12: 256.39
     rank 13: 256.69
     rank 14: 256.29
     rank 15: 256.68
  backward-send:
     rank  4: 51.77
     rank  5: 51.74
     rank  6: 50.65
     rank  7: 51.61
     rank  8: 31.43
     rank  9: 30.57
     rank 10: 31.10
     rank 11: 30.91
     rank 12: 21.35
     rank 13: 21.30
     rank 14: 20.26
     rank 15: 20.89
     rank 16: 10.64
     rank 17: 10.55
     rank 18: 9.89
     rank 19: 10.55
  forward-send-backward-recv:
     rank  0: 2895.83
     rank  1: 2894.77
     rank  2: 2895.45
     rank  3: 2896.55
     rank  4: 1983.82
     rank  5: 1986.06
     rank  6: 1983.72
     rank  7: 1984.97
     rank  8: 742.21
     rank  9: 743.75
     rank 10: 740.61
     rank 11: 741.31
     rank 12: 484.75
     rank 13: 486.55
     rank 14: 483.49
     rank 15: 479.04
  backward-send-forward-recv:
     rank  4: 250.15
     rank  5: 247.84
     rank  6: 248.28
     rank  7: 248.30
     rank  8: 145.71
     rank  9: 145.52
     rank 10: 145.22
     rank 11: 144.27
     rank 12: 894.69
     rank 13: 896.54
     rank 14: 894.86
     rank 15: 895.23
     rank 16: 582.15
     rank 17: 579.06
     rank 18: 575.36
     rank 19: 566.15
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.08
     rank  2: 0.08
     rank  3: 0.09
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.64
     rank  1: 1.70
     rank  2: 1.71
     rank  3: 1.73
     rank  4: 2.47
     rank  5: 2.42
     rank  6: 2.43
     rank  7: 2.43
     rank  8: 2.89
     rank  9: 2.89
     rank 10: 2.89
     rank 11: 2.97
     rank 12: 2.16
     rank 13: 2.22
     rank 14: 2.15
     rank 15: 2.17
     rank 16: 2.36
     rank 17: 2.41
     rank 18: 2.40
     rank 19: 2.33
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.04
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.70
     rank  1: 30.90
     rank  2: 30.83
     rank  3: 30.85
     rank  4: 48.45
     rank  5: 48.36
     rank  6: 48.41
     rank  7: 48.45
     rank  8: 59.01
     rank  9: 59.02
     rank 10: 59.01
     rank 11: 59.08
     rank 12: 42.53
     rank 13: 43.40
     rank 14: 43.02
     rank 15: 42.74
     rank 16: 48.38
     rank 17: 48.50
     rank 18: 48.56
     rank 19: 48.50
  optimizer:
     rank  0: 31.35
     rank  1: 31.55
     rank  2: 31.48
     rank  3: 31.50
     rank  4: 49.09
     rank  5: 49.01
     rank  6: 49.06
     rank  7: 49.09
     rank  8: 59.65
     rank  9: 59.67
     rank 10: 59.66
     rank 11: 59.72
     rank 12: 43.18
     rank 13: 44.05
     rank 14: 43.66
     rank 15: 43.39
     rank 16: 49.02
     rank 17: 49.14
     rank 18: 49.21
     rank 19: 49.15
 [2024-12-05 19:49:21] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10573.0 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.462953E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10479.70
     rank  1: 10479.57
     rank  2: 10479.62
     rank  3: 10479.61
     rank  4: 10481.13
     rank  5: 10481.08
     rank  6: 10481.00
     rank  7: 10481.04
     rank  8: 10482.26
     rank  9: 10482.15
     rank 10: 10482.14
     rank 11: 10482.15
     rank 12: 10480.77
     rank 13: 10480.64
     rank 14: 10480.59
     rank 15: 10480.63
     rank 16: 10481.25
     rank 17: 10481.06
     rank 18: 10481.04
     rank 19: 10481.11
  forward-compute:
     rank  0: 3884.17
     rank  1: 3885.88
     rank  2: 3882.48
     rank  3: 3886.28
     rank  4: 3108.93
     rank  5: 3110.78
     rank  6: 3110.09
     rank  7: 3114.74
     rank  8: 3768.73
     rank  9: 3770.44
     rank 10: 3772.60
     rank 11: 3770.56
     rank 12: 3415.64
     rank 13: 3415.66
     rank 14: 3417.64
     rank 15: 3417.26
     rank 16: 3594.18
     rank 17: 3601.00
     rank 18: 3607.62
     rank 19: 3611.63
  backward-compute:
     rank  0: 2089.22
     rank  1: 2091.42
     rank  2: 2090.50
     rank  3: 2091.85
     rank  4: 3682.31
     rank  5: 3684.21
     rank  6: 3683.41
     rank  7: 3682.82
     rank  8: 4487.08
     rank  9: 4485.03
     rank 10: 4494.22
     rank 11: 4490.29
     rank 12: 3978.21
     rank 13: 3978.81
     rank 14: 3976.93
     rank 15: 3982.42
     rank 16: 4462.46
     rank 17: 4464.96
     rank 18: 4458.34
     rank 19: 4459.96
  pure-backward-compute:
     rank  0: 2088.35
     rank  1: 2090.62
     rank  2: 2089.54
     rank  3: 2091.03
     rank  4: 3681.43
     rank  5: 3683.28
     rank  6: 3682.31
     rank  7: 3681.84
     rank  8: 4486.20
     rank  9: 4484.20
     rank 10: 4493.49
     rank 11: 4489.35
     rank 12: 3976.60
     rank 13: 3977.53
     rank 14: 3975.18
     rank 15: 3981.31
     rank 16: 4460.03
     rank 17: 4462.95
     rank 18: 4456.36
     rank 19: 4457.44
  batch-generator:
     rank  0: 75.93
     rank  1: 81.96
     rank  2: 81.96
     rank  3: 87.84
     rank  4: 53.05
     rank  5: 57.39
     rank  6: 56.01
     rank  7: 61.03
     rank  8: 47.89
     rank  9: 53.09
     rank 10: 55.96
     rank 11: 52.85
     rank 12: 70.97
     rank 13: 74.99
     rank 14: 85.56
     rank 15: 82.16
     rank 16: 65.94
     rank 17: 74.56
     rank 18: 82.38
     rank 19: 88.82
  forward-recv:
     rank  4: 397.68
     rank  5: 398.19
     rank  6: 403.01
     rank  7: 396.10
     rank  8: 474.94
     rank  9: 475.45
     rank 10: 473.11
     rank 11: 475.60
     rank 12: 739.08
     rank 13: 738.08
     rank 14: 737.21
     rank 15: 738.27
     rank 16: 930.20
     rank 17: 930.02
     rank 18: 929.50
     rank 19: 929.61
  forward-send:
     rank  0: 42.57
     rank  1: 41.70
     rank  2: 42.54
     rank  3: 39.28
     rank  4: 38.03
     rank  5: 36.81
     rank  6: 32.13
     rank  7: 36.32
     rank  8: 22.28
     rank  9: 20.95
     rank 10: 19.39
     rank 11: 20.50
     rank 12: 10.82
     rank 13: 10.68
     rank 14: 10.42
     rank 15: 10.37
  backward-recv:
     rank  0: 1559.54
     rank  1: 1560.07
     rank  2: 1559.23
     rank  3: 1560.18
     rank  4: 817.71
     rank  5: 818.01
     rank  6: 817.18
     rank  7: 817.37
     rank  8: 420.42
     rank  9: 422.57
     rank 10: 418.85
     rank 11: 420.39
     rank 12: 256.96
     rank 13: 254.60
     rank 14: 256.73
     rank 15: 257.55
  backward-send:
     rank  4: 49.36
     rank  5: 49.04
     rank  6: 49.41
     rank  7: 49.70
     rank  8: 31.26
     rank  9: 30.76
     rank 10: 31.28
     rank 11: 30.93
     rank 12: 21.25
     rank 13: 23.65
     rank 14: 19.97
     rank 15: 21.00
     rank 16: 10.54
     rank 17: 10.49
     rank 18: 9.62
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 2890.36
     rank  1: 2888.46
     rank  2: 2890.89
     rank  3: 2890.10
     rank  4: 1988.11
     rank  5: 1988.45
     rank  6: 1987.04
     rank  7: 1988.21
     rank  8: 738.10
     rank  9: 738.29
     rank 10: 736.46
     rank 11: 737.25
     rank 12: 489.98
     rank 13: 491.12
     rank 14: 490.45
     rank 15: 487.22
  backward-send-forward-recv:
     rank  4: 249.38
     rank  5: 248.26
     rank  6: 249.15
     rank  7: 248.42
     rank  8: 145.73
     rank  9: 144.75
     rank 10: 145.50
     rank 11: 144.92
     rank 12: 887.09
     rank 13: 888.38
     rank 14: 886.57
     rank 15: 887.00
     rank 16: 540.80
     rank 17: 536.19
     rank 18: 536.20
     rank 19: 528.05
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.07
     rank  2: 0.08
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.15
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.69
     rank  1: 1.65
     rank  2: 1.69
     rank  3: 1.69
     rank  4: 2.40
     rank  5: 2.44
     rank  6: 2.42
     rank  7: 2.44
     rank  8: 2.90
     rank  9: 2.90
     rank 10: 2.90
     rank 11: 2.95
     rank 12: 2.17
     rank 13: 2.14
     rank 14: 2.12
     rank 15: 2.23
     rank 16: 2.66
     rank 17: 2.35
     rank 18: 2.35
     rank 19: 2.38
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.12
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.04
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.72
     rank  1: 30.76
     rank  2: 30.76
     rank  3: 30.78
     rank  4: 48.34
     rank  5: 48.34
     rank  6: 48.50
     rank  7: 48.43
     rank  8: 59.03
     rank  9: 59.04
     rank 10: 59.05
     rank 11: 59.13
     rank 12: 42.64
     rank 13: 43.30
     rank 14: 43.02
     rank 15: 42.82
     rank 16: 48.70
     rank 17: 48.35
     rank 18: 48.43
     rank 19: 48.53
  optimizer:
     rank  0: 31.43
     rank  1: 31.47
     rank  2: 31.47
     rank  3: 31.49
     rank  4: 49.05
     rank  5: 49.05
     rank  6: 49.21
     rank  7: 49.13
     rank  8: 59.73
     rank  9: 59.75
     rank 10: 59.76
     rank 11: 59.84
     rank 12: 43.36
     rank 13: 44.01
     rank 14: 43.74
     rank 15: 43.52
     rank 16: 49.41
     rank 17: 49.07
     rank 18: 49.14
     rank 19: 49.24
 [2024-12-05 19:49:31] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 10579.2 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 6.364578E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10487.34
     rank  1: 10487.27
     rank  2: 10487.36
     rank  3: 10487.37
     rank  4: 10488.77
     rank  5: 10488.67
     rank  6: 10488.71
     rank  7: 10488.69
     rank  8: 10489.89
     rank  9: 10489.79
     rank 10: 10489.84
     rank 11: 10489.88
     rank 12: 10488.33
     rank 13: 10488.31
     rank 14: 10488.29
     rank 15: 10488.31
     rank 16: 10488.93
     rank 17: 10488.75
     rank 18: 10488.74
     rank 19: 10488.79
  forward-compute:
     rank  0: 3887.14
     rank  1: 3889.50
     rank  2: 3885.33
     rank  3: 3889.37
     rank  4: 3099.76
     rank  5: 3102.34
     rank  6: 3100.54
     rank  7: 3105.21
     rank  8: 3758.22
     rank  9: 3758.73
     rank 10: 3758.68
     rank 11: 3759.86
     rank 12: 3415.00
     rank 13: 3413.44
     rank 14: 3414.74
     rank 15: 3416.90
     rank 16: 3608.82
     rank 17: 3615.39
     rank 18: 3619.13
     rank 19: 3624.72
  backward-compute:
     rank  0: 2092.25
     rank  1: 2091.73
     rank  2: 2092.83
     rank  3: 2092.44
     rank  4: 3685.93
     rank  5: 3686.26
     rank  6: 3686.45
     rank  7: 3686.51
     rank  8: 4488.03
     rank  9: 4488.28
     rank 10: 4494.46
     rank 11: 4493.33
     rank 12: 3991.31
     rank 13: 3992.16
     rank 14: 3989.25
     rank 15: 3995.36
     rank 16: 4507.12
     rank 17: 4508.22
     rank 18: 4501.57
     rank 19: 4502.63
  pure-backward-compute:
     rank  0: 2091.36
     rank  1: 2090.90
     rank  2: 2091.86
     rank  3: 2091.62
     rank  4: 3685.06
     rank  5: 3685.44
     rank  6: 3685.60
     rank  7: 3685.61
     rank  8: 4487.21
     rank  9: 4487.46
     rank 10: 4493.77
     rank 11: 4492.31
     rank 12: 3989.75
     rank 13: 3990.89
     rank 14: 3987.44
     rank 15: 3994.29
     rank 16: 4504.56
     rank 17: 4506.30
     rank 18: 4499.56
     rank 19: 4500.10
  batch-generator:
     rank  0: 76.10
     rank  1: 83.60
     rank  2: 82.55
     rank  3: 88.03
     rank  4: 51.79
     rank  5: 56.78
     rank  6: 54.82
     rank  7: 60.35
     rank  8: 48.38
     rank  9: 52.31
     rank 10: 52.78
     rank 11: 53.53
     rank 12: 66.95
     rank 13: 69.76
     rank 14: 79.33
     rank 15: 79.36
     rank 16: 68.54
     rank 17: 76.39
     rank 18: 81.73
     rank 19: 89.23
  forward-recv:
     rank  4: 402.67
     rank  5: 402.38
     rank  6: 403.85
     rank  7: 400.80
     rank  8: 478.13
     rank  9: 477.57
     rank 10: 478.36
     rank 11: 478.82
     rank 12: 733.35
     rank 13: 733.52
     rank 14: 733.28
     rank 15: 733.08
     rank 16: 925.98
     rank 17: 925.71
     rank 18: 925.18
     rank 19: 925.50
  forward-send:
     rank  0: 42.79
     rank  1: 41.49
     rank  2: 42.80
     rank  3: 39.81
     rank  4: 32.40
     rank  5: 31.61
     rank  6: 30.82
     rank  7: 31.27
     rank  8: 21.81
     rank  9: 21.72
     rank 10: 20.77
     rank 11: 20.71
     rank 12: 10.84
     rank 13: 10.73
     rank 14: 10.43
     rank 15: 10.43
  backward-recv:
     rank  0: 1565.63
     rank  1: 1566.44
     rank  2: 1565.31
     rank  3: 1566.31
     rank  4: 820.23
     rank  5: 819.59
     rank  6: 820.59
     rank  7: 820.14
     rank  8: 422.08
     rank  9: 422.39
     rank 10: 421.66
     rank 11: 422.04
     rank 12: 252.11
     rank 13: 252.27
     rank 14: 250.55
     rank 15: 252.54
  backward-send:
     rank  4: 49.71
     rank  5: 49.57
     rank  6: 49.39
     rank  7: 49.54
     rank  8: 31.20
     rank  9: 30.67
     rank 10: 30.95
     rank 11: 30.93
     rank 12: 21.08
     rank 13: 20.99
     rank 14: 21.20
     rank 15: 20.62
     rank 16: 10.76
     rank 17: 10.36
     rank 18: 9.53
     rank 19: 10.38
  forward-send-backward-recv:
     rank  0: 2885.51
     rank  1: 2886.01
     rank  2: 2886.72
     rank  3: 2887.31
     rank  4: 1998.11
     rank  5: 1999.74
     rank  6: 1997.59
     rank  7: 1998.75
     rank  8: 750.69
     rank  9: 750.51
     rank 10: 748.61
     rank 11: 748.98
     rank 12: 497.86
     rank 13: 498.92
     rank 14: 499.07
     rank 15: 495.82
  backward-send-forward-recv:
     rank  4: 250.79
     rank  5: 249.48
     rank  6: 250.98
     rank  7: 248.85
     rank  8: 146.23
     rank  9: 146.09
     rank 10: 145.62
     rank 11: 144.77
     rank 12: 883.06
     rank 13: 884.64
     rank 14: 882.89
     rank 15: 881.89
     rank 16: 493.67
     rank 17: 489.87
     rank 18: 492.71
     rank 19: 482.81
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.07
     rank  2: 0.10
     rank  3: 0.09
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.68
     rank  1: 1.68
     rank  2: 1.77
     rank  3: 1.76
     rank  4: 2.42
     rank  5: 2.40
     rank  6: 2.42
     rank  7: 2.39
     rank  8: 2.89
     rank  9: 2.89
     rank 10: 2.91
     rank 11: 2.97
     rank 12: 2.10
     rank 13: 2.27
     rank 14: 2.20
     rank 15: 2.14
     rank 16: 2.49
     rank 17: 2.32
     rank 18: 2.37
     rank 19: 2.40
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.04
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.08
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.09
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 30.72
     rank  1: 30.79
     rank  2: 30.87
     rank  3: 30.85
     rank  4: 48.39
     rank  5: 48.36
     rank  6: 48.45
     rank  7: 48.40
     rank  8: 59.04
     rank  9: 59.12
     rank 10: 59.02
     rank 11: 59.14
     rank 12: 42.51
     rank 13: 43.29
     rank 14: 43.12
     rank 15: 42.61
     rank 16: 48.60
     rank 17: 48.38
     rank 18: 48.43
     rank 19: 48.67
  optimizer:
     rank  0: 31.36
     rank  1: 31.43
     rank  2: 31.51
     rank  3: 31.49
     rank  4: 49.02
     rank  5: 48.99
     rank  6: 49.09
     rank  7: 49.04
     rank  8: 59.67
     rank  9: 59.77
     rank 10: 59.65
     rank 11: 59.77
     rank 12: 43.15
     rank 13: 43.93
     rank 14: 43.76
     rank 15: 43.24
     rank 16: 49.25
     rank 17: 49.02
     rank 18: 49.07
     rank 19: 49.32
