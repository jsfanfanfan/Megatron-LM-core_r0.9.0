examples/multimodal/pretrain-freeze-llm-hete-3090first.sh: line 4: activate: No such file or directory
4
[2024-12-05 20:15:44,978] torch.distributed.run: [WARNING] 
[2024-12-05 20:15:44,978] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 20:15:44,978] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 20:15:44,978] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4][rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 306229248
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 306229248 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 306229248

 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 306229248
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (306229248 elements):
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.output_layer.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True

name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 99.11
     rank  1: 98.96
     rank  2: 101.24
     rank  3: 103.83
     rank  4: 50.78
     rank  5: 50.83
     rank  6: 34.88
     rank  7: 34.83
     rank  8: 61.51
     rank  9: 65.12
     rank 10: 61.80
     rank 11: 60.18
     rank 12: 71.17
     rank 13: 72.90
     rank 14: 71.52
     rank 15: 71.82
     rank 16: 71.41
     rank 17: 72.55
     rank 18: 72.89
     rank 19: 73.04
  train/valid/test-data-iterators-setup:
     rank  0: 840.22
     rank  1: 840.15
     rank  2: 840.15
     rank  3: 840.52
     rank  4: 914.02
     rank  5: 913.92
     rank  6: 914.76
     rank  7: 913.95
     rank  8: 1181.87
     rank  9: 1182.82
     rank 10: 914.71
     rank 11: 1181.73
     rank 12: 1254.95
     rank 13: 1255.64
     rank 14: 1181.92
     rank 15: 1255.07
     rank 16: 1254.90
     rank 17: 1255.11
     rank 18: 1254.87
     rank 19: 1254.86
 [2024-12-05 20:16:29] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22267.1 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.149047E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[Rank 18] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0

[Rank 17] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 4757.9404296875 | max allocated: 5925.603515625 | reserved: 6660.0 | max reserved: 6660.0
times across ranks (ms):
  forward-backward:
     rank  0: 22182.23
     rank  1: 22182.18
     rank  2: 22182.25
     rank  3: 22182.24
     rank  4: 22182.97
     rank  5: 22182.95
     rank  6: 22183.02
     rank  7: 22182.95
     rank  8: 22183.04
     rank  9: 22185.04
     rank 10: 22183.43
     rank 11: 22182.87
     rank 12: 22184.09
     rank 13: 22184.15
     rank 14: 22184.27
     rank 15: 22184.16
     rank 16: 22184.20
     rank 17: 22184.32
     rank 18: 22184.29
     rank 19: 22184.24
  forward-compute:
     rank  0: 6928.00
     rank  1: 6915.41
     rank  2: 6914.08
     rank  3: 6919.90
     rank  4: 5691.95
     rank  5: 5677.07
     rank  6: 5674.11
     rank  7: 5685.97
     rank  8: 5738.53
     rank  9: 5739.27
     rank 10: 5742.89
     rank 11: 5741.58
     rank 12: 5686.33
     rank 13: 5692.74
     rank 14: 5690.36
     rank 15: 5680.74
     rank 16: 5554.81
     rank 17: 5567.58
     rank 18: 5567.08
     rank 19: 5561.65
  backward-compute:
     rank  0: 2965.79
     rank  1: 2962.81
     rank  2: 2961.04
     rank  3: 2967.38
     rank  4: 3683.72
     rank  5: 3687.03
     rank  6: 3696.18
     rank  7: 3694.09
     rank  8: 4650.30
     rank  9: 4652.14
     rank 10: 4659.70
     rank 11: 4652.38
     rank 12: 4981.64
     rank 13: 4981.91
     rank 14: 4974.91
     rank 15: 4977.29
     rank 16: 5194.51
     rank 17: 5190.51
     rank 18: 5195.30
     rank 19: 5192.43
  pure-backward-compute:
     rank  0: 2965.05
     rank  1: 2961.83
     rank  2: 2959.91
     rank  3: 2966.28
     rank  4: 3682.62
     rank  5: 3685.76
     rank  6: 3694.92
     rank  7: 3693.24
     rank  8: 4648.59
     rank  9: 4649.62
     rank 10: 4656.70
     rank 11: 4650.32
     rank 12: 4979.66
     rank 13: 4980.16
     rank 14: 4972.89
     rank 15: 4974.73
     rank 16: 5191.22
     rank 17: 5187.48
     rank 18: 5193.37
     rank 19: 5190.50
  batch-generator:
     rank  0: 973.08
     rank  1: 971.04
     rank  2: 969.71
     rank  3: 982.37
     rank  4: 1223.60
     rank  5: 1214.09
     rank  6: 1212.30
     rank  7: 1220.67
     rank  8: 1489.23
     rank  9: 1504.77
     rank 10: 1511.07
     rank 11: 1507.59
     rank 12: 1320.18
     rank 13: 1346.79
     rank 14: 1339.02
     rank 15: 1327.22
     rank 16: 1109.79
     rank 17: 1123.78
     rank 18: 1126.04
     rank 19: 1125.15
  forward-recv:
     rank  4: 3789.68
     rank  5: 3805.64
     rank  6: 3802.88
     rank  7: 3795.86
     rank  8: 6548.90
     rank  9: 6552.13
     rank 10: 6550.48
     rank 11: 6545.46
     rank 12: 8342.68
     rank 13: 8342.41
     rank 14: 8342.95
     rank 15: 8348.12
     rank 16: 9981.13
     rank 17: 9974.49
     rank 18: 9977.76
     rank 19: 9980.45
  forward-send:
     rank  0: 5701.98
     rank  1: 5714.83
     rank  2: 5715.59
     rank  3: 5709.36
     rank  4: 3074.96
     rank  5: 3072.28
     rank  6: 3075.06
     rank  7: 3076.02
     rank  8: 1406.48
     rank  9: 1399.91
     rank 10: 1405.22
     rank 11: 1413.24
     rank 12: 30.57
     rank 13: 23.77
     rank 14: 27.45
     rank 15: 30.32
  backward-recv:
     rank  0: 1696.71
     rank  1: 1697.10
     rank  2: 1696.28
     rank  3: 1695.66
     rank  4: 1105.09
     rank  5: 1104.81
     rank  6: 1102.29
     rank  7: 1104.16
     rank  8: 616.60
     rank  9: 616.70
     rank 10: 616.99
     rank 11: 617.21
     rank 12: 285.29
     rank 13: 285.45
     rank 14: 287.44
     rank 15: 284.72
  backward-send:
     rank  4: 41.27
     rank  5: 41.06
     rank  6: 41.30
     rank  7: 40.42
     rank  8: 31.54
     rank  9: 31.36
     rank 10: 29.77
     rank 11: 30.45
     rank 12: 20.84
     rank 13: 20.67
     rank 14: 18.53
     rank 15: 20.57
     rank 16: 10.79
     rank 17: 10.45
     rank 18: 10.70
     rank 19: 10.58
  forward-send-backward-recv:
     rank  0: 4804.31
     rank  1: 4805.64
     rank  2: 4808.10
     rank  3: 4804.36
     rank  4: 4373.58
     rank  5: 4375.55
     rank  6: 4367.30
     rank  7: 4363.95
     rank  8: 2491.16
     rank  9: 2486.40
     rank 10: 2477.44
     rank 11: 2490.14
     rank 12: 1807.38
     rank 13: 1809.11
     rank 14: 1810.98
     rank 15: 1814.31
  backward-send-forward-recv:
     rank  4: 149.78
     rank  5: 149.52
     rank  6: 152.70
     rank  7: 150.30
     rank  8: 185.94
     rank  9: 186.91
     rank 10: 178.90
     rank 11: 179.23
     rank 12: 216.82
     rank 13: 217.41
     rank 14: 214.40
     rank 15: 215.53
     rank 16: 309.27
     rank 17: 308.03
     rank 18: 303.36
     rank 19: 308.64
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.05
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.09
     rank  3: 0.08
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.12
     rank 10: 0.06
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.09
     rank 19: 0.08
  all-grads-sync:
     rank  0: 44.86
     rank  1: 42.00
     rank  2: 46.91
     rank  3: 41.04
     rank  4: 48.52
     rank  5: 46.23
     rank  6: 42.13
     rank  7: 40.16
     rank  8: 69.49
     rank  9: 71.91
     rank 10: 60.64
     rank 11: 57.95
     rank 12: 63.06
     rank 13: 61.95
     rank 14: 59.14
     rank 15: 61.64
     rank 16: 22.65
     rank 17: 23.29
     rank 18: 23.89
     rank 19: 23.92
  optimizer-copy-to-main-grad:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.07
     rank  3: 0.07
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.09
     rank  7: 0.06
     rank  8: 0.07
     rank  9: 0.09
     rank 10: 0.11
     rank 11: 0.06
     rank 12: 0.10
     rank 13: 0.09
     rank 14: 0.08
     rank 15: 0.08
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.10
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.07
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 51.42
     rank  1: 51.44
     rank  2: 51.51
     rank  3: 51.49
     rank  4: 62.82
     rank  5: 59.12
     rank  6: 59.95
     rank  7: 58.92
     rank  8: 61.73
     rank  9: 64.07
     rank 10: 62.37
     rank 11: 60.91
     rank 12: 71.46
     rank 13: 71.24
     rank 14: 75.40
     rank 15: 71.26
     rank 16: 75.26
     rank 17: 75.23
     rank 18: 75.36
     rank 19: 75.33
  optimizer:
     rank  0: 52.60
     rank  1: 52.63
     rank  2: 52.71
     rank  3: 52.67
     rank  4: 64.01
     rank  5: 60.31
     rank  6: 61.16
     rank  7: 60.11
     rank  8: 62.92
     rank  9: 65.27
     rank 10: 63.57
     rank 11: 62.11
     rank 12: 72.65
     rank 13: 72.43
     rank 14: 76.59
     rank 15: 72.44
     rank 16: 76.45
     rank 17: 76.43
     rank 18: 76.56
     rank 19: 76.53
 [2024-12-05 20:16:41] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 12023.5 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.090739E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11922.67
     rank  1: 11922.63
     rank  2: 11922.71
     rank  3: 11923.40
     rank  4: 11923.27
     rank  5: 11923.33
     rank  6: 11923.34
     rank  7: 11923.56
     rank  8: 11922.95
     rank  9: 11923.12
     rank 10: 11922.92
     rank 11: 11923.11
     rank 12: 11923.82
     rank 13: 11923.75
     rank 14: 11923.79
     rank 15: 11923.99
     rank 16: 11924.44
     rank 17: 11924.49
     rank 18: 11924.45
     rank 19: 11924.42
  forward-compute:
     rank  0: 3482.22
     rank  1: 3483.53
     rank  2: 3484.64
     rank  3: 3499.64
     rank  4: 3140.72
     rank  5: 3141.48
     rank  6: 3141.59
     rank  7: 3159.05
     rank  8: 4215.73
     rank  9: 4213.23
     rank 10: 4216.37
     rank 11: 4227.17
     rank 12: 4372.38
     rank 13: 4371.32
     rank 14: 4374.53
     rank 15: 4373.95
     rank 16: 4358.54
     rank 17: 4360.48
     rank 18: 4366.01
     rank 19: 4368.55
  backward-compute:
     rank  0: 2962.81
     rank  1: 2962.48
     rank  2: 2960.81
     rank  3: 2964.77
     rank  4: 3668.26
     rank  5: 3669.12
     rank  6: 3680.17
     rank  7: 3679.74
     rank  8: 4625.25
     rank  9: 4627.39
     rank 10: 4634.93
     rank 11: 4627.23
     rank 12: 5004.05
     rank 13: 5002.15
     rank 14: 4998.95
     rank 15: 5004.88
     rank 16: 5176.99
     rank 17: 5175.34
     rank 18: 5181.96
     rank 19: 5178.42
  pure-backward-compute:
     rank  0: 2962.03
     rank  1: 2961.67
     rank  2: 2960.16
     rank  3: 2963.87
     rank  4: 3667.47
     rank  5: 3668.13
     rank  6: 3679.28
     rank  7: 3678.69
     rank  8: 4623.71
     rank  9: 4625.08
     rank 10: 4631.85
     rank 11: 4625.91
     rank 12: 5002.48
     rank 13: 5000.87
     rank 14: 4996.70
     rank 15: 5003.41
     rank 16: 5174.50
     rank 17: 5172.64
     rank 18: 5179.83
     rank 19: 5176.44
  batch-generator:
     rank  0: 57.00
     rank  1: 63.04
     rank  2: 61.76
     rank  3: 79.55
     rank  4: 52.52
     rank  5: 55.81
     rank  6: 55.81
     rank  7: 71.85
     rank  8: 73.76
     rank  9: 82.30
     rank 10: 90.95
     rank 11: 99.37
     rank 12: 71.30
     rank 13: 74.07
     rank 14: 87.75
     rank 15: 83.99
     rank 16: 69.72
     rank 17: 74.87
     rank 18: 82.75
     rank 19: 86.52
  forward-recv:
     rank  4: 263.13
     rank  5: 264.34
     rank  6: 263.65
     rank  7: 254.24
     rank  8: 457.71
     rank  9: 456.86
     rank 10: 458.08
     rank 11: 453.78
     rank 12: 720.89
     rank 13: 721.00
     rank 14: 720.53
     rank 15: 719.39
     rank 16: 991.90
     rank 17: 991.69
     rank 18: 991.39
     rank 19: 991.46
  forward-send:
     rank  0: 140.67
     rank  1: 139.02
     rank  2: 138.52
     rank  3: 123.76
     rank  4: 166.10
     rank  5: 164.30
     rank  6: 164.69
     rank  7: 158.35
     rank  8: 23.24
     rank  9: 22.97
     rank 10: 22.92
     rank 11: 21.34
     rank 12: 10.84
     rank 13: 10.64
     rank 14: 10.65
     rank 15: 10.43
  backward-recv:
     rank  0: 1720.33
     rank  1: 1720.45
     rank  2: 1721.50
     rank  3: 1719.62
     rank  4: 1124.69
     rank  5: 1123.08
     rank  6: 1120.77
     rank  7: 1122.31
     rank  8: 623.18
     rank  9: 625.25
     rank 10: 625.27
     rank 11: 626.02
     rank 12: 281.43
     rank 13: 281.63
     rank 14: 282.65
     rank 15: 281.99
  backward-send:
     rank  4: 41.74
     rank  5: 41.51
     rank  6: 41.40
     rank  7: 41.10
     rank  8: 33.23
     rank  9: 31.27
     rank 10: 29.36
     rank 11: 30.77
     rank 12: 21.32
     rank 13: 21.23
     rank 14: 19.94
     rank 15: 20.82
     rank 16: 10.85
     rank 17: 10.57
     rank 18: 10.65
     rank 19: 10.35
  forward-send-backward-recv:
     rank  0: 3599.73
     rank  1: 3599.66
     rank  2: 3601.22
     rank  3: 3599.20
     rank  4: 3178.00
     rank  5: 3182.18
     rank  6: 3173.82
     rank  7: 3168.89
     rank  8: 1325.84
     rank  9: 1324.34
     rank 10: 1312.23
     rank 11: 1324.73
     rank 12: 563.73
     rank 13: 566.49
     rank 14: 565.43
     rank 15: 563.92
  backward-send-forward-recv:
     rank  4: 135.11
     rank  5: 134.18
     rank  6: 133.97
     rank  7: 134.25
     rank  8: 175.89
     rank  9: 177.83
     rank 10: 172.23
     rank 11: 169.52
     rank 12: 202.55
     rank 13: 203.75
     rank 14: 199.20
     rank 15: 201.74
     rank 16: 315.09
     rank 17: 315.22
     rank 18: 307.09
     rank 19: 307.31
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.14
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.17
     rank  1: 2.13
     rank  2: 2.23
     rank  3: 2.76
     rank  4: 2.41
     rank  5: 2.45
     rank  6: 2.44
     rank  7: 2.45
     rank  8: 2.24
     rank  9: 2.61
     rank 10: 2.22
     rank 11: 2.21
     rank 12: 2.93
     rank 13: 2.50
     rank 14: 2.60
     rank 15: 2.65
     rank 16: 2.90
     rank 17: 2.92
     rank 18: 2.94
     rank 19: 2.89
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.08
     rank  9: 0.08
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.08
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.08
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.75
     rank  1: 42.69
     rank  2: 42.82
     rank  3: 42.92
     rank  4: 48.35
     rank  5: 48.41
     rank  6: 48.40
     rank  7: 48.39
     rank  8: 43.02
     rank  9: 44.32
     rank 10: 43.50
     rank 11: 42.99
     rank 12: 52.18
     rank 13: 51.91
     rank 14: 52.07
     rank 15: 52.15
     rank 16: 58.41
     rank 17: 58.50
     rank 18: 58.52
     rank 19: 58.48
  optimizer:
     rank  0: 43.76
     rank  1: 43.70
     rank  2: 43.83
     rank  3: 43.87
     rank  4: 49.36
     rank  5: 49.42
     rank  6: 49.42
     rank  7: 49.40
     rank  8: 44.16
     rank  9: 45.33
     rank 10: 44.51
     rank 11: 44.01
     rank 12: 53.20
     rank 13: 52.92
     rank 14: 53.09
     rank 15: 53.17
     rank 16: 59.43
     rank 17: 59.51
     rank 18: 59.52
     rank 19: 59.49
 [2024-12-05 20:16:54] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 12068.0 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 2.732132E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11975.44
     rank  1: 11975.48
     rank  2: 11975.51
     rank  3: 11976.10
     rank  4: 11976.08
     rank  5: 11976.12
     rank  6: 11976.10
     rank  7: 11976.37
     rank  8: 11975.81
     rank  9: 11975.64
     rank 10: 11975.67
     rank 11: 11975.91
     rank 12: 11976.53
     rank 13: 11976.50
     rank 14: 11976.65
     rank 15: 11976.76
     rank 16: 11977.23
     rank 17: 11977.21
     rank 18: 11977.21
     rank 19: 11977.26
  forward-compute:
     rank  0: 3464.57
     rank  1: 3460.63
     rank  2: 3464.47
     rank  3: 3486.17
     rank  4: 3142.14
     rank  5: 3138.44
     rank  6: 3140.14
     rank  7: 3163.99
     rank  8: 4220.43
     rank  9: 4217.28
     rank 10: 4223.24
     rank 11: 4235.79
     rank 12: 4396.94
     rank 13: 4394.91
     rank 14: 4397.54
     rank 15: 4402.81
     rank 16: 4374.27
     rank 17: 4375.94
     rank 18: 4384.12
     rank 19: 4384.53
  backward-compute:
     rank  0: 2954.26
     rank  1: 2955.05
     rank  2: 2951.36
     rank  3: 2955.18
     rank  4: 3663.01
     rank  5: 3662.21
     rank  6: 3673.62
     rank  7: 3671.12
     rank  8: 4639.13
     rank  9: 4640.62
     rank 10: 4647.16
     rank 11: 4641.03
     rank 12: 5040.02
     rank 13: 5038.07
     rank 14: 5036.88
     rank 15: 5039.79
     rank 16: 5188.21
     rank 17: 5186.20
     rank 18: 5192.02
     rank 19: 5188.22
  pure-backward-compute:
     rank  0: 2953.31
     rank  1: 2954.34
     rank  2: 2950.69
     rank  3: 2954.30
     rank  4: 3662.19
     rank  5: 3661.33
     rank  6: 3672.70
     rank  7: 3670.32
     rank  8: 4637.69
     rank  9: 4638.44
     rank 10: 4644.58
     rank 11: 4639.63
     rank 12: 5038.11
     rank 13: 5036.74
     rank 14: 5034.49
     rank 15: 5037.93
     rank 16: 5185.64
     rank 17: 5183.81
     rank 18: 5189.95
     rank 19: 5186.47
  batch-generator:
     rank  0: 66.66
     rank  1: 64.93
     rank  2: 69.46
     rank  3: 91.31
     rank  4: 55.87
     rank  5: 54.40
     rank  6: 55.45
     rank  7: 78.09
     rank  8: 71.46
     rank  9: 72.86
     rank 10: 89.18
     rank 11: 99.37
     rank 12: 72.61
     rank 13: 74.82
     rank 14: 91.76
     rank 15: 90.44
     rank 16: 68.73
     rank 17: 73.85
     rank 18: 84.34
     rank 19: 85.96
  forward-recv:
     rank  4: 258.66
     rank  5: 260.79
     rank  6: 259.34
     rank  7: 252.15
     rank  8: 452.32
     rank  9: 451.86
     rank 10: 452.48
     rank 11: 445.39
     rank 12: 715.03
     rank 13: 715.47
     rank 14: 715.36
     rank 15: 710.45
     rank 16: 992.12
     rank 17: 992.45
     rank 18: 992.26
     rank 19: 990.49
  forward-send:
     rank  0: 154.22
     rank  1: 156.76
     rank  2: 155.19
     rank  3: 133.68
     rank  4: 167.63
     rank  5: 168.44
     rank  6: 168.65
     rank  7: 153.92
     rank  8: 28.59
     rank  9: 29.91
     rank 10: 30.05
     rank 11: 22.97
     rank 12: 10.31
     rank 13: 10.73
     rank 14: 10.75
     rank 15: 9.00
  backward-recv:
     rank  0: 1727.44
     rank  1: 1727.79
     rank  2: 1728.86
     rank  3: 1727.27
     rank  4: 1127.65
     rank  5: 1127.75
     rank  6: 1127.68
     rank  7: 1126.84
     rank  8: 625.37
     rank  9: 625.23
     rank 10: 625.51
     rank 11: 625.52
     rank 12: 283.80
     rank 13: 283.75
     rank 14: 283.89
     rank 15: 284.41
  backward-send:
     rank  4: 41.85
     rank  5: 41.57
     rank  6: 41.33
     rank  7: 41.24
     rank  8: 31.51
     rank  9: 31.73
     rank 10: 30.20
     rank 11: 30.93
     rank 12: 21.33
     rank 13: 21.37
     rank 14: 20.62
     rank 15: 20.74
     rank 16: 10.81
     rank 17: 10.43
     rank 18: 10.70
     rank 19: 10.42
  forward-send-backward-recv:
     rank  0: 3658.22
     rank  1: 3658.58
     rank  2: 3659.93
     rank  3: 3658.06
     rank  4: 3235.99
     rank  5: 3240.31
     rank  6: 3228.79
     rank  7: 3228.06
     rank  8: 1358.49
     rank  9: 1358.03
     rank 10: 1347.94
     rank 11: 1358.77
     rank 12: 564.03
     rank 13: 566.38
     rank 14: 564.53
     rank 15: 564.67
  backward-send-forward-recv:
     rank  4: 135.68
     rank  5: 135.32
     rank  6: 135.25
     rank  7: 135.70
     rank  8: 177.03
     rank  9: 178.06
     rank 10: 170.16
     rank 11: 173.49
     rank 12: 201.20
     rank 13: 202.65
     rank 14: 198.13
     rank 15: 200.45
     rank 16: 343.07
     rank 17: 343.55
     rank 18: 332.61
     rank 19: 337.02
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.14
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.10
  all-grads-sync:
     rank  0: 2.13
     rank  1: 2.18
     rank  2: 2.25
     rank  3: 2.61
     rank  4: 2.42
     rank  5: 2.44
     rank  6: 2.45
     rank  7: 2.43
     rank  8: 2.40
     rank  9: 2.18
     rank 10: 2.22
     rank 11: 2.13
     rank 12: 2.54
     rank 13: 2.52
     rank 14: 2.75
     rank 15: 2.65
     rank 16: 2.89
     rank 17: 2.88
     rank 18: 2.93
     rank 19: 2.99
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.09
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.06
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.10
     rank 15: 0.10
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.70
     rank  1: 42.76
     rank  2: 42.90
     rank  3: 42.80
     rank  4: 48.37
     rank  5: 48.42
     rank  6: 48.38
     rank  7: 48.34
     rank  8: 43.12
     rank  9: 43.42
     rank 10: 43.52
     rank 11: 42.86
     rank 12: 51.95
     rank 13: 51.96
     rank 14: 52.04
     rank 15: 51.98
     rank 16: 58.53
     rank 17: 58.41
     rank 18: 58.39
     rank 19: 58.49
  optimizer:
     rank  0: 43.42
     rank  1: 43.48
     rank  2: 43.62
     rank  3: 43.52
     rank  4: 49.10
     rank  5: 49.15
     rank  6: 49.11
     rank  7: 49.07
     rank  8: 43.85
     rank  9: 44.15
     rank 10: 44.25
     rank 11: 43.59
     rank 12: 52.67
     rank 13: 52.69
     rank 14: 52.79
     rank 15: 52.71
     rank 16: 59.26
     rank 17: 59.14
     rank 18: 59.12
     rank 19: 59.22
 [2024-12-05 20:17:06] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 12122.7 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 5.803400E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12031.22
     rank  1: 12031.22
     rank  2: 12031.28
     rank  3: 12031.35
     rank  4: 12031.87
     rank  5: 12031.87
     rank  6: 12031.86
     rank  7: 12031.97
     rank  8: 12031.36
     rank  9: 12031.54
     rank 10: 12031.40
     rank 11: 12031.50
     rank 12: 12032.29
     rank 13: 12032.38
     rank 14: 12032.29
     rank 15: 12032.34
     rank 16: 12033.04
     rank 17: 12033.06
     rank 18: 12033.01
     rank 19: 12033.00
  forward-compute:
     rank  0: 3450.77
     rank  1: 3450.29
     rank  2: 3458.22
     rank  3: 3471.26
     rank  4: 3137.78
     rank  5: 3137.24
     rank  6: 3143.10
     rank  7: 3159.52
     rank  8: 4232.36
     rank  9: 4231.81
     rank 10: 4239.43
     rank 11: 4249.08
     rank 12: 4419.78
     rank 13: 4417.42
     rank 14: 4420.73
     rank 15: 4428.56
     rank 16: 4382.89
     rank 17: 4385.53
     rank 18: 4387.52
     rank 19: 4394.74
  backward-compute:
     rank  0: 2945.44
     rank  1: 2947.17
     rank  2: 2943.83
     rank  3: 2946.49
     rank  4: 3668.13
     rank  5: 3668.09
     rank  6: 3681.69
     rank  7: 3676.64
     rank  8: 4649.90
     rank  9: 4649.32
     rank 10: 4652.27
     rank 11: 4657.62
     rank 12: 5064.67
     rank 13: 5064.03
     rank 14: 5059.54
     rank 15: 5066.55
     rank 16: 5206.34
     rank 17: 5204.13
     rank 18: 5210.42
     rank 19: 5208.01
  pure-backward-compute:
     rank  0: 2944.67
     rank  1: 2946.38
     rank  2: 2943.09
     rank  3: 2945.56
     rank  4: 3667.17
     rank  5: 3667.23
     rank  6: 3680.89
     rank  7: 3675.85
     rank  8: 4648.23
     rank  9: 4647.46
     rank 10: 4649.83
     rank 11: 4656.24
     rank 12: 5063.09
     rank 13: 5062.69
     rank 14: 5056.26
     rank 15: 5064.97
     rank 16: 5204.02
     rank 17: 5201.83
     rank 18: 5208.49
     rank 19: 5206.02
  batch-generator:
     rank  0: 57.37
     rank  1: 60.82
     rank  2: 67.65
     rank  3: 85.95
     rank  4: 52.24
     rank  5: 55.54
     rank  6: 60.64
     rank  7: 75.67
     rank  8: 75.24
     rank  9: 78.12
     rank 10: 95.67
     rank 11: 103.17
     rank 12: 77.29
     rank 13: 80.79
     rank 14: 100.36
     rank 15: 102.85
     rank 16: 68.60
     rank 17: 74.58
     rank 18: 78.81
     rank 19: 87.37
  forward-recv:
     rank  4: 254.62
     rank  5: 255.54
     rank  6: 252.83
     rank  7: 247.80
     rank  8: 453.51
     rank  9: 452.62
     rank 10: 452.48
     rank 11: 446.25
     rank 12: 720.11
     rank 13: 721.74
     rank 14: 719.60
     rank 15: 717.18
     rank 16: 995.49
     rank 17: 995.04
     rank 18: 994.46
     rank 19: 994.00
  forward-send:
     rank  0: 158.12
     rank  1: 157.79
     rank  2: 151.81
     rank  3: 138.03
     rank  4: 175.33
     rank  5: 174.54
     rank  6: 171.80
     rank  7: 162.11
     rank  8: 28.46
     rank  9: 29.00
     rank 10: 27.24
     rank 11: 24.36
     rank 12: 10.76
     rank 13: 10.61
     rank 14: 10.41
     rank 15: 9.76
  backward-recv:
     rank  0: 1741.70
     rank  1: 1740.99
     rank  2: 1742.18
     rank  3: 1741.23
     rank  4: 1137.03
     rank  5: 1137.65
     rank  6: 1134.50
     rank  7: 1135.75
     rank  8: 628.60
     rank  9: 628.76
     rank 10: 629.37
     rank 11: 628.57
     rank 12: 284.60
     rank 13: 284.21
     rank 14: 284.48
     rank 15: 285.22
  backward-send:
     rank  4: 41.69
     rank  5: 41.09
     rank  6: 41.59
     rank  7: 41.47
     rank  8: 31.41
     rank  9: 31.60
     rank 10: 29.71
     rank 11: 30.81
     rank 12: 21.28
     rank 13: 21.20
     rank 14: 20.62
     rank 15: 20.28
     rank 16: 10.82
     rank 17: 10.42
     rank 18: 10.71
     rank 19: 10.47
  forward-send-backward-recv:
     rank  0: 3719.23
     rank  1: 3718.68
     rank  2: 3719.91
     rank  3: 3718.86
     rank  4: 3279.04
     rank  5: 3283.01
     rank  6: 3271.70
     rank  7: 3269.96
     rank  8: 1387.44
     rank  9: 1388.42
     rank 10: 1381.09
     rank 11: 1380.81
     rank 12: 564.04
     rank 13: 565.45
     rank 14: 564.32
     rank 15: 562.62
  backward-send-forward-recv:
     rank  4: 135.22
     rank  5: 134.42
     rank  6: 134.64
     rank  7: 135.10
     rank  8: 176.97
     rank  9: 177.68
     rank 10: 170.43
     rank 11: 171.42
     rank 12: 200.00
     rank 13: 201.31
     rank 14: 197.83
     rank 15: 194.51
     rank 16: 362.34
     rank 17: 362.61
     rank 18: 358.18
     rank 19: 353.81
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.10
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.07
  all-grads-sync:
     rank  0: 2.14
     rank  1: 2.15
     rank  2: 2.24
     rank  3: 2.23
     rank  4: 2.42
     rank  5: 2.40
     rank  6: 2.39
     rank  7: 2.44
     rank  8: 2.11
     rank  9: 2.44
     rank 10: 2.21
     rank 11: 2.15
     rank 12: 2.59
     rank 13: 2.58
     rank 14: 2.63
     rank 15: 2.50
     rank 16: 3.00
     rank 17: 3.03
     rank 18: 2.97
     rank 19: 2.92
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.08
     rank 18: 0.08
     rank 19: 0.08
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.72
     rank  1: 42.73
     rank  2: 42.84
     rank  3: 42.83
     rank  4: 48.40
     rank  5: 48.37
     rank  6: 48.33
     rank  7: 48.41
     rank  8: 42.80
     rank  9: 43.57
     rank 10: 43.41
     rank 11: 42.76
     rank 12: 51.89
     rank 13: 52.04
     rank 14: 52.09
     rank 15: 51.93
     rank 16: 58.39
     rank 17: 58.46
     rank 18: 58.33
     rank 19: 58.38
  optimizer:
     rank  0: 43.37
     rank  1: 43.37
     rank  2: 43.48
     rank  3: 43.48
     rank  4: 49.04
     rank  5: 49.01
     rank  6: 48.97
     rank  7: 49.05
     rank  8: 43.45
     rank  9: 44.21
     rank 10: 44.05
     rank 11: 43.41
     rank 12: 52.54
     rank 13: 52.69
     rank 14: 52.74
     rank 15: 52.57
     rank 16: 59.04
     rank 17: 59.11
     rank 18: 58.98
     rank 19: 59.03
 [2024-12-05 20:17:18] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 12146.1 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.714217E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12054.38
     rank  1: 12054.69
     rank  2: 12054.48
     rank  3: 12054.97
     rank  4: 12055.02
     rank  5: 12055.09
     rank  6: 12055.11
     rank  7: 12055.22
     rank  8: 12054.60
     rank  9: 12054.63
     rank 10: 12054.63
     rank 11: 12054.74
     rank 12: 12055.43
     rank 13: 12055.47
     rank 14: 12055.54
     rank 15: 12055.57
     rank 16: 12056.14
     rank 17: 12056.12
     rank 18: 12056.10
     rank 19: 12056.12
  forward-compute:
     rank  0: 3483.85
     rank  1: 3484.98
     rank  2: 3488.31
     rank  3: 3502.22
     rank  4: 3144.32
     rank  5: 3143.81
     rank  6: 3146.21
     rank  7: 3163.93
     rank  8: 4251.75
     rank  9: 4252.31
     rank 10: 4261.05
     rank 11: 4261.93
     rank 12: 4422.11
     rank 13: 4420.25
     rank 14: 4426.19
     rank 15: 4425.04
     rank 16: 4389.52
     rank 17: 4393.23
     rank 18: 4395.13
     rank 19: 4404.74
  backward-compute:
     rank  0: 2948.94
     rank  1: 2949.54
     rank  2: 2947.74
     rank  3: 2950.07
     rank  4: 3664.31
     rank  5: 3665.20
     rank  6: 3677.35
     rank  7: 3671.97
     rank  8: 4653.69
     rank  9: 4651.60
     rank 10: 4664.64
     rank 11: 4655.97
     rank 12: 5087.35
     rank 13: 5087.41
     rank 14: 5080.01
     rank 15: 5091.88
     rank 16: 5211.07
     rank 17: 5210.47
     rank 18: 5215.96
     rank 19: 5213.38
  pure-backward-compute:
     rank  0: 2948.05
     rank  1: 2948.84
     rank  2: 2947.02
     rank  3: 2949.22
     rank  4: 3663.57
     rank  5: 3664.43
     rank  6: 3676.56
     rank  7: 3671.11
     rank  8: 4651.67
     rank  9: 4650.12
     rank 10: 4662.48
     rank 11: 4654.75
     rank 12: 5086.06
     rank 13: 5086.08
     rank 14: 5078.06
     rank 15: 5090.63
     rank 16: 5208.50
     rank 17: 5208.24
     rank 18: 5214.29
     rank 19: 5211.46
  batch-generator:
     rank  0: 71.51
     rank  1: 70.72
     rank  2: 74.72
     rank  3: 92.20
     rank  4: 56.77
     rank  5: 58.20
     rank  6: 60.13
     rank  7: 76.69
     rank  8: 76.78
     rank  9: 81.13
     rank 10: 99.40
     rank 11: 98.64
     rank 12: 72.97
     rank 13: 75.11
     rank 14: 91.55
     rank 15: 86.82
     rank 16: 66.92
     rank 17: 74.11
     rank 18: 78.22
     rank 19: 89.13
  forward-recv:
     rank  4: 261.56
     rank  5: 262.70
     rank  6: 261.63
     rank  7: 252.46
     rank  8: 454.11
     rank  9: 454.24
     rank 10: 453.97
     rank 11: 450.20
     rank 12: 718.49
     rank 13: 717.95
     rank 14: 717.56
     rank 15: 716.18
     rank 16: 997.21
     rank 17: 997.01
     rank 18: 996.58
     rank 19: 996.43
  forward-send:
     rank  0: 139.07
     rank  1: 138.01
     rank  2: 135.34
     rank  3: 120.94
     rank  4: 168.81
     rank  5: 167.56
     rank  6: 166.54
     rank  7: 159.91
     rank  8: 28.53
     rank  9: 28.23
     rank 10: 28.34
     rank 11: 26.32
     rank 12: 10.83
     rank 13: 10.70
     rank 14: 10.58
     rank 15: 10.24
  backward-recv:
     rank  0: 1751.64
     rank  1: 1750.69
     rank  2: 1752.06
     rank  3: 1750.38
     rank  4: 1145.57
     rank  5: 1145.85
     rank  6: 1142.83
     rank  7: 1144.29
     rank  8: 631.21
     rank  9: 631.34
     rank 10: 630.87
     rank 11: 631.56
     rank 12: 281.18
     rank 13: 281.08
     rank 14: 282.56
     rank 15: 281.54
  backward-send:
     rank  4: 41.63
     rank  5: 40.99
     rank  6: 41.94
     rank  7: 41.28
     rank  8: 31.51
     rank  9: 31.65
     rank 10: 29.92
     rank 11: 30.87
     rank 12: 21.22
     rank 13: 21.22
     rank 14: 20.05
     rank 15: 20.81
     rank 16: 10.80
     rank 17: 10.44
     rank 18: 10.70
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 3714.41
     rank  1: 3714.50
     rank  2: 3715.83
     rank  3: 3715.35
     rank  4: 3290.83
     rank  5: 3293.55
     rank  6: 3283.75
     rank  7: 3283.80
     rank  8: 1384.05
     rank  9: 1388.31
     rank 10: 1370.36
     rank 11: 1384.69
     rank 12: 568.82
     rank 13: 571.43
     rank 14: 573.78
     rank 15: 567.65
  backward-send-forward-recv:
     rank  4: 135.31
     rank  5: 134.68
     rank  6: 135.00
     rank  7: 134.92
     rank  8: 176.30
     rank  9: 176.73
     rank 10: 166.96
     rank 11: 172.89
     rank 12: 197.88
     rank 13: 200.10
     rank 14: 193.19
     rank 15: 196.46
     rank 16: 368.00
     rank 17: 367.06
     rank 18: 362.88
     rank 19: 355.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.04
     rank  2: 0.02
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.09
     rank  2: 0.05
     rank  3: 0.13
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.14
     rank  1: 2.27
     rank  2: 2.23
     rank  3: 2.38
     rank  4: 2.39
     rank  5: 2.44
     rank  6: 2.49
     rank  7: 2.40
     rank  8: 2.21
     rank  9: 2.24
     rank 10: 2.15
     rank 11: 2.14
     rank 12: 2.60
     rank 13: 2.50
     rank 14: 2.86
     rank 15: 2.53
     rank 16: 2.87
     rank 17: 2.88
     rank 18: 2.85
     rank 19: 2.88
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.07
     rank  2: 0.06
     rank  3: 0.10
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.09
     rank  7: 0.04
     rank  8: 0.11
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.03
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.03
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.73
     rank  1: 43.00
     rank  2: 42.85
     rank  3: 43.24
     rank  4: 48.38
     rank  5: 48.40
     rank  6: 48.37
     rank  7: 48.34
     rank  8: 43.00
     rank  9: 43.37
     rank 10: 43.33
     rank 11: 42.80
     rank 12: 51.82
     rank 13: 51.85
     rank 14: 52.14
     rank 15: 51.87
     rank 16: 58.35
     rank 17: 58.31
     rank 18: 58.31
     rank 19: 58.41
  optimizer:
     rank  0: 43.75
     rank  1: 44.01
     rank  2: 43.87
     rank  3: 44.28
     rank  4: 49.40
     rank  5: 49.43
     rank  6: 49.40
     rank  7: 49.36
     rank  8: 44.02
     rank  9: 44.39
     rank 10: 44.36
     rank 11: 43.82
     rank 12: 52.85
     rank 13: 52.87
     rank 14: 53.16
     rank 15: 52.89
     rank 16: 59.37
     rank 17: 59.33
     rank 18: 59.34
     rank 19: 59.43
 [2024-12-05 20:17:30] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 12181.2 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.600568E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12089.83
     rank  1: 12089.14
     rank  2: 12089.19
     rank  3: 12089.17
     rank  4: 12090.17
     rank  5: 12089.77
     rank  6: 12089.80
     rank  7: 12089.76
     rank  8: 12089.68
     rank  9: 12089.29
     rank 10: 12089.35
     rank 11: 12089.29
     rank 12: 12090.55
     rank 13: 12090.11
     rank 14: 12090.23
     rank 15: 12090.20
     rank 16: 12090.91
     rank 17: 12090.91
     rank 18: 12090.88
     rank 19: 12090.89
  forward-compute:
     rank  0: 3479.20
     rank  1: 3481.15
     rank  2: 3482.90
     rank  3: 3498.96
     rank  4: 3141.72
     rank  5: 3141.35
     rank  6: 3142.91
     rank  7: 3161.44
     rank  8: 4239.49
     rank  9: 4239.08
     rank 10: 4247.45
     rank 11: 4250.82
     rank 12: 4425.62
     rank 13: 4425.03
     rank 14: 4428.90
     rank 15: 4428.20
     rank 16: 4390.53
     rank 17: 4394.33
     rank 18: 4396.57
     rank 19: 4405.00
  backward-compute:
     rank  0: 2951.69
     rank  1: 2950.91
     rank  2: 2951.59
     rank  3: 2952.70
     rank  4: 3669.65
     rank  5: 3669.53
     rank  6: 3683.12
     rank  7: 3675.96
     rank  8: 4655.39
     rank  9: 4655.59
     rank 10: 4666.23
     rank 11: 4656.48
     rank 12: 5119.75
     rank 13: 5120.27
     rank 14: 5112.97
     rank 15: 5124.53
     rank 16: 5225.82
     rank 17: 5225.01
     rank 18: 5229.58
     rank 19: 5227.48
  pure-backward-compute:
     rank  0: 2950.16
     rank  1: 2950.16
     rank  2: 2950.84
     rank  3: 2951.86
     rank  4: 3668.78
     rank  5: 3668.77
     rank  6: 3682.29
     rank  7: 3675.18
     rank  8: 4654.08
     rank  9: 4654.08
     rank 10: 4663.92
     rank 11: 4655.26
     rank 12: 5118.30
     rank 13: 5119.13
     rank 14: 5111.32
     rank 15: 5123.23
     rank 16: 5223.47
     rank 17: 5222.73
     rank 18: 5227.87
     rank 19: 5225.65
  batch-generator:
     rank  0: 87.08
     rank  1: 86.98
     rank  2: 85.89
     rank  3: 104.32
     rank  4: 50.87
     rank  5: 54.43
     rank  6: 55.31
     rank  7: 73.16
     rank  8: 70.85
     rank  9: 74.70
     rank 10: 92.51
     rank 11: 94.23
     rank 12: 71.00
     rank 13: 74.32
     rank 14: 88.48
     rank 15: 84.65
     rank 16: 66.92
     rank 17: 74.41
     rank 18: 78.71
     rank 19: 88.48
  forward-recv:
     rank  4: 259.67
     rank  5: 261.01
     rank  6: 259.43
     rank  7: 252.50
     rank  8: 454.05
     rank  9: 453.96
     rank 10: 453.88
     rank 11: 448.12
     rank 12: 719.54
     rank 13: 719.26
     rank 14: 718.90
     rank 15: 717.23
     rank 16: 998.36
     rank 17: 998.16
     rank 18: 997.87
     rank 19: 997.41
  forward-send:
     rank  0: 163.23
     rank  1: 162.35
     rank  2: 159.99
     rank  3: 144.26
     rank  4: 175.45
     rank  5: 174.09
     rank  6: 173.56
     rank  7: 164.13
     rank  8: 33.15
     rank  9: 33.13
     rank 10: 33.18
     rank 11: 30.63
     rank 12: 10.83
     rank 13: 10.62
     rank 14: 10.72
     rank 15: 10.04
  backward-recv:
     rank  0: 1742.53
     rank  1: 1743.85
     rank  2: 1743.88
     rank  3: 1744.15
     rank  4: 1138.29
     rank  5: 1138.85
     rank  6: 1135.87
     rank  7: 1137.03
     rank  8: 627.57
     rank  9: 627.50
     rank 10: 627.34
     rank 11: 627.94
     rank 12: 284.49
     rank 13: 284.47
     rank 14: 285.87
     rank 15: 284.69
  backward-send:
     rank  4: 41.79
     rank  5: 41.04
     rank  6: 41.36
     rank  7: 41.67
     rank  8: 31.40
     rank  9: 31.58
     rank 10: 29.73
     rank 11: 30.80
     rank 12: 21.26
     rank 13: 21.13
     rank 14: 19.60
     rank 15: 20.77
     rank 16: 10.87
     rank 17: 10.47
     rank 18: 10.81
     rank 19: 10.34
  forward-send-backward-recv:
     rank  0: 3734.40
     rank  1: 3734.04
     rank  2: 3734.88
     rank  3: 3734.19
     rank  4: 3325.79
     rank  5: 3329.47
     rank  6: 3318.37
     rank  7: 3320.20
     rank  8: 1427.33
     rank  9: 1428.84
     rank 10: 1413.90
     rank 11: 1429.02
     rank 12: 567.08
     rank 13: 568.12
     rank 14: 570.87
     rank 15: 564.56
  backward-send-forward-recv:
     rank  4: 135.48
     rank  5: 134.69
     rank  6: 135.49
     rank  7: 135.37
     rank  8: 177.51
     rank  9: 178.01
     rank 10: 168.37
     rank 11: 174.33
     rank 12: 194.96
     rank 13: 196.30
     rank 14: 191.33
     rank 15: 194.62
     rank 16: 388.93
     rank 17: 387.46
     rank 18: 383.53
     rank 19: 377.42
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.16
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.56
     rank  1: 2.14
     rank  2: 2.22
     rank  3: 2.19
     rank  4: 2.46
     rank  5: 2.40
     rank  6: 2.42
     rank  7: 2.40
     rank  8: 2.18
     rank  9: 2.20
     rank 10: 2.16
     rank 11: 2.14
     rank 12: 2.58
     rank 13: 2.50
     rank 14: 2.64
     rank 15: 2.52
     rank 16: 2.88
     rank 17: 2.87
     rank 18: 2.88
     rank 19: 2.86
  optimizer-copy-to-main-grad:
     rank  0: 0.13
     rank  1: 0.08
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.08
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.04
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.04
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.83
     rank  1: 42.73
     rank  2: 42.83
     rank  3: 42.75
     rank  4: 48.48
     rank  5: 48.34
     rank  6: 48.42
     rank  7: 48.35
     rank  8: 42.95
     rank  9: 43.37
     rank 10: 43.33
     rank 11: 42.77
     rank 12: 51.88
     rank 13: 51.94
     rank 14: 51.85
     rank 15: 52.01
     rank 16: 58.28
     rank 17: 58.27
     rank 18: 58.32
     rank 19: 58.39
  optimizer:
     rank  0: 44.08
     rank  1: 44.03
     rank  2: 44.14
     rank  3: 44.05
     rank  4: 49.78
     rank  5: 49.65
     rank  6: 49.72
     rank  7: 49.65
     rank  8: 44.26
     rank  9: 44.68
     rank 10: 44.65
     rank 11: 44.08
     rank 12: 53.19
     rank 13: 53.24
     rank 14: 53.16
     rank 15: 53.31
     rank 16: 59.58
     rank 17: 59.57
     rank 18: 59.63
     rank 19: 59.70
 [2024-12-05 20:17:42] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 12176.2 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.273447E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12085.55
     rank  1: 12085.03
     rank  2: 12085.13
     rank  3: 12085.14
     rank  4: 12085.94
     rank  5: 12085.70
     rank  6: 12085.74
     rank  7: 12085.76
     rank  8: 12085.51
     rank  9: 12085.37
     rank 10: 12085.31
     rank 11: 12085.29
     rank 12: 12086.31
     rank 13: 12086.11
     rank 14: 12086.15
     rank 15: 12086.11
     rank 16: 12086.90
     rank 17: 12086.85
     rank 18: 12086.83
     rank 19: 12086.83
  forward-compute:
     rank  0: 3462.52
     rank  1: 3465.96
     rank  2: 3464.22
     rank  3: 3486.18
     rank  4: 3140.63
     rank  5: 3142.65
     rank  6: 3142.44
     rank  7: 3164.79
     rank  8: 4242.23
     rank  9: 4243.20
     rank 10: 4247.57
     rank 11: 4258.96
     rank 12: 4423.33
     rank 13: 4422.89
     rank 14: 4425.70
     rank 15: 4427.81
     rank 16: 4405.45
     rank 17: 4410.29
     rank 18: 4410.80
     rank 19: 4421.37
  backward-compute:
     rank  0: 2946.30
     rank  1: 2946.06
     rank  2: 2946.97
     rank  3: 2947.25
     rank  4: 3677.69
     rank  5: 3677.82
     rank  6: 3689.44
     rank  7: 3684.79
     rank  8: 4661.93
     rank  9: 4662.18
     rank 10: 4669.63
     rank 11: 4663.74
     rank 12: 5116.31
     rank 13: 5117.22
     rank 14: 5109.33
     rank 15: 5121.50
     rank 16: 5221.52
     rank 17: 5220.66
     rank 18: 5226.92
     rank 19: 5224.18
  pure-backward-compute:
     rank  0: 2945.39
     rank  1: 2945.38
     rank  2: 2946.27
     rank  3: 2946.44
     rank  4: 3676.90
     rank  5: 3677.07
     rank  6: 3688.71
     rank  7: 3683.93
     rank  8: 4660.59
     rank  9: 4660.73
     rank 10: 4667.23
     rank 11: 4662.50
     rank 12: 5114.99
     rank 13: 5115.98
     rank 14: 5107.45
     rank 15: 5120.29
     rank 16: 5219.14
     rank 17: 5218.44
     rank 18: 5225.17
     rank 19: 5222.20
  batch-generator:
     rank  0: 67.12
     rank  1: 71.21
     rank  2: 68.75
     rank  3: 92.54
     rank  4: 51.61
     rank  5: 57.30
     rank  6: 56.42
     rank  7: 77.31
     rank  8: 70.06
     rank  9: 74.97
     rank 10: 89.64
     rank 11: 99.10
     rank 12: 71.01
     rank 13: 74.87
     rank 14: 87.62
     rank 15: 86.40
     rank 16: 67.45
     rank 17: 75.60
     rank 18: 78.57
     rank 19: 90.34
  forward-recv:
     rank  4: 259.84
     rank  5: 260.25
     rank  6: 260.10
     rank  7: 252.25
     rank  8: 455.94
     rank  9: 455.31
     rank 10: 455.71
     rank 11: 447.49
     rank 12: 722.88
     rank 13: 722.15
     rank 14: 722.15
     rank 15: 718.94
     rank 16: 1000.07
     rank 17: 999.85
     rank 18: 999.85
     rank 19: 998.90
  forward-send:
     rank  0: 162.20
     rank  1: 159.10
     rank  2: 159.55
     rank  3: 139.29
     rank  4: 176.99
     rank  5: 174.47
     rank  6: 175.07
     rank  7: 161.79
     rank  8: 31.58
     rank  9: 30.53
     rank 10: 31.10
     rank 11: 26.86
     rank 12: 10.74
     rank 13: 10.55
     rank 14: 10.74
     rank 15: 9.75
  backward-recv:
     rank  0: 1746.39
     rank  1: 1746.68
     rank  2: 1746.90
     rank  3: 1746.13
     rank  4: 1141.15
     rank  5: 1141.48
     rank  6: 1139.01
     rank  7: 1139.94
     rank  8: 629.63
     rank  9: 629.72
     rank 10: 629.35
     rank 11: 629.55
     rank 12: 282.77
     rank 13: 282.72
     rank 14: 284.14
     rank 15: 283.41
  backward-send:
     rank  4: 41.67
     rank  5: 40.87
     rank  6: 41.48
     rank  7: 41.41
     rank  8: 31.54
     rank  9: 31.68
     rank 10: 30.08
     rank 11: 30.84
     rank 12: 21.27
     rank 13: 21.15
     rank 14: 19.75
     rank 15: 20.71
     rank 16: 10.72
     rank 17: 10.32
     rank 18: 10.75
     rank 19: 10.51
  forward-send-backward-recv:
     rank  0: 3751.35
     rank  1: 3751.22
     rank  2: 3752.53
     rank  3: 3751.27
     rank  4: 3310.37
     rank  5: 3314.22
     rank  6: 3304.37
     rank  7: 3303.80
     rank  8: 1414.80
     rank  9: 1415.91
     rank 10: 1403.59
     rank 11: 1415.59
     rank 12: 568.75
     rank 13: 570.08
     rank 14: 572.55
     rank 15: 565.92
  backward-send-forward-recv:
     rank  4: 135.05
     rank  5: 134.02
     rank  6: 134.47
     rank  7: 134.78
     rank  8: 176.15
     rank  9: 176.68
     rank 10: 169.96
     rank 11: 172.21
     rank 12: 195.26
     rank 13: 196.56
     rank 14: 192.19
     rank 15: 194.69
     rank 16: 372.13
     rank 17: 369.98
     rank 18: 366.57
     rank 19: 359.25
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.11
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.41
     rank  1: 2.14
     rank  2: 2.23
     rank  3: 2.20
     rank  4: 2.40
     rank  5: 2.40
     rank  6: 2.40
     rank  7: 2.47
     rank  8: 2.27
     rank  9: 2.19
     rank 10: 2.24
     rank 11: 2.19
     rank 12: 2.55
     rank 13: 2.55
     rank 14: 2.54
     rank 15: 2.52
     rank 16: 2.97
     rank 17: 2.89
     rank 18: 2.83
     rank 19: 2.85
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.70
     rank  1: 42.69
     rank  2: 42.86
     rank  3: 42.76
     rank  4: 48.30
     rank  5: 48.35
     rank  6: 48.31
     rank  7: 48.40
     rank  8: 43.02
     rank  9: 43.43
     rank 10: 43.57
     rank 11: 42.84
     rank 12: 51.87
     rank 13: 51.88
     rank 14: 51.92
     rank 15: 51.82
     rank 16: 58.36
     rank 17: 58.34
     rank 18: 58.30
     rank 19: 58.31
  optimizer:
     rank  0: 43.35
     rank  1: 43.39
     rank  2: 43.56
     rank  3: 43.47
     rank  4: 49.00
     rank  5: 49.05
     rank  6: 49.01
     rank  7: 49.11
     rank  8: 43.73
     rank  9: 44.14
     rank 10: 44.28
     rank 11: 43.55
     rank 12: 52.58
     rank 13: 52.58
     rank 14: 52.63
     rank 15: 52.52
     rank 16: 59.06
     rank 17: 59.05
     rank 18: 59.01
     rank 19: 59.02
 [2024-12-05 20:17:54] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 12166.3 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.403949E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12075.30
     rank  1: 12075.29
     rank  2: 12075.31
     rank  3: 12075.32
     rank  4: 12075.96
     rank  5: 12075.96
     rank  6: 12075.95
     rank  7: 12075.96
     rank  8: 12075.46
     rank  9: 12075.47
     rank 10: 12075.48
     rank 11: 12075.48
     rank 12: 12076.35
     rank 13: 12076.31
     rank 14: 12076.35
     rank 15: 12076.35
     rank 16: 12077.06
     rank 17: 12077.08
     rank 18: 12077.04
     rank 19: 12077.08
  forward-compute:
     rank  0: 3428.95
     rank  1: 3429.59
     rank  2: 3433.17
     rank  3: 3447.11
     rank  4: 3140.29
     rank  5: 3139.56
     rank  6: 3143.89
     rank  7: 3159.63
     rank  8: 4246.70
     rank  9: 4246.11
     rank 10: 4254.84
     rank 11: 4257.48
     rank 12: 4425.54
     rank 13: 4424.10
     rank 14: 4429.07
     rank 15: 4428.36
     rank 16: 4405.90
     rank 17: 4411.08
     rank 18: 4412.78
     rank 19: 4422.63
  backward-compute:
     rank  0: 2955.33
     rank  1: 2954.78
     rank  2: 2955.81
     rank  3: 2956.15
     rank  4: 3672.28
     rank  5: 3672.10
     rank  6: 3685.55
     rank  7: 3678.90
     rank  8: 4670.45
     rank  9: 4669.98
     rank 10: 4679.48
     rank 11: 4672.58
     rank 12: 5104.65
     rank 13: 5105.03
     rank 14: 5098.10
     rank 15: 5109.47
     rank 16: 5240.20
     rank 17: 5238.85
     rank 18: 5243.78
     rank 19: 5241.09
  pure-backward-compute:
     rank  0: 2954.63
     rank  1: 2954.04
     rank  2: 2955.00
     rank  3: 2955.40
     rank  4: 3671.50
     rank  5: 3671.33
     rank  6: 3684.81
     rank  7: 3678.10
     rank  8: 4669.18
     rank  9: 4668.54
     rank 10: 4677.29
     rank 11: 4671.34
     rank 12: 5103.14
     rank 13: 5103.90
     rank 14: 5095.86
     rank 15: 5108.25
     rank 16: 5238.02
     rank 17: 5236.47
     rank 18: 5242.08
     rank 19: 5239.23
  batch-generator:
     rank  0: 54.03
     rank  1: 56.76
     rank  2: 65.05
     rank  3: 77.65
     rank  4: 50.51
     rank  5: 53.62
     rank  6: 57.01
     rank  7: 71.83
     rank  8: 69.43
     rank  9: 72.85
     rank 10: 91.41
     rank 11: 92.42
     rank 12: 78.23
     rank 13: 79.01
     rank 14: 93.96
     rank 15: 90.23
     rank 16: 66.35
     rank 17: 74.99
     rank 18: 78.76
     rank 19: 89.84
  forward-recv:
     rank  4: 256.62
     rank  5: 257.37
     rank  6: 255.85
     rank  7: 249.14
     rank  8: 452.87
     rank  9: 451.98
     rank 10: 452.23
     rank 11: 447.75
     rank 12: 717.08
     rank 13: 718.82
     rank 14: 717.24
     rank 15: 716.01
     rank 16: 996.77
     rank 17: 995.61
     rank 18: 995.41
     rank 19: 995.11
  forward-send:
     rank  0: 161.52
     rank  1: 161.42
     rank  2: 156.90
     rank  3: 144.18
     rank  4: 176.33
     rank  5: 176.24
     rank  6: 173.61
     rank  7: 166.34
     rank  8: 30.65
     rank  9: 31.16
     rank 10: 29.45
     rank 11: 27.47
     rank 12: 11.62
     rank 13: 10.14
     rank 14: 10.07
     rank 15: 9.78
  backward-recv:
     rank  0: 1752.52
     rank  1: 1751.66
     rank  2: 1752.24
     rank  3: 1752.19
     rank  4: 1146.12
     rank  5: 1146.56
     rank  6: 1143.59
     rank  7: 1145.13
     rank  8: 633.68
     rank  9: 633.57
     rank 10: 634.63
     rank 11: 633.56
     rank 12: 283.19
     rank 13: 283.23
     rank 14: 283.38
     rank 15: 284.59
  backward-send:
     rank  4: 41.63
     rank  5: 41.03
     rank  6: 41.91
     rank  7: 41.17
     rank  8: 31.53
     rank  9: 31.68
     rank 10: 30.05
     rank 11: 30.79
     rank 12: 21.24
     rank 13: 21.04
     rank 14: 20.35
     rank 15: 20.04
     rank 16: 10.85
     rank 17: 10.37
     rank 18: 10.80
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 3761.54
     rank  1: 3761.25
     rank  2: 3762.41
     rank  3: 3761.58
     rank  4: 3302.90
     rank  5: 3306.87
     rank  6: 3295.15
     rank  7: 3297.07
     rank  8: 1388.84
     rank  9: 1391.63
     rank 10: 1376.41
     rank 11: 1389.83
     rank 12: 568.42
     rank 13: 570.07
     rank 14: 572.55
     rank 15: 566.27
  backward-send-forward-recv:
     rank  4: 135.05
     rank  5: 134.09
     rank  6: 134.61
     rank  7: 134.50
     rank  8: 176.98
     rank  9: 177.40
     rank 10: 168.87
     rank 11: 173.78
     rank 12: 194.76
     rank 13: 196.18
     rank 14: 191.31
     rank 15: 193.87
     rank 16: 343.95
     rank 17: 341.95
     rank 18: 338.15
     rank 19: 330.90
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.13
     rank  1: 2.14
     rank  2: 2.14
     rank  3: 2.20
     rank  4: 2.39
     rank  5: 2.40
     rank  6: 2.40
     rank  7: 2.40
     rank  8: 2.11
     rank  9: 2.23
     rank 10: 2.15
     rank 11: 2.20
     rank 12: 2.63
     rank 13: 2.48
     rank 14: 2.62
     rank 15: 2.56
     rank 16: 2.85
     rank 17: 2.87
     rank 18: 2.86
     rank 19: 2.88
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.07
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.66
     rank  1: 42.71
     rank  2: 42.78
     rank  3: 42.83
     rank  4: 48.34
     rank  5: 48.39
     rank  6: 48.35
     rank  7: 48.31
     rank  8: 42.72
     rank  9: 43.31
     rank 10: 43.30
     rank 11: 42.77
     rank 12: 51.90
     rank 13: 51.86
     rank 14: 51.98
     rank 15: 51.88
     rank 16: 58.27
     rank 17: 58.27
     rank 18: 58.31
     rank 19: 58.33
  optimizer:
     rank  0: 43.36
     rank  1: 43.40
     rank  2: 43.46
     rank  3: 43.51
     rank  4: 49.06
     rank  5: 49.11
     rank  6: 49.07
     rank  7: 49.02
     rank  8: 43.44
     rank  9: 44.03
     rank 10: 44.01
     rank 11: 43.49
     rank 12: 52.61
     rank 13: 52.57
     rank 14: 52.69
     rank 15: 52.60
     rank 16: 59.00
     rank 17: 58.99
     rank 18: 59.03
     rank 19: 59.04
 [2024-12-05 20:18:07] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 12210.1 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.093220E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12116.76
     rank  1: 12116.80
     rank  2: 12116.80
     rank  3: 12116.80
     rank  4: 12117.39
     rank  5: 12117.44
     rank  6: 12117.42
     rank  7: 12117.44
     rank  8: 12116.91
     rank  9: 12116.99
     rank 10: 12116.93
     rank 11: 12116.95
     rank 12: 12117.72
     rank 13: 12117.77
     rank 14: 12117.76
     rank 15: 12117.79
     rank 16: 12118.52
     rank 17: 12118.52
     rank 18: 12118.51
     rank 19: 12118.57
  forward-compute:
     rank  0: 3518.09
     rank  1: 3519.48
     rank  2: 3520.27
     rank  3: 3534.59
     rank  4: 3139.40
     rank  5: 3140.29
     rank  6: 3142.40
     rank  7: 3158.15
     rank  8: 4254.86
     rank  9: 4254.52
     rank 10: 4260.68
     rank 11: 4262.38
     rank 12: 4433.28
     rank 13: 4432.43
     rank 14: 4436.16
     rank 15: 4434.63
     rank 16: 4407.79
     rank 17: 4412.39
     rank 18: 4413.87
     rank 19: 4422.74
  backward-compute:
     rank  0: 2953.11
     rank  1: 2953.81
     rank  2: 2956.12
     rank  3: 2956.26
     rank  4: 3685.19
     rank  5: 3685.04
     rank  6: 3697.32
     rank  7: 3692.69
     rank  8: 4682.73
     rank  9: 4682.57
     rank 10: 4693.04
     rank 11: 4684.44
     rank 12: 5133.54
     rank 13: 5133.70
     rank 14: 5127.21
     rank 15: 5137.52
     rank 16: 5245.12
     rank 17: 5243.39
     rank 18: 5248.21
     rank 19: 5245.64
  pure-backward-compute:
     rank  0: 2952.39
     rank  1: 2953.11
     rank  2: 2955.41
     rank  3: 2955.25
     rank  4: 3684.42
     rank  5: 3684.28
     rank  6: 3696.55
     rank  7: 3691.90
     rank  8: 4681.47
     rank  9: 4681.16
     rank 10: 4690.76
     rank 11: 4683.23
     rank 12: 5132.13
     rank 13: 5132.25
     rank 14: 5125.32
     rank 15: 5136.29
     rank 16: 5242.64
     rank 17: 5241.20
     rank 18: 5246.49
     rank 19: 5243.92
  batch-generator:
     rank  0: 52.38
     rank  1: 58.43
     rank  2: 59.76
     rank  3: 85.42
     rank  4: 51.01
     rank  5: 55.51
     rank  6: 56.94
     rank  7: 71.55
     rank  8: 69.47
     rank  9: 71.88
     rank 10: 88.49
     rank 11: 88.05
     rank 12: 69.35
     rank 13: 72.68
     rank 14: 86.09
     rank 15: 81.75
     rank 16: 67.14
     rank 17: 75.21
     rank 18: 78.97
     rank 19: 89.00
  forward-recv:
     rank  4: 258.47
     rank  5: 259.92
     rank  6: 258.92
     rank  7: 250.27
     rank  8: 453.30
     rank  9: 453.19
     rank 10: 453.58
     rank 11: 449.95
     rank 12: 717.62
     rank 13: 717.34
     rank 14: 717.25
     rank 15: 716.22
     rank 16: 992.31
     rank 17: 992.11
     rank 18: 991.76
     rank 19: 991.71
  forward-send:
     rank  0: 160.30
     rank  1: 159.24
     rank  2: 157.58
     rank  3: 143.76
     rank  4: 173.64
     rank  5: 172.05
     rank  6: 171.72
     rank  7: 165.70
     rank  8: 26.99
     rank  9: 26.60
     rank 10: 26.68
     rank 11: 25.12
     rank 12: 10.82
     rank 13: 10.68
     rank 14: 10.61
     rank 15: 10.46
  backward-recv:
     rank  0: 1765.44
     rank  1: 1765.73
     rank  2: 1764.39
     rank  3: 1764.96
     rank  4: 1150.54
     rank  5: 1151.76
     rank  6: 1150.25
     rank  7: 1149.90
     rank  8: 640.89
     rank  9: 641.57
     rank 10: 640.48
     rank 11: 641.22
     rank 12: 285.82
     rank 13: 285.92
     rank 14: 285.93
     rank 15: 286.19
  backward-send:
     rank  4: 42.06
     rank  5: 41.05
     rank  6: 40.16
     rank  7: 41.24
     rank  8: 31.47
     rank  9: 31.64
     rank 10: 29.89
     rank 11: 30.78
     rank 12: 21.05
     rank 13: 21.12
     rank 14: 19.34
     rank 15: 20.57
     rank 16: 10.56
     rank 17: 10.57
     rank 18: 9.61
     rank 19: 10.27
  forward-send-backward-recv:
     rank  0: 3704.11
     rank  1: 3701.53
     rank  2: 3703.44
     rank  3: 3702.44
     rank  4: 3327.59
     rank  5: 3331.44
     rank  6: 3320.87
     rank  7: 3321.82
     rank  8: 1406.24
     rank  9: 1407.61
     rank 10: 1393.94
     rank 11: 1406.96
     rank 12: 570.08
     rank 13: 573.37
     rank 14: 576.19
     rank 15: 569.80
  backward-send-forward-recv:
     rank  4: 135.64
     rank  5: 133.73
     rank  6: 133.87
     rank  7: 133.90
     rank  8: 175.39
     rank  9: 176.03
     rank 10: 168.01
     rank 11: 172.67
     rank 12: 193.69
     rank 13: 194.92
     rank 14: 190.00
     rank 15: 193.17
     rank 16: 379.53
     rank 17: 377.49
     rank 18: 373.63
     rank 19: 367.32
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.09
  all-grads-sync:
     rank  0: 2.18
     rank  1: 2.19
     rank  2: 2.18
     rank  3: 2.15
     rank  4: 2.39
     rank  5: 2.40
     rank  6: 2.39
     rank  7: 2.40
     rank  8: 2.17
     rank  9: 2.20
     rank 10: 2.17
     rank 11: 2.19
     rank 12: 2.48
     rank 13: 2.58
     rank 14: 2.51
     rank 15: 2.56
     rank 16: 2.88
     rank 17: 2.88
     rank 18: 2.83
     rank 19: 2.98
  optimizer-copy-to-main-grad:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.05
     rank 13: 0.09
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.68
     rank  1: 42.76
     rank  2: 42.81
     rank  3: 42.78
     rank  4: 48.36
     rank  5: 48.33
     rank  6: 48.33
     rank  7: 48.35
     rank  8: 42.82
     rank  9: 43.43
     rank 10: 43.36
     rank 11: 42.84
     rank 12: 51.68
     rank 13: 51.92
     rank 14: 51.73
     rank 15: 52.04
     rank 16: 58.33
     rank 17: 58.26
     rank 18: 58.27
     rank 19: 58.39
  optimizer:
     rank  0: 43.34
     rank  1: 43.42
     rank  2: 43.47
     rank  3: 43.44
     rank  4: 49.02
     rank  5: 49.00
     rank  6: 48.99
     rank  7: 49.01
     rank  8: 43.50
     rank  9: 44.09
     rank 10: 44.03
     rank 11: 43.51
     rank 12: 52.35
     rank 13: 52.58
     rank 14: 52.40
     rank 15: 52.71
     rank 16: 59.00
     rank 17: 58.93
     rank 18: 58.93
     rank 19: 59.05
 [2024-12-05 20:18:19] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 12224.1 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.137741E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 12133.03
     rank  1: 12133.00
     rank  2: 12133.25
     rank  3: 12133.23
     rank  4: 12133.67
     rank  5: 12133.64
     rank  6: 12133.78
     rank  7: 12133.70
     rank  8: 12133.18
     rank  9: 12133.18
     rank 10: 12133.30
     rank 11: 12133.26
     rank 12: 12134.03
     rank 13: 12134.02
     rank 14: 12134.13
     rank 15: 12134.10
     rank 16: 12134.79
     rank 17: 12134.79
     rank 18: 12134.81
     rank 19: 12134.78
  forward-compute:
     rank  0: 3428.68
     rank  1: 3429.66
     rank  2: 3430.86
     rank  3: 3446.61
     rank  4: 3140.24
     rank  5: 3139.68
     rank  6: 3144.43
     rank  7: 3158.91
     rank  8: 4255.35
     rank  9: 4254.86
     rank 10: 4259.94
     rank 11: 4264.51
     rank 12: 4446.21
     rank 13: 4445.12
     rank 14: 4446.96
     rank 15: 4447.23
     rank 16: 4410.78
     rank 17: 4414.03
     rank 18: 4417.02
     rank 19: 4424.76
  backward-compute:
     rank  0: 2952.48
     rank  1: 2952.16
     rank  2: 2954.28
     rank  3: 2953.64
     rank  4: 3691.01
     rank  5: 3691.01
     rank  6: 3702.39
     rank  7: 3699.16
     rank  8: 4693.27
     rank  9: 4694.61
     rank 10: 4701.04
     rank 11: 4694.59
     rank 12: 5140.92
     rank 13: 5140.09
     rank 14: 5133.49
     rank 15: 5143.81
     rank 16: 5247.01
     rank 17: 5245.67
     rank 18: 5252.04
     rank 19: 5248.90
  pure-backward-compute:
     rank  0: 2951.79
     rank  1: 2951.41
     rank  2: 2953.40
     rank  3: 2952.93
     rank  4: 3690.24
     rank  5: 3690.17
     rank  6: 3701.64
     rank  7: 3698.17
     rank  8: 4692.04
     rank  9: 4693.22
     rank 10: 4698.73
     rank 11: 4693.34
     rank 12: 5139.48
     rank 13: 5138.98
     rank 14: 5131.71
     rank 15: 5142.62
     rank 16: 5244.70
     rank 17: 5243.46
     rank 18: 5249.92
     rank 19: 5246.99
  batch-generator:
     rank  0: 55.98
     rank  1: 60.90
     rank  2: 62.20
     rank  3: 74.52
     rank  4: 50.42
     rank  5: 53.70
     rank  6: 56.07
     rank  7: 70.98
     rank  8: 67.69
     rank  9: 71.08
     rank 10: 86.63
     rank 11: 89.05
     rank 12: 68.75
     rank 13: 72.00
     rank 14: 83.81
     rank 15: 81.26
     rank 16: 66.19
     rank 17: 72.99
     rank 18: 78.03
     rank 19: 87.20
  forward-recv:
     rank  4: 259.19
     rank  5: 260.37
     rank  6: 259.45
     rank  7: 250.71
     rank  8: 454.25
     rank  9: 454.05
     rank 10: 454.32
     rank 11: 449.89
     rank 12: 718.94
     rank 13: 718.94
     rank 14: 718.63
     rank 15: 717.41
     rank 16: 994.68
     rank 17: 994.49
     rank 18: 994.12
     rank 19: 994.06
  forward-send:
     rank  0: 163.55
     rank  1: 162.31
     rank  2: 160.31
     rank  3: 146.30
     rank  4: 174.34
     rank  5: 173.02
     rank  6: 172.30
     rank  7: 165.84
     rank  8: 28.18
     rank  9: 27.79
     rank 10: 27.75
     rank 11: 26.17
     rank 12: 10.83
     rank 13: 10.66
     rank 14: 10.59
     rank 15: 10.40
  backward-recv:
     rank  0: 1766.03
     rank  1: 1765.83
     rank  2: 1764.53
     rank  3: 1765.61
     rank  4: 1151.73
     rank  5: 1152.27
     rank  6: 1149.27
     rank  7: 1150.36
     rank  8: 635.91
     rank  9: 634.66
     rank 10: 634.76
     rank 11: 635.39
     rank 12: 277.92
     rank 13: 279.20
     rank 14: 280.84
     rank 15: 279.88
  backward-send:
     rank  4: 41.74
     rank  5: 40.90
     rank  6: 41.15
     rank  7: 41.41
     rank  8: 31.46
     rank  9: 31.64
     rank 10: 29.65
     rank 11: 30.92
     rank 12: 22.50
     rank 13: 21.09
     rank 14: 19.41
     rank 15: 20.87
     rank 16: 10.68
     rank 17: 10.35
     rank 18: 10.66
     rank 19: 10.32
  forward-send-backward-recv:
     rank  0: 3806.96
     rank  1: 3807.09
     rank  2: 3807.93
     rank  3: 3806.67
     rank  4: 3335.79
     rank  5: 3339.58
     rank  6: 3327.64
     rank  7: 3329.04
     rank  8: 1414.17
     rank  9: 1415.85
     rank 10: 1405.05
     rank 11: 1415.82
     rank 12: 570.82
     rank 13: 573.29
     rank 14: 575.76
     rank 15: 570.62
  backward-send-forward-recv:
     rank  4: 135.26
     rank  5: 134.64
     rank  6: 134.22
     rank  7: 134.08
     rank  8: 175.77
     rank  9: 176.44
     rank 10: 169.33
     rank 11: 172.34
     rank 12: 194.43
     rank 13: 195.74
     rank 14: 192.77
     rank 15: 194.31
     rank 16: 389.53
     rank 17: 388.67
     rank 18: 383.05
     rank 19: 377.86
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.08
     rank  3: 0.08
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 2.17
     rank  1: 2.16
     rank  2: 2.23
     rank  3: 2.22
     rank  4: 2.42
     rank  5: 2.43
     rank  6: 2.41
     rank  7: 2.52
     rank  8: 2.14
     rank  9: 2.14
     rank 10: 2.21
     rank 11: 2.21
     rank 12: 2.50
     rank 13: 2.47
     rank 14: 2.50
     rank 15: 2.57
     rank 16: 2.87
     rank 17: 2.87
     rank 18: 2.93
     rank 19: 2.86
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.04
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 42.74
     rank  1: 42.70
     rank  2: 42.91
     rank  3: 42.91
     rank  4: 48.37
     rank  5: 48.39
     rank  6: 48.37
     rank  7: 48.42
     rank  8: 42.76
     rank  9: 43.34
     rank 10: 43.41
     rank 11: 42.85
     rank 12: 51.76
     rank 13: 51.78
     rank 14: 51.86
     rank 15: 51.89
     rank 16: 58.20
     rank 17: 58.26
     rank 18: 58.36
     rank 19: 58.27
  optimizer:
     rank  0: 43.43
     rank  1: 43.38
     rank  2: 43.61
     rank  3: 43.59
     rank  4: 49.06
     rank  5: 49.08
     rank  6: 49.06
     rank  7: 49.11
     rank  8: 43.45
     rank  9: 44.03
     rank 10: 44.10
     rank 11: 43.54
     rank 12: 52.45
     rank 13: 52.47
     rank 14: 52.55
     rank 15: 52.58
     rank 16: 58.89
     rank 17: 58.96
     rank 18: 59.04
     rank 19: 58.96
