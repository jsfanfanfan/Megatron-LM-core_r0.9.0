examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-05 19:20:41,340] torch.distributed.run: [WARNING] 
[2024-12-05 19:20:41,340] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 19:20:41,340] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 19:20:41,340] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.output_layer.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 108.16
     rank  1: 107.67
     rank  2: 73.55
     rank  3: 73.71
     rank  4: 47.05
     rank  5: 47.19
     rank  6: 46.93
     rank  7: 34.78
     rank  8: 34.15
     rank  9: 44.76
     rank 10: 42.60
     rank 11: 48.06
     rank 12: 47.07
     rank 13: 31.03
     rank 14: 47.10
     rank 15: 46.92
     rank 16: 30.50
     rank 17: 35.50
     rank 18: 47.99
     rank 19: 47.97
  train/valid/test-data-iterators-setup:
     rank  0: 1096.70
     rank  1: 1096.76
     rank  2: 1096.77
     rank  3: 1096.61
     rank  4: 1209.62
     rank  5: 1209.72
     rank  6: 1209.05
     rank  7: 1210.83
     rank  8: 1469.35
     rank  9: 1470.09
     rank 10: 1469.56
     rank 11: 1469.43
     rank 12: 1469.42
     rank 13: 1470.33
     rank 14: 1469.45
     rank 15: 1469.50
     rank 16: 1469.50
     rank 17: 1469.76
     rank 18: 1469.98
     rank 19: 1469.49
 [2024-12-05 19:21:26] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21653.1 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.108734E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10110.0 | max reserved: 10110.0

[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10086.0 | max reserved: 10086.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10058.0 | max reserved: 10058.0
times across ranks (ms):
  forward-backward:
     rank  0: 21585.75
     rank  1: 21585.86
     rank  2: 21585.90
     rank  3: 21585.57
     rank  4: 21590.04
     rank  5: 21589.96
     rank  6: 21589.71
     rank  7: 21589.84
     rank  8: 21589.91
     rank  9: 21589.68
     rank 10: 21589.49
     rank 11: 21589.70
     rank 12: 21590.04
     rank 13: 21589.99
     rank 14: 21590.21
     rank 15: 21589.75
     rank 16: 21589.57
     rank 17: 21589.47
     rank 18: 21589.38
     rank 19: 21589.38
  forward-compute:
     rank  0: 5123.30
     rank  1: 5139.51
     rank  2: 5135.81
     rank  3: 5106.08
     rank  4: 5527.64
     rank  5: 5542.17
     rank  6: 5536.02
     rank  7: 5510.32
     rank  8: 5170.52
     rank  9: 5178.73
     rank 10: 5167.03
     rank 11: 5174.71
     rank 12: 4933.41
     rank 13: 4938.29
     rank 14: 4931.82
     rank 15: 4938.14
     rank 16: 5140.16
     rank 17: 5146.42
     rank 18: 5142.72
     rank 19: 5154.38
  backward-compute:
     rank  0: 53.73
     rank  1: 51.56
     rank  2: 46.92
     rank  3: 53.88
     rank  4: 3336.84
     rank  5: 3339.17
     rank  6: 3336.44
     rank  7: 3340.70
     rank  8: 3306.31
     rank  9: 3300.23
     rank 10: 3301.73
     rank 11: 3305.64
     rank 12: 3293.41
     rank 13: 3294.40
     rank 14: 3295.63
     rank 15: 3303.77
     rank 16: 3523.74
     rank 17: 3520.17
     rank 18: 3524.45
     rank 19: 3524.45
  pure-backward-compute:
     rank  0: 53.02
     rank  1: 50.69
     rank  2: 46.23
     rank  3: 53.19
     rank  4: 3335.70
     rank  5: 3338.04
     rank  6: 3335.47
     rank  7: 3339.71
     rank  8: 3302.60
     rank  9: 3298.82
     rank 10: 3300.31
     rank 11: 3303.92
     rank 12: 3291.69
     rank 13: 3292.25
     rank 14: 3294.54
     rank 15: 3302.82
     rank 16: 3521.13
     rank 17: 3517.58
     rank 18: 3522.15
     rank 19: 3522.72
  batch-generator:
     rank  0: 1145.45
     rank  1: 1165.70
     rank  2: 1175.47
     rank  3: 1150.46
     rank  4: 1289.01
     rank  5: 1312.97
     rank  6: 1308.32
     rank  7: 1286.99
     rank  8: 1151.33
     rank  9: 1162.27
     rank 10: 1149.07
     rank 11: 1155.82
     rank 12: 1070.20
     rank 13: 1078.50
     rank 14: 1073.87
     rank 15: 1077.23
     rank 16: 914.30
     rank 17: 919.68
     rank 18: 918.85
     rank 19: 931.40
  forward-recv:
     rank  4: 4239.19
     rank  5: 4229.83
     rank  6: 4233.08
     rank  7: 4253.43
     rank  8: 7069.52
     rank  9: 7070.28
     rank 10: 7075.52
     rank 11: 7071.77
     rank 12: 9653.94
     rank 13: 9650.09
     rank 14: 9652.73
     rank 15: 9654.24
     rank 16: 12006.15
     rank 17: 12003.45
     rank 18: 12007.34
     rank 19: 12004.63
  forward-send:
     rank  0: 7749.50
     rank  1: 7734.60
     rank  2: 7749.46
     rank  3: 7769.67
     rank  4: 4598.12
     rank  5: 4592.35
     rank  6: 4605.06
     rank  7: 4601.94
     rank  8: 2217.63
     rank  9: 2209.96
     rank 10: 2217.78
     rank 11: 2215.90
     rank 12: 34.46
     rank 13: 31.49
     rank 14: 35.66
     rank 15: 33.07
  backward-recv:
     rank  0: 1655.16
     rank  1: 1655.80
     rank  2: 1657.33
     rank  3: 1655.69
     rank  4: 613.77
     rank  5: 613.39
     rank  6: 613.78
     rank  7: 612.66
     rank  8: 404.15
     rank  9: 406.74
     rank 10: 405.66
     rank 11: 404.36
     rank 12: 205.60
     rank 13: 205.70
     rank 14: 205.88
     rank 15: 205.92
  backward-send:
     rank  4: 22.48
     rank  5: 21.45
     rank  6: 21.36
     rank  7: 22.59
     rank  8: 30.53
     rank  9: 29.53
     rank 10: 30.27
     rank 11: 31.00
     rank 12: 20.95
     rank 13: 20.93
     rank 14: 20.46
     rank 15: 19.94
     rank 16: 10.55
     rank 17: 10.57
     rank 18: 10.25
     rank 19: 9.59
  forward-send-backward-recv:
     rank  0: 6950.92
     rank  1: 6950.79
     rank  2: 6942.63
     rank  3: 6948.01
     rank  4: 3097.33
     rank  5: 3095.56
     rank  6: 3095.29
     rank  7: 3095.98
     rank  8: 2905.70
     rank  9: 2909.05
     rank 10: 2906.51
     rank 11: 2903.32
     rank 12: 2735.48
     rank 13: 2732.62
     rank 14: 2737.26
     rank 15: 2729.96
  backward-send-forward-recv:
     rank  4: 87.73
     rank  5: 88.37
     rank  6: 82.55
     rank  7: 85.35
     rank  8: 203.38
     rank  9: 203.45
     rank 10: 203.78
     rank 11: 201.61
     rank 12: 215.30
     rank 13: 216.99
     rank 14: 215.47
     rank 15: 212.29
     rank 16: 198.10
     rank 17: 198.28
     rank 18: 195.46
     rank 19: 187.37
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.07
     rank  3: 0.04
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.11
     rank 14: 0.08
     rank 15: 0.05
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.11
     rank  1: 0.14
     rank  2: 0.13
     rank  3: 0.09
     rank  4: 0.06
     rank  5: 0.06
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.07
     rank  9: 0.05
     rank 10: 0.03
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.08
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 40.75
     rank  1: 46.44
     rank  2: 43.04
     rank  3: 40.64
     rank  4: 44.75
     rank  5: 44.93
     rank  6: 40.10
     rank  7: 31.47
     rank  8: 46.01
     rank  9: 45.79
     rank 10: 40.05
     rank 11: 44.95
     rank 12: 46.94
     rank 13: 46.49
     rank 14: 46.52
     rank 15: 46.41
     rank 16: 2.71
     rank 17: 2.62
     rank 18: 2.75
     rank 19: 2.73
  optimizer-copy-to-main-grad:
     rank  0: 0.05
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.04
     rank  4: 0.13
     rank  5: 0.11
     rank  6: 0.07
     rank  7: 0.10
     rank  8: 0.14
     rank  9: 0.08
     rank 10: 0.08
     rank 11: 0.09
     rank 12: 0.09
     rank 13: 0.25
     rank 14: 0.18
     rank 15: 0.06
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.07
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.32
     rank  1: 2.31
     rank  2: 2.48
     rank  3: 2.34
     rank  4: 55.91
     rank  5: 55.90
     rank  6: 55.89
     rank  7: 56.22
     rank  8: 52.51
     rank  9: 52.89
     rank 10: 52.48
     rank 11: 52.52
     rank 12: 53.24
     rank 13: 53.49
     rank 14: 53.68
     rank 15: 53.07
     rank 16: 56.56
     rank 17: 55.41
     rank 18: 55.85
     rank 19: 55.57
  optimizer:
     rank  0: 4.41
     rank  1: 4.38
     rank  2: 4.59
     rank  3: 4.44
     rank  4: 58.01
     rank  5: 58.00
     rank  6: 58.00
     rank  7: 58.34
     rank  8: 54.61
     rank  9: 55.00
     rank 10: 54.59
     rank 11: 54.63
     rank 12: 55.35
     rank 13: 55.59
     rank 14: 55.78
     rank 15: 55.18
     rank 16: 58.68
     rank 17: 57.54
     rank 18: 57.96
     rank 19: 57.68
 [2024-12-05 19:21:34] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7972.6 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.285778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7888.11
     rank  1: 7888.16
     rank  2: 7888.19
     rank  3: 7888.16
     rank  4: 7892.29
     rank  5: 7892.34
     rank  6: 7892.38
     rank  7: 7892.37
     rank  8: 7892.05
     rank  9: 7892.08
     rank 10: 7892.15
     rank 11: 7892.06
     rank 12: 7892.09
     rank 13: 7892.15
     rank 14: 7892.16
     rank 15: 7892.10
     rank 16: 7892.25
     rank 17: 7892.28
     rank 18: 7892.37
     rank 19: 7892.31
  forward-compute:
     rank  0: 936.42
     rank  1: 938.72
     rank  2: 945.31
     rank  3: 934.70
     rank  4: 2872.44
     rank  5: 2874.98
     rank  6: 2875.60
     rank  7: 2868.45
     rank  8: 2765.85
     rank  9: 2773.00
     rank 10: 2767.19
     rank 11: 2767.52
     rank 12: 2758.53
     rank 13: 2758.13
     rank 14: 2755.67
     rank 15: 2764.43
     rank 16: 2923.14
     rank 17: 2924.29
     rank 18: 2926.27
     rank 19: 2928.12
  backward-compute:
     rank  0: 51.35
     rank  1: 51.20
     rank  2: 43.68
     rank  3: 50.51
     rank  4: 3317.55
     rank  5: 3319.32
     rank  6: 3315.61
     rank  7: 3317.85
     rank  8: 3279.95
     rank  9: 3278.69
     rank 10: 3279.32
     rank 11: 3286.41
     rank 12: 3270.67
     rank 13: 3271.86
     rank 14: 3276.19
     rank 15: 3273.97
     rank 16: 3506.08
     rank 17: 3504.85
     rank 18: 3507.83
     rank 19: 3508.08
  pure-backward-compute:
     rank  0: 50.70
     rank  1: 50.53
     rank  2: 43.12
     rank  3: 49.93
     rank  4: 3316.62
     rank  5: 3318.35
     rank  6: 3314.86
     rank  7: 3316.74
     rank  8: 3278.25
     rank  9: 3277.69
     rank 10: 3277.80
     rank 11: 3284.34
     rank 12: 3269.39
     rank 13: 3270.19
     rank 14: 3274.94
     rank 15: 3273.21
     rank 16: 3503.64
     rank 17: 3502.45
     rank 18: 3505.76
     rank 19: 3506.22
  batch-generator:
     rank  0: 52.64
     rank  1: 56.42
     rank  2: 62.39
     rank  3: 52.52
     rank  4: 61.80
     rank  5: 66.00
     rank  6: 67.58
     rank  7: 68.93
     rank  8: 55.95
     rank  9: 64.53
     rank 10: 59.01
     rank 11: 59.10
     rank 12: 54.02
     rank 13: 56.93
     rank 14: 55.99
     rank 15: 63.26
     rank 16: 54.76
     rank 17: 58.48
     rank 18: 60.23
     rank 19: 62.64
  forward-recv:
     rank  4: 75.28
     rank  5: 74.94
     rank  6: 76.65
     rank  7: 76.54
     rank  8: 272.26
     rank  9: 271.65
     rank 10: 272.25
     rank 11: 271.73
     rank 12: 448.71
     rank 13: 448.54
     rank 14: 449.83
     rank 15: 450.43
     rank 16: 620.47
     rank 17: 620.61
     rank 18: 620.21
     rank 19: 620.25
  forward-send:
     rank  0: 407.19
     rank  1: 406.12
     rank  2: 409.16
     rank  3: 409.99
     rank  4: 32.18
     rank  5: 31.19
     rank  6: 33.44
     rank  7: 34.05
     rank  8: 20.13
     rank  9: 19.95
     rank 10: 20.73
     rank 11: 20.54
     rank 12: 10.61
     rank 13: 10.63
     rank 14: 10.17
     rank 15: 9.91
  backward-recv:
     rank  0: 1627.97
     rank  1: 1628.13
     rank  2: 1630.08
     rank  3: 1628.36
     rank  4: 591.62
     rank  5: 593.40
     rank  6: 592.69
     rank  7: 591.50
     rank  8: 394.00
     rank  9: 395.00
     rank 10: 394.10
     rank 11: 393.45
     rank 12: 196.74
     rank 13: 196.81
     rank 14: 197.09
     rank 15: 197.25
  backward-send:
     rank  4: 22.33
     rank  5: 20.70
     rank  6: 21.70
     rank  7: 22.31
     rank  8: 31.38
     rank  9: 30.40
     rank 10: 31.08
     rank 11: 31.36
     rank 12: 21.05
     rank 13: 20.84
     rank 14: 20.79
     rank 15: 20.08
     rank 16: 10.57
     rank 17: 10.47
     rank 18: 10.49
     rank 19: 10.03
  forward-send-backward-recv:
     rank  0: 4857.88
     rank  1: 4856.66
     rank  2: 4852.76
     rank  3: 4858.09
     rank  4: 885.01
     rank  5: 881.82
     rank  6: 887.46
     rank  7: 886.86
     rank  8: 748.57
     rank  9: 749.23
     rank 10: 746.87
     rank 11: 739.68
     rank 12: 579.98
     rank 13: 579.25
     rank 14: 582.00
     rank 15: 580.77
  backward-send-forward-recv:
     rank  4: 74.90
     rank  5: 74.63
     rank  6: 70.29
     rank  7: 74.47
     rank  8: 145.67
     rank  9: 140.69
     rank 10: 145.36
     rank 11: 144.94
     rank 12: 156.76
     rank 13: 156.78
     rank 14: 153.23
     rank 15: 149.79
     rank 16: 168.47
     rank 17: 168.54
     rank 18: 166.35
     rank 19: 164.42
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.06
     rank 18: 0.10
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.29
     rank  1: 0.31
     rank  2: 0.29
     rank  3: 0.32
     rank  4: 2.29
     rank  5: 2.33
     rank  6: 2.28
     rank  7: 2.39
     rank  8: 2.50
     rank  9: 2.22
     rank 10: 2.23
     rank 11: 2.23
     rank 12: 2.25
     rank 13: 2.20
     rank 14: 2.31
     rank 15: 2.14
     rank 16: 2.40
     rank 17: 2.42
     rank 18: 2.66
     rank 19: 2.44
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.08
     rank  8: 0.11
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.10
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 2.00
     rank  2: 1.99
     rank  3: 2.05
     rank  4: 46.27
     rank  5: 46.32
     rank  6: 46.26
     rank  7: 46.48
     rank  8: 43.59
     rank  9: 43.10
     rank 10: 43.19
     rank 11: 43.18
     rank 12: 43.10
     rank 13: 43.05
     rank 14: 43.07
     rank 15: 42.94
     rank 16: 46.36
     rank 17: 46.38
     rank 18: 46.46
     rank 19: 46.42
  optimizer:
     rank  0: 2.80
     rank  1: 2.81
     rank  2: 2.80
     rank  3: 2.86
     rank  4: 47.08
     rank  5: 47.14
     rank  6: 47.08
     rank  7: 47.29
     rank  8: 44.40
     rank  9: 43.91
     rank 10: 44.00
     rank 11: 43.99
     rank 12: 43.92
     rank 13: 43.90
     rank 14: 43.88
     rank 15: 43.75
     rank 16: 47.17
     rank 17: 47.19
     rank 18: 47.26
     rank 19: 47.25
 [2024-12-05 19:21:42] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7946.1 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 3.205440E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7866.30
     rank  1: 7866.72
     rank  2: 7866.21
     rank  3: 7866.26
     rank  4: 7870.53
     rank  5: 7870.86
     rank  6: 7870.39
     rank  7: 7870.62
     rank  8: 7870.21
     rank  9: 7870.55
     rank 10: 7870.13
     rank 11: 7870.29
     rank 12: 7870.26
     rank 13: 7870.63
     rank 14: 7870.18
     rank 15: 7870.36
     rank 16: 7870.48
     rank 17: 7871.29
     rank 18: 7870.38
     rank 19: 7870.37
  forward-compute:
     rank  0: 927.83
     rank  1: 931.28
     rank  2: 949.80
     rank  3: 935.91
     rank  4: 2894.09
     rank  5: 2896.83
     rank  6: 2912.24
     rank  7: 2894.55
     rank  8: 2750.74
     rank  9: 2755.64
     rank 10: 2753.65
     rank 11: 2753.02
     rank 12: 2756.80
     rank 13: 2755.73
     rank 14: 2756.78
     rank 15: 2761.26
     rank 16: 2921.45
     rank 17: 2922.87
     rank 18: 2924.44
     rank 19: 2928.28
  backward-compute:
     rank  0: 51.29
     rank  1: 50.43
     rank  2: 44.38
     rank  3: 49.33
     rank  4: 3331.38
     rank  5: 3333.60
     rank  6: 3327.89
     rank  7: 3339.31
     rank  8: 3259.57
     rank  9: 3258.66
     rank 10: 3260.15
     rank 11: 3262.11
     rank 12: 3266.38
     rank 13: 3267.18
     rank 14: 3267.84
     rank 15: 3270.86
     rank 16: 3483.12
     rank 17: 3482.77
     rank 18: 3485.92
     rank 19: 3485.34
  pure-backward-compute:
     rank  0: 50.62
     rank  1: 49.74
     rank  2: 43.77
     rank  3: 48.76
     rank  4: 3329.55
     rank  5: 3332.24
     rank  6: 3327.22
     rank  7: 3338.23
     rank  8: 3258.43
     rank  9: 3257.80
     rank 10: 3258.78
     rank 11: 3260.46
     rank 12: 3265.28
     rank 13: 3265.72
     rank 14: 3266.93
     rank 15: 3270.14
     rank 16: 3480.76
     rank 17: 3480.40
     rank 18: 3484.17
     rank 19: 3483.46
  batch-generator:
     rank  0: 52.96
     rank  1: 57.61
     rank  2: 75.08
     rank  3: 61.86
     rank  4: 90.90
     rank  5: 92.40
     rank  6: 107.52
     rank  7: 90.01
     rank  8: 52.62
     rank  9: 58.90
     rank 10: 57.94
     rank 11: 58.61
     rank 12: 52.70
     rank 13: 55.96
     rank 14: 57.75
     rank 15: 61.27
     rank 16: 59.59
     rank 17: 62.40
     rank 18: 63.78
     rank 19: 68.03
  forward-recv:
     rank  4: 82.07
     rank  5: 82.10
     rank  6: 80.66
     rank  7: 80.84
     rank  8: 278.09
     rank  9: 278.16
     rank 10: 277.24
     rank 11: 277.27
     rank 12: 448.49
     rank 13: 448.44
     rank 14: 449.08
     rank 15: 448.94
     rank 16: 622.20
     rank 17: 622.19
     rank 18: 621.74
     rank 19: 621.81
  forward-send:
     rank  0: 409.52
     rank  1: 409.36
     rank  2: 405.05
     rank  3: 405.33
     rank  4: 31.55
     rank  5: 31.59
     rank  6: 30.00
     rank  7: 30.09
     rank  8: 21.27
     rank  9: 21.33
     rank 10: 20.52
     rank 11: 20.72
     rank 12: 10.57
     rank 13: 10.60
     rank 14: 9.91
     rank 15: 10.04
  backward-recv:
     rank  0: 1637.66
     rank  1: 1638.21
     rank  2: 1641.52
     rank  3: 1634.79
     rank  4: 592.37
     rank  5: 590.69
     rank  6: 596.17
     rank  7: 593.00
     rank  8: 394.85
     rank  9: 395.61
     rank 10: 395.08
     rank 11: 393.59
     rank 12: 194.66
     rank 13: 194.84
     rank 14: 194.65
     rank 15: 194.71
  backward-send:
     rank  4: 22.31
     rank  5: 22.70
     rank  6: 19.61
     rank  7: 21.91
     rank  8: 31.23
     rank  9: 30.53
     rank 10: 31.27
     rank 11: 31.55
     rank 12: 20.98
     rank 13: 20.85
     rank 14: 20.88
     rank 15: 20.11
     rank 16: 10.54
     rank 17: 10.53
     rank 18: 10.48
     rank 19: 9.85
  forward-send-backward-recv:
     rank  0: 4831.33
     rank  1: 4828.41
     rank  2: 4817.02
     rank  3: 4833.39
     rank  4: 818.58
     rank  5: 817.16
     rank  6: 823.13
     rank  7: 821.87
     rank  8: 752.07
     rank  9: 752.97
     rank 10: 750.09
     rank 11: 747.84
     rank 12: 564.43
     rank 13: 564.48
     rank 14: 566.35
     rank 15: 563.62
  backward-send-forward-recv:
     rank  4: 72.56
     rank  5: 71.71
     rank  6: 61.14
     rank  7: 68.90
     rank  8: 147.95
     rank  9: 143.69
     rank 10: 146.77
     rank 11: 147.17
     rank 12: 158.36
     rank 13: 158.58
     rank 14: 156.98
     rank 15: 153.74
     rank 16: 168.97
     rank 17: 168.48
     rank 18: 166.76
     rank 19: 163.67
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.13
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.27
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.32
     rank  1: 0.30
     rank  2: 0.33
     rank  3: 0.30
     rank  4: 2.40
     rank  5: 2.29
     rank  6: 2.28
     rank  7: 2.85
     rank  8: 2.25
     rank  9: 2.15
     rank 10: 2.22
     rank 11: 2.28
     rank 12: 2.19
     rank 13: 2.21
     rank 14: 2.20
     rank 15: 2.15
     rank 16: 2.47
     rank 17: 3.19
     rank 18: 2.42
     rank 19: 2.40
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.07
     rank  9: 0.04
     rank 10: 0.06
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.10
     rank 15: 0.04
     rank 16: 0.08
     rank 17: 0.17
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.05
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 1.99
     rank  1: 2.00
     rank  2: 2.03
     rank  3: 1.99
     rank  4: 46.50
     rank  5: 46.23
     rank  6: 46.29
     rank  7: 46.37
     rank  8: 43.05
     rank  9: 42.98
     rank 10: 43.14
     rank 11: 43.20
     rank 12: 43.08
     rank 13: 43.01
     rank 14: 43.01
     rank 15: 42.98
     rank 16: 46.50
     rank 17: 47.26
     rank 18: 46.44
     rank 19: 46.70
  optimizer:
     rank  0: 3.42
     rank  1: 3.43
     rank  2: 3.46
     rank  3: 3.41
     rank  4: 47.94
     rank  5: 47.66
     rank  6: 47.72
     rank  7: 47.80
     rank  8: 44.48
     rank  9: 44.41
     rank 10: 44.57
     rank 11: 44.63
     rank 12: 44.51
     rank 13: 44.51
     rank 14: 44.44
     rank 15: 44.41
     rank 16: 47.93
     rank 17: 48.66
     rank 18: 47.87
     rank 19: 48.12
 [2024-12-05 19:21:50] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7950.9 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.865407E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7867.10
     rank  1: 7866.88
     rank  2: 7866.91
     rank  3: 7866.89
     rank  4: 7871.29
     rank  5: 7871.05
     rank  6: 7871.07
     rank  7: 7871.06
     rank  8: 7870.98
     rank  9: 7870.80
     rank 10: 7870.83
     rank 11: 7870.79
     rank 12: 7871.03
     rank 13: 7870.85
     rank 14: 7870.86
     rank 15: 7870.81
     rank 16: 7871.62
     rank 17: 7871.09
     rank 18: 7871.12
     rank 19: 7871.04
  forward-compute:
     rank  0: 943.33
     rank  1: 944.68
     rank  2: 960.27
     rank  3: 949.20
     rank  4: 2885.92
     rank  5: 2885.24
     rank  6: 2895.82
     rank  7: 2889.66
     rank  8: 2756.37
     rank  9: 2759.67
     rank 10: 2758.09
     rank 11: 2758.55
     rank 12: 2761.92
     rank 13: 2761.54
     rank 14: 2760.94
     rank 15: 2766.35
     rank 16: 2913.20
     rank 17: 2913.77
     rank 18: 2917.77
     rank 19: 2923.24
  backward-compute:
     rank  0: 51.25
     rank  1: 50.44
     rank  2: 43.40
     rank  3: 50.59
     rank  4: 3332.09
     rank  5: 3334.78
     rank  6: 3330.73
     rank  7: 3333.71
     rank  8: 3258.15
     rank  9: 3257.09
     rank 10: 3258.75
     rank 11: 3260.70
     rank 12: 3273.24
     rank 13: 3272.70
     rank 14: 3275.53
     rank 15: 3279.55
     rank 16: 3490.33
     rank 17: 3489.07
     rank 18: 3492.93
     rank 19: 3491.69
  pure-backward-compute:
     rank  0: 50.58
     rank  1: 49.79
     rank  2: 42.83
     rank  3: 49.98
     rank  4: 3330.89
     rank  5: 3333.39
     rank  6: 3330.05
     rank  7: 3332.90
     rank  8: 3256.95
     rank  9: 3256.07
     rank 10: 3257.32
     rank 11: 3259.23
     rank 12: 3272.13
     rank 13: 3271.48
     rank 14: 3274.63
     rank 15: 3278.61
     rank 16: 3487.60
     rank 17: 3486.38
     rank 18: 3491.07
     rank 19: 3490.11
  batch-generator:
     rank  0: 51.84
     rank  1: 54.18
     rank  2: 68.70
     rank  3: 58.37
     rank  4: 70.72
     rank  5: 77.98
     rank  6: 83.69
     rank  7: 82.96
     rank  8: 53.00
     rank  9: 58.09
     rank 10: 56.96
     rank 11: 58.62
     rank 12: 51.34
     rank 13: 54.87
     rank 14: 55.42
     rank 15: 59.62
     rank 16: 53.83
     rank 17: 57.43
     rank 18: 60.87
     rank 19: 66.64
  forward-recv:
     rank  4: 80.13
     rank  5: 80.34
     rank  6: 79.74
     rank  7: 79.69
     rank  8: 277.18
     rank  9: 276.73
     rank 10: 275.54
     rank 11: 275.52
     rank 12: 447.92
     rank 13: 448.15
     rank 14: 449.11
     rank 15: 448.42
     rank 16: 621.14
     rank 17: 621.25
     rank 18: 620.43
     rank 19: 621.03
  forward-send:
     rank  0: 409.65
     rank  1: 409.23
     rank  2: 405.70
     rank  3: 405.65
     rank  4: 31.56
     rank  5: 31.44
     rank  6: 30.03
     rank  7: 30.01
     rank  8: 20.70
     rank  9: 21.11
     rank 10: 21.06
     rank 11: 21.16
     rank 12: 10.47
     rank 13: 10.52
     rank 14: 9.72
     rank 15: 10.30
  backward-recv:
     rank  0: 1639.64
     rank  1: 1639.38
     rank  2: 1641.80
     rank  3: 1639.89
     rank  4: 590.71
     rank  5: 590.89
     rank  6: 591.80
     rank  7: 591.80
     rank  8: 395.39
     rank  9: 396.11
     rank 10: 395.25
     rank 11: 395.10
     rank 12: 196.21
     rank 13: 194.32
     rank 14: 196.65
     rank 15: 196.75
  backward-send:
     rank  4: 22.78
     rank  5: 22.63
     rank  6: 22.15
     rank  7: 21.90
     rank  8: 31.27
     rank  9: 31.00
     rank 10: 31.33
     rank 11: 31.35
     rank 12: 20.99
     rank 13: 22.90
     rank 14: 20.55
     rank 15: 20.35
     rank 16: 10.44
     rank 17: 10.56
     rank 18: 10.53
     rank 19: 9.97
  forward-send-backward-recv:
     rank  0: 4815.35
     rank  1: 4815.35
     rank  2: 4808.31
     rank  3: 4814.49
     rank  4: 830.00
     rank  5: 828.79
     rank  6: 834.21
     rank  7: 833.04
     rank  8: 750.03
     rank  9: 751.29
     rank 10: 747.36
     rank 11: 745.29
     rank 12: 551.08
     rank 13: 551.70
     rank 14: 552.47
     rank 15: 549.07
  backward-send-forward-recv:
     rank  4: 74.36
     rank  5: 74.42
     rank  6: 67.30
     rank  7: 72.00
     rank  8: 145.86
     rank  9: 143.57
     rank 10: 146.71
     rank 11: 146.25
     rank 12: 158.12
     rank 13: 158.27
     rank 14: 156.76
     rank 15: 153.01
     rank 16: 169.18
     rank 17: 169.58
     rank 18: 166.06
     rank 19: 161.31
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.15
     rank 17: 0.06
     rank 18: 0.09
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.30
     rank  1: 0.30
     rank  2: 0.29
     rank  3: 0.34
     rank  4: 2.35
     rank  5: 2.28
     rank  6: 2.29
     rank  7: 2.31
     rank  8: 2.27
     rank  9: 2.24
     rank 10: 2.37
     rank 11: 2.25
     rank 12: 2.16
     rank 13: 2.20
     rank 14: 2.19
     rank 15: 2.19
     rank 16: 3.08
     rank 17: 2.50
     rank 18: 2.60
     rank 19: 2.39
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.08
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.06
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.14
     rank 17: 0.08
     rank 18: 0.10
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.01
     rank  1: 2.01
     rank  2: 1.99
     rank  3: 2.07
     rank  4: 46.39
     rank  5: 46.27
     rank  6: 46.26
     rank  7: 46.30
     rank  8: 43.14
     rank  9: 43.10
     rank 10: 43.15
     rank 11: 43.38
     rank 12: 42.99
     rank 13: 43.08
     rank 14: 43.08
     rank 15: 43.02
     rank 16: 47.05
     rank 17: 46.47
     rank 18: 46.49
     rank 19: 46.34
  optimizer:
     rank  0: 3.18
     rank  1: 3.18
     rank  2: 3.16
     rank  3: 3.24
     rank  4: 47.56
     rank  5: 47.44
     rank  6: 47.43
     rank  7: 47.47
     rank  8: 44.31
     rank  9: 44.27
     rank 10: 44.32
     rank 11: 44.57
     rank 12: 44.15
     rank 13: 44.25
     rank 14: 44.24
     rank 15: 44.19
     rank 16: 48.19
     rank 17: 47.64
     rank 18: 47.67
     rank 19: 47.51
 [2024-12-05 19:21:58] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7974.4 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.346552E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7891.41
     rank  1: 7891.43
     rank  2: 7891.39
     rank  3: 7891.37
     rank  4: 7895.53
     rank  5: 7895.68
     rank  6: 7895.57
     rank  7: 7895.57
     rank  8: 7895.28
     rank  9: 7895.40
     rank 10: 7895.31
     rank 11: 7895.37
     rank 12: 7895.33
     rank 13: 7895.45
     rank 14: 7895.35
     rank 15: 7895.33
     rank 16: 7895.53
     rank 17: 7895.77
     rank 18: 7895.58
     rank 19: 7895.54
  forward-compute:
     rank  0: 926.66
     rank  1: 929.13
     rank  2: 940.59
     rank  3: 935.55
     rank  4: 2904.23
     rank  5: 2903.80
     rank  6: 2912.56
     rank  7: 2910.45
     rank  8: 2759.28
     rank  9: 2762.26
     rank 10: 2761.81
     rank 11: 2762.42
     rank 12: 2772.98
     rank 13: 2772.64
     rank 14: 2772.52
     rank 15: 2779.08
     rank 16: 2927.54
     rank 17: 2927.81
     rank 18: 2931.08
     rank 19: 2933.36
  backward-compute:
     rank  0: 52.29
     rank  1: 49.01
     rank  2: 43.64
     rank  3: 49.71
     rank  4: 3333.29
     rank  5: 3335.73
     rank  6: 3331.35
     rank  7: 3333.87
     rank  8: 3276.99
     rank  9: 3276.58
     rank 10: 3277.81
     rank 11: 3281.30
     rank 12: 3276.65
     rank 13: 3276.01
     rank 14: 3278.46
     rank 15: 3279.74
     rank 16: 3495.80
     rank 17: 3494.76
     rank 18: 3498.83
     rank 19: 3497.09
  pure-backward-compute:
     rank  0: 51.60
     rank  1: 48.35
     rank  2: 43.06
     rank  3: 49.13
     rank  4: 3332.04
     rank  5: 3334.45
     rank  6: 3330.62
     rank  7: 3333.08
     rank  8: 3275.86
     rank  9: 3275.64
     rank 10: 3276.55
     rank 11: 3280.10
     rank 12: 3275.30
     rank 13: 3274.86
     rank 14: 3277.59
     rank 15: 3279.01
     rank 16: 3493.29
     rank 17: 3492.27
     rank 18: 3497.09
     rank 19: 3495.30
  batch-generator:
     rank  0: 52.33
     rank  1: 57.66
     rank  2: 67.36
     rank  3: 63.09
     rank  4: 98.23
     rank  5: 95.40
     rank  6: 100.89
     rank  7: 99.42
     rank  8: 53.07
     rank  9: 57.97
     rank 10: 56.89
     rank 11: 58.42
     rank 12: 61.78
     rank 13: 62.17
     rank 14: 63.09
     rank 15: 68.12
     rank 16: 54.46
     rank 17: 58.68
     rank 18: 60.73
     rank 19: 64.80
  forward-recv:
     rank  4: 84.47
     rank  5: 84.63
     rank  6: 84.38
     rank  7: 82.38
     rank  8: 281.64
     rank  9: 281.27
     rank 10: 280.91
     rank 11: 280.60
     rank 12: 455.53
     rank 13: 455.69
     rank 14: 456.40
     rank 15: 456.11
     rank 16: 624.47
     rank 17: 624.60
     rank 18: 624.18
     rank 19: 624.14
  forward-send:
     rank  0: 413.79
     rank  1: 413.01
     rank  2: 411.86
     rank  3: 408.02
     rank  4: 31.59
     rank  5: 31.40
     rank  6: 31.16
     rank  7: 29.95
     rank  8: 20.80
     rank  9: 21.06
     rank 10: 20.92
     rank 11: 20.52
     rank 12: 10.63
     rank 13: 10.63
     rank 14: 10.14
     rank 15: 10.04
  backward-recv:
     rank  0: 1636.63
     rank  1: 1638.50
     rank  2: 1639.80
     rank  3: 1638.37
     rank  4: 590.57
     rank  5: 591.14
     rank  6: 593.01
     rank  7: 593.08
     rank  8: 393.71
     rank  9: 394.60
     rank 10: 393.59
     rank 11: 393.14
     rank 12: 195.44
     rank 13: 195.44
     rank 14: 195.50
     rank 15: 195.91
  backward-send:
     rank  4: 22.69
     rank  5: 22.48
     rank  6: 21.34
     rank  7: 21.28
     rank  8: 31.21
     rank  9: 30.70
     rank 10: 31.26
     rank 11: 31.01
     rank 12: 20.99
     rank 13: 20.73
     rank 14: 20.83
     rank 15: 20.24
     rank 16: 10.59
     rank 17: 10.59
     rank 18: 10.40
     rank 19: 10.12
  forward-send-backward-recv:
     rank  0: 4854.61
     rank  1: 4854.08
     rank  2: 4848.06
     rank  3: 4853.34
     rank  4: 829.96
     rank  5: 828.23
     rank  6: 833.34
     rank  7: 832.59
     rank  8: 749.14
     rank  9: 749.58
     rank 10: 746.65
     rank 11: 743.66
     rank 12: 553.59
     rank 13: 554.91
     rank 14: 555.31
     rank 15: 554.34
  backward-send-forward-recv:
     rank  4: 75.62
     rank  5: 76.24
     rank  6: 69.51
     rank  7: 73.32
     rank  8: 146.86
     rank  9: 144.63
     rank 10: 146.01
     rank 11: 146.39
     rank 12: 157.89
     rank 13: 158.25
     rank 14: 156.73
     rank 15: 152.31
     rank 16: 170.16
     rank 17: 170.59
     rank 18: 167.50
     rank 19: 166.23
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.11
     rank 18: 0.08
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.37
     rank  1: 0.30
     rank  2: 0.28
     rank  3: 0.28
     rank  4: 2.29
     rank  5: 2.35
     rank  6: 2.29
     rank  7: 2.32
     rank  8: 2.23
     rank  9: 2.22
     rank 10: 2.29
     rank 11: 2.30
     rank 12: 2.21
     rank 13: 2.17
     rank 14: 2.24
     rank 15: 2.14
     rank 16: 2.41
     rank 17: 2.73
     rank 18: 2.57
     rank 19: 2.40
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.11
     rank 18: 0.10
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.14
     rank  1: 2.00
     rank  2: 1.98
     rank  3: 1.99
     rank  4: 46.29
     rank  5: 46.34
     rank  6: 46.26
     rank  7: 46.33
     rank  8: 43.04
     rank  9: 43.03
     rank 10: 43.15
     rank 11: 43.12
     rank 12: 43.10
     rank 13: 42.99
     rank 14: 43.01
     rank 15: 42.97
     rank 16: 46.39
     rank 17: 46.62
     rank 18: 46.44
     rank 19: 46.41
  optimizer:
     rank  0: 2.79
     rank  1: 2.65
     rank  2: 2.64
     rank  3: 2.64
     rank  4: 46.94
     rank  5: 46.99
     rank  6: 46.91
     rank  7: 46.99
     rank  8: 43.69
     rank  9: 43.68
     rank 10: 43.81
     rank 11: 43.88
     rank 12: 43.75
     rank 13: 43.65
     rank 14: 43.67
     rank 15: 43.63
     rank 16: 47.06
     rank 17: 47.29
     rank 18: 47.09
     rank 19: 47.07
 [2024-12-05 19:22:06] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7973.7 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.426261E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7891.42
     rank  1: 7891.43
     rank  2: 7891.34
     rank  3: 7891.36
     rank  4: 7895.57
     rank  5: 7895.58
     rank  6: 7895.50
     rank  7: 7895.94
     rank  8: 7895.36
     rank  9: 7895.31
     rank 10: 7895.27
     rank 11: 7895.26
     rank 12: 7895.34
     rank 13: 7895.34
     rank 14: 7895.29
     rank 15: 7895.31
     rank 16: 7895.63
     rank 17: 7895.57
     rank 18: 7895.49
     rank 19: 7895.49
  forward-compute:
     rank  0: 925.05
     rank  1: 923.95
     rank  2: 935.71
     rank  3: 930.61
     rank  4: 2897.68
     rank  5: 2894.49
     rank  6: 2905.67
     rank  7: 2900.60
     rank  8: 2759.16
     rank  9: 2762.25
     rank 10: 2762.50
     rank 11: 2759.98
     rank 12: 2754.92
     rank 13: 2754.52
     rank 14: 2755.16
     rank 15: 2759.56
     rank 16: 2922.13
     rank 17: 2921.67
     rank 18: 2926.47
     rank 19: 2929.71
  backward-compute:
     rank  0: 51.51
     rank  1: 50.34
     rank  2: 48.77
     rank  3: 49.09
     rank  4: 3330.62
     rank  5: 3332.91
     rank  6: 3329.95
     rank  7: 3331.09
     rank  8: 3278.99
     rank  9: 3278.32
     rank 10: 3279.82
     rank 11: 3281.46
     rank 12: 3279.13
     rank 13: 3278.91
     rank 14: 3281.06
     rank 15: 3283.56
     rank 16: 3506.68
     rank 17: 3503.96
     rank 18: 3509.55
     rank 19: 3508.52
  pure-backward-compute:
     rank  0: 50.87
     rank  1: 49.69
     rank  2: 48.20
     rank  3: 48.53
     rank  4: 3329.42
     rank  5: 3331.17
     rank  6: 3329.22
     rank  7: 3330.38
     rank  8: 3277.89
     rank  9: 3277.37
     rank 10: 3278.38
     rank 11: 3280.42
     rank 12: 3278.03
     rank 13: 3277.84
     rank 14: 3280.22
     rank 15: 3282.87
     rank 16: 3504.04
     rank 17: 3501.19
     rank 18: 3507.94
     rank 19: 3506.96
  batch-generator:
     rank  0: 52.03
     rank  1: 52.04
     rank  2: 62.90
     rank  3: 58.55
     rank  4: 82.13
     rank  5: 88.21
     rank  6: 93.98
     rank  7: 89.40
     rank  8: 52.80
     rank  9: 57.85
     rank 10: 58.51
     rank 11: 56.82
     rank 12: 52.26
     rank 13: 55.87
     rank 14: 57.50
     rank 15: 60.86
     rank 16: 54.55
     rank 17: 57.52
     rank 18: 61.43
     rank 19: 65.66
  forward-recv:
     rank  4: 82.90
     rank  5: 82.98
     rank  6: 81.05
     rank  7: 82.29
     rank  8: 279.40
     rank  9: 279.36
     rank 10: 278.45
     rank 11: 279.27
     rank 12: 453.69
     rank 13: 453.67
     rank 14: 454.57
     rank 15: 454.07
     rank 16: 620.70
     rank 17: 620.61
     rank 18: 620.23
     rank 19: 620.29
  forward-send:
     rank  0: 414.77
     rank  1: 414.73
     rank  2: 410.46
     rank  3: 412.37
     rank  4: 31.62
     rank  5: 31.75
     rank  6: 30.60
     rank  7: 31.11
     rank  8: 21.19
     rank  9: 21.25
     rank 10: 21.04
     rank 11: 20.79
     rank 12: 10.54
     rank 13: 10.61
     rank 14: 9.88
     rank 15: 10.09
  backward-recv:
     rank  0: 1638.65
     rank  1: 1639.00
     rank  2: 1639.86
     rank  3: 1639.50
     rank  4: 594.24
     rank  5: 595.24
     rank  6: 597.25
     rank  7: 595.82
     rank  8: 395.96
     rank  9: 396.38
     rank 10: 396.18
     rank 11: 396.22
     rank 12: 197.66
     rank 13: 197.60
     rank 14: 197.84
     rank 15: 197.47
  backward-send:
     rank  4: 22.88
     rank  5: 22.31
     rank  6: 21.34
     rank  7: 21.51
     rank  8: 31.28
     rank  9: 30.83
     rank 10: 31.41
     rank 11: 31.06
     rank 12: 20.92
     rank 13: 20.71
     rank 14: 20.82
     rank 15: 20.62
     rank 16: 10.66
     rank 17: 10.44
     rank 18: 10.45
     rank 19: 9.97
  forward-send-backward-recv:
     rank  0: 4854.26
     rank  1: 4855.89
     rank  2: 4850.16
     rank  3: 4853.06
     rank  4: 839.02
     rank  5: 837.68
     rank  6: 841.06
     rank  7: 842.15
     rank  8: 748.82
     rank  9: 749.46
     rank 10: 748.18
     rank 11: 745.54
     rank 12: 570.01
     rank 13: 570.41
     rank 14: 571.26
     rank 15: 569.63
  backward-send-forward-recv:
     rank  4: 74.18
     rank  5: 75.95
     rank  6: 70.25
     rank  7: 72.45
     rank  8: 146.72
     rank  9: 144.02
     rank 10: 144.82
     rank 11: 146.59
     rank 12: 158.84
     rank 13: 159.08
     rank 14: 156.89
     rank 15: 153.95
     rank 16: 169.39
     rank 17: 170.68
     rank 18: 166.09
     rank 19: 163.76
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.04
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.10
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.30
     rank  1: 0.33
     rank  2: 0.29
     rank  3: 0.30
     rank  4: 2.34
     rank  5: 2.28
     rank  6: 2.33
     rank  7: 2.56
     rank  8: 2.28
     rank  9: 2.29
     rank 10: 2.34
     rank 11: 2.30
     rank 12: 2.15
     rank 13: 2.15
     rank 14: 2.18
     rank 15: 2.16
     rank 16: 2.63
     rank 17: 2.48
     rank 18: 2.41
     rank 19: 2.38
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.08
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.11
     rank 11: 0.11
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.12
     rank 17: 0.12
     rank 18: 0.11
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 2.00
     rank  1: 2.07
     rank  2: 2.00
     rank  3: 2.01
     rank  4: 46.36
     rank  5: 46.29
     rank  6: 46.38
     rank  7: 46.34
     rank  8: 43.02
     rank  9: 43.05
     rank 10: 43.25
     rank 11: 43.20
     rank 12: 42.98
     rank 13: 42.95
     rank 14: 43.03
     rank 15: 42.97
     rank 16: 46.49
     rank 17: 46.47
     rank 18: 46.38
     rank 19: 46.35
  optimizer:
     rank  0: 2.60
     rank  1: 2.67
     rank  2: 2.60
     rank  3: 2.61
     rank  4: 46.96
     rank  5: 46.88
     rank  6: 46.98
     rank  7: 46.94
     rank  8: 43.62
     rank  9: 43.65
     rank 10: 43.87
     rank 11: 43.79
     rank 12: 43.58
     rank 13: 43.55
     rank 14: 43.63
     rank 15: 43.56
     rank 16: 47.09
     rank 17: 47.07
     rank 18: 46.98
     rank 19: 46.94
