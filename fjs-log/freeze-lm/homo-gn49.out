examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
[2024-12-27 15:47:36,007] torch.distributed.run: [WARNING] 
[2024-12-27 15:47:36,007] torch.distributed.run: [WARNING] *****************************************
[2024-12-27 15:47:36,007] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-27 15:47:36,007] torch.distributed.run: [WARNING] *****************************************
using world size: 4, data-parallel size: 1, context-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 4, 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:HuggingFaceTokenizer
WARNING: Setting args.overlap_p2p_comm and args.align_param_gather to False since non-interleaved schedule does not support overlapping p2p communication and aligned param AG
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  allow_missing_vision_projection_checkpoint ...... True
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. True
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 0.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/pretrain_dataset.yaml']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. None
  dataloader_type ................................. external
  dataset_config .................................. None
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. 1024
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  disable_vision_class_token ...................... True
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 60
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 576
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... True
  eval_interval ................................... 1000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 230
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_wgrad ....................................... True
  freeze_LM ....................................... True
  freeze_ViT ...................................... True
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 336
  img_w ........................................... 336
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  language_model_type ............................. mistral_7b
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 20000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_num_tiles ................................... 1
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 14
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  prompt_path ..................................... /gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/manual_prompts.json
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 576
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  split_spec ...................................... 30,7,7,6
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /dat/fjs/llama_mistral/homo_mistral_clip_freeze_llm-tp4pp5/output/llava-mistral-7b-instruct-clip336-pretraining/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 2
  timing_log_option ............................... all
  titles_data_path ................................ None
  tokenizer_model ................................. /dat/fjs/llama_mistral/hf-mistral/
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 4
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_te .......................................... True
  use_thumbnail ................................... False
  use_tiling ...................................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  valid_path ...................................... None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_model_type ............................... clip
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 4
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 4
> building HuggingFaceTokenizer tokenizer ...
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
 > padded vocab (size: 32768) with 0 dummy tokens (new size: 32768)
> initializing torch distributed ...
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.076 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
---Rank 1---Tensor Parallel Group GPUs: [0]---Rank 3---Tensor Parallel Group GPUs: [0]

---Rank 2---Tensor Parallel Group GPUs: [0]
---Rank 1---Pipeline Parallel Group GPUs: [1, 1, 1, 1]
---Rank 3---Pipeline Parallel Group GPUs: [3, 3, 3, 3]
---Rank 2---Pipeline Parallel Group GPUs: [2, 2, 2, 2]
[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
>>> done with compiling and loading fused kernels. Compilation time: 2.663 seconds[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 0---Tensor Parallel Group GPUs: [0]
---Rank 0---Pipeline Parallel Group GPUs: [0, 0, 0, 0]
[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 8.161
[after megatron is initialized] datetime: 2024-12-27 15:47:54 
building a multimodal model ...
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1442893824
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1526784000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1526784000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1383571456
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (73400320 elements):
	vision_projection.encoder.linear_fc2.weight
	vision_projection.encoder.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, clip_grad=0.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f38106e7eb0>, config_logger_dir='')
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-12-27 15:47:54 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 40
    test:       40
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
[after dataloaders are built] datetime: 2024-12-27 15:47:55 
done with setup ...
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
training ...
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 177.16
     rank  1: 58.57
     rank  2: 54.67
     rank  3: 50.58
  train/valid/test-data-iterators-setup:
     rank  0: 977.79
     rank  1: 1024.60
     rank  2: 1024.57
     rank  3: 1145.71
[before the start of training step] datetime: 2024-12-27 15:47:55 
Layer: TransformerLayer, Forward time: 613.802282 ms
Layer: TransformerLayer, Forward time: 2.838116 ms
Layer: TransformerLayer, Forward time: 2.467242 ms
Layer: TransformerLayer, Forward time: 2.325477 ms
Layer: TransformerLayer, Forward time: 2.351243 ms
Layer: TransformerLayer, Forward time: 2.318956 ms
Layer: TransformerLayer, Forward time: 2.307910 ms
Layer: TransformerLayer, Forward time: 2.373784 ms
Layer: TransformerLayer, Forward time: 2.295850 ms
Layer: TransformerLayer, Forward time: 2.350218 ms
Layer: TransformerLayer, Forward time: 2.304646 ms
Layer: TransformerLayer, Forward time: 2.341894 ms
Layer: TransformerLayer, Forward time: 2.355433 ms
Layer: TransformerLayer, Forward time: 2.312317 ms
Layer: TransformerLayer, Forward time: 2.352744 ms
Layer: TransformerLayer, Forward time: 2.282962 ms
Layer: TransformerLayer, Forward time: 2.354416 ms
Layer: TransformerLayer, Forward time: 2.402446 ms
Layer: TransformerLayer, Forward time: 2.350289 ms
Layer: TransformerLayer, Forward time: 2.397018 ms
Layer: TransformerLayer, Forward time: 2.329685 ms
Layer: TransformerLayer, Forward time: 2.378678 ms
Layer: TransformerLayer, Forward time: 2.391868 ms
Layer: TransformerLayer, Forward time: 2.380786 ms
Layer: TransformerLayer, Forward time: 18.137310 ms
Layer: TransformerLayer, Forward time: 17.072855 ms
Layer: TransformerLayer, Forward time: 17.080923 ms
Layer: TransformerLayer, Forward time: 17.106807 ms
Layer: TransformerLayer, Forward time: 3.137792 ms
Layer: TransformerLayer, Forward time: 2.502295 ms
Layer: TransformerLayer, Forward time: 2.221873 ms
Layer: TransformerLayer, Forward time: 2.170901 ms
Layer: TransformerLayer, Forward time: 2.209631 ms
Layer: TransformerLayer, Forward time: 2.145927 ms
Layer: TransformerLayer, Forward time: 2.180577 ms
Layer: TransformerLayer, Forward time: 2.156925 ms
Layer: TransformerLayer, Forward time: 2.125360 ms
Layer: TransformerLayer, Forward time: 2.150774 ms
Layer: TransformerLayer, Forward time: 2.121605 ms
Layer: TransformerLayer, Forward time: 2.157396 ms
Layer: TransformerLayer, Forward time: 2.129102 ms
Layer: TransformerLayer, Forward time: 2.154324 ms
Layer: TransformerLayer, Forward time: 2.137994 ms
Layer: TransformerLayer, Forward time: 2.286213 ms
Layer: TransformerLayer, Forward time: 2.388260 ms
Layer: TransformerLayer, Forward time: 2.380244 ms
Layer: TransformerLayer, Forward time: 2.433120 ms
Layer: TransformerLayer, Forward time: 2.404184 ms
Layer: TransformerLayer, Forward time: 2.404442 ms
Layer: TransformerLayer, Forward time: 2.353374 ms
Layer: TransformerLayer, Forward time: 13.886781 ms
Layer: TransformerLayer, Forward time: 2.572605 ms
Layer: TransformerLayer, Forward time: 17.595761 ms
Layer: TransformerLayer, Forward time: 17.414127 ms
Layer: TransformerLayer, Forward time: 17.089900 ms
Layer: TransformerLayer, Forward time: 17.169673 ms
Layer: TransformerLayer, Forward time: 3.569413 ms
Layer: TransformerLayer, Forward time: 1.828829 ms
Layer: TransformerLayer, Forward time: 1.682958 ms
Layer: TransformerLayer, Forward time: 1.600942 ms
Layer: TransformerLayer, Forward time: 1.618440 ms
Layer: TransformerLayer, Forward time: 1.600111 ms
Layer: TransformerLayer, Forward time: 1.574074 ms
Layer: TransformerLayer, Forward time: 1.668106 ms
Layer: TransformerLayer, Forward time: 1.657442 ms
Layer: TransformerLayer, Forward time: 1.681545 ms
Layer: TransformerLayer, Forward time: 1.694517 ms
Layer: TransformerLayer, Forward time: 1.710377 ms
Layer: TransformerLayer, Forward time: 1.748144 ms
Layer: TransformerLayer, Forward time: 1.727654 ms
Layer: TransformerLayer, Forward time: 1.730056 ms
Layer: TransformerLayer, Forward time: 1.706889 ms
Layer: TransformerLayer, Forward time: 1.702946 ms
Layer: TransformerLayer, Forward time: 1.724496 ms
Layer: TransformerLayer, Forward time: 1.703823 ms
Layer: TransformerLayer, Forward time: 1.649097 ms
Layer: TransformerLayer, Forward time: 1.748120 ms
Layer: TransformerLayer, Forward time: 1.677474 ms
Layer: TransformerLayer, Forward time: 1.672013 ms
Layer: TransformerLayer, Forward time: 1.691391 ms
Layer: TransformerLayer, Forward time: 15.371833 ms
Layer: TransformerLayer, Forward time: 15.692171 ms
Layer: TransformerLayer, Forward time: 15.397692 ms
Layer: TransformerLayer, Forward time: 15.328262 ms
Layer: TransformerLayer, Forward time: 2.221757 ms
Layer: TransformerLayer, Forward time: 1.761853 ms
Layer: TransformerLayer, Forward time: 1.652627 ms
Layer: TransformerLayer, Forward time: 1.662398 ms
Layer: TransformerLayer, Forward time: 1.636098 ms
Layer: TransformerLayer, Forward time: 1.663722 ms
Layer: TransformerLayer, Forward time: 1.621812 ms
Layer: TransformerLayer, Forward time: 1.595998 ms
Layer: TransformerLayer, Forward time: 1.560962 ms
Layer: TransformerLayer, Forward time: 1.581412 ms
Layer: TransformerLayer, Forward time: 1.562826 ms
Layer: TransformerLayer, Forward time: 1.552766 ms
Layer: TransformerLayer, Forward time: 1.594777 ms
Layer: TransformerLayer, Forward time: 1.551804 ms
Layer: TransformerLayer, Forward time: 1.544612 ms
Layer: TransformerLayer, Forward time: 1.600963 ms
Layer: TransformerLayer, Forward time: 1.597735 ms
Layer: TransformerLayer, Forward time: 1.589837 ms
Layer: TransformerLayer, Forward time: 1.580890 ms
Layer: TransformerLayer, Forward time: 1.555911 ms
Layer: TransformerLayer, Forward time: 1.650737 ms
Layer: TransformerLayer, Forward time: 1.636133 ms
Layer: TransformerLayer, Forward time: 1.647795 ms
Layer: TransformerLayer, Forward time: 1.676770 ms
Layer: TransformerLayer, Forward time: 15.353198 ms
Layer: TransformerLayer, Forward time: 15.802705 ms
Layer: TransformerLayer, Forward time: 15.869269 ms
Layer: TransformerLayer, Forward time: 15.867399 ms
Layer: TransformerLayer, Backward time: 0.053649 ms
Layer: TransformerLayer, Backward time: 0.089816 ms
Layer: TransformerLayer, Backward time: 0.083635 ms
Layer: TransformerLayer, Backward time: 0.045272 ms
Layer: TransformerLayer, Backward time: 0.086091 ms
Layer: TransformerLayer, Backward time: 0.036650 ms
Layer: TransformerLayer, Backward time: 0.031991 ms
Layer: TransformerLayer, Backward time: 0.030973 ms
Layer: TransformerLayer, Backward time: 0.076125 ms
Layer: TransformerLayer, Backward time: 0.031268 ms
Layer: TransformerLayer, Backward time: 0.029469 ms
Layer: TransformerLayer, Backward time: 0.028303 ms
Layer: TransformerLayer, Backward time: 0.031731 ms
Layer: TransformerLayer, Backward time: 0.030609 ms
Layer: TransformerLayer, Backward time: 0.029217 ms
Layer: TransformerLayer, Backward time: 0.029705 ms
Number of parameters in transformer layers in billions:  5.23
Number of parameters in embedding layers in billions: 0.27
Total number of parameters in billions: 5.50
Number of parameters in most loaded shard in billions: 1.4429
Number of parameters in other shards in billions: 1.3087
Theoretical memory footprints: weight and optimizer=24769.72 MB
 [2024-12-27 15:48:07] iteration        1/      10 | consumed samples:            4 | elapsed time per iteration (ms): 11861.7 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.292803E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 1 iterations) memory (MB) | allocated: 5877.470703125 | max allocated: 14079.63134765625 | reserved: 14598.0 | max reserved: 14598.0
[Rank 2] (after 1 iterations) memory (MB) | allocated: 5877.470703125 | max allocated: 11417.57666015625 | reserved: 11880.0 | max reserved: 11880.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 5557.4560546875 | max allocated: 8025.501953125 | reserved: 8216.0 | max reserved: 8216.0
stage1 GPU utialization:49.6246596177419 %
weighting memory waste ratio T= 0.7468144211669764
[Rank 0] (after 1 iterations) memory (MB) | allocated: 6139.5380859375 | max allocated: 12195.75634765625 | reserved: 13408.0 | max reserved: 13408.0
times across ranks (ms):
  forward-backward:
     rank  0: 11812.08
     rank  1: 11849.85
     rank  2: 11849.59
     rank  3: 11810.83
  forward-compute:
     rank  0: 4303.69
     rank  1: 2538.91
     rank  2: 2327.75
     rank  3: 2548.14
  backward-compute:
     rank  0: 340.24
     rank  1: 554.74
     rank  2: 561.77
     rank  3: 527.55
  pure-backward-compute:
     rank  0: 339.88
     rank  1: 554.29
     rank  2: 561.29
     rank  3: 526.71
  batch-generator:
     rank  0: 937.57
     rank  1: 1013.69
     rank  2: 832.86
     rank  3: 942.38
  forward-recv:
     rank  1: 4076.00
     rank  2: 6268.71
     rank  3: 8258.14
  forward-send:
     rank  0: 4069.48
     rank  1: 1881.85
     rank  2: 12.60
  backward-recv:
     rank  0: 503.32
     rank  1: 226.68
     rank  2: 107.35
  backward-send:
     rank  1: 4.13
     rank  2: 2.86
     rank  3: 1.70
  forward-send-backward-recv:
     rank  0: 2551.15
     rank  1: 2389.53
     rank  2: 2238.39
  backward-send-forward-recv:
     rank  1: 13.23
     rank  2: 29.07
     rank  3: 74.52
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.15
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.09
  all-grads-sync:
     rank  0: 38.74
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 8.83
     rank  1: 0.22
     rank  2: 0.23
     rank  3: 0.13
  optimizer:
     rank  0: 10.11
     rank  1: 1.48
     rank  2: 1.57
     rank  3: 1.38
Layer: TransformerLayer, Forward time: 3.329955 ms
Layer: TransformerLayer, Forward time: 2.568348 ms
Layer: TransformerLayer, Forward time: 2.822952 ms
Layer: TransformerLayer, Forward time: 2.545271 ms
Layer: TransformerLayer, Forward time: 2.483565 ms
Layer: TransformerLayer, Forward time: 2.480386 ms
Layer: TransformerLayer, Forward time: 2.579966 ms
Layer: TransformerLayer, Forward time: 2.584175 ms
Layer: TransformerLayer, Forward time: 2.639477 ms
Layer: TransformerLayer, Forward time: 2.603774 ms
Layer: TransformerLayer, Forward time: 2.568080 ms
Layer: TransformerLayer, Forward time: 2.699185 ms
Layer: TransformerLayer, Forward time: 2.570802 ms
Layer: TransformerLayer, Forward time: 2.636906 ms
Layer: TransformerLayer, Forward time: 2.581343 ms
Layer: TransformerLayer, Forward time: 2.550056 ms
Layer: TransformerLayer, Forward time: 2.682997 ms
Layer: TransformerLayer, Forward time: 2.594808 ms
Layer: TransformerLayer, Forward time: 2.609200 ms
Layer: TransformerLayer, Forward time: 2.590996 ms
Layer: TransformerLayer, Forward time: 2.662870 ms
Layer: TransformerLayer, Forward time: 2.587166 ms
Layer: TransformerLayer, Forward time: 2.580845 ms
Layer: TransformerLayer, Forward time: 2.628117 ms
Layer: TransformerLayer, Forward time: 15.161423 ms
Layer: TransformerLayer, Forward time: 15.346155 ms
Layer: TransformerLayer, Forward time: 15.725795 ms
Layer: TransformerLayer, Forward time: 15.728043 ms
Layer: TransformerLayer, Forward time: 3.176115 ms
Layer: TransformerLayer, Forward time: 2.794178 ms
Layer: TransformerLayer, Forward time: 2.710942 ms
Layer: TransformerLayer, Forward time: 2.710760 ms
Layer: TransformerLayer, Forward time: 2.627826 ms
Layer: TransformerLayer, Forward time: 2.686595 ms
Layer: TransformerLayer, Forward time: 2.644046 ms
Layer: TransformerLayer, Forward time: 2.709195 ms
Layer: TransformerLayer, Forward time: 2.646812 ms
Layer: TransformerLayer, Forward time: 2.702025 ms
Layer: TransformerLayer, Forward time: 2.632587 ms
Layer: TransformerLayer, Forward time: 2.597426 ms
Layer: TransformerLayer, Forward time: 2.669733 ms
Layer: TransformerLayer, Forward time: 2.602328 ms
Layer: TransformerLayer, Forward time: 2.682771 ms
Layer: TransformerLayer, Forward time: 2.623391 ms
Layer: TransformerLayer, Forward time: 2.596242 ms
Layer: TransformerLayer, Forward time: 2.680424 ms
Layer: TransformerLayer, Forward time: 2.596827 ms
Layer: TransformerLayer, Forward time: 2.624283 ms
Layer: TransformerLayer, Forward time: 2.614577 ms
Layer: TransformerLayer, Forward time: 2.601318 ms
Layer: TransformerLayer, Forward time: 2.648340 ms
Layer: TransformerLayer, Forward time: 2.597309 ms
Layer: TransformerLayer, Forward time: 15.196101 ms
Layer: TransformerLayer, Forward time: 15.583803 ms
Layer: TransformerLayer, Forward time: 15.793820 ms
Layer: TransformerLayer, Forward time: 15.681625 ms
Layer: TransformerLayer, Forward time: 2.645281 ms
Layer: TransformerLayer, Forward time: 2.356289 ms
Layer: TransformerLayer, Forward time: 2.308325 ms
Layer: TransformerLayer, Forward time: 2.219390 ms
Layer: TransformerLayer, Forward time: 2.187153 ms
Layer: TransformerLayer, Forward time: 2.218949 ms
Layer: TransformerLayer, Forward time: 2.193079 ms
Layer: TransformerLayer, Forward time: 2.141827 ms
Layer: TransformerLayer, Forward time: 2.219870 ms
Layer: TransformerLayer, Forward time: 2.190072 ms
Layer: TransformerLayer, Forward time: 2.187011 ms
Layer: TransformerLayer, Forward time: 2.153148 ms
Layer: TransformerLayer, Forward time: 2.172382 ms
Layer: TransformerLayer, Forward time: 2.156874 ms
Layer: TransformerLayer, Forward time: 2.153221 ms
Layer: TransformerLayer, Forward time: 2.147417 ms
Layer: TransformerLayer, Forward time: 2.121444 ms
Layer: TransformerLayer, Forward time: 2.188050 ms
Layer: TransformerLayer, Forward time: 2.177803 ms
Layer: TransformerLayer, Forward time: 2.114412 ms
Layer: TransformerLayer, Forward time: 2.232079 ms
Layer: TransformerLayer, Forward time: 2.154555 ms
Layer: TransformerLayer, Forward time: 2.227719 ms
Layer: TransformerLayer, Forward time: 2.160395 ms
Layer: TransformerLayer, Forward time: 15.378960 ms
Layer: TransformerLayer, Forward time: 15.258954 ms
Layer: TransformerLayer, Forward time: 15.613075 ms
Layer: TransformerLayer, Forward time: 15.675685 ms
Layer: TransformerLayer, Forward time: 2.601018 ms
Layer: TransformerLayer, Forward time: 2.310205 ms
Layer: TransformerLayer, Forward time: 2.194603 ms
Layer: TransformerLayer, Forward time: 2.133566 ms
Layer: TransformerLayer, Forward time: 2.165729 ms
Layer: TransformerLayer, Forward time: 2.123674 ms
Layer: TransformerLayer, Forward time: 2.144270 ms
Layer: TransformerLayer, Forward time: 2.160706 ms
Layer: TransformerLayer, Forward time: 2.127306 ms
Layer: TransformerLayer, Forward time: 2.182013 ms
Layer: TransformerLayer, Forward time: 2.120050 ms
Layer: TransformerLayer, Forward time: 2.150154 ms
Layer: TransformerLayer, Forward time: 2.162073 ms
Layer: TransformerLayer, Forward time: 2.131270 ms
Layer: TransformerLayer, Forward time: 2.300375 ms
Layer: TransformerLayer, Forward time: 2.219833 ms
Layer: TransformerLayer, Forward time: 3.474658 ms
Layer: TransformerLayer, Forward time: 4.202691 ms
Layer: TransformerLayer, Forward time: 2.447839 ms
Layer: TransformerLayer, Forward time: 2.219916 ms
Layer: TransformerLayer, Forward time: 2.182264 ms
Layer: TransformerLayer, Forward time: 2.208614 ms
Layer: TransformerLayer, Forward time: 2.141485 ms
Layer: TransformerLayer, Forward time: 2.097389 ms
Layer: TransformerLayer, Forward time: 15.091218 ms
Layer: TransformerLayer, Forward time: 15.302891 ms
Layer: TransformerLayer, Forward time: 15.436773 ms
Layer: TransformerLayer, Forward time: 15.697849 ms
Layer: TransformerLayer, Backward time: 0.087678 ms
Layer: TransformerLayer, Backward time: 0.043412 ms
Layer: TransformerLayer, Backward time: 0.056218 ms
Layer: TransformerLayer, Backward time: 0.041214 ms
Layer: TransformerLayer, Backward time: 0.040740 ms
Layer: TransformerLayer, Backward time: 0.032759 ms
Layer: TransformerLayer, Backward time: 0.031031 ms
Layer: TransformerLayer, Backward time: 0.030749 ms
Layer: TransformerLayer, Backward time: 0.079641 ms
Layer: TransformerLayer, Backward time: 0.031177 ms
Layer: TransformerLayer, Backward time: 0.030311 ms
Layer: TransformerLayer, Backward time: 0.029320 ms
Layer: TransformerLayer, Backward time: 0.035530 ms
Layer: TransformerLayer, Backward time: 0.035064 ms
Layer: TransformerLayer, Backward time: 0.033495 ms
Layer: TransformerLayer, Backward time: 0.031225 ms
 [2024-12-27 15:48:09] iteration        2/      10 | consumed samples:            8 | elapsed time per iteration (ms): 1757.7 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.219714E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1738.19
     rank  1: 1737.24
     rank  2: 1737.28
     rank  3: 1737.27
  forward-compute:
     rank  0: 538.20
     rank  1: 449.76
     rank  2: 453.11
     rank  3: 448.07
  backward-compute:
     rank  0: 331.15
     rank  1: 541.92
     rank  2: 550.57
     rank  3: 529.71
  pure-backward-compute:
     rank  0: 330.87
     rank  1: 541.60
     rank  2: 550.11
     rank  3: 528.89
  batch-generator:
     rank  0: 14.79
     rank  1: 14.41
     rank  2: 13.24
     rank  3: 14.50
  forward-recv:
     rank  1: 183.30
     rank  2: 281.90
     rank  3: 370.00
  forward-send:
     rank  0: 3.58
     rank  1: 3.20
     rank  2: 0.98
  backward-recv:
     rank  0: 509.35
     rank  1: 228.66
     rank  2: 106.62
  backward-send:
     rank  1: 3.47
     rank  2: 2.92
     rank  3: 1.12
  forward-send-backward-recv:
     rank  0: 350.85
     rank  1: 237.74
     rank  2: 111.35
  backward-send-forward-recv:
     rank  1: 1.83
     rank  2: 4.49
     rank  3: 23.71
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.11
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.08
  all-grads-sync:
     rank  0: 0.87
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.03
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.64
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.05
  optimizer:
     rank  0: 8.61
     rank  1: 1.04
     rank  2: 1.06
     rank  3: 1.04
Layer: TransformerLayer, Forward time: 5.235817 ms
Layer: TransformerLayer, Forward time: 2.834287 ms
Layer: TransformerLayer, Forward time: 2.630073 ms
Layer: TransformerLayer, Forward time: 2.624761 ms
Layer: TransformerLayer, Forward time: 2.574400 ms
Layer: TransformerLayer, Forward time: 2.651162 ms
Layer: TransformerLayer, Forward time: 2.558753 ms
Layer: TransformerLayer, Forward time: 2.655095 ms
Layer: TransformerLayer, Forward time: 2.566035 ms
Layer: TransformerLayer, Forward time: 2.596281 ms
Layer: TransformerLayer, Forward time: 2.612307 ms
Layer: TransformerLayer, Forward time: 2.598440 ms
Layer: TransformerLayer, Forward time: 2.563284 ms
Layer: TransformerLayer, Forward time: 2.570208 ms
Layer: TransformerLayer, Forward time: 2.592639 ms
Layer: TransformerLayer, Forward time: 2.528787 ms
Layer: TransformerLayer, Forward time: 2.580743 ms
Layer: TransformerLayer, Forward time: 2.545221 ms
Layer: TransformerLayer, Forward time: 2.577239 ms
Layer: TransformerLayer, Forward time: 2.575903 ms
Layer: TransformerLayer, Forward time: 2.539710 ms
Layer: TransformerLayer, Forward time: 2.533200 ms
Layer: TransformerLayer, Forward time: 2.616511 ms
Layer: TransformerLayer, Forward time: 2.593280 ms
Layer: TransformerLayer, Forward time: 15.147137 ms
Layer: TransformerLayer, Forward time: 15.361031 ms
Layer: TransformerLayer, Forward time: 15.735255 ms
Layer: TransformerLayer, Forward time: 15.712224 ms
Layer: TransformerLayer, Forward time: 3.226024 ms
Layer: TransformerLayer, Forward time: 2.859288 ms
Layer: TransformerLayer, Forward time: 2.754037 ms
Layer: TransformerLayer, Forward time: 2.735751 ms
Layer: TransformerLayer, Forward time: 2.659513 ms
Layer: TransformerLayer, Forward time: 2.738494 ms
Layer: TransformerLayer, Forward time: 2.682553 ms
Layer: TransformerLayer, Forward time: 2.632502 ms
Layer: TransformerLayer, Forward time: 2.642095 ms
Layer: TransformerLayer, Forward time: 2.683080 ms
Layer: TransformerLayer, Forward time: 2.621397 ms
Layer: TransformerLayer, Forward time: 2.628856 ms
Layer: TransformerLayer, Forward time: 2.676618 ms
Layer: TransformerLayer, Forward time: 2.601876 ms
Layer: TransformerLayer, Forward time: 2.628999 ms
Layer: TransformerLayer, Forward time: 2.681465 ms
Layer: TransformerLayer, Forward time: 2.553304 ms
Layer: TransformerLayer, Forward time: 2.654872 ms
Layer: TransformerLayer, Forward time: 2.597037 ms
Layer: TransformerLayer, Forward time: 2.598090 ms
Layer: TransformerLayer, Forward time: 2.649742 ms
Layer: TransformerLayer, Forward time: 2.592158 ms
Layer: TransformerLayer, Forward time: 2.612648 ms
Layer: TransformerLayer, Forward time: 2.703962 ms
Layer: TransformerLayer, Forward time: 15.314272 ms
Layer: TransformerLayer, Forward time: 15.499221 ms
Layer: TransformerLayer, Forward time: 15.575105 ms
Layer: TransformerLayer, Forward time: 15.742979 ms
Layer: TransformerLayer, Forward time: 2.704371 ms
Layer: TransformerLayer, Forward time: 2.413386 ms
Layer: TransformerLayer, Forward time: 2.282224 ms
Layer: TransformerLayer, Forward time: 2.297407 ms
Layer: TransformerLayer, Forward time: 2.228242 ms
Layer: TransformerLayer, Forward time: 2.196885 ms
Layer: TransformerLayer, Forward time: 2.214936 ms
Layer: TransformerLayer, Forward time: 2.167213 ms
Layer: TransformerLayer, Forward time: 2.171477 ms
Layer: TransformerLayer, Forward time: 2.251242 ms
Layer: TransformerLayer, Forward time: 2.160297 ms
Layer: TransformerLayer, Forward time: 2.168884 ms
Layer: TransformerLayer, Forward time: 2.161852 ms
Layer: TransformerLayer, Forward time: 2.197079 ms
Layer: TransformerLayer, Forward time: 2.177581 ms
Layer: TransformerLayer, Forward time: 2.130291 ms
Layer: TransformerLayer, Forward time: 2.141155 ms
Layer: TransformerLayer, Forward time: 2.084968 ms
Layer: TransformerLayer, Forward time: 2.171408 ms
Layer: TransformerLayer, Forward time: 2.200440 ms
Layer: TransformerLayer, Forward time: 2.096642 ms
Layer: TransformerLayer, Forward time: 2.201661 ms
Layer: TransformerLayer, Forward time: 2.172160 ms
Layer: TransformerLayer, Forward time: 3.923374 ms
Layer: TransformerLayer, Forward time: 16.092394 ms
Layer: TransformerLayer, Forward time: 15.340718 ms
Layer: TransformerLayer, Forward time: 15.756112 ms
Layer: TransformerLayer, Forward time: 15.735614 ms
Layer: TransformerLayer, Forward time: 2.721547 ms
Layer: TransformerLayer, Forward time: 2.285382 ms
Layer: TransformerLayer, Forward time: 2.211146 ms
Layer: TransformerLayer, Forward time: 2.229834 ms
Layer: TransformerLayer, Forward time: 2.227363 ms
Layer: TransformerLayer, Forward time: 2.181477 ms
Layer: TransformerLayer, Forward time: 2.134079 ms
Layer: TransformerLayer, Forward time: 2.223885 ms
Layer: TransformerLayer, Forward time: 2.195799 ms
Layer: TransformerLayer, Forward time: 2.152839 ms
Layer: TransformerLayer, Forward time: 2.214955 ms
Layer: TransformerLayer, Forward time: 2.190281 ms
Layer: TransformerLayer, Forward time: 2.187311 ms
Layer: TransformerLayer, Forward time: 2.518165 ms
Layer: TransformerLayer, Forward time: 4.231458 ms
Layer: TransformerLayer, Forward time: 3.129533 ms
Layer: TransformerLayer, Forward time: 2.254268 ms
Layer: TransformerLayer, Forward time: 2.193241 ms
Layer: TransformerLayer, Forward time: 2.180754 ms
Layer: TransformerLayer, Forward time: 2.129286 ms
Layer: TransformerLayer, Forward time: 2.162451 ms
Layer: TransformerLayer, Forward time: 2.163221 ms
Layer: TransformerLayer, Forward time: 2.148728 ms
Layer: TransformerLayer, Forward time: 2.137328 ms
Layer: TransformerLayer, Forward time: 15.129040 ms
Layer: TransformerLayer, Forward time: 15.194671 ms
Layer: TransformerLayer, Forward time: 15.689846 ms
Layer: TransformerLayer, Forward time: 15.698715 ms
Layer: TransformerLayer, Backward time: 0.087128 ms
Layer: TransformerLayer, Backward time: 0.046472 ms
Layer: TransformerLayer, Backward time: 0.063956 ms
Layer: TransformerLayer, Backward time: 0.041608 ms
Layer: TransformerLayer, Backward time: 0.088508 ms
Layer: TransformerLayer, Backward time: 0.034804 ms
Layer: TransformerLayer, Backward time: 0.031412 ms
Layer: TransformerLayer, Backward time: 0.030792 ms
Layer: TransformerLayer, Backward time: 0.038026 ms
Layer: TransformerLayer, Backward time: 0.034521 ms
Layer: TransformerLayer, Backward time: 0.034046 ms
Layer: TransformerLayer, Backward time: 0.032987 ms
Layer: TransformerLayer, Backward time: 0.037942 ms
Layer: TransformerLayer, Backward time: 0.034110 ms
Layer: TransformerLayer, Backward time: 0.032991 ms
Layer: TransformerLayer, Backward time: 0.034450 ms
 [2024-12-27 15:48:11] iteration        3/      10 | consumed samples:           12 | elapsed time per iteration (ms): 1766.8 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.838976E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1748.05
     rank  1: 1747.38
     rank  2: 1747.57
     rank  3: 1747.62
  forward-compute:
     rank  0: 549.17
     rank  1: 451.84
     rank  2: 456.79
     rank  3: 449.74
  backward-compute:
     rank  0: 332.49
     rank  1: 541.83
     rank  2: 552.18
     rank  3: 533.46
  pure-backward-compute:
     rank  0: 332.12
     rank  1: 541.45
     rank  2: 551.57
     rank  3: 532.35
  batch-generator:
     rank  0: 16.89
     rank  1: 14.77
     rank  2: 13.78
     rank  3: 14.30
  forward-recv:
     rank  1: 191.17
     rank  2: 284.29
     rank  3: 373.97
  forward-send:
     rank  0: 3.68
     rank  1: 3.08
     rank  2: 1.00
  backward-recv:
     rank  0: 512.88
     rank  1: 230.95
     rank  2: 108.52
  backward-send:
     rank  1: 4.27
     rank  2: 3.10
     rank  3: 1.45
  forward-send-backward-recv:
     rank  0: 345.31
     rank  1: 235.24
     rank  2: 112.31
  backward-send-forward-recv:
     rank  1: 1.88
     rank  2: 4.92
     rank  3: 24.78
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.06
     rank  3: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.02
     rank  2: 0.05
     rank  3: 0.12
  all-grads-sync:
     rank  0: 0.69
     rank  1: 0.02
     rank  2: 0.09
     rank  3: 0.04
  optimizer-copy-to-main-grad:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.51
     rank  1: 0.05
     rank  2: 0.12
     rank  3: 0.11
  optimizer:
     rank  0: 8.14
     rank  1: 0.72
     rank  2: 0.78
     rank  3: 0.77
Layer: TransformerLayer, Forward time: 3.255370 ms
Layer: TransformerLayer, Forward time: 2.769480 ms
Layer: TransformerLayer, Forward time: 4.400088 ms
Layer: TransformerLayer, Forward time: 2.697237 ms
Layer: TransformerLayer, Forward time: 2.582804 ms
Layer: TransformerLayer, Forward time: 2.572454 ms
Layer: TransformerLayer, Forward time: 2.616594 ms
Layer: TransformerLayer, Forward time: 2.555400 ms
Layer: TransformerLayer, Forward time: 3.102880 ms
Layer: TransformerLayer, Forward time: 2.623430 ms
Layer: TransformerLayer, Forward time: 2.602566 ms
Layer: TransformerLayer, Forward time: 2.651623 ms
Layer: TransformerLayer, Forward time: 2.603240 ms
Layer: TransformerLayer, Forward time: 2.610704 ms
Layer: TransformerLayer, Forward time: 2.586061 ms
Layer: TransformerLayer, Forward time: 2.682760 ms
Layer: TransformerLayer, Forward time: 2.604484 ms
Layer: TransformerLayer, Forward time: 2.586767 ms
Layer: TransformerLayer, Forward time: 2.628402 ms
Layer: TransformerLayer, Forward time: 2.581050 ms
Layer: TransformerLayer, Forward time: 2.573637 ms
Layer: TransformerLayer, Forward time: 2.578852 ms
Layer: TransformerLayer, Forward time: 2.571333 ms
Layer: TransformerLayer, Forward time: 2.610341 ms
Layer: TransformerLayer, Forward time: 15.183287 ms
Layer: TransformerLayer, Forward time: 15.420589 ms
Layer: TransformerLayer, Forward time: 15.482201 ms
Layer: TransformerLayer, Forward time: 15.549586 ms
Layer: TransformerLayer, Forward time: 3.262900 ms
Layer: TransformerLayer, Forward time: 2.857230 ms
Layer: TransformerLayer, Forward time: 2.683900 ms
Layer: TransformerLayer, Forward time: 2.737321 ms
Layer: TransformerLayer, Forward time: 2.658725 ms
Layer: TransformerLayer, Forward time: 2.695931 ms
Layer: TransformerLayer, Forward time: 2.637017 ms
Layer: TransformerLayer, Forward time: 2.730927 ms
Layer: TransformerLayer, Forward time: 2.634303 ms
Layer: TransformerLayer, Forward time: 2.646832 ms
Layer: TransformerLayer, Forward time: 2.748095 ms
Layer: TransformerLayer, Forward time: 2.662939 ms
Layer: TransformerLayer, Forward time: 2.620973 ms
Layer: TransformerLayer, Forward time: 2.713881 ms
Layer: TransformerLayer, Forward time: 2.643013 ms
Layer: TransformerLayer, Forward time: 2.544917 ms
Layer: TransformerLayer, Forward time: 2.675859 ms
Layer: TransformerLayer, Forward time: 2.593893 ms
Layer: TransformerLayer, Forward time: 2.668878 ms
Layer: TransformerLayer, Forward time: 2.626548 ms
Layer: TransformerLayer, Forward time: 2.685795 ms
Layer: TransformerLayer, Forward time: 2.586918 ms
Layer: TransformerLayer, Forward time: 2.607304 ms
Layer: TransformerLayer, Forward time: 2.582081 ms
Layer: TransformerLayer, Forward time: 16.864886 ms
Layer: TransformerLayer, Forward time: 16.771915 ms
Layer: TransformerLayer, Forward time: 15.557554 ms
Layer: TransformerLayer, Forward time: 15.562819 ms
Layer: TransformerLayer, Forward time: 3.239178 ms
Layer: TransformerLayer, Forward time: 2.846909 ms
Layer: TransformerLayer, Forward time: 2.810154 ms
Layer: TransformerLayer, Forward time: 2.743338 ms
Layer: TransformerLayer, Forward time: 2.752926 ms
Layer: TransformerLayer, Forward time: 2.691419 ms
Layer: TransformerLayer, Forward time: 2.662431 ms
Layer: TransformerLayer, Forward time: 2.671798 ms
Layer: TransformerLayer, Forward time: 2.648077 ms
Layer: TransformerLayer, Forward time: 2.593990 ms
Layer: TransformerLayer, Forward time: 2.716640 ms
Layer: TransformerLayer, Forward time: 2.671028 ms
Layer: TransformerLayer, Forward time: 2.656037 ms
Layer: TransformerLayer, Forward time: 2.605303 ms
Layer: TransformerLayer, Forward time: 2.558786 ms
Layer: TransformerLayer, Forward time: 2.650350 ms
Layer: TransformerLayer, Forward time: 2.577477 ms
Layer: TransformerLayer, Forward time: 2.564018 ms
Layer: TransformerLayer, Forward time: 2.534218 ms
Layer: TransformerLayer, Forward time: 2.593175 ms
Layer: TransformerLayer, Forward time: 2.556336 ms
Layer: TransformerLayer, Forward time: 2.537568 ms
Layer: TransformerLayer, Forward time: 2.582145 ms
Layer: TransformerLayer, Forward time: 2.534083 ms
Layer: TransformerLayer, Forward time: 15.252444 ms
Layer: TransformerLayer, Forward time: 15.220551 ms
Layer: TransformerLayer, Forward time: 15.704720 ms
Layer: TransformerLayer, Forward time: 15.692887 ms
Layer: TransformerLayer, Forward time: 3.758609 ms
Layer: TransformerLayer, Forward time: 2.028578 ms
Layer: TransformerLayer, Forward time: 1.906916 ms
Layer: TransformerLayer, Forward time: 1.904613 ms
Layer: TransformerLayer, Forward time: 1.865127 ms
Layer: TransformerLayer, Forward time: 1.837343 ms
Layer: TransformerLayer, Forward time: 1.846648 ms
Layer: TransformerLayer, Forward time: 1.791461 ms
Layer: TransformerLayer, Forward time: 1.811262 ms
Layer: TransformerLayer, Forward time: 1.802691 ms
Layer: TransformerLayer, Forward time: 1.770649 ms
Layer: TransformerLayer, Forward time: 1.826463 ms
Layer: TransformerLayer, Forward time: 1.746044 ms
Layer: TransformerLayer, Forward time: 1.613064 ms
Layer: TransformerLayer, Forward time: 1.651092 ms
Layer: TransformerLayer, Forward time: 1.611322 ms
Layer: TransformerLayer, Forward time: 1.643364 ms
Layer: TransformerLayer, Forward time: 1.617740 ms
Layer: TransformerLayer, Forward time: 1.649236 ms
Layer: TransformerLayer, Forward time: 1.779878 ms
Layer: TransformerLayer, Forward time: 1.790778 ms
Layer: TransformerLayer, Forward time: 1.795113 ms
Layer: TransformerLayer, Forward time: 1.650187 ms
Layer: TransformerLayer, Forward time: 1.629512 ms
Layer: TransformerLayer, Forward time: 15.207149 ms
Layer: TransformerLayer, Forward time: 15.241340 ms
Layer: TransformerLayer, Forward time: 15.642192 ms
Layer: TransformerLayer, Forward time: 15.674052 ms
Layer: TransformerLayer, Backward time: 0.084657 ms
Layer: TransformerLayer, Backward time: 0.039790 ms
Layer: TransformerLayer, Backward time: 0.051964 ms
Layer: TransformerLayer, Backward time: 0.045871 ms
Layer: TransformerLayer, Backward time: 0.085270 ms
Layer: TransformerLayer, Backward time: 0.035441 ms
Layer: TransformerLayer, Backward time: 0.030858 ms
Layer: TransformerLayer, Backward time: 0.032992 ms
Layer: TransformerLayer, Backward time: 0.076268 ms
Layer: TransformerLayer, Backward time: 0.032165 ms
Layer: TransformerLayer, Backward time: 0.028783 ms
Layer: TransformerLayer, Backward time: 0.029535 ms
Layer: TransformerLayer, Backward time: 0.032025 ms
Layer: TransformerLayer, Backward time: 0.029243 ms
Layer: TransformerLayer, Backward time: 0.028201 ms
Layer: TransformerLayer, Backward time: 0.027952 ms
 [2024-12-27 15:48:12] iteration        4/      10 | consumed samples:           16 | elapsed time per iteration (ms): 1761.0 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.115121E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1743.42
     rank  1: 1742.86
     rank  2: 1742.95
     rank  3: 1742.96
  forward-compute:
     rank  0: 541.50
     rank  1: 452.68
     rank  2: 458.74
     rank  3: 451.51
  backward-compute:
     rank  0: 330.32
     rank  1: 542.33
     rank  2: 551.52
     rank  3: 529.02
  pure-backward-compute:
     rank  0: 330.15
     rank  1: 542.02
     rank  2: 550.96
     rank  3: 528.18
  batch-generator:
     rank  0: 15.24
     rank  1: 15.58
     rank  2: 15.57
     rank  3: 14.44
  forward-recv:
     rank  1: 198.34
     rank  2: 285.24
     rank  3: 377.14
  forward-send:
     rank  0: 4.26
     rank  1: 3.20
     rank  2: 1.68
  backward-recv:
     rank  0: 508.89
     rank  1: 226.35
     rank  2: 106.08
  backward-send:
     rank  1: 2.75
     rank  2: 2.63
     rank  3: 0.93
  forward-send-backward-recv:
     rank  0: 354.94
     rank  1: 229.04
     rank  2: 110.56
  backward-send-forward-recv:
     rank  1: 1.84
     rank  2: 4.00
     rank  3: 23.09
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.07
  all-grads-sync:
     rank  0: 0.57
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.38
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
  optimizer:
     rank  0: 7.87
     rank  1: 0.55
     rank  2: 0.56
     rank  3: 0.55
Layer: TransformerLayer, Forward time: 2.103290 ms
Layer: TransformerLayer, Forward time: 1.777065 ms
Layer: TransformerLayer, Forward time: 1.738379 ms
Layer: TransformerLayer, Forward time: 1.689091 ms
Layer: TransformerLayer, Forward time: 1.652652 ms
Layer: TransformerLayer, Forward time: 1.687757 ms
Layer: TransformerLayer, Forward time: 1.651546 ms
Layer: TransformerLayer, Forward time: 1.671768 ms
Layer: TransformerLayer, Forward time: 1.657029 ms
Layer: TransformerLayer, Forward time: 1.737237 ms
Layer: TransformerLayer, Forward time: 1.665042 ms
Layer: TransformerLayer, Forward time: 1.627443 ms
Layer: TransformerLayer, Forward time: 2.830746 ms
Layer: TransformerLayer, Forward time: 1.681081 ms
Layer: TransformerLayer, Forward time: 1.630560 ms
Layer: TransformerLayer, Forward time: 1.645502 ms
Layer: TransformerLayer, Forward time: 1.573597 ms
Layer: TransformerLayer, Forward time: 1.561476 ms
Layer: TransformerLayer, Forward time: 1.559002 ms
Layer: TransformerLayer, Forward time: 1.558187 ms
Layer: TransformerLayer, Forward time: 1.576043 ms
Layer: TransformerLayer, Forward time: 1.559114 ms
Layer: TransformerLayer, Forward time: 1.582164 ms
Layer: TransformerLayer, Forward time: 1.623034 ms
Layer: TransformerLayer, Forward time: 15.008643 ms
Layer: TransformerLayer, Forward time: 15.224289 ms
Layer: TransformerLayer, Forward time: 15.756635 ms
Layer: TransformerLayer, Forward time: 15.679194 ms
Layer: TransformerLayer, Forward time: 2.096655 ms
Layer: TransformerLayer, Forward time: 1.736101 ms
Layer: TransformerLayer, Forward time: 1.716295 ms
Layer: TransformerLayer, Forward time: 1.760096 ms
Layer: TransformerLayer, Forward time: 1.711158 ms
Layer: TransformerLayer, Forward time: 1.761714 ms
Layer: TransformerLayer, Forward time: 1.735130 ms
Layer: TransformerLayer, Forward time: 1.681744 ms
Layer: TransformerLayer, Forward time: 1.628604 ms
Layer: TransformerLayer, Forward time: 1.625011 ms
Layer: TransformerLayer, Forward time: 1.555965 ms
Layer: TransformerLayer, Forward time: 1.557734 ms
Layer: TransformerLayer, Forward time: 1.597112 ms
Layer: TransformerLayer, Forward time: 1.642258 ms
Layer: TransformerLayer, Forward time: 1.617225 ms
Layer: TransformerLayer, Forward time: 1.638806 ms
Layer: TransformerLayer, Forward time: 1.646705 ms
Layer: TransformerLayer, Forward time: 1.583457 ms
Layer: TransformerLayer, Forward time: 1.599161 ms
Layer: TransformerLayer, Forward time: 1.624793 ms
Layer: TransformerLayer, Forward time: 1.638957 ms
Layer: TransformerLayer, Forward time: 1.634278 ms
Layer: TransformerLayer, Forward time: 1.643323 ms
Layer: TransformerLayer, Forward time: 2.942801 ms
Layer: TransformerLayer, Forward time: 15.166017 ms
Layer: TransformerLayer, Forward time: 15.230622 ms
Layer: TransformerLayer, Forward time: 15.623461 ms
Layer: TransformerLayer, Forward time: 15.674396 ms
Layer: TransformerLayer, Forward time: 3.610665 ms
Layer: TransformerLayer, Forward time: 1.991003 ms
Layer: TransformerLayer, Forward time: 1.886988 ms
Layer: TransformerLayer, Forward time: 1.889469 ms
Layer: TransformerLayer, Forward time: 1.816835 ms
Layer: TransformerLayer, Forward time: 1.785161 ms
Layer: TransformerLayer, Forward time: 1.817214 ms
Layer: TransformerLayer, Forward time: 1.805896 ms
Layer: TransformerLayer, Forward time: 1.777801 ms
Layer: TransformerLayer, Forward time: 1.809328 ms
Layer: TransformerLayer, Forward time: 1.786575 ms
Layer: TransformerLayer, Forward time: 1.637694 ms
Layer: TransformerLayer, Forward time: 1.637857 ms
Layer: TransformerLayer, Forward time: 1.616425 ms
Layer: TransformerLayer, Forward time: 1.654900 ms
Layer: TransformerLayer, Forward time: 1.611594 ms
Layer: TransformerLayer, Forward time: 1.680121 ms
Layer: TransformerLayer, Forward time: 1.743972 ms
Layer: TransformerLayer, Forward time: 1.765362 ms
Layer: TransformerLayer, Forward time: 1.809684 ms
Layer: TransformerLayer, Forward time: 1.784122 ms
Layer: TransformerLayer, Forward time: 1.805783 ms
Layer: TransformerLayer, Forward time: 1.618520 ms
Layer: TransformerLayer, Forward time: 1.636632 ms
Layer: TransformerLayer, Forward time: 17.010632 ms
Layer: TransformerLayer, Forward time: 15.282350 ms
Layer: TransformerLayer, Forward time: 15.711990 ms
Layer: TransformerLayer, Forward time: 15.688521 ms
Layer: TransformerLayer, Forward time: 2.221572 ms
Layer: TransformerLayer, Forward time: 1.931801 ms
Layer: TransformerLayer, Forward time: 1.853525 ms
Layer: TransformerLayer, Forward time: 1.676009 ms
Layer: TransformerLayer, Forward time: 1.691723 ms
Layer: TransformerLayer, Forward time: 1.623090 ms
Layer: TransformerLayer, Forward time: 1.654986 ms
Layer: TransformerLayer, Forward time: 1.632820 ms
Layer: TransformerLayer, Forward time: 1.625380 ms
Layer: TransformerLayer, Forward time: 1.651413 ms
Layer: TransformerLayer, Forward time: 1.628049 ms
Layer: TransformerLayer, Forward time: 1.668185 ms
Layer: TransformerLayer, Forward time: 1.659579 ms
Layer: TransformerLayer, Forward time: 1.628895 ms
Layer: TransformerLayer, Forward time: 1.638332 ms
Layer: TransformerLayer, Forward time: 1.615972 ms
Layer: TransformerLayer, Forward time: 1.605075 ms
Layer: TransformerLayer, Forward time: 1.609265 ms
Layer: TransformerLayer, Forward time: 1.671204 ms
Layer: TransformerLayer, Forward time: 1.733549 ms
Layer: TransformerLayer, Forward time: 1.844350 ms
Layer: TransformerLayer, Forward time: 1.809645 ms
Layer: TransformerLayer, Forward time: 1.802293 ms
Layer: TransformerLayer, Forward time: 1.840231 ms
Layer: TransformerLayer, Forward time: 15.195070 ms
Layer: TransformerLayer, Forward time: 15.345697 ms
Layer: TransformerLayer, Forward time: 15.645997 ms
Layer: TransformerLayer, Forward time: 15.650969 ms
Layer: TransformerLayer, Backward time: 0.066980 ms
Layer: TransformerLayer, Backward time: 0.053901 ms
Layer: TransformerLayer, Backward time: 0.059046 ms
Layer: TransformerLayer, Backward time: 0.053034 ms
Layer: TransformerLayer, Backward time: 0.038439 ms
Layer: TransformerLayer, Backward time: 0.033682 ms
Layer: TransformerLayer, Backward time: 0.031695 ms
Layer: TransformerLayer, Backward time: 0.030641 ms
Layer: TransformerLayer, Backward time: 0.041053 ms
Layer: TransformerLayer, Backward time: 0.031168 ms
Layer: TransformerLayer, Backward time: 0.030610 ms
Layer: TransformerLayer, Backward time: 0.030504 ms
Layer: TransformerLayer, Backward time: 0.031810 ms
Layer: TransformerLayer, Backward time: 0.033669 ms
Layer: TransformerLayer, Backward time: 0.029055 ms
Layer: TransformerLayer, Backward time: 0.030630 ms
 [2024-12-27 15:48:14] iteration        5/      10 | consumed samples:           20 | elapsed time per iteration (ms): 1735.2 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.523976E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1718.50
     rank  1: 1717.93
     rank  2: 1717.93
     rank  3: 1717.98
  forward-compute:
     rank  0: 458.95
     rank  1: 454.33
     rank  2: 459.76
     rank  3: 453.30
  backward-compute:
     rank  0: 330.09
     rank  1: 542.96
     rank  2: 552.66
     rank  3: 531.47
  pure-backward-compute:
     rank  0: 329.90
     rank  1: 542.60
     rank  2: 551.98
     rank  3: 530.64
  batch-generator:
     rank  0: 11.22
     rank  1: 14.46
     rank  2: 14.58
     rank  3: 16.38
  forward-recv:
     rank  1: 118.72
     rank  2: 229.83
     rank  3: 343.65
  forward-send:
     rank  0: 4.32
     rank  1: 2.57
     rank  2: 0.95
  backward-recv:
     rank  0: 513.60
     rank  1: 229.59
     rank  2: 107.92
  backward-send:
     rank  1: 2.74
     rank  2: 2.59
     rank  3: 0.96
  forward-send-backward-recv:
     rank  0: 407.97
     rank  1: 279.21
     rank  2: 137.71
  backward-send-forward-recv:
     rank  1: 1.87
     rank  2: 4.27
     rank  3: 27.92
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.08
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.51
     rank  1: 0.09
     rank  2: 0.10
     rank  3: 0.09
  optimizer:
     rank  0: 8.06
     rank  1: 0.66
     rank  2: 0.66
     rank  3: 0.65
Layer: TransformerLayer, Forward time: 2.512728 ms
Layer: TransformerLayer, Forward time: 1.765072 ms
Layer: TransformerLayer, Forward time: 1.656290 ms
Layer: TransformerLayer, Forward time: 1.679044 ms
Layer: TransformerLayer, Forward time: 1.623375 ms
Layer: TransformerLayer, Forward time: 1.623713 ms
Layer: TransformerLayer, Forward time: 1.639501 ms
Layer: TransformerLayer, Forward time: 1.626697 ms
Layer: TransformerLayer, Forward time: 1.608923 ms
Layer: TransformerLayer, Forward time: 1.615633 ms
Layer: TransformerLayer, Forward time: 1.598235 ms
Layer: TransformerLayer, Forward time: 1.612358 ms
Layer: TransformerLayer, Forward time: 1.618290 ms
Layer: TransformerLayer, Forward time: 1.597634 ms
Layer: TransformerLayer, Forward time: 1.588583 ms
Layer: TransformerLayer, Forward time: 1.646736 ms
Layer: TransformerLayer, Forward time: 1.679364 ms
Layer: TransformerLayer, Forward time: 1.654745 ms
Layer: TransformerLayer, Forward time: 1.646685 ms
Layer: TransformerLayer, Forward time: 1.607306 ms
Layer: TransformerLayer, Forward time: 1.547670 ms
Layer: TransformerLayer, Forward time: 1.589130 ms
Layer: TransformerLayer, Forward time: 1.557137 ms
Layer: TransformerLayer, Forward time: 1.529379 ms
Layer: TransformerLayer, Forward time: 15.119805 ms
Layer: TransformerLayer, Forward time: 15.029075 ms
Layer: TransformerLayer, Forward time: 15.555854 ms
Layer: TransformerLayer, Forward time: 15.545931 ms
Layer: TransformerLayer, Forward time: 2.044203 ms
Layer: TransformerLayer, Forward time: 1.786288 ms
Layer: TransformerLayer, Forward time: 1.659721 ms
Layer: TransformerLayer, Forward time: 1.661248 ms
Layer: TransformerLayer, Forward time: 1.541385 ms
Layer: TransformerLayer, Forward time: 1.535791 ms
Layer: TransformerLayer, Forward time: 1.551475 ms
Layer: TransformerLayer, Forward time: 1.630502 ms
Layer: TransformerLayer, Forward time: 1.664631 ms
Layer: TransformerLayer, Forward time: 1.607391 ms
Layer: TransformerLayer, Forward time: 1.615345 ms
Layer: TransformerLayer, Forward time: 1.657974 ms
Layer: TransformerLayer, Forward time: 1.616720 ms
Layer: TransformerLayer, Forward time: 1.630590 ms
Layer: TransformerLayer, Forward time: 1.614217 ms
Layer: TransformerLayer, Forward time: 1.602829 ms
Layer: TransformerLayer, Forward time: 1.613481 ms
Layer: TransformerLayer, Forward time: 1.639147 ms
Layer: TransformerLayer, Forward time: 1.596148 ms
Layer: TransformerLayer, Forward time: 1.639253 ms
Layer: TransformerLayer, Forward time: 1.626700 ms
Layer: TransformerLayer, Forward time: 1.657027 ms
Layer: TransformerLayer, Forward time: 1.689423 ms
Layer: TransformerLayer, Forward time: 2.887051 ms
Layer: TransformerLayer, Forward time: 15.144157 ms
Layer: TransformerLayer, Forward time: 15.313521 ms
Layer: TransformerLayer, Forward time: 15.417675 ms
Layer: TransformerLayer, Forward time: 15.544599 ms
Layer: TransformerLayer, Forward time: 2.206361 ms
Layer: TransformerLayer, Forward time: 1.956202 ms
Layer: TransformerLayer, Forward time: 1.894647 ms
Layer: TransformerLayer, Forward time: 1.850295 ms
Layer: TransformerLayer, Forward time: 1.786742 ms
Layer: TransformerLayer, Forward time: 1.849331 ms
Layer: TransformerLayer, Forward time: 1.813725 ms
Layer: TransformerLayer, Forward time: 1.744922 ms
Layer: TransformerLayer, Forward time: 1.771783 ms
Layer: TransformerLayer, Forward time: 1.770017 ms
Layer: TransformerLayer, Forward time: 1.762077 ms
Layer: TransformerLayer, Forward time: 1.774926 ms
Layer: TransformerLayer, Forward time: 1.743991 ms
Layer: TransformerLayer, Forward time: 1.613377 ms
Layer: TransformerLayer, Forward time: 1.617296 ms
Layer: TransformerLayer, Forward time: 1.581723 ms
Layer: TransformerLayer, Forward time: 1.645236 ms
Layer: TransformerLayer, Forward time: 1.609435 ms
Layer: TransformerLayer, Forward time: 1.664765 ms
Layer: TransformerLayer, Forward time: 1.622269 ms
Layer: TransformerLayer, Forward time: 1.639018 ms
Layer: TransformerLayer, Forward time: 1.627192 ms
Layer: TransformerLayer, Forward time: 1.647451 ms
Layer: TransformerLayer, Forward time: 1.718104 ms
Layer: TransformerLayer, Forward time: 15.160786 ms
Layer: TransformerLayer, Forward time: 15.363571 ms
Layer: TransformerLayer, Forward time: 15.556618 ms
Layer: TransformerLayer, Forward time: 15.787711 ms
Layer: TransformerLayer, Forward time: 2.231760 ms
Layer: TransformerLayer, Forward time: 1.928525 ms
Layer: TransformerLayer, Forward time: 1.865912 ms
Layer: TransformerLayer, Forward time: 1.807388 ms
Layer: TransformerLayer, Forward time: 1.654495 ms
Layer: TransformerLayer, Forward time: 1.782638 ms
Layer: TransformerLayer, Forward time: 1.668865 ms
Layer: TransformerLayer, Forward time: 1.692938 ms
Layer: TransformerLayer, Forward time: 1.793745 ms
Layer: TransformerLayer, Forward time: 1.760099 ms
Layer: TransformerLayer, Forward time: 1.731189 ms
Layer: TransformerLayer, Forward time: 1.730173 ms
Layer: TransformerLayer, Forward time: 1.814548 ms
Layer: TransformerLayer, Forward time: 1.851908 ms
Layer: TransformerLayer, Forward time: 1.863450 ms
Layer: TransformerLayer, Forward time: 1.786803 ms
Layer: TransformerLayer, Forward time: 1.832518 ms
Layer: TransformerLayer, Forward time: 1.788916 ms
Layer: TransformerLayer, Forward time: 1.812749 ms
Layer: TransformerLayer, Forward time: 1.769287 ms
Layer: TransformerLayer, Forward time: 1.765427 ms
Layer: TransformerLayer, Forward time: 1.777292 ms
Layer: TransformerLayer, Forward time: 1.759440 ms
Layer: TransformerLayer, Forward time: 1.764467 ms
Layer: TransformerLayer, Forward time: 15.133627 ms
Layer: TransformerLayer, Forward time: 15.367172 ms
Layer: TransformerLayer, Forward time: 15.592995 ms
Layer: TransformerLayer, Forward time: 15.701850 ms
Layer: TransformerLayer, Backward time: 0.083300 ms
Layer: TransformerLayer, Backward time: 0.045091 ms
Layer: TransformerLayer, Backward time: 0.062407 ms
Layer: TransformerLayer, Backward time: 0.039183 ms
Layer: TransformerLayer, Backward time: 0.038976 ms
Layer: TransformerLayer, Backward time: 0.033751 ms
Layer: TransformerLayer, Backward time: 0.031634 ms
Layer: TransformerLayer, Backward time: 0.030252 ms
Layer: TransformerLayer, Backward time: 0.049608 ms
Layer: TransformerLayer, Backward time: 0.031119 ms
Layer: TransformerLayer, Backward time: 0.029431 ms
Layer: TransformerLayer, Backward time: 0.029209 ms
Layer: TransformerLayer, Backward time: 0.030664 ms
Layer: TransformerLayer, Backward time: 0.029413 ms
Layer: TransformerLayer, Backward time: 0.029492 ms
Layer: TransformerLayer, Backward time: 0.029078 ms
 [2024-12-27 15:48:16] iteration        6/      10 | consumed samples:           24 | elapsed time per iteration (ms): 1724.4 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.451799E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1707.74
     rank  1: 1707.16
     rank  2: 1707.30
     rank  3: 1707.30
  forward-compute:
     rank  0: 455.24
     rank  1: 452.88
     rank  2: 456.62
     rank  3: 450.15
  backward-compute:
     rank  0: 330.05
     rank  1: 542.27
     rank  2: 551.71
     rank  3: 529.62
  pure-backward-compute:
     rank  0: 329.78
     rank  1: 541.99
     rank  2: 551.20
     rank  3: 528.84
  batch-generator:
     rank  0: 11.37
     rank  1: 13.17
     rank  2: 13.15
     rank  3: 14.30
  forward-recv:
     rank  1: 114.31
     rank  2: 228.15
     rank  3: 340.90
  forward-send:
     rank  0: 4.12
     rank  1: 2.53
     rank  2: 0.96
  backward-recv:
     rank  0: 510.01
     rank  1: 227.25
     rank  2: 106.17
  backward-send:
     rank  1: 2.75
     rank  2: 2.53
     rank  3: 0.95
  forward-send-backward-recv:
     rank  0: 404.79
     rank  1: 277.45
     rank  2: 134.15
  backward-send-forward-recv:
     rank  1: 1.83
     rank  2: 4.15
     rank  3: 24.15
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.07
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.38
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
  optimizer:
     rank  0: 7.88
     rank  1: 0.54
     rank  2: 0.53
     rank  3: 0.54
Layer: TransformerLayer, Forward time: 2.038392 ms
Layer: TransformerLayer, Forward time: 1.791307 ms
Layer: TransformerLayer, Forward time: 1.704522 ms
Layer: TransformerLayer, Forward time: 1.660587 ms
Layer: TransformerLayer, Forward time: 1.688766 ms
Layer: TransformerLayer, Forward time: 1.675828 ms
Layer: TransformerLayer, Forward time: 1.701606 ms
Layer: TransformerLayer, Forward time: 1.674698 ms
Layer: TransformerLayer, Forward time: 1.645041 ms
Layer: TransformerLayer, Forward time: 1.662960 ms
Layer: TransformerLayer, Forward time: 1.644693 ms
Layer: TransformerLayer, Forward time: 1.637534 ms
Layer: TransformerLayer, Forward time: 1.692630 ms
Layer: TransformerLayer, Forward time: 1.661315 ms
Layer: TransformerLayer, Forward time: 1.655742 ms
Layer: TransformerLayer, Forward time: 1.617078 ms
Layer: TransformerLayer, Forward time: 1.662161 ms
Layer: TransformerLayer, Forward time: 1.625527 ms
Layer: TransformerLayer, Forward time: 1.609151 ms
Layer: TransformerLayer, Forward time: 1.660982 ms
Layer: TransformerLayer, Forward time: 1.611311 ms
Layer: TransformerLayer, Forward time: 1.636468 ms
Layer: TransformerLayer, Forward time: 1.612206 ms
Layer: TransformerLayer, Forward time: 1.588108 ms
Layer: TransformerLayer, Forward time: 14.987569 ms
Layer: TransformerLayer, Forward time: 15.069584 ms
Layer: TransformerLayer, Forward time: 15.055076 ms
Layer: TransformerLayer, Forward time: 15.277631 ms
Layer: TransformerLayer, Forward time: 2.038220 ms
Layer: TransformerLayer, Forward time: 1.740434 ms
Layer: TransformerLayer, Forward time: 1.657539 ms
Layer: TransformerLayer, Forward time: 1.582435 ms
Layer: TransformerLayer, Forward time: 1.529048 ms
Layer: TransformerLayer, Forward time: 1.588442 ms
Layer: TransformerLayer, Forward time: 1.635273 ms
Layer: TransformerLayer, Forward time: 1.660538 ms
Layer: TransformerLayer, Forward time: 1.629859 ms
Layer: TransformerLayer, Forward time: 1.612722 ms
Layer: TransformerLayer, Forward time: 1.643263 ms
Layer: TransformerLayer, Forward time: 1.542969 ms
Layer: TransformerLayer, Forward time: 1.548254 ms
Layer: TransformerLayer, Forward time: 1.538200 ms
Layer: TransformerLayer, Forward time: 1.573732 ms
Layer: TransformerLayer, Forward time: 1.565148 ms
Layer: TransformerLayer, Forward time: 1.583522 ms
Layer: TransformerLayer, Forward time: 1.627162 ms
Layer: TransformerLayer, Forward time: 1.629390 ms
Layer: TransformerLayer, Forward time: 1.597040 ms
Layer: TransformerLayer, Forward time: 1.626318 ms
Layer: TransformerLayer, Forward time: 1.579666 ms
Layer: TransformerLayer, Forward time: 1.510800 ms
Layer: TransformerLayer, Forward time: 1.568088 ms
Layer: TransformerLayer, Forward time: 15.281008 ms
Layer: TransformerLayer, Forward time: 15.543326 ms
Layer: TransformerLayer, Forward time: 15.536694 ms
Layer: TransformerLayer, Forward time: 15.613683 ms
Layer: TransformerLayer, Forward time: 2.367610 ms
Layer: TransformerLayer, Forward time: 1.967073 ms
Layer: TransformerLayer, Forward time: 1.907648 ms
Layer: TransformerLayer, Forward time: 1.752964 ms
Layer: TransformerLayer, Forward time: 1.664738 ms
Layer: TransformerLayer, Forward time: 1.697927 ms
Layer: TransformerLayer, Forward time: 1.639374 ms
Layer: TransformerLayer, Forward time: 1.634221 ms
Layer: TransformerLayer, Forward time: 1.676227 ms
Layer: TransformerLayer, Forward time: 1.625302 ms
Layer: TransformerLayer, Forward time: 1.629355 ms
Layer: TransformerLayer, Forward time: 1.671123 ms
Layer: TransformerLayer, Forward time: 1.655582 ms
Layer: TransformerLayer, Forward time: 1.600547 ms
Layer: TransformerLayer, Forward time: 1.639669 ms
Layer: TransformerLayer, Forward time: 1.632900 ms
Layer: TransformerLayer, Forward time: 1.606255 ms
Layer: TransformerLayer, Forward time: 1.645783 ms
Layer: TransformerLayer, Forward time: 1.599989 ms
Layer: TransformerLayer, Forward time: 1.589177 ms
Layer: TransformerLayer, Forward time: 1.634955 ms
Layer: TransformerLayer, Forward time: 1.628519 ms
Layer: TransformerLayer, Forward time: 3.183140 ms
Layer: TransformerLayer, Forward time: 1.686925 ms
Layer: TransformerLayer, Forward time: 15.213107 ms
Layer: TransformerLayer, Forward time: 15.423120 ms
Layer: TransformerLayer, Forward time: 15.499340 ms
Layer: TransformerLayer, Forward time: 15.564074 ms
Layer: TransformerLayer, Forward time: 2.217778 ms
Layer: TransformerLayer, Forward time: 1.916455 ms
Layer: TransformerLayer, Forward time: 1.854276 ms
Layer: TransformerLayer, Forward time: 1.804102 ms
Layer: TransformerLayer, Forward time: 1.831040 ms
Layer: TransformerLayer, Forward time: 1.773214 ms
Layer: TransformerLayer, Forward time: 1.821159 ms
Layer: TransformerLayer, Forward time: 1.756419 ms
Layer: TransformerLayer, Forward time: 1.746356 ms
Layer: TransformerLayer, Forward time: 1.777581 ms
Layer: TransformerLayer, Forward time: 1.741052 ms
Layer: TransformerLayer, Forward time: 1.725100 ms
Layer: TransformerLayer, Forward time: 1.658954 ms
Layer: TransformerLayer, Forward time: 1.629097 ms
Layer: TransformerLayer, Forward time: 1.575859 ms
Layer: TransformerLayer, Forward time: 1.651279 ms
Layer: TransformerLayer, Forward time: 1.597250 ms
Layer: TransformerLayer, Forward time: 1.571120 ms
Layer: TransformerLayer, Forward time: 1.599521 ms
Layer: TransformerLayer, Forward time: 1.593036 ms
Layer: TransformerLayer, Forward time: 1.657034 ms
Layer: TransformerLayer, Forward time: 1.627987 ms
Layer: TransformerLayer, Forward time: 1.585921 ms
Layer: TransformerLayer, Forward time: 1.613608 ms
Layer: TransformerLayer, Forward time: 15.234451 ms
Layer: TransformerLayer, Forward time: 15.601484 ms
Layer: TransformerLayer, Forward time: 15.613623 ms
Layer: TransformerLayer, Forward time: 15.666047 ms
Layer: TransformerLayer, Backward time: 0.084714 ms
Layer: TransformerLayer, Backward time: 0.037618 ms
Layer: TransformerLayer, Backward time: 0.053976 ms
Layer: TransformerLayer, Backward time: 0.039113 ms
Layer: TransformerLayer, Backward time: 0.038643 ms
Layer: TransformerLayer, Backward time: 0.032575 ms
Layer: TransformerLayer, Backward time: 0.031975 ms
Layer: TransformerLayer, Backward time: 0.030803 ms
Layer: TransformerLayer, Backward time: 0.033629 ms
Layer: TransformerLayer, Backward time: 0.031954 ms
Layer: TransformerLayer, Backward time: 0.028562 ms
Layer: TransformerLayer, Backward time: 0.028095 ms
Layer: TransformerLayer, Backward time: 0.076485 ms
Layer: TransformerLayer, Backward time: 0.030205 ms
Layer: TransformerLayer, Backward time: 0.028303 ms
Layer: TransformerLayer, Backward time: 0.028123 ms
 [2024-12-27 15:48:18] iteration        7/      10 | consumed samples:           28 | elapsed time per iteration (ms): 1737.9 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.145542E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1720.73
     rank  1: 1720.04
     rank  2: 1720.18
     rank  3: 1720.24
  forward-compute:
     rank  0: 449.40
     rank  1: 451.29
     rank  2: 459.36
     rank  3: 460.07
  backward-compute:
     rank  0: 331.48
     rank  1: 542.52
     rank  2: 552.43
     rank  3: 529.06
  pure-backward-compute:
     rank  0: 331.32
     rank  1: 542.32
     rank  2: 551.92
     rank  3: 528.33
  batch-generator:
     rank  0: 10.71
     rank  1: 11.96
     rank  2: 14.54
     rank  3: 19.59
  forward-recv:
     rank  1: 113.14
     rank  2: 226.43
     rank  3: 342.28
  forward-send:
     rank  0: 8.49
     rank  1: 4.47
     rank  2: 1.10
  backward-recv:
     rank  0: 510.03
     rank  1: 228.13
     rank  2: 106.76
  backward-send:
     rank  1: 2.69
     rank  2: 2.38
     rank  3: 0.95
  forward-send-backward-recv:
     rank  0: 418.20
     rank  1: 289.89
     rank  2: 144.65
  backward-send-forward-recv:
     rank  1: 1.75
     rank  2: 4.05
     rank  3: 25.51
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.07
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.01
     rank  2: 0.03
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
  optimizer-inner-step:
     rank  0: 7.37
     rank  1: 0.03
     rank  2: 0.08
     rank  3: 0.08
  optimizer:
     rank  0: 7.90
     rank  1: 0.57
     rank  2: 0.62
     rank  3: 0.63
Layer: TransformerLayer, Forward time: 2.005490 ms
Layer: TransformerLayer, Forward time: 1.659911 ms
Layer: TransformerLayer, Forward time: 1.562146 ms
Layer: TransformerLayer, Forward time: 1.595865 ms
Layer: TransformerLayer, Forward time: 1.549935 ms
Layer: TransformerLayer, Forward time: 1.516736 ms
Layer: TransformerLayer, Forward time: 1.559299 ms
Layer: TransformerLayer, Forward time: 1.541736 ms
Layer: TransformerLayer, Forward time: 1.513139 ms
Layer: TransformerLayer, Forward time: 1.561693 ms
Layer: TransformerLayer, Forward time: 1.515153 ms
Layer: TransformerLayer, Forward time: 1.552335 ms
Layer: TransformerLayer, Forward time: 1.544208 ms
Layer: TransformerLayer, Forward time: 1.523170 ms
Layer: TransformerLayer, Forward time: 1.541564 ms
Layer: TransformerLayer, Forward time: 1.520169 ms
Layer: TransformerLayer, Forward time: 1.554850 ms
Layer: TransformerLayer, Forward time: 1.529647 ms
Layer: TransformerLayer, Forward time: 1.556656 ms
Layer: TransformerLayer, Forward time: 1.539047 ms
Layer: TransformerLayer, Forward time: 1.513633 ms
Layer: TransformerLayer, Forward time: 1.558069 ms
Layer: TransformerLayer, Forward time: 1.551535 ms
Layer: TransformerLayer, Forward time: 1.509300 ms
Layer: TransformerLayer, Forward time: 15.074671 ms
Layer: TransformerLayer, Forward time: 15.435490 ms
Layer: TransformerLayer, Forward time: 15.724302 ms
Layer: TransformerLayer, Forward time: 15.702525 ms
Layer: TransformerLayer, Forward time: 2.050861 ms
Layer: TransformerLayer, Forward time: 1.751522 ms
Layer: TransformerLayer, Forward time: 1.678596 ms
Layer: TransformerLayer, Forward time: 1.711850 ms
Layer: TransformerLayer, Forward time: 1.644383 ms
Layer: TransformerLayer, Forward time: 1.641153 ms
Layer: TransformerLayer, Forward time: 1.659923 ms
Layer: TransformerLayer, Forward time: 1.634298 ms
Layer: TransformerLayer, Forward time: 1.753262 ms
Layer: TransformerLayer, Forward time: 1.754422 ms
Layer: TransformerLayer, Forward time: 1.744266 ms
Layer: TransformerLayer, Forward time: 1.766953 ms
Layer: TransformerLayer, Forward time: 1.779034 ms
Layer: TransformerLayer, Forward time: 1.728360 ms
Layer: TransformerLayer, Forward time: 1.773401 ms
Layer: TransformerLayer, Forward time: 1.731981 ms
Layer: TransformerLayer, Forward time: 1.793849 ms
Layer: TransformerLayer, Forward time: 1.735107 ms
Layer: TransformerLayer, Forward time: 1.729216 ms
Layer: TransformerLayer, Forward time: 1.788007 ms
Layer: TransformerLayer, Forward time: 1.736763 ms
Layer: TransformerLayer, Forward time: 1.790478 ms
Layer: TransformerLayer, Forward time: 1.761224 ms
Layer: TransformerLayer, Forward time: 1.758174 ms
Layer: TransformerLayer, Forward time: 15.230339 ms
Layer: TransformerLayer, Forward time: 15.442386 ms
Layer: TransformerLayer, Forward time: 15.538794 ms
Layer: TransformerLayer, Forward time: 15.592205 ms
Layer: TransformerLayer, Forward time: 2.257573 ms
Layer: TransformerLayer, Forward time: 2.036316 ms
Layer: TransformerLayer, Forward time: 1.893827 ms
Layer: TransformerLayer, Forward time: 1.893300 ms
Layer: TransformerLayer, Forward time: 1.794002 ms
Layer: TransformerLayer, Forward time: 1.752392 ms
Layer: TransformerLayer, Forward time: 1.654505 ms
Layer: TransformerLayer, Forward time: 1.649322 ms
Layer: TransformerLayer, Forward time: 1.597295 ms
Layer: TransformerLayer, Forward time: 1.632147 ms
Layer: TransformerLayer, Forward time: 1.599447 ms
Layer: TransformerLayer, Forward time: 1.577192 ms
Layer: TransformerLayer, Forward time: 1.642531 ms
Layer: TransformerLayer, Forward time: 1.601652 ms
Layer: TransformerLayer, Forward time: 1.587181 ms
Layer: TransformerLayer, Forward time: 1.631544 ms
Layer: TransformerLayer, Forward time: 1.608771 ms
Layer: TransformerLayer, Forward time: 1.599311 ms
Layer: TransformerLayer, Forward time: 1.628275 ms
Layer: TransformerLayer, Forward time: 1.618416 ms
Layer: TransformerLayer, Forward time: 1.614391 ms
Layer: TransformerLayer, Forward time: 1.612599 ms
Layer: TransformerLayer, Forward time: 1.618958 ms
Layer: TransformerLayer, Forward time: 1.637733 ms
Layer: TransformerLayer, Forward time: 15.165155 ms
Layer: TransformerLayer, Forward time: 15.474047 ms
Layer: TransformerLayer, Forward time: 15.638430 ms
Layer: TransformerLayer, Forward time: 15.784435 ms
Layer: TransformerLayer, Forward time: 2.219249 ms
Layer: TransformerLayer, Forward time: 1.918144 ms
Layer: TransformerLayer, Forward time: 1.844391 ms
Layer: TransformerLayer, Forward time: 1.864428 ms
Layer: TransformerLayer, Forward time: 1.780837 ms
Layer: TransformerLayer, Forward time: 1.799283 ms
Layer: TransformerLayer, Forward time: 1.777284 ms
Layer: TransformerLayer, Forward time: 1.836551 ms
Layer: TransformerLayer, Forward time: 1.761723 ms
Layer: TransformerLayer, Forward time: 1.766392 ms
Layer: TransformerLayer, Forward time: 1.753388 ms
Layer: TransformerLayer, Forward time: 1.744478 ms
Layer: TransformerLayer, Forward time: 1.791479 ms
Layer: TransformerLayer, Forward time: 1.761412 ms
Layer: TransformerLayer, Forward time: 1.758509 ms
Layer: TransformerLayer, Forward time: 1.782032 ms
Layer: TransformerLayer, Forward time: 1.664218 ms
Layer: TransformerLayer, Forward time: 1.625628 ms
Layer: TransformerLayer, Forward time: 1.734480 ms
Layer: TransformerLayer, Forward time: 1.746872 ms
Layer: TransformerLayer, Forward time: 1.773417 ms
Layer: TransformerLayer, Forward time: 1.766369 ms
Layer: TransformerLayer, Forward time: 1.780872 ms
Layer: TransformerLayer, Forward time: 1.722751 ms
Layer: TransformerLayer, Forward time: 15.294636 ms
Layer: TransformerLayer, Forward time: 16.691657 ms
Layer: TransformerLayer, Forward time: 15.618616 ms
Layer: TransformerLayer, Forward time: 15.773705 ms
Layer: TransformerLayer, Backward time: 0.087669 ms
Layer: TransformerLayer, Backward time: 0.044408 ms
Layer: TransformerLayer, Backward time: 0.053457 ms
Layer: TransformerLayer, Backward time: 0.041631 ms
Layer: TransformerLayer, Backward time: 0.039180 ms
Layer: TransformerLayer, Backward time: 0.033938 ms
Layer: TransformerLayer, Backward time: 0.032414 ms
Layer: TransformerLayer, Backward time: 0.030320 ms
Layer: TransformerLayer, Backward time: 0.076694 ms
Layer: TransformerLayer, Backward time: 0.032324 ms
Layer: TransformerLayer, Backward time: 0.031933 ms
Layer: TransformerLayer, Backward time: 0.046844 ms
Layer: TransformerLayer, Backward time: 0.032720 ms
Layer: TransformerLayer, Backward time: 0.030892 ms
Layer: TransformerLayer, Backward time: 0.029560 ms
Layer: TransformerLayer, Backward time: 0.028381 ms
 [2024-12-27 15:48:19] iteration        8/      10 | consumed samples:           32 | elapsed time per iteration (ms): 1724.6 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.929564E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1707.57
     rank  1: 1706.79
     rank  2: 1707.24
     rank  3: 1706.99
  forward-compute:
     rank  0: 453.10
     rank  1: 451.28
     rank  2: 456.94
     rank  3: 451.25
  backward-compute:
     rank  0: 333.15
     rank  1: 544.20
     rank  2: 551.28
     rank  3: 531.96
  pure-backward-compute:
     rank  0: 332.92
     rank  1: 544.01
     rank  2: 550.78
     rank  3: 531.10
  batch-generator:
     rank  0: 10.53
     rank  1: 11.16
     rank  2: 13.17
     rank  3: 14.90
  forward-recv:
     rank  1: 112.14
     rank  2: 224.56
     rank  3: 338.15
  forward-send:
     rank  0: 4.24
     rank  1: 2.59
     rank  2: 1.01
  backward-recv:
     rank  0: 507.16
     rank  1: 226.64
     rank  2: 107.62
  backward-send:
     rank  1: 2.55
     rank  2: 2.50
     rank  3: 0.97
  forward-send-backward-recv:
     rank  0: 406.31
     rank  1: 279.65
     rank  2: 135.59
  backward-send-forward-recv:
     rank  1: 1.78
     rank  2: 4.12
     rank  3: 23.26
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.05
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.07
  all-grads-sync:
     rank  0: 0.61
     rank  1: 0.01
     rank  2: 0.07
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.42
     rank  1: 0.03
     rank  2: 0.09
     rank  3: 0.05
  optimizer:
     rank  0: 8.02
     rank  1: 0.62
     rank  2: 0.67
     rank  3: 0.64
Layer: TransformerLayer, Forward time: 2.647013 ms
Layer: TransformerLayer, Forward time: 2.265632 ms
Layer: TransformerLayer, Forward time: 2.158253 ms
Layer: TransformerLayer, Forward time: 2.175594 ms
Layer: TransformerLayer, Forward time: 2.161710 ms
Layer: TransformerLayer, Forward time: 2.104075 ms
Layer: TransformerLayer, Forward time: 2.217465 ms
Layer: TransformerLayer, Forward time: 2.130324 ms
Layer: TransformerLayer, Forward time: 2.143957 ms
Layer: TransformerLayer, Forward time: 2.151597 ms
Layer: TransformerLayer, Forward time: 2.124801 ms
Layer: TransformerLayer, Forward time: 2.110819 ms
Layer: TransformerLayer, Forward time: 2.151277 ms
Layer: TransformerLayer, Forward time: 2.106792 ms
Layer: TransformerLayer, Forward time: 2.102088 ms
Layer: TransformerLayer, Forward time: 2.142877 ms
Layer: TransformerLayer, Forward time: 2.165394 ms
Layer: TransformerLayer, Forward time: 2.128537 ms
Layer: TransformerLayer, Forward time: 2.093551 ms
Layer: TransformerLayer, Forward time: 2.098904 ms
Layer: TransformerLayer, Forward time: 2.162302 ms
Layer: TransformerLayer, Forward time: 2.089078 ms
Layer: TransformerLayer, Forward time: 2.146875 ms
Layer: TransformerLayer, Forward time: 2.113751 ms
Layer: TransformerLayer, Forward time: 15.203317 ms
Layer: TransformerLayer, Forward time: 15.274429 ms
Layer: TransformerLayer, Forward time: 15.757979 ms
Layer: TransformerLayer, Forward time: 15.794108 ms
Layer: TransformerLayer, Forward time: 2.519163 ms
Layer: TransformerLayer, Forward time: 2.303304 ms
Layer: TransformerLayer, Forward time: 2.162980 ms
Layer: TransformerLayer, Forward time: 2.143800 ms
Layer: TransformerLayer, Forward time: 2.146221 ms
Layer: TransformerLayer, Forward time: 2.111199 ms
Layer: TransformerLayer, Forward time: 2.131653 ms
Layer: TransformerLayer, Forward time: 2.132170 ms
Layer: TransformerLayer, Forward time: 2.181979 ms
Layer: TransformerLayer, Forward time: 3.860861 ms
Layer: TransformerLayer, Forward time: 2.943181 ms
Layer: TransformerLayer, Forward time: 2.205424 ms
Layer: TransformerLayer, Forward time: 2.180776 ms
Layer: TransformerLayer, Forward time: 2.142969 ms
Layer: TransformerLayer, Forward time: 2.190919 ms
Layer: TransformerLayer, Forward time: 2.138431 ms
Layer: TransformerLayer, Forward time: 2.142319 ms
Layer: TransformerLayer, Forward time: 2.164097 ms
Layer: TransformerLayer, Forward time: 2.111536 ms
Layer: TransformerLayer, Forward time: 2.071422 ms
Layer: TransformerLayer, Forward time: 2.127609 ms
Layer: TransformerLayer, Forward time: 2.109624 ms
Layer: TransformerLayer, Forward time: 2.148702 ms
Layer: TransformerLayer, Forward time: 2.093685 ms
Layer: TransformerLayer, Forward time: 15.200224 ms
Layer: TransformerLayer, Forward time: 15.516556 ms
Layer: TransformerLayer, Forward time: 15.560498 ms
Layer: TransformerLayer, Forward time: 15.623699 ms
Layer: TransformerLayer, Forward time: 2.513021 ms
Layer: TransformerLayer, Forward time: 2.257106 ms
Layer: TransformerLayer, Forward time: 2.189964 ms
Layer: TransformerLayer, Forward time: 2.202054 ms
Layer: TransformerLayer, Forward time: 2.147931 ms
Layer: TransformerLayer, Forward time: 2.193061 ms
Layer: TransformerLayer, Forward time: 2.136724 ms
Layer: TransformerLayer, Forward time: 2.087195 ms
Layer: TransformerLayer, Forward time: 2.188494 ms
Layer: TransformerLayer, Forward time: 2.113601 ms
Layer: TransformerLayer, Forward time: 2.125977 ms
Layer: TransformerLayer, Forward time: 2.187275 ms
Layer: TransformerLayer, Forward time: 2.128021 ms
Layer: TransformerLayer, Forward time: 2.183886 ms
Layer: TransformerLayer, Forward time: 2.426201 ms
Layer: TransformerLayer, Forward time: 4.143920 ms
Layer: TransformerLayer, Forward time: 2.800036 ms
Layer: TransformerLayer, Forward time: 2.192202 ms
Layer: TransformerLayer, Forward time: 2.216434 ms
Layer: TransformerLayer, Forward time: 2.156115 ms
Layer: TransformerLayer, Forward time: 2.152309 ms
Layer: TransformerLayer, Forward time: 2.141519 ms
Layer: TransformerLayer, Forward time: 2.078894 ms
Layer: TransformerLayer, Forward time: 2.123968 ms
Layer: TransformerLayer, Forward time: 15.197610 ms
Layer: TransformerLayer, Forward time: 15.366983 ms
Layer: TransformerLayer, Forward time: 15.671250 ms
Layer: TransformerLayer, Forward time: 15.759903 ms
Layer: TransformerLayer, Forward time: 2.102006 ms
Layer: TransformerLayer, Forward time: 1.777722 ms
Layer: TransformerLayer, Forward time: 1.731105 ms
Layer: TransformerLayer, Forward time: 1.740413 ms
Layer: TransformerLayer, Forward time: 1.723987 ms
Layer: TransformerLayer, Forward time: 1.710562 ms
Layer: TransformerLayer, Forward time: 1.699032 ms
Layer: TransformerLayer, Forward time: 1.780234 ms
Layer: TransformerLayer, Forward time: 1.814208 ms
Layer: TransformerLayer, Forward time: 1.819624 ms
Layer: TransformerLayer, Forward time: 1.762102 ms
Layer: TransformerLayer, Forward time: 1.823925 ms
Layer: TransformerLayer, Forward time: 1.790483 ms
Layer: TransformerLayer, Forward time: 1.771905 ms
Layer: TransformerLayer, Forward time: 1.809009 ms
Layer: TransformerLayer, Forward time: 1.851853 ms
Layer: TransformerLayer, Forward time: 1.805364 ms
Layer: TransformerLayer, Forward time: 1.833505 ms
Layer: TransformerLayer, Forward time: 1.810059 ms
Layer: TransformerLayer, Forward time: 1.803270 ms
Layer: TransformerLayer, Forward time: 1.693349 ms
Layer: TransformerLayer, Forward time: 2.427254 ms
Layer: TransformerLayer, Forward time: 2.938185 ms
Layer: TransformerLayer, Forward time: 1.969581 ms
Layer: TransformerLayer, Forward time: 15.221823 ms
Layer: TransformerLayer, Forward time: 15.254110 ms
Layer: TransformerLayer, Forward time: 16.054336 ms
Layer: TransformerLayer, Forward time: 15.814041 ms
Layer: TransformerLayer, Backward time: 0.084397 ms
Layer: TransformerLayer, Backward time: 0.040194 ms
Layer: TransformerLayer, Backward time: 0.045905 ms
Layer: TransformerLayer, Backward time: 0.045262 ms
Layer: TransformerLayer, Backward time: 0.040887 ms
Layer: TransformerLayer, Backward time: 0.035255 ms
Layer: TransformerLayer, Backward time: 0.030875 ms
Layer: TransformerLayer, Backward time: 0.028909 ms
Layer: TransformerLayer, Backward time: 0.077432 ms
Layer: TransformerLayer, Backward time: 0.033540 ms
Layer: TransformerLayer, Backward time: 0.030258 ms
Layer: TransformerLayer, Backward time: 0.030000 ms
Layer: TransformerLayer, Backward time: 0.031377 ms
Layer: TransformerLayer, Backward time: 0.029281 ms
Layer: TransformerLayer, Backward time: 0.029316 ms
Layer: TransformerLayer, Backward time: 0.028116 ms
 [2024-12-27 15:48:21] iteration        9/      10 | consumed samples:           36 | elapsed time per iteration (ms): 1739.1 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 5.279637E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1722.74
     rank  1: 1722.01
     rank  2: 1722.09
     rank  3: 1722.15
  forward-compute:
     rank  0: 501.05
     rank  1: 451.23
     rank  2: 457.22
     rank  3: 450.49
  backward-compute:
     rank  0: 332.10
     rank  1: 544.57
     rank  2: 549.28
     rank  3: 531.61
  pure-backward-compute:
     rank  0: 331.83
     rank  1: 544.37
     rank  2: 548.77
     rank  3: 530.85
  batch-generator:
     rank  0: 12.35
     rank  1: 10.92
     rank  2: 14.61
     rank  3: 14.83
  forward-recv:
     rank  1: 157.59
     rank  2: 253.91
     rank  3: 357.43
  forward-send:
     rank  0: 2.85
     rank  1: 2.41
     rank  2: 1.05
  backward-recv:
     rank  0: 504.19
     rank  1: 224.27
     rank  2: 106.07
  backward-send:
     rank  1: 2.66
     rank  2: 2.39
     rank  3: 0.96
  forward-send-backward-recv:
     rank  0: 378.68
     rank  1: 251.66
     rank  2: 125.07
  backward-send-forward-recv:
     rank  1: 1.77
     rank  2: 4.02
     rank  3: 20.34
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.07
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.42
     rank  1: 0.03
     rank  2: 0.05
     rank  3: 0.09
  optimizer:
     rank  0: 7.91
     rank  1: 0.55
     rank  2: 0.58
     rank  3: 0.61
Layer: TransformerLayer, Forward time: 2.562566 ms
Layer: TransformerLayer, Forward time: 2.199505 ms
Layer: TransformerLayer, Forward time: 2.081330 ms
Layer: TransformerLayer, Forward time: 2.136683 ms
Layer: TransformerLayer, Forward time: 2.072921 ms
Layer: TransformerLayer, Forward time: 2.069423 ms
Layer: TransformerLayer, Forward time: 2.120536 ms
Layer: TransformerLayer, Forward time: 2.091950 ms
Layer: TransformerLayer, Forward time: 2.059861 ms
Layer: TransformerLayer, Forward time: 2.114521 ms
Layer: TransformerLayer, Forward time: 2.105325 ms
Layer: TransformerLayer, Forward time: 2.046725 ms
Layer: TransformerLayer, Forward time: 2.090308 ms
Layer: TransformerLayer, Forward time: 2.069826 ms
Layer: TransformerLayer, Forward time: 2.043237 ms
Layer: TransformerLayer, Forward time: 2.129541 ms
Layer: TransformerLayer, Forward time: 2.086884 ms
Layer: TransformerLayer, Forward time: 2.051314 ms
Layer: TransformerLayer, Forward time: 2.078807 ms
Layer: TransformerLayer, Forward time: 2.028294 ms
Layer: TransformerLayer, Forward time: 2.079409 ms
Layer: TransformerLayer, Forward time: 2.051626 ms
Layer: TransformerLayer, Forward time: 2.017019 ms
Layer: TransformerLayer, Forward time: 2.065515 ms
Layer: TransformerLayer, Forward time: 15.158797 ms
Layer: TransformerLayer, Forward time: 15.224984 ms
Layer: TransformerLayer, Forward time: 15.737862 ms
Layer: TransformerLayer, Forward time: 15.761907 ms
Layer: TransformerLayer, Forward time: 2.495300 ms
Layer: TransformerLayer, Forward time: 2.285076 ms
Layer: TransformerLayer, Forward time: 2.174850 ms
Layer: TransformerLayer, Forward time: 2.100532 ms
Layer: TransformerLayer, Forward time: 2.142759 ms
Layer: TransformerLayer, Forward time: 2.117414 ms
Layer: TransformerLayer, Forward time: 2.086630 ms
Layer: TransformerLayer, Forward time: 2.120002 ms
Layer: TransformerLayer, Forward time: 2.091974 ms
Layer: TransformerLayer, Forward time: 2.083263 ms
Layer: TransformerLayer, Forward time: 2.187551 ms
Layer: TransformerLayer, Forward time: 2.749405 ms
Layer: TransformerLayer, Forward time: 3.826652 ms
Layer: TransformerLayer, Forward time: 2.414368 ms
Layer: TransformerLayer, Forward time: 2.218471 ms
Layer: TransformerLayer, Forward time: 2.135884 ms
Layer: TransformerLayer, Forward time: 2.120461 ms
Layer: TransformerLayer, Forward time: 2.170580 ms
Layer: TransformerLayer, Forward time: 2.091631 ms
Layer: TransformerLayer, Forward time: 2.114880 ms
Layer: TransformerLayer, Forward time: 2.056996 ms
Layer: TransformerLayer, Forward time: 2.107677 ms
Layer: TransformerLayer, Forward time: 2.059635 ms
Layer: TransformerLayer, Forward time: 2.084357 ms
Layer: TransformerLayer, Forward time: 15.196962 ms
Layer: TransformerLayer, Forward time: 15.455696 ms
Layer: TransformerLayer, Forward time: 15.592701 ms
Layer: TransformerLayer, Forward time: 15.812330 ms
Layer: TransformerLayer, Forward time: 2.525982 ms
Layer: TransformerLayer, Forward time: 2.224786 ms
Layer: TransformerLayer, Forward time: 2.191330 ms
Layer: TransformerLayer, Forward time: 2.140001 ms
Layer: TransformerLayer, Forward time: 2.119578 ms
Layer: TransformerLayer, Forward time: 2.145431 ms
Layer: TransformerLayer, Forward time: 2.092178 ms
Layer: TransformerLayer, Forward time: 2.083739 ms
Layer: TransformerLayer, Forward time: 2.112621 ms
Layer: TransformerLayer, Forward time: 2.159345 ms
Layer: TransformerLayer, Forward time: 2.090318 ms
Layer: TransformerLayer, Forward time: 2.155163 ms
Layer: TransformerLayer, Forward time: 2.132529 ms
Layer: TransformerLayer, Forward time: 2.218330 ms
Layer: TransformerLayer, Forward time: 2.162885 ms
Layer: TransformerLayer, Forward time: 2.791339 ms
Layer: TransformerLayer, Forward time: 4.320845 ms
Layer: TransformerLayer, Forward time: 2.737094 ms
Layer: TransformerLayer, Forward time: 2.201638 ms
Layer: TransformerLayer, Forward time: 2.132251 ms
Layer: TransformerLayer, Forward time: 2.204098 ms
Layer: TransformerLayer, Forward time: 2.119837 ms
Layer: TransformerLayer, Forward time: 2.098779 ms
Layer: TransformerLayer, Forward time: 2.048098 ms
Layer: TransformerLayer, Forward time: 15.200620 ms
Layer: TransformerLayer, Forward time: 15.339299 ms
Layer: TransformerLayer, Forward time: 16.349320 ms
Layer: TransformerLayer, Forward time: 15.641480 ms
Layer: TransformerLayer, Forward time: 2.230607 ms
Layer: TransformerLayer, Forward time: 1.974401 ms
Layer: TransformerLayer, Forward time: 1.913492 ms
Layer: TransformerLayer, Forward time: 1.898218 ms
Layer: TransformerLayer, Forward time: 1.855450 ms
Layer: TransformerLayer, Forward time: 1.902364 ms
Layer: TransformerLayer, Forward time: 1.822198 ms
Layer: TransformerLayer, Forward time: 1.826724 ms
Layer: TransformerLayer, Forward time: 1.863290 ms
Layer: TransformerLayer, Forward time: 1.847832 ms
Layer: TransformerLayer, Forward time: 1.900565 ms
Layer: TransformerLayer, Forward time: 1.820106 ms
Layer: TransformerLayer, Forward time: 1.793245 ms
Layer: TransformerLayer, Forward time: 1.840484 ms
Layer: TransformerLayer, Forward time: 1.859681 ms
Layer: TransformerLayer, Forward time: 1.732969 ms
Layer: TransformerLayer, Forward time: 2.845930 ms
Layer: TransformerLayer, Forward time: 3.912611 ms
Layer: TransformerLayer, Forward time: 4.914353 ms
Layer: TransformerLayer, Forward time: 4.384326 ms
Layer: TransformerLayer, Forward time: 3.789277 ms
Layer: TransformerLayer, Forward time: 2.289079 ms
Layer: TransformerLayer, Forward time: 2.202020 ms
Layer: TransformerLayer, Forward time: 2.115020 ms
Layer: TransformerLayer, Forward time: 15.183251 ms
Layer: TransformerLayer, Forward time: 15.634625 ms
Layer: TransformerLayer, Forward time: 15.678130 ms
Layer: TransformerLayer, Forward time: 15.697160 ms
Layer: TransformerLayer, Backward time: 0.086371 ms
Layer: TransformerLayer, Backward time: 0.042697 ms
Layer: TransformerLayer, Backward time: 0.053665 ms
Layer: TransformerLayer, Backward time: 0.039926 ms
Layer: TransformerLayer, Backward time: 0.039232 ms
Layer: TransformerLayer, Backward time: 0.032490 ms
Layer: TransformerLayer, Backward time: 0.030873 ms
Layer: TransformerLayer, Backward time: 0.028758 ms
Layer: TransformerLayer, Backward time: 0.078789 ms
Layer: TransformerLayer, Backward time: 0.030458 ms
Layer: TransformerLayer, Backward time: 0.028747 ms
Layer: TransformerLayer, Backward time: 0.028602 ms
Layer: TransformerLayer, Backward time: 0.031820 ms
Layer: TransformerLayer, Backward time: 0.029869 ms
Layer: TransformerLayer, Backward time: 0.029089 ms
Layer: TransformerLayer, Backward time: 0.027924 ms
 [2024-12-27 15:48:23] iteration       10/      10 | consumed samples:           40 | elapsed time per iteration (ms): 1730.3 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 5.309896E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 1713.98
     rank  1: 1713.20
     rank  2: 1713.43
     rank  3: 1713.19
  forward-compute:
     rank  0: 512.80
     rank  1: 450.55
     rank  2: 454.92
     rank  3: 442.08
  backward-compute:
     rank  0: 332.22
     rank  1: 544.88
     rank  2: 550.07
     rank  3: 525.76
  pure-backward-compute:
     rank  0: 331.91
     rank  1: 544.66
     rank  2: 549.64
     rank  3: 525.07
  batch-generator:
     rank  0: 12.39
     rank  1: 10.91
     rank  2: 12.93
     rank  3: 10.74
  forward-recv:
     rank  1: 157.75
     rank  2: 253.57
     rank  3: 352.60
  forward-send:
     rank  0: 2.72
     rank  1: 2.50
     rank  2: 0.97
  backward-recv:
     rank  0: 504.42
     rank  1: 223.39
     rank  2: 104.29
  backward-send:
     rank  1: 2.54
     rank  2: 2.39
     rank  3: 0.88
  forward-send-backward-recv:
     rank  0: 357.75
     rank  1: 243.96
     rank  2: 119.74
  backward-send-forward-recv:
     rank  1: 1.74
     rank  2: 4.05
     rank  3: 31.14
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.05
  all-grads-sync:
     rank  0: 0.60
     rank  1: 0.01
     rank  2: 0.07
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 7.43
     rank  1: 0.03
     rank  2: 0.08
     rank  3: 0.03
  optimizer:
     rank  0: 7.90
     rank  1: 0.53
     rank  2: 0.59
     rank  3: 0.53
[after training is done] datetime: 2024-12-27 15:48:23 
