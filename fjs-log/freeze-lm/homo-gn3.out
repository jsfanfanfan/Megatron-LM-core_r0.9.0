examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
[2024-12-27 13:27:05,976] torch.distributed.run: [WARNING] 
[2024-12-27 13:27:05,976] torch.distributed.run: [WARNING] *****************************************
[2024-12-27 13:27:05,976] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-27 13:27:05,976] torch.distributed.run: [WARNING] *****************************************
using world size: 4, data-parallel size: 1, context-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 4, 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:HuggingFaceTokenizer
WARNING: Setting args.overlap_p2p_comm and args.align_param_gather to False since non-interleaved schedule does not support overlapping p2p communication and aligned param AG
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  allow_missing_vision_projection_checkpoint ...... True
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. True
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 0.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/pretrain_dataset.yaml']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. None
  dataloader_type ................................. external
  dataset_config .................................. None
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. 1024
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  disable_vision_class_token ...................... True
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 60
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 1
  encoder_seq_length .............................. 576
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... True
  eval_interval ................................... 1000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 230
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_wgrad ....................................... True
  freeze_LM ....................................... True
  freeze_ViT ...................................... False
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 336
  img_w ........................................... 336
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  language_model_type ............................. mistral_7b
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 20000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_num_tiles ................................... 1
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 14
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  prompt_path ..................................... /gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/manual_prompts.json
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 576
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  split_spec ...................................... 12,12,2,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /dat/fjs/llama_mistral/homo_mistral_clip_freeze_llm-tp4pp5/output/llava-mistral-7b-instruct-clip336-pretraining/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 2
  timing_log_option ............................... all
  titles_data_path ................................ None
  tokenizer_model ................................. /dat/fjs/llama_mistral/hf-mistral/
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 4
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_te .......................................... True
  use_thumbnail ................................... False
  use_tiling ...................................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  valid_path ...................................... None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_model_type ............................... clip
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 4
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 4
> building HuggingFaceTokenizer tokenizer ...
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
 > padded vocab (size: 32768) with 0 dummy tokens (new size: 32768)
> initializing torch distributed ...
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:4------
------world_size:4------
------total_model_size:4------
------num_pipeline_model_parallel_groups:1------
make: Nothing to be done for 'default'.
make: Leaving directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.075 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 0.537 seconds---Rank 3---Tensor Parallel Group GPUs: [0]---Rank 2---Tensor Parallel Group GPUs: [0]


---Rank 1---Tensor Parallel Group GPUs: [0]---Rank 3---Pipeline Parallel Group GPUs: [3, 3, 3, 3]---Rank 2---Pipeline Parallel Group GPUs: [2, 2, 2, 2]


---Rank 1---Pipeline Parallel Group GPUs: [1, 1, 1, 1]
---Rank 0---Tensor Parallel Group GPUs: [0]
---Rank 0---Pipeline Parallel Group GPUs: [0, 0, 0, 0]
[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 6.154
[after megatron is initialized] datetime: 2024-12-27 13:27:20 
building a multimodal model ...
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 73400320
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (73400320 elements):
	vision_projection.encoder.linear_fc2.weight
	vision_projection.encoder.linear_fc1.weight
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 151154688
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (151154688 elements):
	vision_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.9.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.10.self_attention.linear_proj.weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.mlp.linear_fc2.bias
	vision_model.decoder.layers.0.self_attention.linear_proj.weight
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.self_attention.linear_qkv.bias
	vision_model.decoder.layers.11.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.self_attention.linear_proj.weight
	vision_model.decoder.layers.4.self_attention.linear_proj.bias
	vision_model.decoder.layers.4.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.1.self_attention.linear_proj.weight
	vision_model.decoder.layers.0.mlp.linear_fc2.bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.9.self_attention.linear_qkv.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.9.mlp.linear_fc1.weight
	vision_model.decoder.layers.8.mlp.linear_fc2.weight
	vision_model.decoder.layers.3.self_attention.linear_proj.weight
	vision_model.decoder.layers.11.self_attention.linear_proj.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.6.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.mlp.linear_fc1.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.8.self_attention.linear_proj.bias
	vision_model.decoder.layers.3.mlp.linear_fc2.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.2.self_attention.linear_proj.weight
	vision_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.10.self_attention.linear_qkv.bias
	vision_model.decoder.layers.9.mlp.linear_fc2.bias
	vision_model.decoder.layers.5.mlp.linear_fc2.weight
	vision_model.decoder.layers.3.mlp.linear_fc2.bias
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.10.mlp.linear_fc1.bias
	vision_model.decoder.layers.0.mlp.linear_fc1.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.bias
	vision_model.decoder.layers.11.self_attention.linear_proj.weight
	vision_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.bias
	vision_model.decoder.layers.3.mlp.linear_fc1.weight
	vision_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.7.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.bias
	vision_model.decoder.layers.4.mlp.linear_fc1.weight
	vision_model.decoder.layers.8.self_attention.linear_proj.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.weight
	vision_model.decoder.layers.1.mlp.linear_fc2.bias
	vision_model.decoder.layers.11.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.10.mlp.linear_fc1.weight
	vision_model.decoder.layers.9.mlp.linear_fc2.weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.3.mlp.linear_fc1.bias
	vision_model.decoder.layers.2.mlp.linear_fc1.bias
	vision_model.decoder.layers.0.mlp.linear_fc1.bias
	vision_model.decoder.layers.8.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.7.mlp.linear_fc1.weight
	vision_model.decoder.layers.6.mlp.linear_fc2.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.weight
	vision_model.decoder.layers.9.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.2.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.mlp.linear_fc1.weight
	vision_model.decoder.layers.11.self_attention.linear_qkv.bias
	vision_model.decoder.layers.10.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.mlp.linear_fc2.weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.11.mlp.linear_fc1.bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.bias
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.8.self_attention.linear_qkv.bias
	vision_model.decoder.layers.7.mlp.linear_fc2.bias
	vision_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.8.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.mlp.linear_fc2.bias
	vision_model.decoder.layers.5.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.self_attention.linear_proj.bias
	vision_model.decoder.layers.9.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.bias
	vision_model.decoder.layers.0.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.self_attention.linear_proj.bias
	vision_model.decoder.layers.11.self_attention.linear_qkv.weight
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.5.mlp.linear_fc1.bias
	vision_model.decoder.layers.4.mlp.linear_fc2.bias
	vision_model.decoder.layers.11.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.mlp.linear_fc2.weight
	vision_model.decoder.layers.2.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.mlp.linear_fc2.weight
	vision_model.decoder.layers.1.self_attention.linear_proj.bias
	vision_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.8.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.9.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.8.mlp.linear_fc1.weight
	vision_model.decoder.layers.7.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.10.self_attention.linear_proj.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.11.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.4.mlp.linear_fc1.bias
	vision_model.decoder.layers.7.self_attention.linear_proj.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.bias
	vision_model.decoder.layers.8.mlp.linear_fc2.bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.weight
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 486551552
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 152350720
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (152350720 elements):
	vision_model.decoder.layers.11.self_attention.linear_qkv.weight
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.weight
	vision_model.ln_pre.bias
	vision_model.decoder.layers.11.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.mlp.linear_fc2.weight
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.5.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.7.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.self_attention.linear_proj.bias
	vision_model.decoder.layers.0.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.bias
	vision_model.decoder.layers.8.mlp.linear_fc2.bias
	vision_model.decoder.layers.3.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.mlp.linear_fc2.bias
	vision_model.decoder.layers.0.mlp.linear_fc2.bias
	vision_model.decoder.layers.10.self_attention.linear_proj.bias
	vision_model.decoder.layers.9.mlp.linear_fc1.bias
	vision_model.decoder.layers.4.self_attention.linear_proj.bias
	vision_model.decoder.layers.3.mlp.linear_fc1.bias
	vision_model.decoder.layers.11.mlp.linear_fc2.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.6.self_attention.linear_qkv.bias
	vision_model.decoder.layers.5.mlp.linear_fc2.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.mlp.linear_fc1.bias
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.0.mlp.linear_fc1.bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.7.self_attention.linear_proj.weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.1.self_attention.linear_proj.weight
	vision_model.position_embeddings.weight
	vision_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.weight
	vision_model.decoder.layers.10.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.mlp.linear_fc1.weight
	vision_model.decoder.layers.8.mlp.linear_fc2.weight
	vision_model.decoder.layers.4.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.weight
	vision_model.decoder.layers.2.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.weight
	vision_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.5.mlp.linear_fc2.weight
	vision_model.decoder.layers.11.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.6.mlp.linear_fc1.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.0.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.8.self_attention.linear_proj.bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.2.self_attention.linear_proj.bias
	vision_model.conv1.weight
	vision_model.decoder.layers.10.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.9.mlp.linear_fc2.bias
	vision_model.decoder.layers.4.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.3.mlp.linear_fc2.bias
	vision_model.decoder.layers.11.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.self_attention.linear_proj.bias
	vision_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.bias
	vision_model.decoder.layers.6.mlp.linear_fc2.bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.1.self_attention.linear_qkv.bias
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.7.mlp.linear_fc1.bias
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.1.mlp.linear_fc1.bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.bias
	vision_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.8.self_attention.linear_proj.weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.4.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.self_attention.linear_proj.weight
	vision_model.class_token
	vision_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.10.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.4.mlp.linear_fc1.bias
	vision_model.decoder.layers.11.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.weight
	vision_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.8.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.7.mlp.linear_fc1.weight
	vision_model.decoder.layers.6.mlp.linear_fc2.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.1.mlp.linear_fc1.weight
	vision_model.decoder.layers.0.mlp.linear_fc2.weight
	vision_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.weight
	vision_model.decoder.layers.9.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.11.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.10.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.mlp.linear_fc1.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.bias
	vision_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.8.self_attention.linear_qkv.bias
	vision_model.decoder.layers.7.mlp.linear_fc2.bias
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.bias
	vision_model.decoder.layers.1.mlp.linear_fc2.bias
	vision_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.8.mlp.linear_fc1.bias
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.2.mlp.linear_fc1.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	vision_model.decoder.layers.11.self_attention.linear_qkv.bias
	vision_model.decoder.layers.10.mlp.linear_fc2.bias
	vision_model.decoder.layers.9.self_attention.linear_proj.weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.bias
	vision_model.decoder.layers.4.mlp.linear_fc2.bias
	vision_model.decoder.layers.3.self_attention.linear_proj.weight
	vision_model.ln_pre.weight
	vision_model.decoder.layers.11.mlp.linear_fc1.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.5.mlp.linear_fc1.bias
	vision_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	vision_model.decoder.layers.6.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.8.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
	vision_model.decoder.layers.2.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.bias
	vision_model.decoder.layers.9.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.8.mlp.linear_fc1.weight
	vision_model.decoder.layers.7.mlp.linear_fc2.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.layer_norm_bias
	vision_model.decoder.layers.2.mlp.linear_fc1.weight
	vision_model.decoder.layers.1.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, clip_grad=0.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f405f683f40>, config_logger_dir='')
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-12-27 13:27:20 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 40
    test:       40
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
[after dataloaders are built] datetime: 2024-12-27 13:27:22 
done with setup ...
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 61.02
     rank  1: 47.72
     rank  2: 6.48
     rank  3: 45.27
  train/valid/test-data-iterators-setup:
     rank  0: 1262.45
     rank  1: 1262.44
     rank  2: 1358.58
     rank  3: 1358.30
training ...
[before the start of training step] datetime: 2024-12-27 13:27:22 
Layer: TransformerLayer, Forward time: 869.066513 ms
Layer: TransformerLayer, Forward time: 3.962488 ms
Layer: TransformerLayer, Forward time: 3.547309 ms
Layer: TransformerLayer, Forward time: 3.707341 ms
Layer: TransformerLayer, Forward time: 3.728690 ms
Layer: TransformerLayer, Forward time: 3.716749 ms
Layer: TransformerLayer, Forward time: 3.698679 ms
Layer: TransformerLayer, Forward time: 3.738087 ms
Layer: TransformerLayer, Forward time: 3.554324 ms
Layer: TransformerLayer, Forward time: 3.545022 ms
Layer: TransformerLayer, Forward time: 3.588034 ms
Layer: TransformerLayer, Forward time: 3.760357 ms
Layer: TransformerLayer, Forward time: 5.821903 ms
Layer: TransformerLayer, Forward time: 5.093939 ms
Layer: TransformerLayer, Forward time: 4.380800 ms
Layer: TransformerLayer, Forward time: 4.504062 ms
Layer: TransformerLayer, Forward time: 4.433467 ms
Layer: TransformerLayer, Forward time: 4.324115 ms
Layer: TransformerLayer, Forward time: 4.184209 ms
Layer: TransformerLayer, Forward time: 4.211157 ms
Layer: TransformerLayer, Forward time: 4.226010 ms
Layer: TransformerLayer, Forward time: 4.746150 ms
Layer: TransformerLayer, Forward time: 4.227895 ms
Layer: TransformerLayer, Forward time: 3.949621 ms
Layer: TransformerLayer, Forward time: 6.132555 ms
Layer: TransformerLayer, Forward time: 3.809624 ms
Layer: TransformerLayer, Forward time: 3.726230 ms
Layer: TransformerLayer, Forward time: 3.708296 ms
Layer: TransformerLayer, Forward time: 3.223220 ms
Layer: TransformerLayer, Forward time: 3.182218 ms
Layer: TransformerLayer, Forward time: 3.260998 ms
Layer: TransformerLayer, Forward time: 3.899077 ms
Layer: TransformerLayer, Forward time: 3.777117 ms
Layer: TransformerLayer, Forward time: 3.718482 ms
Layer: TransformerLayer, Forward time: 3.744997 ms
Layer: TransformerLayer, Forward time: 3.770370 ms
Layer: TransformerLayer, Forward time: 6.615502 ms
Layer: TransformerLayer, Forward time: 3.632018 ms
Layer: TransformerLayer, Forward time: 3.564092 ms
Layer: TransformerLayer, Forward time: 3.694334 ms
Layer: TransformerLayer, Forward time: 3.633000 ms
Layer: TransformerLayer, Forward time: 3.959400 ms
Layer: TransformerLayer, Forward time: 3.994145 ms
Layer: TransformerLayer, Forward time: 3.905118 ms
Layer: TransformerLayer, Forward time: 16.034516 ms
Layer: TransformerLayer, Forward time: 3.869838 ms
Layer: TransformerLayer, Forward time: 3.629027 ms
Layer: TransformerLayer, Forward time: 8.038677 ms
Layer: TransformerLayer, Backward time: 0.266255 ms
Layer: TransformerLayer, Backward time: 0.046336 ms
Layer: TransformerLayer, Backward time: 0.039047 ms
Layer: TransformerLayer, Backward time: 0.040822 ms
Layer: TransformerLayer, Backward time: 0.038579 ms
Layer: TransformerLayer, Backward time: 0.040113 ms
Layer: TransformerLayer, Backward time: 0.038567 ms
Layer: TransformerLayer, Backward time: 0.034977 ms
Layer: TransformerLayer, Backward time: 0.039081 ms
Layer: TransformerLayer, Backward time: 0.037212 ms
Layer: TransformerLayer, Backward time: 0.038466 ms
Layer: TransformerLayer, Backward time: 0.037150 ms
Layer: TransformerLayer, Backward time: 0.037320 ms
Layer: TransformerLayer, Backward time: 0.052924 ms
Layer: TransformerLayer, Backward time: 0.041288 ms
Layer: TransformerLayer, Backward time: 0.035091 ms
Layer: TransformerLayer, Backward time: 0.033738 ms
Layer: TransformerLayer, Backward time: 0.033521 ms
Layer: TransformerLayer, Backward time: 0.034082 ms
Layer: TransformerLayer, Backward time: 0.032658 ms
Layer: TransformerLayer, Backward time: 0.033088 ms
Layer: TransformerLayer, Backward time: 0.032561 ms
Layer: TransformerLayer, Backward time: 0.036368 ms
Layer: TransformerLayer, Backward time: 0.032254 ms
Layer: TransformerLayer, Backward time: 0.033717 ms
Layer: TransformerLayer, Backward time: 0.033641 ms
Layer: TransformerLayer, Backward time: 0.040305 ms
Layer: TransformerLayer, Backward time: 0.040967 ms
Layer: TransformerLayer, Backward time: 0.041044 ms
Layer: TransformerLayer, Backward time: 0.033638 ms
Layer: TransformerLayer, Backward time: 0.033558 ms
Layer: TransformerLayer, Backward time: 0.034134 ms
Layer: TransformerLayer, Backward time: 0.032363 ms
Layer: TransformerLayer, Backward time: 0.032655 ms
Layer: TransformerLayer, Backward time: 0.034785 ms
Layer: TransformerLayer, Backward time: 0.032741 ms
Layer: TransformerLayer, Backward time: 0.033387 ms
Layer: TransformerLayer, Backward time: 0.041123 ms
Layer: TransformerLayer, Backward time: 0.037912 ms
Layer: TransformerLayer, Backward time: 0.038358 ms
Layer: TransformerLayer, Backward time: 0.031918 ms
Layer: TransformerLayer, Backward time: 0.031679 ms
Layer: TransformerLayer, Backward time: 0.031487 ms
Layer: TransformerLayer, Backward time: 0.031461 ms
Layer: TransformerLayer, Backward time: 0.040826 ms
Layer: TransformerLayer, Backward time: 0.034967 ms
Layer: TransformerLayer, Backward time: 0.032909 ms
Layer: TransformerLayer, Backward time: 0.032769 ms
Number of parameters in transformer layers in billions:  0.22
Number of parameters in embedding layers in billions: 0.27
Total number of parameters in billions: 0.49
Number of parameters in most loaded shard in billions: 0.1887
Number of parameters in other shards in billions: 0.0545
Theoretical memory footprints: weight and optimizer=3240.11 MB
 [2024-12-27 13:27:31] iteration        1/      10 | consumed samples:            4 | elapsed time per iteration (ms): 9422.4 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.279898E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 1 iterations) memory (MB) | allocated: 2332.06396484375 | max allocated: 3252.0673828125 | reserved: 3520.0 | max reserved: 3520.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 1895.2998046875 | max allocated: 2491.30810546875 | reserved: 2672.0 | max reserved: 2672.0
stage1 GPU utialization:16.110618909200035 %
weighting memory waste ratio T= 1.2436600746711095
[Rank 0] (after 1 iterations) memory (MB) | allocated: 2346.29150390625 | max allocated: 3959.345703125 | reserved: 4256.0 | max reserved: 4256.0
[Rank 2] (after 1 iterations) memory (MB) | allocated: 1128.50146484375 | max allocated: 1408.50146484375 | reserved: 1658.0 | max reserved: 1658.0
times across ranks (ms):
  forward-backward:
     rank  0: 9368.44
     rank  1: 9366.73
     rank  2: 9368.19
     rank  3: 9361.08
  forward-compute:
     rank  0: 2348.29
     rank  1: 2365.52
     rank  2: 1223.34
     rank  3: 1604.05
  backward-compute:
     rank  0: 1073.77
     rank  1: 1110.10
     rank  2: 54.27
     rank  3: 296.23
  pure-backward-compute:
     rank  0: 1073.20
     rank  1: 1109.54
     rank  2: 53.85
     rank  3: 295.47
  batch-generator:
     rank  0: 1061.91
     rank  1: 1081.87
     rank  2: 1158.33
     rank  3: 1154.68
  forward-recv:
     rank  1: 2295.23
     rank  2: 4511.50
     rank  3: 5711.69
  forward-send:
     rank  0: 3449.41
     rank  1: 1181.36
     rank  2: 16.47
  backward-recv:
     rank  0: 2.24
     rank  1: 1.05
     rank  2: 1.06
  backward-send:
     rank  1: 854.52
     rank  2: 894.69
     rank  3: 732.53
  forward-send-backward-recv:
     rank  0: 2412.35
     rank  1: 1415.46
     rank  2: 1571.43
  backward-send-forward-recv:
     rank  1: 9.90
     rank  2: 895.42
     rank  3: 781.95
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.05
     rank  2: 0.12
     rank  3: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.04
     rank  2: 0.29
     rank  3: 0.10
  all-grads-sync:
     rank  0: 64.93
     rank  1: 56.72
     rank  2: 68.02
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.20
     rank  2: 0.06
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 47.90
     rank  1: 50.31
     rank  2: 39.95
     rank  3: 0.14
  optimizer:
     rank  0: 48.88
     rank  1: 51.36
     rank  2: 40.99
     rank  3: 1.16
Layer: TransformerLayer, Forward time: 3.768511 ms
Layer: TransformerLayer, Forward time: 2.971155 ms
Layer: TransformerLayer, Forward time: 2.880821 ms
Layer: TransformerLayer, Forward time: 2.860320 ms
Layer: TransformerLayer, Forward time: 2.841364 ms
Layer: TransformerLayer, Forward time: 2.861591 ms
Layer: TransformerLayer, Forward time: 2.847478 ms
Layer: TransformerLayer, Forward time: 2.831692 ms
Layer: TransformerLayer, Forward time: 2.879445 ms
Layer: TransformerLayer, Forward time: 2.832889 ms
Layer: TransformerLayer, Forward time: 3.132131 ms
Layer: TransformerLayer, Forward time: 2.921673 ms
Layer: TransformerLayer, Forward time: 3.054745 ms
Layer: TransformerLayer, Forward time: 2.919990 ms
Layer: TransformerLayer, Forward time: 3.009119 ms
Layer: TransformerLayer, Forward time: 3.019233 ms
Layer: TransformerLayer, Forward time: 3.081747 ms
Layer: TransformerLayer, Forward time: 3.032290 ms
Layer: TransformerLayer, Forward time: 2.927758 ms
Layer: TransformerLayer, Forward time: 3.028664 ms
Layer: TransformerLayer, Forward time: 3.032698 ms
Layer: TransformerLayer, Forward time: 3.114640 ms
Layer: TransformerLayer, Forward time: 2.893606 ms
Layer: TransformerLayer, Forward time: 2.948075 ms
Layer: TransformerLayer, Forward time: 3.954510 ms
Layer: TransformerLayer, Forward time: 4.197819 ms
Layer: TransformerLayer, Forward time: 4.185250 ms
Layer: TransformerLayer, Forward time: 4.108074 ms
Layer: TransformerLayer, Forward time: 4.149758 ms
Layer: TransformerLayer, Forward time: 4.038118 ms
Layer: TransformerLayer, Forward time: 5.053503 ms
Layer: TransformerLayer, Forward time: 5.019832 ms
Layer: TransformerLayer, Forward time: 4.961617 ms
Layer: TransformerLayer, Forward time: 5.170905 ms
Layer: TransformerLayer, Forward time: 4.590569 ms
Layer: TransformerLayer, Forward time: 4.816883 ms
Layer: TransformerLayer, Forward time: 5.449634 ms
Layer: TransformerLayer, Forward time: 5.092332 ms
Layer: TransformerLayer, Forward time: 4.343139 ms
Layer: TransformerLayer, Forward time: 3.770841 ms
Layer: TransformerLayer, Forward time: 20.273644 ms
Layer: TransformerLayer, Forward time: 5.709008 ms
Layer: TransformerLayer, Forward time: 4.899562 ms
Layer: TransformerLayer, Forward time: 4.237789 ms
Layer: TransformerLayer, Forward time: 3.674082 ms
Layer: TransformerLayer, Forward time: 4.090646 ms
Layer: TransformerLayer, Forward time: 3.872551 ms
Layer: TransformerLayer, Forward time: 3.842065 ms
Layer: TransformerLayer, Backward time: 0.045131 ms
Layer: TransformerLayer, Backward time: 0.081695 ms
Layer: TransformerLayer, Backward time: 0.052508 ms
Layer: TransformerLayer, Backward time: 0.050167 ms
Layer: TransformerLayer, Backward time: 0.037672 ms
Layer: TransformerLayer, Backward time: 0.040013 ms
Layer: TransformerLayer, Backward time: 0.060929 ms
Layer: TransformerLayer, Backward time: 0.039976 ms
Layer: TransformerLayer, Backward time: 0.039157 ms
Layer: TransformerLayer, Backward time: 0.039439 ms
Layer: TransformerLayer, Backward time: 0.038193 ms
Layer: TransformerLayer, Backward time: 0.038802 ms
Layer: TransformerLayer, Backward time: 0.050416 ms
Layer: TransformerLayer, Backward time: 0.261738 ms
Layer: TransformerLayer, Backward time: 0.049445 ms
Layer: TransformerLayer, Backward time: 0.056548 ms
Layer: TransformerLayer, Backward time: 0.061383 ms
Layer: TransformerLayer, Backward time: 0.041460 ms
Layer: TransformerLayer, Backward time: 0.038960 ms
Layer: TransformerLayer, Backward time: 0.039754 ms
Layer: TransformerLayer, Backward time: 0.059306 ms
Layer: TransformerLayer, Backward time: 0.055477 ms
Layer: TransformerLayer, Backward time: 0.036610 ms
Layer: TransformerLayer, Backward time: 0.065597 ms
Layer: TransformerLayer, Backward time: 0.056666 ms
Layer: TransformerLayer, Backward time: 0.044244 ms
Layer: TransformerLayer, Backward time: 0.042691 ms
Layer: TransformerLayer, Backward time: 0.039685 ms
Layer: TransformerLayer, Backward time: 0.039118 ms
Layer: TransformerLayer, Backward time: 0.035598 ms
Layer: TransformerLayer, Backward time: 0.036143 ms
Layer: TransformerLayer, Backward time: 0.034447 ms
Layer: TransformerLayer, Backward time: 0.034155 ms
Layer: TransformerLayer, Backward time: 0.033414 ms
Layer: TransformerLayer, Backward time: 0.032371 ms
Layer: TransformerLayer, Backward time: 0.032407 ms
Layer: TransformerLayer, Backward time: 0.091466 ms
Layer: TransformerLayer, Backward time: 0.078250 ms
Layer: TransformerLayer, Backward time: 0.037108 ms
Layer: TransformerLayer, Backward time: 0.033746 ms
Layer: TransformerLayer, Backward time: 0.034394 ms
Layer: TransformerLayer, Backward time: 0.033490 ms
Layer: TransformerLayer, Backward time: 0.033618 ms
Layer: TransformerLayer, Backward time: 0.033179 ms
Layer: TransformerLayer, Backward time: 0.034920 ms
Layer: TransformerLayer, Backward time: 0.035778 ms
Layer: TransformerLayer, Backward time: 0.033156 ms
Layer: TransformerLayer, Backward time: 0.033300 ms
 [2024-12-27 13:27:32] iteration        2/      10 | consumed samples:            8 | elapsed time per iteration (ms): 782.9 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.427397E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 745.72
     rank  1: 745.33
     rank  2: 744.14
     rank  3: 742.99
  forward-compute:
     rank  0: 226.28
     rank  1: 201.81
     rank  2: 45.19
     rank  3: 263.28
  backward-compute:
     rank  0: 194.55
     rank  1: 166.20
     rank  2: 48.32
     rank  3: 274.99
  pure-backward-compute:
     rank  0: 194.04
     rank  1: 165.71
     rank  2: 47.44
     rank  3: 274.32
  batch-generator:
     rank  0: 16.81
     rank  1: 18.92
     rank  2: 17.45
     rank  3: 15.31
  forward-recv:
     rank  1: 62.77
     rank  2: 117.25
     rank  3: 96.80
  forward-send:
     rank  0: 1.29
     rank  1: 0.98
     rank  2: 0.99
  backward-recv:
     rank  0: 250.48
     rank  1: 186.18
     rank  2: 122.25
  backward-send:
     rank  1: 1.71
     rank  2: 0.89
     rank  3: 0.98
  forward-send-backward-recv:
     rank  0: 63.40
     rank  1: 68.85
     rank  2: 317.49
  backward-send-forward-recv:
     rank  1: 1.00
     rank  2: 1.70
     rank  3: 4.92
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.03
     rank  2: 0.07
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.59
     rank  1: 1.39
     rank  2: 0.93
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.15
     rank  2: 0.02
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.69
     rank  1: 24.31
     rank  2: 11.79
     rank  3: 0.04
  optimizer:
     rank  0: 25.69
     rank  1: 25.34
     rank  2: 12.79
     rank  3: 1.03
Layer: TransformerLayer, Forward time: 3.216052 ms
Layer: TransformerLayer, Forward time: 2.878258 ms
Layer: TransformerLayer, Forward time: 2.958177 ms
Layer: TransformerLayer, Forward time: 2.858059 ms
Layer: TransformerLayer, Forward time: 2.756706 ms
Layer: TransformerLayer, Forward time: 2.741407 ms
Layer: TransformerLayer, Forward time: 2.929903 ms
Layer: TransformerLayer, Forward time: 2.826058 ms
Layer: TransformerLayer, Forward time: 2.916181 ms
Layer: TransformerLayer, Forward time: 2.751356 ms
Layer: TransformerLayer, Forward time: 2.928419 ms
Layer: TransformerLayer, Forward time: 3.154187 ms
Layer: TransformerLayer, Forward time: 3.229056 ms
Layer: TransformerLayer, Forward time: 3.134509 ms
Layer: TransformerLayer, Forward time: 3.115039 ms
Layer: TransformerLayer, Forward time: 3.063349 ms
Layer: TransformerLayer, Forward time: 3.045434 ms
Layer: TransformerLayer, Forward time: 3.013436 ms
Layer: TransformerLayer, Forward time: 3.038725 ms
Layer: TransformerLayer, Forward time: 3.016345 ms
Layer: TransformerLayer, Forward time: 3.061600 ms
Layer: TransformerLayer, Forward time: 3.044420 ms
Layer: TransformerLayer, Forward time: 4.055327 ms
Layer: TransformerLayer, Forward time: 3.169906 ms
Layer: TransformerLayer, Forward time: 3.347760 ms
Layer: TransformerLayer, Forward time: 3.221540 ms
Layer: TransformerLayer, Forward time: 3.184125 ms
Layer: TransformerLayer, Forward time: 3.185682 ms
Layer: TransformerLayer, Forward time: 3.165385 ms
Layer: TransformerLayer, Forward time: 3.180294 ms
Layer: TransformerLayer, Forward time: 3.173628 ms
Layer: TransformerLayer, Forward time: 3.177058 ms
Layer: TransformerLayer, Forward time: 3.184818 ms
Layer: TransformerLayer, Forward time: 3.146799 ms
Layer: TransformerLayer, Forward time: 3.227183 ms
Layer: TransformerLayer, Forward time: 3.424674 ms
Layer: TransformerLayer, Forward time: 3.445711 ms
Layer: TransformerLayer, Forward time: 3.319163 ms
Layer: TransformerLayer, Forward time: 3.307187 ms
Layer: TransformerLayer, Forward time: 3.288559 ms
Layer: TransformerLayer, Forward time: 3.279248 ms
Layer: TransformerLayer, Forward time: 3.298450 ms
Layer: TransformerLayer, Forward time: 3.269962 ms
Layer: TransformerLayer, Forward time: 3.255741 ms
Layer: TransformerLayer, Forward time: 3.232102 ms
Layer: TransformerLayer, Forward time: 3.240042 ms
Layer: TransformerLayer, Forward time: 3.420617 ms
Layer: TransformerLayer, Forward time: 3.233327 ms
Layer: TransformerLayer, Backward time: 0.094127 ms
Layer: TransformerLayer, Backward time: 0.084189 ms
Layer: TransformerLayer, Backward time: 0.079432 ms
Layer: TransformerLayer, Backward time: 0.036340 ms
Layer: TransformerLayer, Backward time: 0.036765 ms
Layer: TransformerLayer, Backward time: 0.034777 ms
Layer: TransformerLayer, Backward time: 0.033774 ms
Layer: TransformerLayer, Backward time: 0.034510 ms
Layer: TransformerLayer, Backward time: 0.035090 ms
Layer: TransformerLayer, Backward time: 0.035007 ms
Layer: TransformerLayer, Backward time: 0.033974 ms
Layer: TransformerLayer, Backward time: 0.033884 ms
Layer: TransformerLayer, Backward time: 0.053088 ms
Layer: TransformerLayer, Backward time: 0.074729 ms
Layer: TransformerLayer, Backward time: 0.039237 ms
Layer: TransformerLayer, Backward time: 0.046198 ms
Layer: TransformerLayer, Backward time: 0.055444 ms
Layer: TransformerLayer, Backward time: 0.077905 ms
Layer: TransformerLayer, Backward time: 0.043652 ms
Layer: TransformerLayer, Backward time: 0.036691 ms
Layer: TransformerLayer, Backward time: 0.035475 ms
Layer: TransformerLayer, Backward time: 0.036935 ms
Layer: TransformerLayer, Backward time: 0.034613 ms
Layer: TransformerLayer, Backward time: 0.034720 ms
Layer: TransformerLayer, Backward time: 0.056020 ms
Layer: TransformerLayer, Backward time: 0.045430 ms
Layer: TransformerLayer, Backward time: 0.036121 ms
Layer: TransformerLayer, Backward time: 0.035594 ms
Layer: TransformerLayer, Backward time: 0.035268 ms
Layer: TransformerLayer, Backward time: 0.037093 ms
Layer: TransformerLayer, Backward time: 0.038442 ms
Layer: TransformerLayer, Backward time: 0.045320 ms
Layer: TransformerLayer, Backward time: 0.037337 ms
Layer: TransformerLayer, Backward time: 0.035225 ms
Layer: TransformerLayer, Backward time: 0.033046 ms
Layer: TransformerLayer, Backward time: 0.033549 ms
Layer: TransformerLayer, Backward time: 0.045745 ms
Layer: TransformerLayer, Backward time: 0.036690 ms
Layer: TransformerLayer, Backward time: 0.034977 ms
Layer: TransformerLayer, Backward time: 0.033573 ms
Layer: TransformerLayer, Backward time: 0.033842 ms
Layer: TransformerLayer, Backward time: 0.034440 ms
Layer: TransformerLayer, Backward time: 0.033186 ms
Layer: TransformerLayer, Backward time: 0.034706 ms
Layer: TransformerLayer, Backward time: 0.033187 ms
Layer: TransformerLayer, Backward time: 0.033135 ms
Layer: TransformerLayer, Backward time: 0.034476 ms
Layer: TransformerLayer, Backward time: 0.033784 ms
 [2024-12-27 13:27:33] iteration        3/      10 | consumed samples:           12 | elapsed time per iteration (ms): 775.6 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.980520E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 739.23
     rank  1: 738.70
     rank  2: 737.53
     rank  3: 736.55
  forward-compute:
     rank  0: 177.03
     rank  1: 152.11
     rank  2: 41.69
     rank  3: 266.87
  backward-compute:
     rank  0: 179.31
     rank  1: 164.89
     rank  2: 47.56
     rank  3: 274.99
  pure-backward-compute:
     rank  0: 178.82
     rank  1: 164.47
     rank  2: 47.19
     rank  3: 274.24
  batch-generator:
     rank  0: 14.36
     rank  1: 13.61
     rank  2: 14.73
     rank  3: 19.01
  forward-recv:
     rank  1: 56.97
     rank  2: 113.48
     rank  3: 91.28
  forward-send:
     rank  0: 1.27
     rank  1: 0.92
     rank  2: 0.93
  backward-recv:
     rank  0: 264.23
     rank  1: 193.48
     rank  2: 126.93
  backward-send:
     rank  1: 1.87
     rank  2: 0.95
     rank  3: 1.18
  forward-send-backward-recv:
     rank  0: 107.75
     rank  1: 116.62
     rank  2: 319.67
  backward-send-forward-recv:
     rank  1: 0.69
     rank  2: 1.50
     rank  3: 5.18
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.09
     rank  3: 0.09
  all-grads-sync:
     rank  0: 1.55
     rank  1: 1.30
     rank  2: 0.77
     rank  3: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.31
     rank  1: 0.14
     rank  2: 0.02
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
  optimizer-inner-step:
     rank  0: 24.97
     rank  1: 24.23
     rank  2: 11.73
     rank  3: 0.06
  optimizer:
     rank  0: 26.41
     rank  1: 25.74
     rank  2: 13.23
     rank  3: 1.55
Layer: TransformerLayer, Forward time: 3.318947 ms
Layer: TransformerLayer, Forward time: 2.973853 ms
Layer: TransformerLayer, Forward time: 2.908989 ms
Layer: TransformerLayer, Forward time: 2.834179 ms
Layer: TransformerLayer, Forward time: 2.864113 ms
Layer: TransformerLayer, Forward time: 2.866140 ms
Layer: TransformerLayer, Forward time: 2.869903 ms
Layer: TransformerLayer, Forward time: 2.859035 ms
Layer: TransformerLayer, Forward time: 2.850894 ms
Layer: TransformerLayer, Forward time: 2.870657 ms
Layer: TransformerLayer, Forward time: 2.872079 ms
Layer: TransformerLayer, Forward time: 3.072407 ms
Layer: TransformerLayer, Forward time: 3.074555 ms
Layer: TransformerLayer, Forward time: 2.966686 ms
Layer: TransformerLayer, Forward time: 2.938302 ms
Layer: TransformerLayer, Forward time: 2.930772 ms
Layer: TransformerLayer, Forward time: 2.887356 ms
Layer: TransformerLayer, Forward time: 2.914719 ms
Layer: TransformerLayer, Forward time: 2.884184 ms
Layer: TransformerLayer, Forward time: 2.869962 ms
Layer: TransformerLayer, Forward time: 3.060366 ms
Layer: TransformerLayer, Forward time: 2.914542 ms
Layer: TransformerLayer, Forward time: 3.070164 ms
Layer: TransformerLayer, Forward time: 2.945523 ms
Layer: TransformerLayer, Forward time: 3.186353 ms
Layer: TransformerLayer, Forward time: 3.003193 ms
Layer: TransformerLayer, Forward time: 3.200044 ms
Layer: TransformerLayer, Forward time: 3.276232 ms
Layer: TransformerLayer, Forward time: 2.974227 ms
Layer: TransformerLayer, Forward time: 2.949726 ms
Layer: TransformerLayer, Forward time: 2.951209 ms
Layer: TransformerLayer, Forward time: 2.954665 ms
Layer: TransformerLayer, Forward time: 3.135131 ms
Layer: TransformerLayer, Forward time: 3.087848 ms
Layer: TransformerLayer, Forward time: 3.191975 ms
Layer: TransformerLayer, Forward time: 3.165275 ms
Layer: TransformerLayer, Forward time: 3.479486 ms
Layer: TransformerLayer, Forward time: 3.279469 ms
Layer: TransformerLayer, Forward time: 3.233256 ms
Layer: TransformerLayer, Forward time: 3.231942 ms
Layer: TransformerLayer, Forward time: 3.246826 ms
Layer: TransformerLayer, Forward time: 3.200458 ms
Layer: TransformerLayer, Forward time: 3.233952 ms
Layer: TransformerLayer, Forward time: 3.244170 ms
Layer: TransformerLayer, Forward time: 3.188945 ms
Layer: TransformerLayer, Forward time: 3.178523 ms
Layer: TransformerLayer, Forward time: 3.339201 ms
Layer: TransformerLayer, Forward time: 3.167981 ms
Layer: TransformerLayer, Backward time: 0.053628 ms
Layer: TransformerLayer, Backward time: 0.038269 ms
Layer: TransformerLayer, Backward time: 0.034796 ms
Layer: TransformerLayer, Backward time: 0.034466 ms
Layer: TransformerLayer, Backward time: 0.034606 ms
Layer: TransformerLayer, Backward time: 0.033909 ms
Layer: TransformerLayer, Backward time: 0.033959 ms
Layer: TransformerLayer, Backward time: 0.035648 ms
Layer: TransformerLayer, Backward time: 0.033596 ms
Layer: TransformerLayer, Backward time: 0.034025 ms
Layer: TransformerLayer, Backward time: 0.032841 ms
Layer: TransformerLayer, Backward time: 0.034595 ms
Layer: TransformerLayer, Backward time: 0.077670 ms
Layer: TransformerLayer, Backward time: 0.365536 ms
Layer: TransformerLayer, Backward time: 0.059132 ms
Layer: TransformerLayer, Backward time: 0.039361 ms
Layer: TransformerLayer, Backward time: 0.035656 ms
Layer: TransformerLayer, Backward time: 0.036070 ms
Layer: TransformerLayer, Backward time: 0.036517 ms
Layer: TransformerLayer, Backward time: 0.034372 ms
Layer: TransformerLayer, Backward time: 0.033804 ms
Layer: TransformerLayer, Backward time: 0.032277 ms
Layer: TransformerLayer, Backward time: 0.032924 ms
Layer: TransformerLayer, Backward time: 0.033974 ms
Layer: TransformerLayer, Backward time: 0.051812 ms
Layer: TransformerLayer, Backward time: 0.070220 ms
Layer: TransformerLayer, Backward time: 0.078182 ms
Layer: TransformerLayer, Backward time: 0.067654 ms
Layer: TransformerLayer, Backward time: 0.043551 ms
Layer: TransformerLayer, Backward time: 0.034846 ms
Layer: TransformerLayer, Backward time: 0.033948 ms
Layer: TransformerLayer, Backward time: 0.036518 ms
Layer: TransformerLayer, Backward time: 0.042297 ms
Layer: TransformerLayer, Backward time: 0.033367 ms
Layer: TransformerLayer, Backward time: 0.033578 ms
Layer: TransformerLayer, Backward time: 0.033964 ms
Layer: TransformerLayer, Backward time: 0.090752 ms
Layer: TransformerLayer, Backward time: 0.071581 ms
Layer: TransformerLayer, Backward time: 0.041048 ms
Layer: TransformerLayer, Backward time: 0.033655 ms
Layer: TransformerLayer, Backward time: 0.032346 ms
Layer: TransformerLayer, Backward time: 0.031534 ms
Layer: TransformerLayer, Backward time: 0.032581 ms
Layer: TransformerLayer, Backward time: 0.032150 ms
Layer: TransformerLayer, Backward time: 0.032419 ms
Layer: TransformerLayer, Backward time: 0.033898 ms
Layer: TransformerLayer, Backward time: 0.031811 ms
Layer: TransformerLayer, Backward time: 0.032406 ms
 [2024-12-27 13:27:33] iteration        4/      10 | consumed samples:           16 | elapsed time per iteration (ms): 784.9 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.199298E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 749.38
     rank  1: 748.81
     rank  2: 747.82
     rank  3: 746.51
  forward-compute:
     rank  0: 173.64
     rank  1: 155.66
     rank  2: 42.16
     rank  3: 274.65
  backward-compute:
     rank  0: 186.25
     rank  1: 165.28
     rank  2: 47.81
     rank  3: 274.02
  pure-backward-compute:
     rank  0: 185.83
     rank  1: 164.87
     rank  2: 47.46
     rank  3: 273.25
  batch-generator:
     rank  0: 15.62
     rank  1: 13.54
     rank  2: 14.90
     rank  3: 20.52
  forward-recv:
     rank  1: 52.34
     rank  2: 112.02
     rank  3: 90.41
  forward-send:
     rank  0: 1.23
     rank  1: 0.93
     rank  2: 0.93
  backward-recv:
     rank  0: 271.28
     rank  1: 191.34
     rank  2: 127.47
  backward-send:
     rank  1: 1.25
     rank  2: 0.92
     rank  3: 1.19
  forward-send-backward-recv:
     rank  0: 107.38
     rank  1: 127.00
     rank  2: 326.01
  backward-send-forward-recv:
     rank  1: 0.70
     rank  2: 1.56
     rank  3: 5.03
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.08
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.75
     rank  1: 1.28
     rank  2: 0.87
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.12
     rank  2: 0.02
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.73
     rank  1: 24.14
     rank  2: 11.84
     rank  3: 0.03
  optimizer:
     rank  0: 25.87
     rank  1: 25.30
     rank  2: 13.00
     rank  3: 1.19
Layer: TransformerLayer, Forward time: 3.216008 ms
Layer: TransformerLayer, Forward time: 2.644985 ms
Layer: TransformerLayer, Forward time: 2.605340 ms
Layer: TransformerLayer, Forward time: 2.559590 ms
Layer: TransformerLayer, Forward time: 2.547788 ms
Layer: TransformerLayer, Forward time: 2.557123 ms
Layer: TransformerLayer, Forward time: 2.573870 ms
Layer: TransformerLayer, Forward time: 2.560553 ms
Layer: TransformerLayer, Forward time: 2.556193 ms
Layer: TransformerLayer, Forward time: 2.573214 ms
Layer: TransformerLayer, Forward time: 2.545761 ms
Layer: TransformerLayer, Forward time: 2.725565 ms
Layer: TransformerLayer, Forward time: 2.907241 ms
Layer: TransformerLayer, Forward time: 2.852830 ms
Layer: TransformerLayer, Forward time: 2.607231 ms
Layer: TransformerLayer, Forward time: 2.640414 ms
Layer: TransformerLayer, Forward time: 2.806953 ms
Layer: TransformerLayer, Forward time: 2.917610 ms
Layer: TransformerLayer, Forward time: 2.608111 ms
Layer: TransformerLayer, Forward time: 2.876176 ms
Layer: TransformerLayer, Forward time: 2.831274 ms
Layer: TransformerLayer, Forward time: 2.656250 ms
Layer: TransformerLayer, Forward time: 3.075873 ms
Layer: TransformerLayer, Forward time: 3.003081 ms
Layer: TransformerLayer, Forward time: 3.138789 ms
Layer: TransformerLayer, Forward time: 3.052422 ms
Layer: TransformerLayer, Forward time: 3.013602 ms
Layer: TransformerLayer, Forward time: 3.015070 ms
Layer: TransformerLayer, Forward time: 2.989235 ms
Layer: TransformerLayer, Forward time: 2.991148 ms
Layer: TransformerLayer, Forward time: 2.971024 ms
Layer: TransformerLayer, Forward time: 2.996673 ms
Layer: TransformerLayer, Forward time: 2.957814 ms
Layer: TransformerLayer, Forward time: 2.972209 ms
Layer: TransformerLayer, Forward time: 3.151492 ms
Layer: TransformerLayer, Forward time: 3.133244 ms
Layer: TransformerLayer, Forward time: 3.367537 ms
Layer: TransformerLayer, Forward time: 3.209806 ms
Layer: TransformerLayer, Forward time: 3.175713 ms
Layer: TransformerLayer, Forward time: 3.085608 ms
Layer: TransformerLayer, Forward time: 3.110509 ms
Layer: TransformerLayer, Forward time: 3.166958 ms
Layer: TransformerLayer, Forward time: 3.143462 ms
Layer: TransformerLayer, Forward time: 3.176826 ms
Layer: TransformerLayer, Forward time: 3.096210 ms
Layer: TransformerLayer, Forward time: 3.008767 ms
Layer: TransformerLayer, Forward time: 3.274093 ms
Layer: TransformerLayer, Forward time: 3.103265 ms
Layer: TransformerLayer, Backward time: 0.053225 ms
Layer: TransformerLayer, Backward time: 0.044567 ms
Layer: TransformerLayer, Backward time: 0.042682 ms
Layer: TransformerLayer, Backward time: 0.038706 ms
Layer: TransformerLayer, Backward time: 0.034371 ms
Layer: TransformerLayer, Backward time: 0.034226 ms
Layer: TransformerLayer, Backward time: 0.036907 ms
Layer: TransformerLayer, Backward time: 0.033915 ms
Layer: TransformerLayer, Backward time: 0.032327 ms
Layer: TransformerLayer, Backward time: 0.035634 ms
Layer: TransformerLayer, Backward time: 0.032585 ms
Layer: TransformerLayer, Backward time: 0.032105 ms
Layer: TransformerLayer, Backward time: 0.052164 ms
Layer: TransformerLayer, Backward time: 0.051217 ms
Layer: TransformerLayer, Backward time: 0.035629 ms
Layer: TransformerLayer, Backward time: 0.033821 ms
Layer: TransformerLayer, Backward time: 0.033513 ms
Layer: TransformerLayer, Backward time: 0.033572 ms
Layer: TransformerLayer, Backward time: 0.033986 ms
Layer: TransformerLayer, Backward time: 0.032575 ms
Layer: TransformerLayer, Backward time: 0.035523 ms
Layer: TransformerLayer, Backward time: 0.033343 ms
Layer: TransformerLayer, Backward time: 0.033240 ms
Layer: TransformerLayer, Backward time: 0.032649 ms
Layer: TransformerLayer, Backward time: 0.052133 ms
Layer: TransformerLayer, Backward time: 0.046023 ms
Layer: TransformerLayer, Backward time: 0.035141 ms
Layer: TransformerLayer, Backward time: 0.060264 ms
Layer: TransformerLayer, Backward time: 0.034397 ms
Layer: TransformerLayer, Backward time: 0.027063 ms
Layer: TransformerLayer, Backward time: 0.027372 ms
Layer: TransformerLayer, Backward time: 0.026445 ms
Layer: TransformerLayer, Backward time: 0.030674 ms
Layer: TransformerLayer, Backward time: 0.027952 ms
Layer: TransformerLayer, Backward time: 0.030040 ms
Layer: TransformerLayer, Backward time: 0.027196 ms
Layer: TransformerLayer, Backward time: 0.046213 ms
Layer: TransformerLayer, Backward time: 0.044308 ms
Layer: TransformerLayer, Backward time: 0.028575 ms
Layer: TransformerLayer, Backward time: 0.025868 ms
Layer: TransformerLayer, Backward time: 0.027626 ms
Layer: TransformerLayer, Backward time: 0.027210 ms
Layer: TransformerLayer, Backward time: 0.026302 ms
Layer: TransformerLayer, Backward time: 0.025707 ms
Layer: TransformerLayer, Backward time: 0.026632 ms
Layer: TransformerLayer, Backward time: 0.026067 ms
Layer: TransformerLayer, Backward time: 0.025633 ms
Layer: TransformerLayer, Backward time: 0.026123 ms
 [2024-12-27 13:27:34] iteration        5/      10 | consumed samples:           20 | elapsed time per iteration (ms): 763.5 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.098997E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 726.68
     rank  1: 726.23
     rank  2: 725.15
     rank  3: 724.00
  forward-compute:
     rank  0: 164.04
     rank  1: 146.44
     rank  2: 43.78
     rank  3: 265.70
  backward-compute:
     rank  0: 170.72
     rank  1: 167.15
     rank  2: 47.97
     rank  3: 267.49
  pure-backward-compute:
     rank  0: 170.25
     rank  1: 166.85
     rank  2: 47.57
     rank  3: 266.76
  batch-generator:
     rank  0: 13.59
     rank  1: 13.09
     rank  2: 16.77
     rank  3: 18.89
  forward-recv:
     rank  1: 49.20
     rank  2: 103.65
     rank  3: 84.76
  forward-send:
     rank  0: 1.21
     rank  1: 0.94
     rank  2: 0.90
  backward-recv:
     rank  0: 274.02
     rank  1: 181.66
     rank  2: 117.62
  backward-send:
     rank  1: 1.29
     rank  2: 0.99
     rank  3: 1.19
  forward-send-backward-recv:
     rank  0: 107.25
     rank  1: 129.21
     rank  2: 320.93
  backward-send-forward-recv:
     rank  1: 0.74
     rank  2: 1.47
     rank  3: 5.11
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.09
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.55
     rank  1: 1.27
     rank  2: 0.88
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.32
     rank  1: 0.12
     rank  2: 0.03
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.04
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.03
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 25.02
     rank  1: 24.18
     rank  2: 11.82
     rank  3: 0.04
  optimizer:
     rank  0: 26.41
     rank  1: 25.60
     rank  2: 13.24
     rank  3: 1.45
Layer: TransformerLayer, Forward time: 3.375289 ms
Layer: TransformerLayer, Forward time: 2.922293 ms
Layer: TransformerLayer, Forward time: 2.771816 ms
Layer: TransformerLayer, Forward time: 2.790842 ms
Layer: TransformerLayer, Forward time: 2.786892 ms
Layer: TransformerLayer, Forward time: 2.678801 ms
Layer: TransformerLayer, Forward time: 2.633609 ms
Layer: TransformerLayer, Forward time: 2.577991 ms
Layer: TransformerLayer, Forward time: 2.577937 ms
Layer: TransformerLayer, Forward time: 2.654934 ms
Layer: TransformerLayer, Forward time: 2.656626 ms
Layer: TransformerLayer, Forward time: 2.741494 ms
Layer: TransformerLayer, Forward time: 3.074891 ms
Layer: TransformerLayer, Forward time: 3.004911 ms
Layer: TransformerLayer, Forward time: 2.972516 ms
Layer: TransformerLayer, Forward time: 2.977227 ms
Layer: TransformerLayer, Forward time: 2.973873 ms
Layer: TransformerLayer, Forward time: 2.970934 ms
Layer: TransformerLayer, Forward time: 2.960252 ms
Layer: TransformerLayer, Forward time: 2.995829 ms
Layer: TransformerLayer, Forward time: 2.964104 ms
Layer: TransformerLayer, Forward time: 3.013462 ms
Layer: TransformerLayer, Forward time: 3.225478 ms
Layer: TransformerLayer, Forward time: 3.089197 ms
Layer: TransformerLayer, Forward time: 3.198801 ms
Layer: TransformerLayer, Forward time: 3.092260 ms
Layer: TransformerLayer, Forward time: 3.047426 ms
Layer: TransformerLayer, Forward time: 3.047841 ms
Layer: TransformerLayer, Forward time: 2.999350 ms
Layer: TransformerLayer, Forward time: 3.035530 ms
Layer: TransformerLayer, Forward time: 3.003585 ms
Layer: TransformerLayer, Forward time: 3.021338 ms
Layer: TransformerLayer, Forward time: 2.995776 ms
Layer: TransformerLayer, Forward time: 2.983433 ms
Layer: TransformerLayer, Forward time: 3.135911 ms
Layer: TransformerLayer, Forward time: 2.995627 ms
Layer: TransformerLayer, Forward time: 3.451282 ms
Layer: TransformerLayer, Forward time: 3.118551 ms
Layer: TransformerLayer, Forward time: 3.139882 ms
Layer: TransformerLayer, Forward time: 3.050481 ms
Layer: TransformerLayer, Forward time: 3.139079 ms
Layer: TransformerLayer, Forward time: 3.136636 ms
Layer: TransformerLayer, Forward time: 3.154517 ms
Layer: TransformerLayer, Forward time: 3.236672 ms
Layer: TransformerLayer, Forward time: 4.733148 ms
Layer: TransformerLayer, Forward time: 3.369711 ms
Layer: TransformerLayer, Forward time: 3.481783 ms
Layer: TransformerLayer, Forward time: 3.245343 ms
Layer: TransformerLayer, Backward time: 0.069780 ms
Layer: TransformerLayer, Backward time: 0.057094 ms
Layer: TransformerLayer, Backward time: 0.051859 ms
Layer: TransformerLayer, Backward time: 0.031362 ms
Layer: TransformerLayer, Backward time: 0.031802 ms
Layer: TransformerLayer, Backward time: 0.033820 ms
Layer: TransformerLayer, Backward time: 0.033553 ms
Layer: TransformerLayer, Backward time: 0.031590 ms
Layer: TransformerLayer, Backward time: 0.033787 ms
Layer: TransformerLayer, Backward time: 0.032115 ms
Layer: TransformerLayer, Backward time: 0.032468 ms
Layer: TransformerLayer, Backward time: 0.030672 ms
Layer: TransformerLayer, Backward time: 0.176093 ms
Layer: TransformerLayer, Backward time: 0.069674 ms
Layer: TransformerLayer, Backward time: 0.046070 ms
Layer: TransformerLayer, Backward time: 0.032661 ms
Layer: TransformerLayer, Backward time: 0.031840 ms
Layer: TransformerLayer, Backward time: 0.033477 ms
Layer: TransformerLayer, Backward time: 0.029834 ms
Layer: TransformerLayer, Backward time: 0.031313 ms
Layer: TransformerLayer, Backward time: 0.035318 ms
Layer: TransformerLayer, Backward time: 0.027220 ms
Layer: TransformerLayer, Backward time: 0.033683 ms
Layer: TransformerLayer, Backward time: 0.031829 ms
Layer: TransformerLayer, Backward time: 0.055848 ms
Layer: TransformerLayer, Backward time: 0.049168 ms
Layer: TransformerLayer, Backward time: 0.040967 ms
Layer: TransformerLayer, Backward time: 0.030667 ms
Layer: TransformerLayer, Backward time: 0.028788 ms
Layer: TransformerLayer, Backward time: 0.029556 ms
Layer: TransformerLayer, Backward time: 0.034782 ms
Layer: TransformerLayer, Backward time: 0.026724 ms
Layer: TransformerLayer, Backward time: 0.026840 ms
Layer: TransformerLayer, Backward time: 0.027878 ms
Layer: TransformerLayer, Backward time: 0.028519 ms
Layer: TransformerLayer, Backward time: 0.027847 ms
Layer: TransformerLayer, Backward time: 0.047771 ms
Layer: TransformerLayer, Backward time: 0.027398 ms
Layer: TransformerLayer, Backward time: 0.026087 ms
Layer: TransformerLayer, Backward time: 0.025965 ms
Layer: TransformerLayer, Backward time: 0.027177 ms
Layer: TransformerLayer, Backward time: 0.027506 ms
Layer: TransformerLayer, Backward time: 0.025708 ms
Layer: TransformerLayer, Backward time: 0.026040 ms
Layer: TransformerLayer, Backward time: 0.025604 ms
Layer: TransformerLayer, Backward time: 0.025443 ms
Layer: TransformerLayer, Backward time: 0.026344 ms
Layer: TransformerLayer, Backward time: 0.026354 ms
 [2024-12-27 13:27:35] iteration        6/      10 | consumed samples:           24 | elapsed time per iteration (ms): 778.0 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 7.464402E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 741.16
     rank  1: 740.50
     rank  2: 739.37
     rank  3: 738.14
  forward-compute:
     rank  0: 171.03
     rank  1: 169.29
     rank  2: 42.59
     rank  3: 271.28
  backward-compute:
     rank  0: 178.40
     rank  1: 153.67
     rank  2: 48.11
     rank  3: 274.71
  pure-backward-compute:
     rank  0: 177.85
     rank  1: 153.38
     rank  2: 47.73
     rank  3: 273.99
  batch-generator:
     rank  0: 13.82
     rank  1: 14.92
     rank  2: 15.54
     rank  3: 18.55
  forward-recv:
     rank  1: 45.79
     rank  2: 120.45
     rank  3: 88.71
  forward-send:
     rank  0: 8.31
     rank  1: 0.99
     rank  2: 0.87
  backward-recv:
     rank  0: 277.67
     rank  1: 202.33
     rank  2: 128.78
  backward-send:
     rank  1: 1.29
     rank  2: 0.95
     rank  3: 1.18
  forward-send-backward-recv:
     rank  0: 94.98
     rank  1: 115.93
     rank  2: 310.67
  backward-send-forward-recv:
     rank  1: 0.74
     rank  2: 1.51
     rank  3: 5.02
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.03
     rank  2: 0.08
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.89
     rank  1: 1.29
     rank  2: 0.87
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.13
     rank  2: 0.02
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.03
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 25.00
     rank  1: 24.29
     rank  2: 11.80
     rank  3: 0.04
  optimizer:
     rank  0: 26.11
     rank  1: 25.42
     rank  2: 12.94
     rank  3: 1.16
Layer: TransformerLayer, Forward time: 3.486347 ms
Layer: TransformerLayer, Forward time: 2.649328 ms
Layer: TransformerLayer, Forward time: 2.542037 ms
Layer: TransformerLayer, Forward time: 2.742217 ms
Layer: TransformerLayer, Forward time: 2.539441 ms
Layer: TransformerLayer, Forward time: 2.533794 ms
Layer: TransformerLayer, Forward time: 2.681806 ms
Layer: TransformerLayer, Forward time: 2.628307 ms
Layer: TransformerLayer, Forward time: 2.518409 ms
Layer: TransformerLayer, Forward time: 2.616696 ms
Layer: TransformerLayer, Forward time: 2.645730 ms
Layer: TransformerLayer, Forward time: 2.694397 ms
Layer: TransformerLayer, Forward time: 3.019847 ms
Layer: TransformerLayer, Forward time: 2.957549 ms
Layer: TransformerLayer, Forward time: 2.917230 ms
Layer: TransformerLayer, Forward time: 2.885382 ms
Layer: TransformerLayer, Forward time: 2.865932 ms
Layer: TransformerLayer, Forward time: 2.885770 ms
Layer: TransformerLayer, Forward time: 2.901443 ms
Layer: TransformerLayer, Forward time: 2.892509 ms
Layer: TransformerLayer, Forward time: 2.931686 ms
Layer: TransformerLayer, Forward time: 2.909358 ms
Layer: TransformerLayer, Forward time: 3.059761 ms
Layer: TransformerLayer, Forward time: 2.966039 ms
Layer: TransformerLayer, Forward time: 3.267260 ms
Layer: TransformerLayer, Forward time: 3.008585 ms
Layer: TransformerLayer, Forward time: 3.015197 ms
Layer: TransformerLayer, Forward time: 2.993548 ms
Layer: TransformerLayer, Forward time: 2.953661 ms
Layer: TransformerLayer, Forward time: 2.965294 ms
Layer: TransformerLayer, Forward time: 3.000541 ms
Layer: TransformerLayer, Forward time: 2.965019 ms
Layer: TransformerLayer, Forward time: 2.919157 ms
Layer: TransformerLayer, Forward time: 3.028503 ms
Layer: TransformerLayer, Forward time: 3.058917 ms
Layer: TransformerLayer, Forward time: 3.056182 ms
Layer: TransformerLayer, Forward time: 3.368100 ms
Layer: TransformerLayer, Forward time: 3.101262 ms
Layer: TransformerLayer, Forward time: 3.017226 ms
Layer: TransformerLayer, Forward time: 3.114663 ms
Layer: TransformerLayer, Forward time: 3.115878 ms
Layer: TransformerLayer, Forward time: 3.157390 ms
Layer: TransformerLayer, Forward time: 3.064692 ms
Layer: TransformerLayer, Forward time: 2.976758 ms
Layer: TransformerLayer, Forward time: 3.097427 ms
Layer: TransformerLayer, Forward time: 3.054678 ms
Layer: TransformerLayer, Forward time: 3.297761 ms
Layer: TransformerLayer, Forward time: 3.062538 ms
Layer: TransformerLayer, Backward time: 0.115117 ms
Layer: TransformerLayer, Backward time: 0.070409 ms
Layer: TransformerLayer, Backward time: 0.049925 ms
Layer: TransformerLayer, Backward time: 0.038425 ms
Layer: TransformerLayer, Backward time: 0.040238 ms
Layer: TransformerLayer, Backward time: 0.038907 ms
Layer: TransformerLayer, Backward time: 0.048510 ms
Layer: TransformerLayer, Backward time: 0.050218 ms
Layer: TransformerLayer, Backward time: 0.037421 ms
Layer: TransformerLayer, Backward time: 0.037599 ms
Layer: TransformerLayer, Backward time: 0.037746 ms
Layer: TransformerLayer, Backward time: 0.035662 ms
Layer: TransformerLayer, Backward time: 0.160646 ms
Layer: TransformerLayer, Backward time: 0.075333 ms
Layer: TransformerLayer, Backward time: 0.044308 ms
Layer: TransformerLayer, Backward time: 0.040170 ms
Layer: TransformerLayer, Backward time: 0.046128 ms
Layer: TransformerLayer, Backward time: 0.039275 ms
Layer: TransformerLayer, Backward time: 0.037200 ms
Layer: TransformerLayer, Backward time: 0.035978 ms
Layer: TransformerLayer, Backward time: 0.040901 ms
Layer: TransformerLayer, Backward time: 0.044228 ms
Layer: TransformerLayer, Backward time: 0.033678 ms
Layer: TransformerLayer, Backward time: 0.036028 ms
Layer: TransformerLayer, Backward time: 0.077879 ms
Layer: TransformerLayer, Backward time: 0.076034 ms
Layer: TransformerLayer, Backward time: 0.088373 ms
Layer: TransformerLayer, Backward time: 0.032350 ms
Layer: TransformerLayer, Backward time: 0.032798 ms
Layer: TransformerLayer, Backward time: 0.035404 ms
Layer: TransformerLayer, Backward time: 0.031210 ms
Layer: TransformerLayer, Backward time: 0.031127 ms
Layer: TransformerLayer, Backward time: 0.031659 ms
Layer: TransformerLayer, Backward time: 0.031663 ms
Layer: TransformerLayer, Backward time: 0.034854 ms
Layer: TransformerLayer, Backward time: 0.031826 ms
Layer: TransformerLayer, Backward time: 0.032242 ms
Layer: TransformerLayer, Backward time: 0.031742 ms
Layer: TransformerLayer, Backward time: 0.030992 ms
Layer: TransformerLayer, Backward time: 0.030760 ms
Layer: TransformerLayer, Backward time: 0.030848 ms
Layer: TransformerLayer, Backward time: 0.031206 ms
Layer: TransformerLayer, Backward time: 0.031822 ms
Layer: TransformerLayer, Backward time: 0.031915 ms
Layer: TransformerLayer, Backward time: 0.031519 ms
Layer: TransformerLayer, Backward time: 0.031616 ms
Layer: TransformerLayer, Backward time: 0.030415 ms
Layer: TransformerLayer, Backward time: 0.031113 ms
 [2024-12-27 13:27:36] iteration        7/      10 | consumed samples:           28 | elapsed time per iteration (ms): 765.5 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.000817E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 732.66
     rank  1: 732.61
     rank  2: 731.59
     rank  3: 730.43
  forward-compute:
     rank  0: 166.63
     rank  1: 150.19
     rank  2: 42.07
     rank  3: 265.15
  backward-compute:
     rank  0: 189.68
     rank  1: 152.90
     rank  2: 47.80
     rank  3: 276.07
  pure-backward-compute:
     rank  0: 189.33
     rank  1: 152.62
     rank  2: 47.40
     rank  3: 275.29
  batch-generator:
     rank  0: 14.39
     rank  1: 13.08
     rank  2: 14.87
     rank  3: 17.13
  forward-recv:
     rank  1: 49.43
     rank  2: 107.56
     rank  3: 88.50
  forward-send:
     rank  0: 1.25
     rank  1: 0.93
     rank  2: 0.89
  backward-recv:
     rank  0: 256.96
     rank  1: 200.25
     rank  2: 123.45
  backward-send:
     rank  1: 1.23
     rank  2: 0.98
     rank  3: 0.98
  forward-send-backward-recv:
     rank  0: 110.68
     rank  1: 128.47
     rank  2: 324.09
  backward-send-forward-recv:
     rank  1: 0.73
     rank  2: 1.50
     rank  3: 4.81
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.08
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.33
     rank  1: 1.28
     rank  2: 0.82
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.11
     rank  1: 0.13
     rank  2: 0.03
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.38
     rank  1: 24.15
     rank  2: 11.83
     rank  3: 0.04
  optimizer:
     rank  0: 24.98
     rank  1: 24.73
     rank  2: 12.45
     rank  3: 0.64
Layer: TransformerLayer, Forward time: 3.394007 ms
Layer: TransformerLayer, Forward time: 2.958785 ms
Layer: TransformerLayer, Forward time: 4.108703 ms
Layer: TransformerLayer, Forward time: 2.944582 ms
Layer: TransformerLayer, Forward time: 2.954595 ms
Layer: TransformerLayer, Forward time: 2.602888 ms
Layer: TransformerLayer, Forward time: 2.872724 ms
Layer: TransformerLayer, Forward time: 2.802917 ms
Layer: TransformerLayer, Forward time: 2.686122 ms
Layer: TransformerLayer, Forward time: 2.565859 ms
Layer: TransformerLayer, Forward time: 2.697460 ms
Layer: TransformerLayer, Forward time: 3.032821 ms
Layer: TransformerLayer, Forward time: 3.087002 ms
Layer: TransformerLayer, Forward time: 2.972774 ms
Layer: TransformerLayer, Forward time: 2.966678 ms
Layer: TransformerLayer, Forward time: 2.940224 ms
Layer: TransformerLayer, Forward time: 2.918594 ms
Layer: TransformerLayer, Forward time: 2.908464 ms
Layer: TransformerLayer, Forward time: 2.879469 ms
Layer: TransformerLayer, Forward time: 2.864233 ms
Layer: TransformerLayer, Forward time: 2.937485 ms
Layer: TransformerLayer, Forward time: 2.927002 ms
Layer: TransformerLayer, Forward time: 3.072450 ms
Layer: TransformerLayer, Forward time: 2.950711 ms
Layer: TransformerLayer, Forward time: 3.181205 ms
Layer: TransformerLayer, Forward time: 3.158528 ms
Layer: TransformerLayer, Forward time: 3.127889 ms
Layer: TransformerLayer, Forward time: 3.103332 ms
Layer: TransformerLayer, Forward time: 3.065595 ms
Layer: TransformerLayer, Forward time: 3.004283 ms
Layer: TransformerLayer, Forward time: 2.991929 ms
Layer: TransformerLayer, Forward time: 3.044066 ms
Layer: TransformerLayer, Forward time: 3.068497 ms
Layer: TransformerLayer, Forward time: 3.052323 ms
Layer: TransformerLayer, Forward time: 3.162328 ms
Layer: TransformerLayer, Forward time: 2.945169 ms
Layer: TransformerLayer, Forward time: 3.205113 ms
Layer: TransformerLayer, Forward time: 3.041300 ms
Layer: TransformerLayer, Forward time: 3.035389 ms
Layer: TransformerLayer, Forward time: 3.021792 ms
Layer: TransformerLayer, Forward time: 3.030628 ms
Layer: TransformerLayer, Forward time: 3.007532 ms
Layer: TransformerLayer, Forward time: 3.010346 ms
Layer: TransformerLayer, Forward time: 3.005969 ms
Layer: TransformerLayer, Forward time: 3.009950 ms
Layer: TransformerLayer, Forward time: 2.960412 ms
Layer: TransformerLayer, Forward time: 3.132992 ms
Layer: TransformerLayer, Forward time: 3.036718 ms
Layer: TransformerLayer, Backward time: 0.080360 ms
Layer: TransformerLayer, Backward time: 0.057692 ms
Layer: TransformerLayer, Backward time: 0.055669 ms
Layer: TransformerLayer, Backward time: 0.053916 ms
Layer: TransformerLayer, Backward time: 0.044703 ms
Layer: TransformerLayer, Backward time: 0.036174 ms
Layer: TransformerLayer, Backward time: 0.035044 ms
Layer: TransformerLayer, Backward time: 0.038825 ms
Layer: TransformerLayer, Backward time: 0.038476 ms
Layer: TransformerLayer, Backward time: 0.039631 ms
Layer: TransformerLayer, Backward time: 0.038676 ms
Layer: TransformerLayer, Backward time: 0.037112 ms
Layer: TransformerLayer, Backward time: 0.120552 ms
Layer: TransformerLayer, Backward time: 0.069019 ms
Layer: TransformerLayer, Backward time: 0.062960 ms
Layer: TransformerLayer, Backward time: 0.072700 ms
Layer: TransformerLayer, Backward time: 0.132123 ms
Layer: TransformerLayer, Backward time: 0.065165 ms
Layer: TransformerLayer, Backward time: 0.079634 ms
Layer: TransformerLayer, Backward time: 0.052155 ms
Layer: TransformerLayer, Backward time: 0.047816 ms
Layer: TransformerLayer, Backward time: 0.046676 ms
Layer: TransformerLayer, Backward time: 0.060118 ms
Layer: TransformerLayer, Backward time: 0.037522 ms
Layer: TransformerLayer, Backward time: 0.037221 ms
Layer: TransformerLayer, Backward time: 0.044195 ms
Layer: TransformerLayer, Backward time: 0.045825 ms
Layer: TransformerLayer, Backward time: 0.039828 ms
Layer: TransformerLayer, Backward time: 0.040284 ms
Layer: TransformerLayer, Backward time: 0.028104 ms
Layer: TransformerLayer, Backward time: 0.024217 ms
Layer: TransformerLayer, Backward time: 0.025641 ms
Layer: TransformerLayer, Backward time: 0.029749 ms
Layer: TransformerLayer, Backward time: 0.024651 ms
Layer: TransformerLayer, Backward time: 0.023601 ms
Layer: TransformerLayer, Backward time: 0.024562 ms
Layer: TransformerLayer, Backward time: 0.033744 ms
Layer: TransformerLayer, Backward time: 0.054665 ms
Layer: TransformerLayer, Backward time: 0.079934 ms
Layer: TransformerLayer, Backward time: 0.050884 ms
Layer: TransformerLayer, Backward time: 0.024289 ms
Layer: TransformerLayer, Backward time: 0.032531 ms
Layer: TransformerLayer, Backward time: 0.023429 ms
Layer: TransformerLayer, Backward time: 0.023329 ms
Layer: TransformerLayer, Backward time: 0.026105 ms
Layer: TransformerLayer, Backward time: 0.025994 ms
Layer: TransformerLayer, Backward time: 0.028244 ms
Layer: TransformerLayer, Backward time: 0.023821 ms
 [2024-12-27 13:27:37] iteration        8/      10 | consumed samples:           32 | elapsed time per iteration (ms): 779.3 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 6.722784E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 744.29
     rank  1: 743.77
     rank  2: 742.76
     rank  3: 741.51
  forward-compute:
     rank  0: 169.92
     rank  1: 148.15
     rank  2: 49.72
     rank  3: 269.89
  backward-compute:
     rank  0: 191.32
     rank  1: 154.24
     rank  2: 48.12
     rank  3: 275.29
  pure-backward-compute:
     rank  0: 190.94
     rank  1: 153.87
     rank  2: 47.58
     rank  3: 274.53
  batch-generator:
     rank  0: 14.76
     rank  1: 13.05
     rank  2: 21.99
     rank  3: 19.41
  forward-recv:
     rank  1: 52.93
     rank  2: 111.52
     rank  3: 91.06
  forward-send:
     rank  0: 1.22
     rank  1: 0.97
     rank  2: 0.93
  backward-recv:
     rank  0: 264.61
     rank  1: 199.19
     rank  2: 126.68
  backward-send:
     rank  1: 1.43
     rank  2: 1.09
     rank  3: 1.04
  forward-send-backward-recv:
     rank  0: 108.11
     rank  1: 134.39
     rank  2: 315.19
  backward-send-forward-recv:
     rank  1: 0.71
     rank  2: 1.63
     rank  3: 5.06
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.02
     rank  2: 0.08
     rank  3: 0.07
  all-grads-sync:
     rank  0: 1.66
     rank  1: 1.32
     rank  2: 0.92
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.15
     rank  2: 0.03
     rank  3: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.73
     rank  1: 24.19
     rank  2: 11.83
     rank  3: 0.08
  optimizer:
     rank  0: 25.66
     rank  1: 25.08
     rank  2: 13.21
     rank  3: 0.97
Layer: TransformerLayer, Forward time: 3.793801 ms
Layer: TransformerLayer, Forward time: 3.314251 ms
Layer: TransformerLayer, Forward time: 3.170331 ms
Layer: TransformerLayer, Forward time: 3.176477 ms
Layer: TransformerLayer, Forward time: 3.151672 ms
Layer: TransformerLayer, Forward time: 3.224741 ms
Layer: TransformerLayer, Forward time: 3.301992 ms
Layer: TransformerLayer, Forward time: 3.221985 ms
Layer: TransformerLayer, Forward time: 3.011717 ms
Layer: TransformerLayer, Forward time: 3.079259 ms
Layer: TransformerLayer, Forward time: 3.197694 ms
Layer: TransformerLayer, Forward time: 3.294602 ms
Layer: TransformerLayer, Forward time: 3.531903 ms
Layer: TransformerLayer, Forward time: 3.427119 ms
Layer: TransformerLayer, Forward time: 3.613650 ms
Layer: TransformerLayer, Forward time: 3.463664 ms
Layer: TransformerLayer, Forward time: 3.418699 ms
Layer: TransformerLayer, Forward time: 3.431692 ms
Layer: TransformerLayer, Forward time: 3.225870 ms
Layer: TransformerLayer, Forward time: 2.922148 ms
Layer: TransformerLayer, Forward time: 2.911052 ms
Layer: TransformerLayer, Forward time: 2.899551 ms
Layer: TransformerLayer, Forward time: 3.181398 ms
Layer: TransformerLayer, Forward time: 2.922998 ms
Layer: TransformerLayer, Forward time: 6.576603 ms
Layer: TransformerLayer, Forward time: 3.606664 ms
Layer: TransformerLayer, Forward time: 3.008902 ms
Layer: TransformerLayer, Forward time: 3.185170 ms
Layer: TransformerLayer, Forward time: 2.953982 ms
Layer: TransformerLayer, Forward time: 2.985735 ms
Layer: TransformerLayer, Forward time: 2.983293 ms
Layer: TransformerLayer, Forward time: 2.948462 ms
Layer: TransformerLayer, Forward time: 2.966214 ms
Layer: TransformerLayer, Forward time: 2.982012 ms
Layer: TransformerLayer, Forward time: 3.313398 ms
Layer: TransformerLayer, Forward time: 2.917897 ms
Layer: TransformerLayer, Forward time: 3.274397 ms
Layer: TransformerLayer, Forward time: 3.083200 ms
Layer: TransformerLayer, Forward time: 3.041967 ms
Layer: TransformerLayer, Forward time: 3.039714 ms
Layer: TransformerLayer, Forward time: 3.040827 ms
Layer: TransformerLayer, Forward time: 3.206627 ms
Layer: TransformerLayer, Forward time: 3.019424 ms
Layer: TransformerLayer, Forward time: 2.969513 ms
Layer: TransformerLayer, Forward time: 2.963647 ms
Layer: TransformerLayer, Forward time: 2.959522 ms
Layer: TransformerLayer, Forward time: 3.158196 ms
Layer: TransformerLayer, Forward time: 2.973821 ms
Layer: TransformerLayer, Backward time: 0.038466 ms
Layer: TransformerLayer, Backward time: 0.048689 ms
Layer: TransformerLayer, Backward time: 0.044829 ms
Layer: TransformerLayer, Backward time: 0.042614 ms
Layer: TransformerLayer, Backward time: 0.045331 ms
Layer: TransformerLayer, Backward time: 0.032856 ms
Layer: TransformerLayer, Backward time: 0.032973 ms
Layer: TransformerLayer, Backward time: 0.029367 ms
Layer: TransformerLayer, Backward time: 0.032780 ms
Layer: TransformerLayer, Backward time: 0.031301 ms
Layer: TransformerLayer, Backward time: 0.029806 ms
Layer: TransformerLayer, Backward time: 0.029751 ms
Layer: TransformerLayer, Backward time: 0.049901 ms
Layer: TransformerLayer, Backward time: 0.044396 ms
Layer: TransformerLayer, Backward time: 0.043099 ms
Layer: TransformerLayer, Backward time: 0.042486 ms
Layer: TransformerLayer, Backward time: 0.035473 ms
Layer: TransformerLayer, Backward time: 0.032612 ms
Layer: TransformerLayer, Backward time: 0.049986 ms
Layer: TransformerLayer, Backward time: 0.047315 ms
Layer: TransformerLayer, Backward time: 0.038971 ms
Layer: TransformerLayer, Backward time: 0.037989 ms
Layer: TransformerLayer, Backward time: 0.036968 ms
Layer: TransformerLayer, Backward time: 0.071615 ms
Layer: TransformerLayer, Backward time: 0.044864 ms
Layer: TransformerLayer, Backward time: 0.076610 ms
Layer: TransformerLayer, Backward time: 0.077066 ms
Layer: TransformerLayer, Backward time: 0.039063 ms
Layer: TransformerLayer, Backward time: 0.029016 ms
Layer: TransformerLayer, Backward time: 0.024793 ms
Layer: TransformerLayer, Backward time: 0.024911 ms
Layer: TransformerLayer, Backward time: 0.027366 ms
Layer: TransformerLayer, Backward time: 0.025026 ms
Layer: TransformerLayer, Backward time: 0.026628 ms
Layer: TransformerLayer, Backward time: 0.028997 ms
Layer: TransformerLayer, Backward time: 0.025192 ms
Layer: TransformerLayer, Backward time: 0.123407 ms
Layer: TransformerLayer, Backward time: 0.057032 ms
Layer: TransformerLayer, Backward time: 0.055316 ms
Layer: TransformerLayer, Backward time: 0.060087 ms
Layer: TransformerLayer, Backward time: 0.025586 ms
Layer: TransformerLayer, Backward time: 0.026703 ms
Layer: TransformerLayer, Backward time: 0.026030 ms
Layer: TransformerLayer, Backward time: 0.025328 ms
Layer: TransformerLayer, Backward time: 0.026656 ms
Layer: TransformerLayer, Backward time: 0.023193 ms
Layer: TransformerLayer, Backward time: 0.025153 ms
Layer: TransformerLayer, Backward time: 0.024071 ms
 [2024-12-27 13:27:37] iteration        9/      10 | consumed samples:           36 | elapsed time per iteration (ms): 784.1 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 2.981273E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 747.79
     rank  1: 747.43
     rank  2: 746.34
     rank  3: 745.18
  forward-compute:
     rank  0: 182.72
     rank  1: 155.59
     rank  2: 44.67
     rank  3: 265.54
  backward-compute:
     rank  0: 189.08
     rank  1: 155.70
     rank  2: 48.52
     rank  3: 275.15
  pure-backward-compute:
     rank  0: 188.67
     rank  1: 155.21
     rank  2: 48.06
     rank  3: 274.45
  batch-generator:
     rank  0: 15.69
     rank  1: 15.45
     rank  2: 17.25
     rank  3: 15.71
  forward-recv:
     rank  1: 64.51
     rank  2: 120.08
     rank  3: 97.07
  forward-send:
     rank  0: 1.33
     rank  1: 0.94
     rank  2: 0.91
  backward-recv:
     rank  0: 267.01
     rank  1: 192.80
     rank  2: 127.85
  backward-send:
     rank  1: 1.76
     rank  2: 1.07
     rank  3: 1.00
  forward-send-backward-recv:
     rank  0: 98.62
     rank  1: 120.02
     rank  2: 311.97
  backward-send-forward-recv:
     rank  1: 1.08
     rank  2: 1.63
     rank  3: 4.77
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.08
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.63
     rank  1: 1.39
     rank  2: 0.86
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.02
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.04
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.03
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.92
     rank  1: 24.19
     rank  2: 11.81
     rank  3: 0.03
  optimizer:
     rank  0: 26.09
     rank  1: 25.38
     rank  2: 13.00
     rank  3: 1.21
Layer: TransformerLayer, Forward time: 3.075049 ms
Layer: TransformerLayer, Forward time: 2.825731 ms
Layer: TransformerLayer, Forward time: 2.907924 ms
Layer: TransformerLayer, Forward time: 2.770986 ms
Layer: TransformerLayer, Forward time: 2.734097 ms
Layer: TransformerLayer, Forward time: 2.726400 ms
Layer: TransformerLayer, Forward time: 2.714047 ms
Layer: TransformerLayer, Forward time: 2.698483 ms
Layer: TransformerLayer, Forward time: 2.703016 ms
Layer: TransformerLayer, Forward time: 2.735541 ms
Layer: TransformerLayer, Forward time: 2.730329 ms
Layer: TransformerLayer, Forward time: 2.889900 ms
Layer: TransformerLayer, Forward time: 3.152721 ms
Layer: TransformerLayer, Forward time: 3.107505 ms
Layer: TransformerLayer, Forward time: 3.087065 ms
Layer: TransformerLayer, Forward time: 3.028689 ms
Layer: TransformerLayer, Forward time: 3.046150 ms
Layer: TransformerLayer, Forward time: 3.030434 ms
Layer: TransformerLayer, Forward time: 3.026725 ms
Layer: TransformerLayer, Forward time: 3.067901 ms
Layer: TransformerLayer, Forward time: 3.078008 ms
Layer: TransformerLayer, Forward time: 3.050587 ms
Layer: TransformerLayer, Forward time: 3.255770 ms
Layer: TransformerLayer, Forward time: 3.057647 ms
Layer: TransformerLayer, Forward time: 3.388230 ms
Layer: TransformerLayer, Forward time: 3.190661 ms
Layer: TransformerLayer, Forward time: 3.222153 ms
Layer: TransformerLayer, Forward time: 3.237349 ms
Layer: TransformerLayer, Forward time: 3.219509 ms
Layer: TransformerLayer, Forward time: 3.240917 ms
Layer: TransformerLayer, Forward time: 3.146838 ms
Layer: TransformerLayer, Forward time: 3.245794 ms
Layer: TransformerLayer, Forward time: 3.124334 ms
Layer: TransformerLayer, Forward time: 3.151525 ms
Layer: TransformerLayer, Forward time: 3.401436 ms
Layer: TransformerLayer, Forward time: 3.193127 ms
Layer: TransformerLayer, Forward time: 3.450882 ms
Layer: TransformerLayer, Forward time: 3.315045 ms
Layer: TransformerLayer, Forward time: 3.242966 ms
Layer: TransformerLayer, Forward time: 3.244265 ms
Layer: TransformerLayer, Forward time: 3.205860 ms
Layer: TransformerLayer, Forward time: 3.179860 ms
Layer: TransformerLayer, Forward time: 3.136338 ms
Layer: TransformerLayer, Forward time: 3.109256 ms
Layer: TransformerLayer, Forward time: 3.213095 ms
Layer: TransformerLayer, Forward time: 3.182847 ms
Layer: TransformerLayer, Forward time: 3.292223 ms
Layer: TransformerLayer, Forward time: 5.676271 ms
Layer: TransformerLayer, Backward time: 0.045275 ms
Layer: TransformerLayer, Backward time: 0.049658 ms
Layer: TransformerLayer, Backward time: 0.048715 ms
Layer: TransformerLayer, Backward time: 0.058253 ms
Layer: TransformerLayer, Backward time: 0.044255 ms
Layer: TransformerLayer, Backward time: 0.046540 ms
Layer: TransformerLayer, Backward time: 0.029945 ms
Layer: TransformerLayer, Backward time: 0.031234 ms
Layer: TransformerLayer, Backward time: 0.030250 ms
Layer: TransformerLayer, Backward time: 0.029560 ms
Layer: TransformerLayer, Backward time: 0.041147 ms
Layer: TransformerLayer, Backward time: 0.032415 ms
Layer: TransformerLayer, Backward time: 0.037459 ms
Layer: TransformerLayer, Backward time: 0.039713 ms
Layer: TransformerLayer, Backward time: 0.039441 ms
Layer: TransformerLayer, Backward time: 0.039724 ms
Layer: TransformerLayer, Backward time: 0.037180 ms
Layer: TransformerLayer, Backward time: 0.027690 ms
Layer: TransformerLayer, Backward time: 0.030163 ms
Layer: TransformerLayer, Backward time: 0.027370 ms
Layer: TransformerLayer, Backward time: 0.029748 ms
Layer: TransformerLayer, Backward time: 0.030682 ms
Layer: TransformerLayer, Backward time: 0.027058 ms
Layer: TransformerLayer, Backward time: 0.026406 ms
Layer: TransformerLayer, Backward time: 0.038261 ms
Layer: TransformerLayer, Backward time: 0.025945 ms
Layer: TransformerLayer, Backward time: 0.048077 ms
Layer: TransformerLayer, Backward time: 0.047415 ms
Layer: TransformerLayer, Backward time: 0.048352 ms
Layer: TransformerLayer, Backward time: 0.031350 ms
Layer: TransformerLayer, Backward time: 0.026631 ms
Layer: TransformerLayer, Backward time: 0.023628 ms
Layer: TransformerLayer, Backward time: 0.024041 ms
Layer: TransformerLayer, Backward time: 0.024232 ms
Layer: TransformerLayer, Backward time: 0.023651 ms
Layer: TransformerLayer, Backward time: 0.026257 ms
Layer: TransformerLayer, Backward time: 0.038429 ms
Layer: TransformerLayer, Backward time: 0.027760 ms
Layer: TransformerLayer, Backward time: 0.051454 ms
Layer: TransformerLayer, Backward time: 0.050639 ms
Layer: TransformerLayer, Backward time: 0.050863 ms
Layer: TransformerLayer, Backward time: 0.025351 ms
Layer: TransformerLayer, Backward time: 0.026259 ms
Layer: TransformerLayer, Backward time: 0.026427 ms
Layer: TransformerLayer, Backward time: 0.029652 ms
Layer: TransformerLayer, Backward time: 0.023063 ms
Layer: TransformerLayer, Backward time: 0.023803 ms
Layer: TransformerLayer, Backward time: 0.025239 ms
 [2024-12-27 13:27:38] iteration       10/      10 | consumed samples:           40 | elapsed time per iteration (ms): 779.3 | learning rate: 1.000000E-05 | global batch size:     4 | lm loss: 2.369648E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 742.50
     rank  1: 743.08
     rank  2: 741.59
     rank  3: 740.17
  forward-compute:
     rank  0: 177.12
     rank  1: 156.05
     rank  2: 44.33
     rank  3: 264.17
  backward-compute:
     rank  0: 170.28
     rank  1: 165.21
     rank  2: 48.75
     rank  3: 277.18
  pure-backward-compute:
     rank  0: 169.96
     rank  1: 164.74
     rank  2: 48.29
     rank  3: 276.42
  batch-generator:
     rank  0: 14.50
     rank  1: 16.12
     rank  2: 17.19
     rank  3: 15.40
  forward-recv:
     rank  1: 54.47
     rank  2: 111.58
     rank  3: 89.19
  forward-send:
     rank  0: 1.28
     rank  1: 0.90
     rank  2: 0.90
  backward-recv:
     rank  0: 283.76
     rank  1: 194.93
     rank  2: 127.98
  backward-send:
     rank  1: 1.71
     rank  2: 1.05
     rank  3: 0.97
  forward-send-backward-recv:
     rank  0: 101.56
     rank  1: 117.07
     rank  2: 313.60
  backward-send-forward-recv:
     rank  1: 1.02
     rank  2: 1.66
     rank  3: 4.79
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.06
     rank  2: 0.03
     rank  3: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.06
     rank  2: 0.08
     rank  3: 0.06
  all-grads-sync:
     rank  0: 1.38
     rank  1: 1.82
     rank  2: 0.86
     rank  3: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.22
     rank  2: 0.03
     rank  3: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
  optimizer-inner-step:
     rank  0: 24.39
     rank  1: 24.44
     rank  2: 11.84
     rank  3: 0.04
  optimizer:
     rank  0: 25.52
     rank  1: 25.55
     rank  2: 12.91
     rank  3: 1.16
[after training is done] datetime: 2024-12-27 13:27:38 
