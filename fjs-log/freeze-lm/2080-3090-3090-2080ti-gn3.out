examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
3
[2024-12-25 14:43:57,350] torch.distributed.run: [WARNING] 
[2024-12-25 14:43:57,350] torch.distributed.run: [WARNING] *****************************************
[2024-12-25 14:43:57,350] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-25 14:43:57,350] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:4------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:4------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:4------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:4------
------num_pipeline_model_parallel_groups:4------
---Rank 12---Tensor Parallel Group GPUs: [0]
---Rank 12---Pipeline Parallel Group GPUs: [3, 3, 3, 3]
---Rank 14---Tensor Parallel Group GPUs: [0]
---Rank 14---Pipeline Parallel Group GPUs: [3, 3, 3, 3]
---Rank 13---Tensor Parallel Group GPUs: [0]
---Rank 13---Pipeline Parallel Group GPUs: [3, 3, 3, 3]
[rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 15---Tensor Parallel Group GPUs: [0]
---Rank 15---Pipeline Parallel Group GPUs: [3, 3, 3, 3]
[rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 352333824
rank=2, worker=0: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<619>, pretrain-37.tar[2200, 2300), pretrain-37.tar[2300, 2400), pretrain-37.tar[2400, 2500)] sum(count)=62500
rank=2, worker=1: shard_range=[pretrain-37.tar[2500, 2600), pretrain-37.tar[2600, 2700), pretrain-37.tar[2700, 2800), ...<619>, pretrain-42.tar[4700, 4800), pretrain-42.tar[4800, 4900), pretrain-42.tar[4900, 5000)] sum(count)=62500
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<619>, pretrain-14.tar[2200, 2300), pretrain-14.tar[2300, 2400), pretrain-14.tar[2400, 2500)] sum(count)=62500
rank=0, worker=1: shard_range=[pretrain-14.tar[2500, 2600), pretrain-14.tar[2600, 2700), pretrain-14.tar[2700, 2800), ...<619>, pretrain-2.tar[4700, 4800), pretrain-2.tar[4800, 4900), pretrain-2.tar[4900, 5000)] sum(count)=62500
rank=1, worker=0: shard_range=[pretrain-2.tar[5000, 5100), pretrain-2.tar[5100, 5200), pretrain-2.tar[5200, 5300), ...<619>, pretrain-25.tar[7200, 7300), pretrain-25.tar[7300, 7400), pretrain-25.tar[7400, 7500)] sum(count)=62500
rank=1, worker=1: shard_range=[pretrain-25.tar[7500, 7600), pretrain-25.tar[7600, 7700), pretrain-25.tar[7700, 7800), ...<619>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=62500
rank=3, worker=0: shard_range=[pretrain-42.tar[5000, 5100), pretrain-42.tar[5100, 5200), pretrain-42.tar[5200, 5300), ...<619>, pretrain-48.tar[7200, 7300), pretrain-48.tar[7300, 7400), pretrain-48.tar[7400, 7500)] sum(count)=62500
rank=3, worker=1: shard_range=[pretrain-48.tar[7500, 7600), pretrain-48.tar[7600, 7700), pretrain-48.tar[7700, 7800), ...<619>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=62500
rank=2, worker=0: shard_range=[pretrain-7.tar[936, 8202)] sum(count)=7266
rank=2, worker=1: shard_range=[pretrain-7.tar[8202, 10000), pretrain-8.tar[0, 5468)] sum(count)=7266
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 7266)] sum(count)=7266
rank=0, worker=1: shard_range=[pretrain-54.tar[7266, 10000), pretrain-55.tar[0, 4532)] sum(count)=7266
rank=3, worker=0: shard_range=[pretrain-8.tar[5468, 10000), pretrain-9.tar[0, 2734)] sum(count)=7266
rank=3, worker=1: shard_range=[pretrain-9.tar[2734, 10000)] sum(count)=7266
rank=1, worker=0: shard_range=[pretrain-55.tar[4532, 8128), pretrain-6.tar[0, 3670)] sum(count)=7266
rank=1, worker=1: shard_range=[pretrain-6.tar[3670, 10000), pretrain-7.tar[0, 936)] sum(count)=7266
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 68.95
     rank  1: 66.76
     rank  2: 70.07
     rank  3: 65.67
     rank  4: 112.98
     rank  5: 97.43
     rank  6: 114.19
     rank  7: 108.93
     rank  8: 40.11
     rank  9: 31.23
     rank 10: 44.98
     rank 11: 39.79
     rank 12: 45.45
     rank 13: 49.25
     rank 14: 43.49
     rank 15: 45.15
  train/valid/test-data-iterators-setup:
     rank  0: 1062.26
     rank  1: 1067.61
     rank  2: 1489.63
     rank  3: 1084.64
     rank  4: 1091.30
     rank  5: 1091.36
     rank  6: 1129.34
     rank  7: 1129.27
     rank  8: 1129.38
     rank  9: 1129.05
     rank 10: 1131.92
     rank 11: 1131.98
     rank 12: 1285.57
     rank 13: 1489.91
     rank 14: 1137.11
     rank 15: 1489.68
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1499, in forward_backward_pipelining_without_interleaving
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1499, in forward_backward_pipelining_without_interleaving
    input_tensor = recv_forward(recv_tensor_shapes, config)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1280, in recv_forward
    input_tensor = recv_forward(recv_tensor_shapes, config)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1280, in recv_forward
    input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 421, in recv_forward
    input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 421, in recv_forward
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1499, in forward_backward_pipelining_without_interleaving
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    input_tensor = recv_forward(recv_tensor_shapes, config)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1280, in recv_forward
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 421, in recv_forward
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1499, in forward_backward_pipelining_without_interleaving
    input_tensor = recv_forward(recv_tensor_shapes, config)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1280, in recv_forward
    input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 421, in recv_forward
    input_tensor, _, _ = _communicate(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 370, in _communicate
            input_tensor, _, _ = _communicate(input_tensor, _, _ = _communicate(input_tensor, _, _ = _communicate(


  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 370, in _communicate
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 370, in _communicate
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 370, in _communicate
    p2p_func(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 153, in _batched_p2p_ops
    p2p_func(    
    p2p_func(reqs = torch.distributed.batch_isend_irecv(ops)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 153, in _batched_p2p_ops

  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 153, in _batched_p2p_ops
    p2p_func(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1865, in batch_isend_irecv
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/p2p_communication.py", line 153, in _batched_p2p_ops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1865, in batch_isend_irecv
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1865, in batch_isend_irecv
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1865, in batch_isend_irecv
    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1631, in irecv
    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1631, in irecv
    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1631, in irecv
    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1631, in irecv
    return pg.recv([tensor], group_src_rank, tag)
torch.distributed.DistBackendError: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f2e3883bd87 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5894fde (0x7f2e74534fde in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f2e7452f7f0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f2e7452fb32 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f2e74530961 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f2e744e5dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f2e744e5dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f2e744e5dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f2e744e5dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f2e744e5dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f2e399b74e9 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f2e399be4db in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::recv(std::vector<at::Tensor, std::allocator<at::Tensor> >&, int, int) + 0x550 (0x7f2e399e13e0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #13: <unknown function> + 0x5838289 (0x7f2e744d8289 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x5843180 (0x7f2e744e3180 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x5843215 (0x7f2e744e3215 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x4e8937c (0x7f2e73b2937c in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x1a08a38 (0x7f2e706a8a38 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x58498d4 (0x7f2e744e98d4 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x584eb85 (0x7f2e744eeb85 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0xc99c3e (0x7f2e86d99c3e in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x413f64 (0x7f2e86513f64 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #22: <unknown function> + 0x1457e6 (0x55cd2bfa37e6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #23: _PyObject_MakeTpCall + 0x26b (0x55cd2bf9c9db in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #24: <unknown function> + 0x151ec6 (0x55cd2bfafec6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #25: _PyEval_EvalFrameDefault + 0x4c1a (0x55cd2bf97f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #26: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #27: _PyEval_EvalFrameDefault + 0x4c1a (0x55cd2bf97f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #28: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #29: _PyEval_EvalFrameDefault + 0x4c1a (0x55cd2bf97f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #30: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #31: _PyEval_EvalFrameDefault + 0x13cc (0x55cd2bf946dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #32: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #33: _PyEval_EvalFrameDefault + 0x13cc (0x55cd2bf946dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #34: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #35: _PyEval_EvalFrameDefault + 0x4c1a (0x55cd2bf97f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #36: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #37: _PyEval_EvalFrameDefault + 0x320 (0x55cd2bf93630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #38: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #39: _PyEval_EvalFrameDefault + 0x13cc (0x55cd2bf946dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #40: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #41: _PyEval_EvalFrameDefault + 0x320 (0x55cd2bf93630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #42: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #43: _PyEval_EvalFrameDefault + 0x320 (0x55cd2bf93630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #44: _PyFunction_Vectorcall + 0x6c (0x55cd2bfa3c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #45: _PyEval_EvalFrameDefault + 0x13cc (0x55cd2bf946dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #46: <unknown function> + 0x1da820 (0x55cd2c038820 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #47: PyEval_EvalCode + 0x87 (0x55cd2c038767 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #48: <unknown function> + 0x20ac2a (0x55cd2c068c2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #49: <unknown function> + 0x206023 (0x55cd2c064023 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #50: <unknown function> + 0x9a558 (0x55cd2bef8558 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #51: _PyRun_SimpleFileObject + 0x1ae (0x55cd2c05e50e in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #52: _PyRun_AnyFileObject + 0x44 (0x55cd2c05e0a4 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #53: Py_RunMain + 0x38b (0x55cd2c05b29b in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #54: Py_BytesMain + 0x37 (0x55cd2c02c257 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #55: __libc_start_main + 0xe7 (0x7f2e883fdb97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: <unknown function> + 0x1ce151 (0x55cd2c02c151 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
. This may indicate a possible application crash on rank 0 or a network set up issue.    
return pg.recv([tensor], group_src_rank, tag)
torch.distributed.DistBackendError: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7ff2144a2d87 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5894fde (0x7ff25019bfde in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7ff2501967f0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7ff250196b32 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7ff250197961 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff25014cdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff25014cdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff25014cdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff25014cdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff25014cdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7ff21561e4e9 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7ff2156254db in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::recv(std::vector<at::Tensor, std::allocator<at::Tensor> >&, int, int) + 0x550 (0x7ff2156483e0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #13: <unknown function> + 0x5838289 (0x7ff25013f289 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x5843180 (0x7ff25014a180 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x5843215 (0x7ff25014a215 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x4e8937c (0x7ff24f79037c in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x1a08a38 (0x7ff24c30fa38 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x58498d4 (0x7ff2501508d4 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x584eb85 (0x7ff250155b85 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0xc99c3e (0x7ff262a00c3e in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x413f64 (0x7ff26217af64 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #22: <unknown function> + 0x1457e6 (0x55ff44beb7e6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #23: _PyObject_MakeTpCall + 0x26b (0x55ff44be49db in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #24: <unknown function> + 0x151ec6 (0x55ff44bf7ec6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #25: _PyEval_EvalFrameDefault + 0x4c1a (0x55ff44bdff2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #26: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #27: _PyEval_EvalFrameDefault + 0x4c1a (0x55ff44bdff2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #28: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #29: _PyEval_EvalFrameDefault + 0x4c1a (0x55ff44bdff2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #30: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #31: _PyEval_EvalFrameDefault + 0x13cc (0x55ff44bdc6dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #32: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #33: _PyEval_EvalFrameDefault + 0x13cc (0x55ff44bdc6dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #34: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #35: _PyEval_EvalFrameDefault + 0x4c1a (0x55ff44bdff2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #36: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #37: _PyEval_EvalFrameDefault + 0x320 (0x55ff44bdb630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #38: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #39: _PyEval_EvalFrameDefault + 0x13cc (0x55ff44bdc6dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #40: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #41: _PyEval_EvalFrameDefault + 0x320 (0x55ff44bdb630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #42: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #43: _PyEval_EvalFrameDefault + 0x320 (0x55ff44bdb630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #44: _PyFunction_Vectorcall + 0x6c (0x55ff44bebc6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #45: _PyEval_EvalFrameDefault + 0x13cc (0x55ff44bdc6dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #46: <unknown function> + 0x1da820 (0x55ff44c80820 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #47: PyEval_EvalCode + 0x87 (0x55ff44c80767 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #48: <unknown function> + 0x20ac2a (0x55ff44cb0c2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #49: <unknown function> + 0x206023 (0x55ff44cac023 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #50: <unknown function> + 0x9a558 (0x55ff44b40558 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #51: _PyRun_SimpleFileObject + 0x1ae (0x55ff44ca650e in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #52: _PyRun_AnyFileObject + 0x44 (0x55ff44ca60a4 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #53: Py_RunMain + 0x38b (0x55ff44ca329b in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #54: Py_BytesMain + 0x37 (0x55ff44c74257 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #55: __libc_start_main + 0xe7 (0x7ff264064b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: <unknown function> + 0x1ce151 (0x55ff44c74151 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    return pg.recv([tensor], group_src_rank, tag)
torch.distributed.DistBackendError: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1d8901ed87 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5894fde (0x7f1dc4d17fde in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f1dc4d127f0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f1dc4d12b32 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f1dc4d13961 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f1dc4cc8dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f1dc4cc8dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f1dc4cc8dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f1dc4cc8dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f1dc4cc8dd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f1d8a19a4e9 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f1d8a1a14db in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::recv(std::vector<at::Tensor, std::allocator<at::Tensor> >&, int, int) + 0x550 (0x7f1d8a1c43e0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #13: <unknown function> + 0x5838289 (0x7f1dc4cbb289 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x5843180 (0x7f1dc4cc6180 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x5843215 (0x7f1dc4cc6215 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x4e8937c (0x7f1dc430c37c in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x1a08a38 (0x7f1dc0e8ba38 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x58498d4 (0x7f1dc4ccc8d4 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x584eb85 (0x7f1dc4cd1b85 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0xc99c3e (0x7f1dd757cc3e in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x413f64 (0x7f1dd6cf6f64 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #22: <unknown function> + 0x1457e6 (0x558a12b547e6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #23: _PyObject_MakeTpCall + 0x26b (0x558a12b4d9db in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #24: <unknown function> + 0x151ec6 (0x558a12b60ec6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #25: _PyEval_EvalFrameDefault + 0x4c1a (0x558a12b48f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #26: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #27: _PyEval_EvalFrameDefault + 0x4c1a (0x558a12b48f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #28: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #29: _PyEval_EvalFrameDefault + 0x4c1a (0x558a12b48f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #30: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #31: _PyEval_EvalFrameDefault + 0x13cc (0x558a12b456dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #32: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #33: _PyEval_EvalFrameDefault + 0x13cc (0x558a12b456dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #34: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #35: _PyEval_EvalFrameDefault + 0x4c1a (0x558a12b48f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #36: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #37: _PyEval_EvalFrameDefault + 0x320 (0x558a12b44630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #38: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #39: _PyEval_EvalFrameDefault + 0x13cc (0x558a12b456dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #40: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #41: _PyEval_EvalFrameDefault + 0x320 (0x558a12b44630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #42: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #43: _PyEval_EvalFrameDefault + 0x320 (0x558a12b44630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #44: _PyFunction_Vectorcall + 0x6c (0x558a12b54c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #45: _PyEval_EvalFrameDefault + 0x13cc (0x558a12b456dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #46: <unknown function> + 0x1da820 (0x558a12be9820 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #47: PyEval_EvalCode + 0x87 (0x558a12be9767 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #48: <unknown function> + 0x20ac2a (0x558a12c19c2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #49: <unknown function> + 0x206023 (0x558a12c15023 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #50: <unknown function> + 0x9a558 (0x558a12aa9558 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #51: _PyRun_SimpleFileObject + 0x1ae (0x558a12c0f50e in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #52: _PyRun_AnyFileObject + 0x44 (0x558a12c0f0a4 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #53: Py_RunMain + 0x38b (0x558a12c0c29b in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #54: Py_BytesMain + 0x37 (0x558a12bdd257 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #55: __libc_start_main + 0xe7 (0x7f1dd8be0b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: <unknown function> + 0x1ce151 (0x558a12bdd151 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    return pg.recv([tensor], group_src_rank, tag)
torch.distributed.DistBackendError: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fcb52c35d87 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5894fde (0x7fcb8e92efde in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7fcb8e9297f0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7fcb8e929b32 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7fcb8e92a961 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fcb8e8dfdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fcb8e8dfdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fcb8e8dfdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fcb8e8dfdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fcb8e8dfdd1 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7fcb53db14e9 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7fcb53db84db in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::recv(std::vector<at::Tensor, std::allocator<at::Tensor> >&, int, int) + 0x550 (0x7fcb53ddb3e0 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #13: <unknown function> + 0x5838289 (0x7fcb8e8d2289 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x5843180 (0x7fcb8e8dd180 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x5843215 (0x7fcb8e8dd215 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x4e8937c (0x7fcb8df2337c in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x1a08a38 (0x7fcb8aaa2a38 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x58498d4 (0x7fcb8e8e38d4 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x584eb85 (0x7fcb8e8e8b85 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0xc99c3e (0x7fcba1193c3e in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x413f64 (0x7fcba090df64 in /gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #22: <unknown function> + 0x1457e6 (0x5562505057e6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #23: _PyObject_MakeTpCall + 0x26b (0x5562504fe9db in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #24: <unknown function> + 0x151ec6 (0x556250511ec6 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #25: _PyEval_EvalFrameDefault + 0x4c1a (0x5562504f9f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #26: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #27: _PyEval_EvalFrameDefault + 0x4c1a (0x5562504f9f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #28: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #29: _PyEval_EvalFrameDefault + 0x4c1a (0x5562504f9f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #30: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #31: _PyEval_EvalFrameDefault + 0x13cc (0x5562504f66dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #32: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #33: _PyEval_EvalFrameDefault + 0x13cc (0x5562504f66dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #34: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #35: _PyEval_EvalFrameDefault + 0x4c1a (0x5562504f9f2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #36: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #37: _PyEval_EvalFrameDefault + 0x320 (0x5562504f5630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #38: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #39: _PyEval_EvalFrameDefault + 0x13cc (0x5562504f66dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #40: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #41: _PyEval_EvalFrameDefault + 0x320 (0x5562504f5630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #42: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #43: _PyEval_EvalFrameDefault + 0x320 (0x5562504f5630 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #44: _PyFunction_Vectorcall + 0x6c (0x556250505c6c in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #45: _PyEval_EvalFrameDefault + 0x13cc (0x5562504f66dc in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #46: <unknown function> + 0x1da820 (0x55625059a820 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #47: PyEval_EvalCode + 0x87 (0x55625059a767 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #48: <unknown function> + 0x20ac2a (0x5562505cac2a in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #49: <unknown function> + 0x206023 (0x5562505c6023 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #50: <unknown function> + 0x9a558 (0x55625045a558 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #51: _PyRun_SimpleFileObject + 0x1ae (0x5562505c050e in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #52: _PyRun_AnyFileObject + 0x44 (0x5562505c00a4 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #53: Py_RunMain + 0x38b (0x5562505bd29b in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #54: Py_BytesMain + 0x37 (0x55625058e257 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
frame #55: __libc_start_main + 0xe7 (0x7fcba27f7b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: <unknown function> + 0x1ce151 (0x55625058e151 in /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-12-25 14:45:16,018] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1780) of binary: /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10
Traceback (most recent call last):
  File "/gf3/home/fjs/anaconda3/envs/megatron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
examples/multimodal/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-12-25_14:45:16
  host      : gn3
  rank      : 13 (local_rank: 1)
  exitcode  : 1 (pid: 1781)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-12-25_14:45:16
  host      : gn3
  rank      : 14 (local_rank: 2)
  exitcode  : 1 (pid: 1782)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-12-25_14:45:16
  host      : gn3
  rank      : 15 (local_rank: 3)
  exitcode  : 1 (pid: 1783)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-25_14:45:16
  host      : gn3
  rank      : 12 (local_rank: 0)
  exitcode  : 1 (pid: 1780)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
