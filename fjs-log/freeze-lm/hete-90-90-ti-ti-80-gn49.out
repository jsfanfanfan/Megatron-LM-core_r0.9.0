examples/multimodal/pretrain-freeze-llm-hete-3090first.sh: line 4: activate: No such file or directory
0
[2024-12-31 12:43:42,977] torch.distributed.run: [WARNING] 
[2024-12-31 12:43:42,977] torch.distributed.run: [WARNING] *****************************************
[2024-12-31 12:43:42,977] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-31 12:43:42,977] torch.distributed.run: [WARNING] *****************************************
using world size: 20, data-parallel size: 4, context-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 5, 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:HuggingFaceTokenizer
WARNING: Setting args.overlap_p2p_comm and args.align_param_gather to False since non-interleaved schedule does not support overlapping p2p communication and aligned param AG
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  allow_missing_vision_projection_checkpoint ...... True
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. True
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 0.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 4
  data_path ....................................... ['/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/pretrain_dataset.yaml']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. None
  dataloader_type ................................. external
  dataset_config .................................. None
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. 1024
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  disable_vision_class_token ...................... True
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 60
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 576
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... True
  eval_interval ................................... 1000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 230
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_wgrad ....................................... True
  freeze_LM ....................................... True
  freeze_ViT ...................................... False
  global_batch_size ............................... 32
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 336
  img_w ........................................... 336
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  language_model_type ............................. mistral_7b
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 20000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_num_tiles ................................... 1
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 14
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 5
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  prompt_path ..................................... /gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/manual_prompts.json
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 576
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  split_spec ...................................... 32,11,5,6,4
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /dat/fjs/llama_mistral/hete_mistral_clip_freeze_llm-tp4pp5/output/llava-mistral-7b-instruct-clip336-pretraining/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 2
  timing_log_option ............................... all
  titles_data_path ................................ None
  tokenizer_model ................................. /dat/fjs/llama_mistral/hf-mistral/
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 5
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_te .......................................... False
  use_thumbnail ................................... False
  use_tiling ...................................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  valid_path ...................................... None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_model_type ............................... clip
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 20
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 8
> building HuggingFaceTokenizer tokenizer ...
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
 > padded vocab (size: 32768) with 0 dummy tokens (new size: 32768)
> initializing torch distributed ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:5------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:5------
------num_pipeline_model_parallel_groups:4------
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 5
> setting random seeds to 1234 ...
> compiling dataset index builder ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:5------
------num_pipeline_model_parallel_groups:4------
make: Entering directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.078 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:5------
------num_pipeline_model_parallel_groups:4------
---Rank 2---Tensor Parallel Group GPUs: [0]---Rank 1---Tensor Parallel Group GPUs: [0]

---Rank 3---Tensor Parallel Group GPUs: [0]
---Rank 2---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]
>>> done with compiling and loading fused kernels. Compilation time: 2.922 seconds---Rank 1---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]---Rank 3---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]


[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 0---Tensor Parallel Group GPUs: [0]
---Rank 0---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]
[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 11.985
[after megatron is initialized] datetime: 2024-12-31 12:44:17 
building a multimodal model ...
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1819795456
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (376905728 elements):
	vision_model.decoder.layers.23.self_attention.linear_proj.weight
	vision_model.decoder.layers.22.self_attention.linear_proj.weight
	vision_model.decoder.layers.21.self_attention.linear_proj.bias
	vision_model.decoder.layers.18.mlp.linear_fc2.weight
	vision_model.decoder.layers.16.mlp.linear_fc2.weight
	vision_model.decoder.layers.15.mlp.linear_fc2.bias
	vision_model.decoder.layers.14.mlp.linear_fc2.bias
	vision_model.decoder.layers.13.mlp.linear_fc2.weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.weight
	vision_model.decoder.layers.5.input_layernorm.weight
	vision_model.decoder.layers.23.pre_mlp_layernorm.weight
	vision_model.decoder.layers.22.pre_mlp_layernorm.bias
	vision_model.decoder.layers.21.pre_mlp_layernorm.weight
	vision_model.decoder.layers.15.self_attention.linear_proj.weight
	vision_model.decoder.layers.12.self_attention.linear_qkv.bias
	vision_model.decoder.layers.23.input_layernorm.bias
	vision_model.decoder.layers.22.input_layernorm.bias
	vision_model.decoder.layers.21.input_layernorm.weight
	vision_model.decoder.layers.19.pre_mlp_layernorm.bias
	vision_model.decoder.layers.15.pre_mlp_layernorm.bias
	vision_model.decoder.layers.14.pre_mlp_layernorm.bias
	vision_model.decoder.layers.13.self_attention.linear_qkv.bias
	vision_model.decoder.layers.12.self_attention.linear_qkv.weight
	vision_model.decoder.layers.11.self_attention.linear_qkv.weight
	vision_model.decoder.layers.8.self_attention.linear_proj.bias
	vision_model.decoder.layers.5.mlp.linear_fc2.bias
	vision_model.decoder.layers.20.mlp.linear_fc2.bias
	vision_model.decoder.layers.15.input_layernorm.bias
	vision_model.decoder.layers.12.mlp.linear_fc1.weight
	vision_model.decoder.layers.11.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.mlp.linear_fc1.weight
	vision_model.decoder.layers.7.pre_mlp_layernorm.weight
	vision_model.decoder.layers.6.pre_mlp_layernorm.weight
	vision_model.decoder.layers.5.self_attention.linear_proj.weight
	vision_model.decoder.layers.3.self_attention.linear_proj.weight
	vision_model.decoder.layers.2.pre_mlp_layernorm.bias
	vision_model.decoder.layers.23.mlp.linear_fc2.bias
	vision_model.decoder.layers.22.mlp.linear_fc2.weight
	vision_model.decoder.layers.21.mlp.linear_fc2.bias
	vision_model.decoder.layers.8.input_layernorm.weight
	vision_model.decoder.layers.7.input_layernorm.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.weight
	vision_model.decoder.layers.6.input_layernorm.weight
	vision_model.decoder.layers.5.pre_mlp_layernorm.weight
	vision_model.decoder.layers.4.mlp.linear_fc2.bias
	vision_model.decoder.layers.4.pre_mlp_layernorm.bias
	vision_model.ln_pre.bias
	vision_model.decoder.layers.21.self_attention.linear_proj.weight
	vision_model.decoder.layers.20.self_attention.linear_proj.bias
	vision_model.decoder.layers.15.mlp.linear_fc2.weight
	vision_model.decoder.layers.14.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.input_layernorm.bias
	vision_model.decoder.layers.3.input_layernorm.bias
	vision_model.decoder.layers.23.pre_mlp_layernorm.bias
	vision_model.decoder.layers.21.pre_mlp_layernorm.bias
	vision_model.decoder.layers.20.pre_mlp_layernorm.weight
	vision_model.decoder.layers.18.self_attention.linear_qkv.bias
	vision_model.decoder.layers.9.pre_mlp_layernorm.bias
	vision_model.decoder.layers.9.pre_mlp_layernorm.weight
	vision_model.decoder.layers.7.mlp.linear_fc2.bias
	vision_model.decoder.layers.6.mlp.linear_fc2.bias
	vision_model.decoder.layers.0.self_attention.linear_proj.weight
	vision_model.decoder.layers.21.input_layernorm.bias
	vision_model.decoder.layers.20.input_layernorm.weight
	vision_model.decoder.layers.18.mlp.linear_fc1.bias
	vision_model.decoder.layers.17.mlp.linear_fc1.bias
	vision_model.decoder.layers.16.mlp.linear_fc1.bias
	vision_model.decoder.layers.13.self_attention.linear_qkv.weight
	vision_model.decoder.layers.9.mlp.linear_fc2.bias
	vision_model.decoder.layers.9.self_attention.linear_proj.bias
	vision_model.decoder.layers.8.self_attention.linear_proj.weight
	vision_model.decoder.layers.7.self_attention.linear_proj.weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.weight
	vision_model.decoder.layers.10.mlp.linear_fc1.bias
	vision_model.decoder.layers.8.pre_mlp_layernorm.weight
	vision_model.decoder.layers.7.pre_mlp_layernorm.bias
	vision_model.decoder.layers.6.pre_mlp_layernorm.bias
	vision_model.decoder.layers.3.pre_mlp_layernorm.bias
	vision_model.decoder.layers.3.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.bias
	vision_model.decoder.layers.0.self_attention.linear_qkv.weight
	vision_model.decoder.layers.6.self_attention.linear_qkv.weight
	vision_model.decoder.layers.23.mlp.linear_fc2.weight
	vision_model.decoder.layers.21.mlp.linear_fc2.weight
	vision_model.decoder.layers.20.mlp.linear_fc2.weight
	vision_model.decoder.layers.19.mlp.linear_fc2.weight
	vision_model.decoder.layers.11.self_attention.linear_proj.bias
	vision_model.decoder.layers.9.input_layernorm.weight
	vision_model.decoder.layers.8.input_layernorm.bias
	vision_model.decoder.layers.7.input_layernorm.bias
	vision_model.decoder.layers.6.input_layernorm.bias
	vision_model.decoder.layers.5.pre_mlp_layernorm.bias
	vision_model.decoder.layers.20.self_attention.linear_proj.weight
	vision_model.decoder.layers.17.self_attention.linear_qkv.bias
	vision_model.decoder.layers.22.mlp.linear_fc1.bias
	vision_model.decoder.layers.20.pre_mlp_layernorm.bias
	vision_model.decoder.layers.19.self_attention.linear_qkv.bias
	vision_model.decoder.layers.18.self_attention.linear_qkv.weight
	vision_model.decoder.layers.17.self_attention.linear_qkv.weight
	vision_model.decoder.layers.16.self_attention.linear_qkv.weight
	vision_model.decoder.layers.14.self_attention.linear_qkv.bias
	vision_model.decoder.layers.11.input_layernorm.weight
	vision_model.decoder.layers.8.mlp.linear_fc2.bias
	vision_model.decoder.layers.7.mlp.linear_fc2.weight
	vision_model.ln_pre.weight
	vision_model.decoder.layers.20.input_layernorm.bias
	vision_model.decoder.layers.19.mlp.linear_fc1.bias
	vision_model.decoder.layers.18.mlp.linear_fc1.weight
	vision_model.decoder.layers.17.mlp.linear_fc1.weight
	vision_model.decoder.layers.16.mlp.linear_fc1.weight
	vision_model.decoder.layers.15.mlp.linear_fc1.bias
	vision_model.decoder.layers.14.mlp.linear_fc1.bias
	vision_model.decoder.layers.13.mlp.linear_fc1.weight
	vision_model.decoder.layers.9.self_attention.linear_proj.weight
	vision_model.decoder.layers.6.self_attention.linear_proj.bias
	vision_model.decoder.layers.2.input_layernorm.bias
	vision_projection.encoder.linear_fc2.weight
	vision_model.decoder.layers.8.pre_mlp_layernorm.bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.bias
	vision_model.decoder.layers.4.self_attention.linear_proj.weight
	vision_model.decoder.layers.2.self_attention.linear_qkv.bias
	vision_model.decoder.layers.1.self_attention.linear_qkv.weight
	vision_model.conv1.weight
	vision_model.decoder.layers.7.self_attention.linear_qkv.weight
	vision_model.decoder.layers.1.input_layernorm.bias
	vision_model.decoder.layers.19.mlp.linear_fc2.bias
	vision_model.decoder.layers.13.self_attention.linear_proj.bias
	vision_model.decoder.layers.12.self_attention.linear_proj.bias
	vision_model.decoder.layers.11.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.input_layernorm.bias
	vision_model.decoder.layers.4.mlp.linear_fc1.bias
	vision_model.decoder.layers.3.mlp.linear_fc1.bias
	vision_model.decoder.layers.2.mlp.linear_fc1.bias
	vision_model.decoder.layers.1.mlp.linear_fc1.weight
	vision_model.decoder.layers.23.self_attention.linear_qkv.bias
	vision_model.decoder.layers.22.self_attention.linear_qkv.bias
	vision_model.decoder.layers.16.self_attention.linear_qkv.bias
	vision_model.decoder.layers.15.self_attention.linear_qkv.bias
	vision_model.decoder.layers.12.pre_mlp_layernorm.weight
	vision_model.decoder.layers.11.pre_mlp_layernorm.weight
	vision_model.decoder.layers.10.pre_mlp_layernorm.weight
	vision_model.decoder.layers.5.self_attention.linear_qkv.weight
	vision_model.decoder.layers.23.mlp.linear_fc1.bias
	vision_model.decoder.layers.22.mlp.linear_fc1.weight
	vision_model.decoder.layers.21.mlp.linear_fc1.bias
	vision_model.decoder.layers.19.self_attention.linear_qkv.weight
	vision_model.decoder.layers.15.self_attention.linear_qkv.weight
	vision_model.decoder.layers.14.self_attention.linear_qkv.weight
	vision_model.decoder.layers.13.input_layernorm.weight
	vision_model.decoder.layers.12.input_layernorm.weight
	vision_model.decoder.layers.11.input_layernorm.bias
	vision_model.decoder.layers.10.self_attention.linear_proj.bias
	vision_model.decoder.layers.7.self_attention.linear_proj.bias
	vision_model.decoder.layers.19.mlp.linear_fc1.weight
	vision_model.decoder.layers.15.mlp.linear_fc1.weight
	vision_model.decoder.layers.14.mlp.linear_fc1.weight
	vision_model.decoder.layers.13.mlp.linear_fc1.bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.bias
	vision_model.decoder.layers.3.self_attention.linear_qkv.bias
	vision_model.decoder.layers.3.input_layernorm.weight
	vision_model.decoder.layers.0.input_layernorm.weight
	vision_model.decoder.layers.18.self_attention.linear_proj.bias
	vision_model.decoder.layers.12.mlp.linear_fc2.bias
	vision_model.decoder.layers.11.mlp.linear_fc2.bias
	vision_model.decoder.layers.10.mlp.linear_fc2.weight
	vision_model.decoder.layers.7.mlp.linear_fc1.bias
	vision_model.decoder.layers.6.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.self_attention.linear_qkv.bias
	vision_model.decoder.layers.5.self_attention.linear_proj.bias
	vision_model.decoder.layers.4.self_attention.linear_qkv.weight
	vision_model.decoder.layers.3.self_attention.linear_qkv.weight
	vision_model.decoder.layers.0.mlp.linear_fc1.weight
	vision_model.decoder.layers.0.self_attention.linear_qkv.bias
	vision_model.decoder.layers.2.input_layernorm.weight
	vision_model.decoder.layers.21.self_attention.linear_qkv.bias
	vision_model.decoder.layers.20.self_attention.linear_qkv.bias
	vision_model.decoder.layers.13.self_attention.linear_proj.weight
	vision_model.decoder.layers.12.self_attention.linear_proj.weight
	vision_model.decoder.layers.9.mlp.linear_fc1.bias
	vision_model.decoder.layers.5.mlp.linear_fc1.bias
	vision_model.decoder.layers.4.mlp.linear_fc1.weight
	vision_model.decoder.layers.3.mlp.linear_fc1.weight
	vision_model.decoder.layers.2.mlp.linear_fc2.weight
	vision_model.decoder.layers.2.mlp.linear_fc1.weight
	vision_model.decoder.layers.18.input_layernorm.weight
	vision_model.decoder.layers.23.self_attention.linear_qkv.weight
	vision_model.decoder.layers.22.self_attention.linear_qkv.weight
	vision_model.decoder.layers.21.self_attention.linear_qkv.weight
	vision_model.decoder.layers.12.pre_mlp_layernorm.bias
	vision_model.decoder.layers.11.pre_mlp_layernorm.bias
	vision_model.decoder.layers.10.pre_mlp_layernorm.bias
	vision_model.decoder.layers.10.input_layernorm.bias
	vision_model.decoder.layers.9.mlp.linear_fc2.weight
	vision_model.decoder.layers.1.self_attention.linear_proj.bias
	vision_model.position_embeddings.weight
	vision_model.decoder.layers.17.mlp.linear_fc2.bias
	vision_model.decoder.layers.23.mlp.linear_fc1.weight
	vision_model.decoder.layers.21.mlp.linear_fc1.weight
	vision_model.decoder.layers.20.mlp.linear_fc1.bias
	vision_model.decoder.layers.13.input_layernorm.bias
	vision_model.decoder.layers.12.input_layernorm.bias
	vision_model.decoder.layers.1.mlp.linear_fc2.weight
	vision_model.decoder.layers.0.pre_mlp_layernorm.weight
	vision_model.decoder.layers.8.self_attention.linear_qkv.bias
	vision_model.decoder.layers.7.self_attention.linear_qkv.bias
	vision_model.decoder.layers.6.self_attention.linear_qkv.bias
	vision_model.decoder.layers.4.input_layernorm.bias
	vision_model.decoder.layers.1.input_layernorm.weight
	vision_model.decoder.layers.0.input_layernorm.bias
	vision_model.decoder.layers.0.mlp.linear_fc2.weight
	vision_model.decoder.layers.19.self_attention.linear_proj.bias
	vision_model.decoder.layers.18.self_attention.linear_proj.weight
	vision_model.decoder.layers.17.self_attention.linear_proj.bias
	vision_model.decoder.layers.16.self_attention.linear_proj.bias
	vision_model.decoder.layers.14.self_attention.linear_proj.bias
	vision_model.decoder.layers.12.mlp.linear_fc2.weight
	vision_model.decoder.layers.11.mlp.linear_fc2.weight
	vision_model.decoder.layers.8.mlp.linear_fc1.bias
	vision_model.decoder.layers.7.mlp.linear_fc1.weight
	vision_model.decoder.layers.6.mlp.linear_fc1.weight
	vision_model.decoder.layers.1.mlp.linear_fc1.bias
	vision_model.decoder.layers.18.pre_mlp_layernorm.weight
	vision_model.decoder.layers.17.pre_mlp_layernorm.weight
	vision_model.decoder.layers.16.pre_mlp_layernorm.weight
	vision_model.decoder.layers.13.pre_mlp_layernorm.weight
	vision_model.decoder.layers.10.input_layernorm.weight
	vision_model.decoder.layers.5.mlp.linear_fc1.weight
	vision_model.decoder.layers.3.mlp.linear_fc2.bias
	vision_model.decoder.layers.2.self_attention.linear_proj.weight
	vision_model.decoder.layers.0.mlp.linear_fc2.bias
	vision_model.class_token
	vision_model.decoder.layers.20.self_attention.linear_qkv.weight
	vision_model.decoder.layers.19.input_layernorm.weight
	vision_model.decoder.layers.18.input_layernorm.bias
	vision_model.decoder.layers.17.input_layernorm.weight
	vision_model.decoder.layers.16.input_layernorm.weight
	vision_model.decoder.layers.14.input_layernorm.weight
	vision_model.decoder.layers.4.self_attention.linear_proj.bias
	vision_model.decoder.layers.2.self_attention.linear_proj.bias
	vision_model.decoder.layers.1.self_attention.linear_proj.weight
	vision_model.decoder.layers.0.mlp.linear_fc1.bias
	vision_model.decoder.layers.20.mlp.linear_fc1.weight
	vision_model.decoder.layers.10.self_attention.linear_proj.weight
	vision_model.decoder.layers.2.mlp.linear_fc2.bias
	vision_model.decoder.layers.1.pre_mlp_layernorm.weight
	vision_model.decoder.layers.0.pre_mlp_layernorm.bias
	vision_projection.encoder.linear_fc1.weight
	vision_model.decoder.layers.23.self_attention.linear_proj.bias
	vision_model.decoder.layers.22.self_attention.linear_proj.bias
	vision_model.decoder.layers.18.mlp.linear_fc2.bias
	vision_model.decoder.layers.17.mlp.linear_fc2.weight
	vision_model.decoder.layers.16.mlp.linear_fc2.bias
	vision_model.decoder.layers.13.mlp.linear_fc2.bias
	vision_model.decoder.layers.10.self_attention.linear_qkv.weight
	vision_model.decoder.layers.9.self_attention.linear_qkv.bias
	vision_model.decoder.layers.8.self_attention.linear_qkv.weight
	vision_model.decoder.layers.6.mlp.linear_fc2.weight
	vision_model.decoder.layers.5.mlp.linear_fc2.weight
	vision_model.decoder.layers.1.mlp.linear_fc2.bias
	vision_model.decoder.layers.22.pre_mlp_layernorm.weight
	vision_model.decoder.layers.19.self_attention.linear_proj.weight
	vision_model.decoder.layers.17.self_attention.linear_proj.weight
	vision_model.decoder.layers.16.self_attention.linear_proj.weight
	vision_model.decoder.layers.15.self_attention.linear_proj.bias
	vision_model.decoder.layers.14.self_attention.linear_proj.weight
	vision_model.decoder.layers.8.mlp.linear_fc1.weight
	vision_model.decoder.layers.4.input_layernorm.weight
	vision_model.decoder.layers.23.input_layernorm.weight
	vision_model.decoder.layers.22.input_layernorm.weight
	vision_model.decoder.layers.19.pre_mlp_layernorm.weight
	vision_model.decoder.layers.18.pre_mlp_layernorm.bias
	vision_model.decoder.layers.17.pre_mlp_layernorm.bias
	vision_model.decoder.layers.16.pre_mlp_layernorm.bias
	vision_model.decoder.layers.15.pre_mlp_layernorm.weight
	vision_model.decoder.layers.14.pre_mlp_layernorm.weight
	vision_model.decoder.layers.13.pre_mlp_layernorm.bias
	vision_model.decoder.layers.11.self_attention.linear_qkv.bias
	vision_model.decoder.layers.4.mlp.linear_fc2.weight
	vision_model.decoder.layers.19.input_layernorm.bias
	vision_model.decoder.layers.17.input_layernorm.bias
	vision_model.decoder.layers.16.input_layernorm.bias
	vision_model.decoder.layers.15.input_layernorm.weight
	vision_model.decoder.layers.14.input_layernorm.bias
	vision_model.decoder.layers.12.mlp.linear_fc1.bias
	vision_model.decoder.layers.11.mlp.linear_fc1.bias
	vision_model.decoder.layers.10.mlp.linear_fc2.bias
	vision_model.decoder.layers.9.mlp.linear_fc1.weight
	vision_model.decoder.layers.8.mlp.linear_fc2.weight
	vision_model.decoder.layers.22.mlp.linear_fc2.bias
	vision_model.decoder.layers.4.pre_mlp_layernorm.weight
	vision_model.decoder.layers.3.mlp.linear_fc2.weight
	vision_model.decoder.layers.3.pre_mlp_layernorm.weight
	vision_model.decoder.layers.2.pre_mlp_layernorm.weight
	vision_model.decoder.layers.1.pre_mlp_layernorm.bias
	vision_model.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, clip_grad=0.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fa7d12e3fd0>, config_logger_dir='')
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-12-31 12:44:18 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      320
    validation: 320
    test:       320
> building HuggingFaceTokenizer tokenizer ...
rank=2, worker=0: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<619>, pretrain-37.tar[2200, 2300), pretrain-37.tar[2300, 2400), pretrain-37.tar[2400, 2500)] sum(count)=62500
rank=2, worker=1: shard_range=[pretrain-37.tar[2500, 2600), pretrain-37.tar[2600, 2700), pretrain-37.tar[2700, 2800), ...<619>, pretrain-42.tar[4700, 4800), pretrain-42.tar[4800, 4900), pretrain-42.tar[4900, 5000)] sum(count)=62500
rank=1, worker=0: shard_range=[pretrain-2.tar[5000, 5100), pretrain-2.tar[5100, 5200), pretrain-2.tar[5200, 5300), ...<619>, pretrain-25.tar[7200, 7300), pretrain-25.tar[7300, 7400), pretrain-25.tar[7400, 7500)] sum(count)=62500
rank=1, worker=1: shard_range=[pretrain-25.tar[7500, 7600), pretrain-25.tar[7600, 7700), pretrain-25.tar[7700, 7800), ...<619>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=62500
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<619>, pretrain-14.tar[2200, 2300), pretrain-14.tar[2300, 2400), pretrain-14.tar[2400, 2500)] sum(count)=62500
rank=0, worker=1: shard_range=[pretrain-14.tar[2500, 2600), pretrain-14.tar[2600, 2700), pretrain-14.tar[2700, 2800), ...<619>, pretrain-2.tar[4700, 4800), pretrain-2.tar[4800, 4900), pretrain-2.tar[4900, 5000)] sum(count)=62500
> building HuggingFaceTokenizer tokenizer ...
rank=3, worker=0: shard_range=[pretrain-42.tar[5000, 5100), pretrain-42.tar[5100, 5200), pretrain-42.tar[5200, 5300), ...<619>, pretrain-48.tar[7200, 7300), pretrain-48.tar[7300, 7400), pretrain-48.tar[7400, 7500)] sum(count)=62500
rank=3, worker=1: shard_range=[pretrain-48.tar[7500, 7600), pretrain-48.tar[7600, 7700), pretrain-48.tar[7700, 7800), ...<619>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=62500
rank=2, worker=0: shard_range=[pretrain-7.tar[936, 8202)] sum(count)=7266
rank=2, worker=1: shard_range=[pretrain-7.tar[8202, 10000), pretrain-8.tar[0, 5468)] sum(count)=7266
rank=1, worker=0: shard_range=[pretrain-55.tar[4532, 8128), pretrain-6.tar[0, 3670)] sum(count)=7266
rank=1, worker=1: shard_range=[pretrain-6.tar[3670, 10000), pretrain-7.tar[0, 936)] sum(count)=7266
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 7266)] sum(count)=7266
rank=0, worker=1: shard_range=[pretrain-54.tar[7266, 10000), pretrain-55.tar[0, 4532)] sum(count)=7266
rank=3, worker=0: shard_range=[pretrain-8.tar[5468, 10000), pretrain-9.tar[0, 2734)] sum(count)=7266
rank=3, worker=1: shard_range=[pretrain-9.tar[2734, 10000)] sum(count)=7266
[after dataloaders are built] datetime: 2024-12-31 12:44:19 
done with setup ...
training ...
[before the start of training step] datetime: 2024-12-31 12:44:19 
Layer: TransformerLayer, Forward time: 576.287261 ms
Layer: TransformerLayer, Forward time: 2.410440 ms
Layer: TransformerLayer, Forward time: 1.819596 ms
Layer: TransformerLayer, Forward time: 1.936496 ms
Layer: TransformerLayer, Forward time: 1.917318 ms
Layer: TransformerLayer, Forward time: 1.893999 ms
Layer: TransformerLayer, Forward time: 1.856116 ms
Layer: TransformerLayer, Forward time: 1.913616 ms
Layer: TransformerLayer, Forward time: 1.872562 ms
Layer: TransformerLayer, Forward time: 1.904130 ms
Layer: TransformerLayer, Forward time: 1.740111 ms
Layer: TransformerLayer, Forward time: 1.985087 ms
Layer: TransformerLayer, Forward time: 1.920661 ms
Layer: TransformerLayer, Forward time: 1.910782 ms
Layer: TransformerLayer, Forward time: 1.965287 ms
Layer: TransformerLayer, Forward time: 2.062966 ms
Layer: TransformerLayer, Forward time: 2.066156 ms
Layer: TransformerLayer, Forward time: 2.068222 ms
Layer: TransformerLayer, Forward time: 1.892896 ms
Layer: TransformerLayer, Forward time: 2.048552 ms
Layer: TransformerLayer, Forward time: 1.935411 ms
Layer: TransformerLayer, Forward time: 1.905559 ms
Layer: TransformerLayer, Forward time: 1.997970 ms
Layer: TransformerLayer, Forward time: 1.966468 ms
Layer: TransformerLayer, Forward time: 23.392872 ms
Layer: TransformerLayer, Forward time: 24.259845 ms
Layer: TransformerLayer, Forward time: 25.413946 ms
Layer: TransformerLayer, Forward time: 25.415338 ms
Layer: TransformerLayer, Forward time: 25.411902 ms
Layer: TransformerLayer, Forward time: 25.418790 ms
Layer: TransformerLayer, Forward time: 739.912164 ms
Layer: TransformerLayer, Forward time: 2.567677 ms
Layer: TransformerLayer, Forward time: 1.982756 ms
Layer: TransformerLayer, Forward time: 2.135753 ms
Layer: TransformerLayer, Forward time: 2.171823 ms
Layer: TransformerLayer, Forward time: 2.085206 ms
Layer: TransformerLayer, Forward time: 2.063782 ms
Layer: TransformerLayer, Forward time: 2.053823 ms
Layer: TransformerLayer, Forward time: 2.054924 ms
Layer: TransformerLayer, Forward time: 2.110794 ms
Layer: TransformerLayer, Forward time: 2.105518 ms
Layer: TransformerLayer, Forward time: 2.198147 ms
Layer: TransformerLayer, Forward time: 2.105575 ms
Layer: TransformerLayer, Forward time: 2.116066 ms
Layer: TransformerLayer, Forward time: 2.072300 ms
Layer: TransformerLayer, Forward time: 2.321136 ms
Layer: TransformerLayer, Forward time: 2.119564 ms
Layer: TransformerLayer, Forward time: 2.082286 ms
Layer: TransformerLayer, Forward time: 1.955158 ms
Layer: TransformerLayer, Forward time: 2.164663 ms
Layer: TransformerLayer, Forward time: 2.131555 ms
Layer: TransformerLayer, Forward time: 2.065013 ms
Layer: TransformerLayer, Forward time: 2.188145 ms
Layer: TransformerLayer, Forward time: 2.086151 ms
Layer: TransformerLayer, Forward time: 23.481020 ms
Layer: TransformerLayer, Forward time: 25.924473 ms
Layer: TransformerLayer, Forward time: 26.032610 ms
Layer: TransformerLayer, Forward time: 775.817989 ms
Layer: TransformerLayer, Forward time: 2.835996 ms
Layer: TransformerLayer, Forward time: 2.051565 ms
Layer: TransformerLayer, Forward time: 2.127003 ms
Layer: TransformerLayer, Forward time: 1.770850 ms
Layer: TransformerLayer, Forward time: 2.162789 ms
Layer: TransformerLayer, Forward time: 25.890465 ms
Layer: TransformerLayer, Forward time: 1.962188 ms
Layer: TransformerLayer, Forward time: 2.051352 ms
Layer: TransformerLayer, Forward time: 2.987728 ms
Layer: TransformerLayer, Forward time: 2.211025 ms
Layer: TransformerLayer, Forward time: 1.659014 ms
Layer: TransformerLayer, Forward time: 2.061467 ms
Layer: TransformerLayer, Forward time: 1.847688 ms
Layer: TransformerLayer, Forward time: 2.083460 ms
Layer: TransformerLayer, Forward time: 1.845608 ms
Layer: TransformerLayer, Forward time: 2.060824 ms
Layer: TransformerLayer, Forward time: 1.817484 ms
Layer: TransformerLayer, Forward time: 2.065350 ms
Layer: TransformerLayer, Forward time: 1.954774 ms
Layer: TransformerLayer, Forward time: 2.261673 ms
Layer: TransformerLayer, Forward time: 1.950009 ms
Layer: TransformerLayer, Forward time: 2.220782 ms
Layer: TransformerLayer, Forward time: 805.351472 ms
Layer: TransformerLayer, Forward time: 2.376422 ms
Layer: TransformerLayer, Forward time: 2.036974 ms
Layer: TransformerLayer, Forward time: 2.177354 ms
Layer: TransformerLayer, Forward time: 3.005207 ms
Layer: TransformerLayer, Forward time: 1.990117 ms
Layer: TransformerLayer, Forward time: 2.034287 ms
Layer: TransformerLayer, Forward time: 2.287146 ms
Layer: TransformerLayer, Forward time: 1.893711 ms
Layer: TransformerLayer, Forward time: 2.050113 ms
Layer: TransformerLayer, Forward time: 2.079272 ms
Layer: TransformerLayer, Forward time: 2.531368 ms
Layer: TransformerLayer, Forward time: 25.902287 ms
Layer: TransformerLayer, Forward time: 2.267934 ms
Layer: TransformerLayer, Forward time: 2.192125 ms
Layer: TransformerLayer, Forward time: 2.309058 ms
Layer: TransformerLayer, Forward time: 3.177529 ms
Layer: TransformerLayer, Forward time: 1.976690 ms
Layer: TransformerLayer, Forward time: 1.960115 ms
Layer: TransformerLayer, Forward time: 2.549046 ms
Layer: TransformerLayer, Forward time: 1.878894 ms
Layer: TransformerLayer, Forward time: 2.001573 ms
Layer: TransformerLayer, Forward time: 2.039006 ms
Layer: TransformerLayer, Forward time: 2.407534 ms
Layer: TransformerLayer, Forward time: 2.029136 ms
Layer: TransformerLayer, Forward time: 2.461654 ms
Layer: TransformerLayer, Forward time: 2.109925 ms
Layer: TransformerLayer, Forward time: 3.425514 ms
Layer: TransformerLayer, Forward time: 2.619569 ms
Layer: TransformerLayer, Forward time: 2.474714 ms
Layer: TransformerLayer, Forward time: 2.174623 ms
Layer: TransformerLayer, Forward time: 2.499989 ms
Layer: TransformerLayer, Forward time: 2.230823 ms
Layer: TransformerLayer, Forward time: 2.362742 ms
Layer: TransformerLayer, Forward time: 2.185682 ms
Layer: TransformerLayer, Forward time: 2.393882 ms
Layer: TransformerLayer, Forward time: 2.000613 ms
Layer: TransformerLayer, Forward time: 2.741299 ms
Layer: TransformerLayer, Forward time: 1.861492 ms
Layer: TransformerLayer, Forward time: 2.594518 ms
Layer: TransformerLayer, Forward time: 25.870415 ms
Layer: TransformerLayer, Forward time: 2.688863 ms
Layer: TransformerLayer, Forward time: 2.639081 ms
Layer: TransformerLayer, Forward time: 2.593518 ms
Layer: TransformerLayer, Forward time: 2.482254 ms
Layer: TransformerLayer, Forward time: 2.641437 ms
Layer: TransformerLayer, Forward time: 2.831884 ms
Layer: TransformerLayer, Forward time: 2.419745 ms
Layer: TransformerLayer, Forward time: 2.448726 ms
Layer: TransformerLayer, Forward time: 23.298911 ms
Layer: TransformerLayer, Forward time: 2.430254 ms
Layer: TransformerLayer, Forward time: 2.590521 ms
Layer: TransformerLayer, Forward time: 22.861632 ms
Layer: TransformerLayer, Forward time: 2.435150 ms
Layer: TransformerLayer, Forward time: 23.613016 ms
Layer: TransformerLayer, Forward time: 25.544452 ms
Layer: TransformerLayer, Forward time: 23.415337 ms
Layer: TransformerLayer, Forward time: 25.864555 ms
Layer: TransformerLayer, Forward time: 25.761478 ms
Layer: TransformerLayer, Forward time: 24.758711 ms
Layer: TransformerLayer, Forward time: 25.878072 ms
Layer: TransformerLayer, Forward time: 25.694610 ms
Layer: TransformerLayer, Forward time: 24.858174 ms
Layer: TransformerLayer, Forward time: 25.524450 ms
Layer: TransformerLayer, Forward time: 25.721726 ms
Layer: TransformerLayer, Forward time: 24.857059 ms
Layer: TransformerLayer, Forward time: 25.385617 ms
Layer: TransformerLayer, Forward time: 25.765607 ms
Layer: TransformerLayer, Forward time: 24.887978 ms
Layer: TransformerLayer, Forward time: 25.354568 ms
Layer: TransformerLayer, Forward time: 1.953096 ms
Layer: TransformerLayer, Forward time: 2.021589 ms
Layer: TransformerLayer, Forward time: 3.569988 ms
Layer: TransformerLayer, Forward time: 1.870564 ms
Layer: TransformerLayer, Forward time: 2.074460 ms
Layer: TransformerLayer, Forward time: 1.949193 ms
Layer: TransformerLayer, Forward time: 2.056007 ms
Layer: TransformerLayer, Forward time: 2.020816 ms
Layer: TransformerLayer, Forward time: 2.078179 ms
Layer: TransformerLayer, Forward time: 2.172905 ms
Layer: TransformerLayer, Forward time: 2.120754 ms
Layer: TransformerLayer, Forward time: 2.144946 ms
Layer: TransformerLayer, Forward time: 2.091599 ms
Layer: TransformerLayer, Forward time: 2.180668 ms
Layer: TransformerLayer, Forward time: 2.106328 ms
Layer: TransformerLayer, Forward time: 2.381362 ms
Layer: TransformerLayer, Forward time: 2.001262 ms
Layer: TransformerLayer, Forward time: 2.164372 ms
Layer: TransformerLayer, Forward time: 2.236210 ms
Layer: TransformerLayer, Forward time: 2.126548 ms
Layer: TransformerLayer, Forward time: 2.243258 ms
Layer: TransformerLayer, Forward time: 2.119476 ms
Layer: TransformerLayer, Forward time: 2.251505 ms
Layer: TransformerLayer, Forward time: 2.066881 ms
Layer: TransformerLayer, Forward time: 23.324421 ms
Layer: TransformerLayer, Forward time: 25.174437 ms
Layer: TransformerLayer, Forward time: 1.948615 ms
Layer: TransformerLayer, Forward time: 2.020723 ms
Layer: TransformerLayer, Forward time: 3.119195 ms
Layer: TransformerLayer, Forward time: 1.703551 ms
Layer: TransformerLayer, Forward time: 26.250992 ms
Layer: TransformerLayer, Forward time: 1.892101 ms
Layer: TransformerLayer, Forward time: 2.135985 ms
Layer: TransformerLayer, Forward time: 2.791396 ms
Layer: TransformerLayer, Forward time: 1.831171 ms
Layer: TransformerLayer, Forward time: 2.191493 ms
Layer: TransformerLayer, Forward time: 3.075775 ms
Layer: TransformerLayer, Forward time: 1.867887 ms
Layer: TransformerLayer, Forward time: 2.045815 ms
Layer: TransformerLayer, Forward time: 4.101970 ms
Layer: TransformerLayer, Forward time: 1.943444 ms
Layer: TransformerLayer, Forward time: 2.215500 ms
Layer: TransformerLayer, Forward time: 1.899647 ms
Layer: TransformerLayer, Forward time: 2.475549 ms
Layer: TransformerLayer, Forward time: 1.956081 ms
Layer: TransformerLayer, Forward time: 2.067616 ms
Layer: TransformerLayer, Forward time: 2.508409 ms
Layer: TransformerLayer, Forward time: 2.079447 ms
Layer: TransformerLayer, Forward time: 2.475428 ms
Layer: TransformerLayer, Forward time: 2.084266 ms
Layer: TransformerLayer, Forward time: 2.405008 ms
Layer: TransformerLayer, Forward time: 1.842503 ms
Layer: TransformerLayer, Forward time: 26.186362 ms
Layer: TransformerLayer, Forward time: 2.419372 ms
Layer: TransformerLayer, Forward time: 1.987545 ms
Layer: TransformerLayer, Forward time: 2.231350 ms
Layer: TransformerLayer, Forward time: 2.875492 ms
Layer: TransformerLayer, Forward time: 2.116279 ms
Layer: TransformerLayer, Forward time: 2.646908 ms
Layer: TransformerLayer, Forward time: 2.043366 ms
Layer: TransformerLayer, Forward time: 2.516765 ms
Layer: TransformerLayer, Forward time: 1.991419 ms
Layer: TransformerLayer, Forward time: 2.543124 ms
Layer: TransformerLayer, Forward time: 2.010085 ms
Layer: TransformerLayer, Forward time: 2.195385 ms
Layer: TransformerLayer, Forward time: 2.794128 ms
Layer: TransformerLayer, Forward time: 2.531208 ms
Layer: TransformerLayer, Forward time: 2.559910 ms
Layer: TransformerLayer, Forward time: 2.281636 ms
Layer: TransformerLayer, Forward time: 2.560147 ms
Layer: TransformerLayer, Forward time: 26.099457 ms
Layer: TransformerLayer, Forward time: 2.676760 ms
Layer: TransformerLayer, Forward time: 2.524239 ms
Layer: TransformerLayer, Forward time: 2.623452 ms
Layer: TransformerLayer, Forward time: 2.468988 ms
Layer: TransformerLayer, Forward time: 2.531182 ms
Layer: TransformerLayer, Forward time: 2.836836 ms
Layer: TransformerLayer, Forward time: 22.976142 ms
Layer: TransformerLayer, Forward time: 25.897045 ms
Layer: TransformerLayer, Forward time: 24.515310 ms
Layer: TransformerLayer, Forward time: 22.642140 ms
Layer: TransformerLayer, Forward time: 25.898670 ms
Layer: TransformerLayer, Forward time: 23.772733 ms
Layer: TransformerLayer, Forward time: 25.838259 ms
Layer: TransformerLayer, Forward time: 25.019217 ms
Layer: TransformerLayer, Forward time: 25.062528 ms
Layer: TransformerLayer, Forward time: 25.684152 ms
Layer: TransformerLayer, Forward time: 25.267539 ms
Layer: TransformerLayer, Forward time: 25.732213 ms
Layer: TransformerLayer, Forward time: 25.328867 ms
Layer: TransformerLayer, Forward time: 2.411827 ms
Layer: TransformerLayer, Forward time: 2.438789 ms
Layer: TransformerLayer, Forward time: 2.170053 ms
Layer: TransformerLayer, Forward time: 2.314997 ms
Layer: TransformerLayer, Forward time: 2.440819 ms
Layer: TransformerLayer, Forward time: 2.506293 ms
Layer: TransformerLayer, Forward time: 2.576314 ms
Layer: TransformerLayer, Forward time: 2.594635 ms
Layer: TransformerLayer, Forward time: 2.592591 ms
Layer: TransformerLayer, Forward time: 2.552160 ms
Layer: TransformerLayer, Forward time: 2.543940 ms
Layer: TransformerLayer, Forward time: 2.556480 ms
Layer: TransformerLayer, Forward time: 2.581240 ms
Layer: TransformerLayer, Forward time: 2.333788 ms
Layer: TransformerLayer, Forward time: 2.716686 ms
Layer: TransformerLayer, Forward time: 2.541480 ms
Layer: TransformerLayer, Forward time: 2.522298 ms
Layer: TransformerLayer, Forward time: 2.549383 ms
Layer: TransformerLayer, Forward time: 2.507902 ms
Layer: TransformerLayer, Forward time: 2.475647 ms
Layer: TransformerLayer, Forward time: 2.438052 ms
Layer: TransformerLayer, Forward time: 2.228660 ms
Layer: TransformerLayer, Forward time: 2.454401 ms
Layer: TransformerLayer, Forward time: 2.503154 ms
Layer: TransformerLayer, Forward time: 22.977456 ms
Layer: TransformerLayer, Forward time: 25.439657 ms
Layer: TransformerLayer, Forward time: 25.734769 ms
Layer: TransformerLayer, Forward time: 25.590876 ms
Layer: TransformerLayer, Forward time: 25.651105 ms
Layer: TransformerLayer, Forward time: 25.647169 ms
Layer: TransformerLayer, Forward time: 1.886395 ms
Layer: TransformerLayer, Forward time: 1.958349 ms
Layer: TransformerLayer, Forward time: 1.894061 ms
Layer: TransformerLayer, Forward time: 1.958531 ms
Layer: TransformerLayer, Forward time: 1.935055 ms
Layer: TransformerLayer, Forward time: 1.936737 ms
Layer: TransformerLayer, Forward time: 1.914495 ms
Layer: TransformerLayer, Forward time: 1.972514 ms
Layer: TransformerLayer, Forward time: 1.920013 ms
Layer: TransformerLayer, Forward time: 2.011583 ms
Layer: TransformerLayer, Forward time: 1.966627 ms
Layer: TransformerLayer, Forward time: 1.936620 ms
Layer: TransformerLayer, Forward time: 1.937219 ms
Layer: TransformerLayer, Forward time: 1.782986 ms
Layer: TransformerLayer, Forward time: 1.919393 ms
Layer: TransformerLayer, Forward time: 1.964983 ms
Layer: TransformerLayer, Forward time: 1.967452 ms
Layer: TransformerLayer, Forward time: 1.994014 ms
Layer: TransformerLayer, Forward time: 1.999278 ms
Layer: TransformerLayer, Forward time: 2.354693 ms
Layer: TransformerLayer, Forward time: 2.099942 ms
Layer: TransformerLayer, Forward time: 1.852985 ms
Layer: TransformerLayer, Forward time: 1.987773 ms
Layer: TransformerLayer, Forward time: 1.972592 ms
Layer: TransformerLayer, Forward time: 23.145760 ms
Layer: TransformerLayer, Forward time: 25.387665 ms
Layer: TransformerLayer, Forward time: 26.011521 ms
Layer: TransformerLayer, Forward time: 25.986184 ms
Layer: TransformerLayer, Forward time: 1.916634 ms
Layer: TransformerLayer, Forward time: 2.048894 ms
Layer: TransformerLayer, Forward time: 26.114823 ms
Layer: TransformerLayer, Forward time: 1.725625 ms
Layer: TransformerLayer, Forward time: 1.919052 ms
Layer: TransformerLayer, Forward time: 1.993035 ms
Layer: TransformerLayer, Forward time: 1.976549 ms
Layer: TransformerLayer, Forward time: 2.000082 ms
Layer: TransformerLayer, Forward time: 2.105792 ms
Layer: TransformerLayer, Forward time: 2.128808 ms
Layer: TransformerLayer, Forward time: 2.092677 ms
Layer: TransformerLayer, Forward time: 2.072583 ms
Layer: TransformerLayer, Forward time: 1.970433 ms
Layer: TransformerLayer, Forward time: 1.985575 ms
Layer: TransformerLayer, Forward time: 1.812063 ms
Layer: TransformerLayer, Forward time: 26.152114 ms
Layer: TransformerLayer, Forward time: 2.002817 ms
Layer: TransformerLayer, Forward time: 2.040561 ms
Layer: TransformerLayer, Forward time: 2.114626 ms
Layer: TransformerLayer, Forward time: 2.184635 ms
Layer: TransformerLayer, Forward time: 2.095244 ms
Layer: TransformerLayer, Forward time: 2.049495 ms
Layer: TransformerLayer, Forward time: 2.016207 ms
Layer: TransformerLayer, Forward time: 1.805161 ms
Layer: TransformerLayer, Forward time: 3.120895 ms
Layer: TransformerLayer, Forward time: 2.325820 ms
Layer: TransformerLayer, Forward time: 23.193008 ms
Layer: TransformerLayer, Forward time: 24.557723 ms
Layer: TransformerLayer, Forward time: 1.852720 ms
Layer: TransformerLayer, Forward time: 1.909967 ms
Layer: TransformerLayer, Forward time: 1.712170 ms
Layer: TransformerLayer, Forward time: 1.878683 ms
Layer: TransformerLayer, Forward time: 1.950532 ms
Layer: TransformerLayer, Forward time: 25.788358 ms
Layer: TransformerLayer, Forward time: 1.983311 ms
Layer: TransformerLayer, Forward time: 2.115097 ms
Layer: TransformerLayer, Forward time: 2.085204 ms
Layer: TransformerLayer, Forward time: 1.940650 ms
Layer: TransformerLayer, Forward time: 2.034392 ms
Layer: TransformerLayer, Forward time: 1.952460 ms
Layer: TransformerLayer, Forward time: 1.986958 ms
Layer: TransformerLayer, Forward time: 1.962945 ms
Layer: TransformerLayer, Forward time: 1.805047 ms
Layer: TransformerLayer, Forward time: 1.975756 ms
Layer: TransformerLayer, Forward time: 1.989451 ms
Layer: TransformerLayer, Forward time: 1.953681 ms
Layer: TransformerLayer, Forward time: 2.014312 ms
Layer: TransformerLayer, Forward time: 25.893486 ms
Layer: TransformerLayer, Forward time: 2.080695 ms
Layer: TransformerLayer, Forward time: 2.041723 ms
Layer: TransformerLayer, Forward time: 2.026873 ms
Layer: TransformerLayer, Forward time: 1.902647 ms
Layer: TransformerLayer, Forward time: 1.971044 ms
Layer: TransformerLayer, Forward time: 1.963921 ms
Layer: TransformerLayer, Forward time: 25.899828 ms
Layer: TransformerLayer, Forward time: 22.842000 ms
Layer: TransformerLayer, Forward time: 25.959540 ms
Layer: TransformerLayer, Forward time: 24.041706 ms
Layer: TransformerLayer, Forward time: 25.313264 ms
Layer: TransformerLayer, Forward time: 25.327543 ms
Layer: TransformerLayer, Forward time: 25.245132 ms
Layer: TransformerLayer, Forward time: 24.746095 ms
Layer: TransformerLayer, Forward time: 1.837901 ms
Layer: TransformerLayer, Forward time: 3.110276 ms
Layer: TransformerLayer, Forward time: 1.713597 ms
Layer: TransformerLayer, Forward time: 1.674854 ms
Layer: TransformerLayer, Forward time: 2.018006 ms
Layer: TransformerLayer, Forward time: 2.190943 ms
Layer: TransformerLayer, Forward time: 2.059426 ms
Layer: TransformerLayer, Forward time: 2.067942 ms
Layer: TransformerLayer, Forward time: 2.013149 ms
Layer: TransformerLayer, Forward time: 2.041713 ms
Layer: TransformerLayer, Forward time: 2.033628 ms
Layer: TransformerLayer, Forward time: 1.904999 ms
Layer: TransformerLayer, Forward time: 2.447404 ms
Layer: TransformerLayer, Forward time: 2.127750 ms
Layer: TransformerLayer, Forward time: 2.110012 ms
Layer: TransformerLayer, Forward time: 2.042020 ms
Layer: TransformerLayer, Forward time: 2.205058 ms
Layer: TransformerLayer, Forward time: 2.117781 ms
Layer: TransformerLayer, Forward time: 1.981135 ms
Layer: TransformerLayer, Forward time: 1.840181 ms
Layer: TransformerLayer, Forward time: 2.012256 ms
Layer: TransformerLayer, Forward time: 2.000915 ms
Layer: TransformerLayer, Forward time: 1.945806 ms
Layer: TransformerLayer, Forward time: 2.005516 ms
Layer: TransformerLayer, Forward time: 23.095121 ms
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1473, in forward_backward_pipelining_without_interleaving
    output_tensor, num_tokens = forward_step(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 273, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 368, in forward_step
    output_tensor, loss_mask = model(images, tokens, position_ids, attention_mask, labels, loss_mask, num_image_tiles=num_image_tiles)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/distributed/distributed_data_parallel.py", line 308, in forward
    return self.module(*inputs, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/multimodal/llava_model.py", line 600, in forward
    output = self.language_model(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/gpt/gpt_model.py", line 231, in forward
    hidden_states = self.decoder(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_block.py", line 525, in forward
    hidden_states, context = layer(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 373, in __call__
    return super(MegatronModule, self).__call__(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 280, in forward
    attention_output_with_bias = self.self_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/attention.py", line 300, in forward
    core_attn_out = self.core_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/dot_product_attention.py", line 157, in forward
    attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 148, in forward
Layer: TransformerLayer, Forward time: 1.816481 ms
    return self.forward_torch_softmax(input, mask)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 206, in forward_torch_softmax
    probs = torch.nn.Softmax(dim=-1)(mask_output)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
Layer: TransformerLayer, Forward time: 1.946278 ms
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1514, in forward
Layer: TransformerLayer, Forward time: 1.852843 ms
Layer: TransformerLayer, Forward time: 1.820755 ms
Layer: TransformerLayer, Forward time: 2.175359 ms
Layer: TransformerLayer, Forward time: 2.126774 ms
Layer: TransformerLayer, Forward time: 2.064739 ms
Layer: TransformerLayer, Forward time: 2.008529 ms
Layer: TransformerLayer, Forward time: 1.970218 ms
Layer: TransformerLayer, Forward time: 1.973245 ms
Layer: TransformerLayer, Forward time: 1.957928 ms
Layer: TransformerLayer, Forward time: 1.793515 ms
Layer: TransformerLayer, Forward time: 2.017485 ms
Layer: TransformerLayer, Forward time: 1.972081 ms
Layer: TransformerLayer, Forward time: 1.978038 ms
Layer: TransformerLayer, Forward time: 1.981205 ms
Layer: TransformerLayer, Forward time: 2.051222 ms
Layer: TransformerLayer, Forward time: 1.970768 ms
Layer: TransformerLayer, Forward time: 1.985556 ms
Layer: TransformerLayer, Forward time: 1.863159 ms
Layer: TransformerLayer, Forward time: 2.034101 ms
Layer: TransformerLayer, Forward time: 2.142745 ms
Layer: TransformerLayer, Forward time: 2.050738 ms
Layer: TransformerLayer, Forward time: 2.055695 ms
Layer: TransformerLayer, Forward time: 23.327415 ms
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1473, in forward_backward_pipelining_without_interleaving
    output_tensor, num_tokens = forward_step(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 273, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 368, in forward_step
    output_tensor, loss_mask = model(images, tokens, position_ids, attention_mask, labels, loss_mask, num_image_tiles=num_image_tiles)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/distributed/distributed_data_parallel.py", line 308, in forward
    return self.module(*inputs, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/multimodal/llava_model.py", line 600, in forward
    output = self.language_model(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/gpt/gpt_model.py", line 231, in forward
    hidden_states = self.decoder(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_block.py", line 525, in forward
    hidden_states, context = layer(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 373, in __call__
    return super(MegatronModule, self).__call__(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 280, in forward
    attention_output_with_bias = self.self_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/attention.py", line 300, in forward
    core_attn_out = self.core_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/dot_product_attention.py", line 157, in forward
    attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 148, in forward
    return self.forward_torch_softmax(input, mask)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 206, in forward_torch_softmax
    probs = torch.nn.Softmax(dim=-1)(mask_output)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1514, in forward
        return F.softmax(input, self.dim, _stacklevel=5)return F.softmax(input, self.dim, _stacklevel=5)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/functional.py", line 1858, in softmax

  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/functional.py", line 1858, in softmax
    ret = input.softmax(dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 2 has a total capacity of 23.69 GiB of which 81.19 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 508.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    ret = input.softmax(dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacity of 23.69 GiB of which 81.19 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 508.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Layer: TransformerLayer, Forward time: 2.061419 ms
Layer: TransformerLayer, Forward time: 2.290639 ms
Layer: TransformerLayer, Forward time: 1.854679 ms
Layer: TransformerLayer, Forward time: 1.784421 ms
Layer: TransformerLayer, Forward time: 2.134735 ms
Layer: TransformerLayer, Forward time: 2.499558 ms
Layer: TransformerLayer, Forward time: 2.175805 ms
Layer: TransformerLayer, Forward time: 2.742742 ms
Layer: TransformerLayer, Forward time: 2.117722 ms
Layer: TransformerLayer, Forward time: 2.096161 ms
Layer: TransformerLayer, Forward time: 2.149784 ms
Layer: TransformerLayer, Forward time: 1.883865 ms
Layer: TransformerLayer, Forward time: 1.994884 ms
Layer: TransformerLayer, Forward time: 1.831800 ms
Layer: TransformerLayer, Forward time: 2.060890 ms
Layer: TransformerLayer, Forward time: 2.198985 ms
Layer: TransformerLayer, Forward time: 1.828651 ms
Layer: TransformerLayer, Forward time: 2.537920 ms
Layer: TransformerLayer, Forward time: 2.198147 ms
Layer: TransformerLayer, Forward time: 2.172801 ms
Layer: TransformerLayer, Forward time: 2.084142 ms
Layer: TransformerLayer, Forward time: 2.208773 ms
Layer: TransformerLayer, Forward time: 2.120885 ms
Layer: TransformerLayer, Forward time: 2.149247 ms
Layer: TransformerLayer, Forward time: 2.053348 ms
Layer: TransformerLayer, Forward time: 2.218826 ms
Layer: TransformerLayer, Forward time: 2.324011 ms
Layer: TransformerLayer, Forward time: 2.289192 ms
Layer: TransformerLayer, Forward time: 2.315730 ms
Layer: TransformerLayer, Forward time: 2.058701 ms
Layer: TransformerLayer, Forward time: 2.034794 ms
Layer: TransformerLayer, Forward time: 1.937913 ms
Layer: TransformerLayer, Forward time: 2.216103 ms
Layer: TransformerLayer, Forward time: 2.129520 ms
Layer: TransformerLayer, Forward time: 2.100491 ms
Layer: TransformerLayer, Forward time: 2.071424 ms
Layer: TransformerLayer, Forward time: 2.084390 ms
Layer: TransformerLayer, Forward time: 2.025085 ms
Layer: TransformerLayer, Forward time: 2.159136 ms
Layer: TransformerLayer, Forward time: 1.976269 ms
Layer: TransformerLayer, Forward time: 2.087654 ms
Layer: TransformerLayer, Forward time: 2.119287 ms
Layer: TransformerLayer, Forward time: 2.000485 ms
Layer: TransformerLayer, Forward time: 1.892451 ms
Layer: TransformerLayer, Forward time: 2.246460 ms
Layer: TransformerLayer, Forward time: 2.163192 ms
Layer: TransformerLayer, Forward time: 2.011721 ms
Layer: TransformerLayer, Forward time: 1.989992 ms
Layer: TransformerLayer, Forward time: 22.974877 ms
Layer: TransformerLayer, Forward time: 23.153695 ms
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1473, in forward_backward_pipelining_without_interleaving
    output_tensor, num_tokens = forward_step(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 273, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 368, in forward_step
    output_tensor, loss_mask = model(images, tokens, position_ids, attention_mask, labels, loss_mask, num_image_tiles=num_image_tiles)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/distributed/distributed_data_parallel.py", line 308, in forward
    return self.module(*inputs, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/multimodal/llava_model.py", line 600, in forward
    output = self.language_model(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/gpt/gpt_model.py", line 231, in forward
    hidden_states = self.decoder(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_block.py", line 525, in forward
    hidden_states, context = layer(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 373, in __call__
    return super(MegatronModule, self).__call__(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 280, in forward
    attention_output_with_bias = self.self_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/attention.py", line 300, in forward
    core_attn_out = self.core_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/dot_product_attention.py", line 157, in forward
    attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 148, in forward
    return self.forward_torch_softmax(input, mask)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 206, in forward_torch_softmax
    probs = torch.nn.Softmax(dim=-1)(mask_output)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1514, in forward
    return F.softmax(input, self.dim, _stacklevel=5)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/functional.py", line 1858, in softmax
    ret = input.softmax(dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 81.19 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 508.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 456, in <module>
    pretrain(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 407, in pretrain
    iteration, num_floating_point_operations_so_far = train( # 调用 1172 行进行训练
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 1375, in train
    train_step(forward_step_func, # 调用 805 行
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/training/training.py", line 854, in train_step
    losses_reduced = forward_backward_func( # schedules.py 1345 行 forward_backward_pipelining_without_interleaving
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 1473, in forward_backward_pipelining_without_interleaving
    output_tensor, num_tokens = forward_step(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/pipeline_parallel/schedules.py", line 273, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/train.py", line 368, in forward_step
    output_tensor, loss_mask = model(images, tokens, position_ids, attention_mask, labels, loss_mask, num_image_tiles=num_image_tiles)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/distributed/distributed_data_parallel.py", line 308, in forward
    return self.module(*inputs, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/multimodal/llava_model.py", line 600, in forward
    output = self.language_model(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/models/gpt/gpt_model.py", line 231, in forward
    hidden_states = self.decoder(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_block.py", line 525, in forward
    hidden_states, context = layer(
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 373, in __call__
    return super(MegatronModule, self).__call__(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/transformer_layer.py", line 280, in forward
    attention_output_with_bias = self.self_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/attention.py", line 300, in forward
    core_attn_out = self.core_attention(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/transformer/dot_product_attention.py", line 157, in forward
    attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 148, in forward
    return self.forward_torch_softmax(input, mask)
  File "/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/fusions/fused_softmax.py", line 206, in forward_torch_softmax
    probs = torch.nn.Softmax(dim=-1)(mask_output)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1514, in forward
    return F.softmax(input, self.dim, _stacklevel=5)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/nn/functional.py", line 1858, in softmax
    ret = input.softmax(dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacity of 23.69 GiB of which 81.19 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 508.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-12-31 12:44:43,811] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 15237) of binary: /gf3/home/fjs/anaconda3/envs/megatron/bin/python3.10
Traceback (most recent call last):
  File "/gf3/home/fjs/anaconda3/envs/megatron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gf3/home/fjs/anaconda3/envs/megatron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
examples/multimodal/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-12-31_12:44:43
  host      : gn49
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 15238)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-12-31_12:44:43
  host      : gn49
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 15239)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-12-31_12:44:43
  host      : gn49
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 15240)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-31_12:44:43
  host      : gn49
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 15237)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
