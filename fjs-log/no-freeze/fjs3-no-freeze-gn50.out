examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
4
[2024-12-05 23:06:50,703] torch.distributed.run: [WARNING] 
[2024-12-05 23:06:50,703] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 23:06:50,703] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 23:06:50,703] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]


---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 687968256
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 687968256
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 687968256
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (687968256 elements):
	language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.9.mlp.linear_fc2.weight
	language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.8.self_attention.linear_proj.weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.11.self_attention.linear_qkv.weight
	language_model.decoder.layers.9.mlp.linear_fc1.weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.11.mlp.linear_fc1.weight
	language_model.decoder.layers.10.mlp.linear_fc2.weight
	language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.9.self_attention.linear_proj.weight
	language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.output_layer.weight
	language_model.decoder.layers.11.mlp.linear_fc2.weight
	language_model.decoder.layers.10.mlp.linear_fc1.weight
	language_model.decoder.layers.8.self_attention.linear_qkv.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.10.self_attention.linear_proj.weight
	language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.9.self_attention.linear_qkv.weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.11.self_attention.linear_proj.weight
	language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.8.mlp.linear_fc2.weight
	language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.10.self_attention.linear_qkv.weight
	language_model.decoder.layers.8.mlp.linear_fc1.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True

name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 687968256
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.8.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.9.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.10.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.11.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 101.03
     rank  1: 100.62
     rank  2: 101.75
     rank  3: 102.16
     rank  4: 55.14
     rank  5: 52.36
     rank  6: 56.02
     rank  7: 55.66
     rank  8: 59.11
     rank  9: 59.09
     rank 10: 59.36
     rank 11: 59.46
     rank 12: 69.51
     rank 13: 69.50
     rank 14: 72.10
     rank 15: 69.07
     rank 16: 48.20
     rank 17: 50.08
     rank 18: 48.16
     rank 19: 67.24
  train/valid/test-data-iterators-setup:
     rank  0: 1027.49
     rank  1: 1027.47
     rank  2: 1242.55
     rank  3: 1027.51
     rank  4: 1237.62
     rank  5: 1237.91
     rank  6: 1027.69
     rank  7: 1237.60
     rank  8: 1242.19
     rank  9: 1242.47
     rank 10: 1237.97
     rank 11: 1242.33
     rank 12: 1242.15
     rank 13: 1242.22
     rank 14: 1242.17
     rank 15: 1242.14
     rank 16: 1242.18
     rank 17: 1242.17
     rank 18: 1242.14
     rank 19: 1242.21
 [2024-12-05 23:07:32] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21162.8 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 7.001200E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 10582.8154296875 | max allocated: 13206.041015625 | reserved: 14738.0 | max reserved: 14738.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 10582.8154296875 | max allocated: 13206.041015625 | reserved: 14698.0 | max reserved: 14698.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 10582.8154296875 | max allocated: 13206.041015625 | reserved: 14730.0 | max reserved: 14730.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 10582.8154296875 | max allocated: 13206.041015625 | reserved: 14722.0 | max reserved: 14722.0
times across ranks (ms):
  forward-backward:
     rank  0: 21073.32
     rank  1: 21073.96
     rank  2: 21073.08
     rank  3: 21073.16
     rank  4: 21072.09
     rank  5: 21071.68
     rank  6: 21071.98
     rank  7: 21071.87
     rank  8: 21073.02
     rank  9: 21072.63
     rank 10: 21072.66
     rank 11: 21072.59
     rank 12: 21073.60
     rank 13: 21073.42
     rank 14: 21073.46
     rank 15: 21073.33
     rank 16: 21073.45
     rank 17: 21073.51
     rank 18: 21073.43
     rank 19: 21073.45
  forward-compute:
     rank  0: 3922.16
     rank  1: 3932.09
     rank  2: 3915.12
     rank  3: 3913.53
     rank  4: 3841.79
     rank  5: 3851.98
     rank  6: 3829.52
     rank  7: 3831.65
     rank  8: 4930.38
     rank  9: 4939.39
     rank 10: 4933.03
     rank 11: 4922.46
     rank 12: 6362.21
     rank 13: 6362.92
     rank 14: 6353.61
     rank 15: 6352.86
     rank 16: 6602.18
     rank 17: 6611.56
     rank 18: 6611.20
     rank 19: 6610.09
  backward-compute:
     rank  0: 2758.71
     rank  1: 2760.46
     rank  2: 2758.21
     rank  3: 2763.26
     rank  4: 2583.31
     rank  5: 2589.06
     rank  6: 2607.19
     rank  7: 2592.71
     rank  8: 4168.57
     rank  9: 4168.60
     rank 10: 4160.77
     rank 11: 4165.47
     rank 12: 4981.98
     rank 13: 4985.52
     rank 14: 4983.27
     rank 15: 4984.55
     rank 16: 5123.51
     rank 17: 5128.06
     rank 18: 5122.78
     rank 19: 5118.03
  pure-backward-compute:
     rank  0: 2757.73
     rank  1: 2759.45
     rank  2: 2757.31
     rank  3: 2762.33
     rank  4: 2581.77
     rank  5: 2587.65
     rank  6: 2605.58
     rank  7: 2590.93
     rank  8: 4166.64
     rank  9: 4166.46
     rank 10: 4158.68
     rank 11: 4163.63
     rank 12: 4980.46
     rank 13: 4983.85
     rank 14: 4981.28
     rank 15: 4983.43
     rank 16: 5120.40
     rank 17: 5126.22
     rank 18: 5120.92
     rank 19: 5114.98
  batch-generator:
     rank  0: 1052.42
     rank  1: 1068.57
     rank  2: 1057.27
     rank  3: 1054.62
     rank  4: 1322.88
     rank  5: 1339.95
     rank  6: 1329.25
     rank  7: 1334.30
     rank  8: 1408.80
     rank  9: 1433.49
     rank 10: 1434.37
     rank 11: 1425.43
     rank 12: 931.15
     rank 13: 936.76
     rank 14: 928.92
     rank 15: 925.96
     rank 16: 1023.55
     rank 17: 1033.89
     rank 18: 1028.00
     rank 19: 1028.57
  forward-recv:
     rank  4: 2228.22
     rank  5: 2224.51
     rank  6: 2222.70
     rank  7: 2224.06
     rank  8: 3825.56
     rank  9: 3819.79
     rank 10: 3826.45
     rank 11: 3825.04
     rank 12: 5501.46
     rank 13: 5508.20
     rank 14: 5512.36
     rank 15: 5514.20
     rank 16: 7976.83
     rank 17: 7969.96
     rank 18: 7974.40
     rank 19: 7975.83
  forward-send:
     rank  0: 5518.69
     rank  1: 5508.48
     rank  2: 5523.11
     rank  3: 5525.69
     rank  4: 3882.23
     rank  5: 3875.73
     rank  6: 3891.98
     rank  7: 3893.10
     rank  8: 2282.86
     rank  9: 2282.42
     rank 10: 2291.61
     rank 11: 2294.50
     rank 12: 38.20
     rank 13: 31.05
     rank 14: 35.62
     rank 15: 37.46
  backward-recv:
     rank  0: 1899.58
     rank  1: 1899.75
     rank  2: 1900.12
     rank  3: 1899.78
     rank  4: 1314.17
     rank  5: 1313.63
     rank  6: 1312.93
     rank  7: 1311.24
     rank  8: 652.54
     rank  9: 652.32
     rank 10: 655.00
     rank 11: 654.58
     rank 12: 276.89
     rank 13: 277.03
     rank 14: 276.70
     rank 15: 276.55
  backward-send:
     rank  4: 4.79
     rank  5: 4.44
     rank  6: 4.86
     rank  7: 4.70
     rank  8: 31.47
     rank  9: 31.26
     rank 10: 31.67
     rank 11: 29.74
     rank 12: 21.82
     rank 13: 20.85
     rank 14: 22.01
     rank 15: 21.95
     rank 16: 10.46
     rank 17: 10.23
     rank 18: 10.49
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 6888.53
     rank  1: 6887.01
     rank  2: 6892.05
     rank  3: 6887.60
     rank  4: 6393.90
     rank  5: 6390.63
     rank  6: 6374.13
     rank  7: 6390.52
     rank  8: 4319.58
     rank  9: 4322.01
     rank 10: 4328.30
     rank 11: 4324.50
     rank 12: 2868.32
     rank 13: 2865.23
     rank 14: 2865.27
     rank 15: 2865.69
  backward-send-forward-recv:
     rank  4: 625.64
     rank  5: 623.24
     rank  6: 627.06
     rank  7: 622.37
     rank  8: 498.07
     rank  9: 495.66
     rank 10: 485.51
     rank 11: 495.33
     rank 12: 390.10
     rank 13: 389.26
     rank 14: 389.61
     rank 15: 388.42
     rank 16: 405.30
     rank 17: 402.56
     rank 18: 403.89
     rank 19: 403.90
  layernorm-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.06
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.08
     rank  5: 0.08
     rank  6: 0.08
     rank  7: 0.06
     rank  8: 0.08
     rank  9: 0.08
     rank 10: 0.08
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.03
     rank  3: 0.04
     rank  4: 0.24
     rank  5: 0.22
     rank  6: 0.24
     rank  7: 0.17
     rank  8: 0.07
     rank  9: 0.08
     rank 10: 0.06
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 53.23
     rank  1: 56.33
     rank  2: 56.54
     rank  3: 56.08
     rank  4: 62.94
     rank  5: 58.22
     rank  6: 60.56
     rank  7: 62.21
     rank  8: 60.30
     rank  9: 58.18
     rank 10: 61.44
     rank 11: 62.88
     rank 12: 46.69
     rank 13: 41.33
     rank 14: 43.64
     rank 15: 41.68
     rank 16: 3.60
     rank 17: 3.68
     rank 18: 3.47
     rank 19: 3.70
  optimizer-copy-to-main-grad:
     rank  0: 0.32
     rank  1: 0.31
     rank  2: 0.30
     rank  3: 0.30
     rank  4: 0.10
     rank  5: 0.10
     rank  6: 0.11
     rank  7: 0.07
     rank  8: 0.10
     rank  9: 0.08
     rank 10: 0.12
     rank 11: 0.09
     rank 12: 0.08
     rank 13: 0.09
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.12
     rank 19: 0.13
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.09
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 48.99
     rank  1: 49.08
     rank  2: 48.77
     rank  3: 49.04
     rank  4: 55.99
     rank  5: 52.33
     rank  6: 56.15
     rank  7: 55.92
     rank  8: 64.09
     rank  9: 64.17
     rank 10: 65.55
     rank 11: 64.72
     rank 12: 76.56
     rank 13: 76.92
     rank 14: 76.88
     rank 15: 76.72
     rank 16: 80.64
     rank 17: 80.96
     rank 18: 80.31
     rank 19: 81.05
  optimizer:
     rank  0: 50.60
     rank  1: 50.66
     rank  2: 50.38
     rank  3: 50.65
     rank  4: 57.58
     rank  5: 53.91
     rank  6: 57.73
     rank  7: 57.50
     rank  8: 65.69
     rank  9: 65.78
     rank 10: 67.17
     rank 11: 66.33
     rank 12: 78.17
     rank 13: 78.54
     rank 14: 78.50
     rank 15: 78.33
     rank 16: 82.26
     rank 17: 82.57
     rank 18: 81.92
     rank 19: 82.66
 [2024-12-05 23:07:43] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 11364.4 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.253815E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11256.64
     rank  1: 11256.66
     rank  2: 11256.66
     rank  3: 11256.67
     rank  4: 11258.50
     rank  5: 11258.50
     rank  6: 11258.76
     rank  7: 11258.56
     rank  8: 11259.41
     rank  9: 11259.37
     rank 10: 11259.57
     rank 11: 11259.43
     rank 12: 11261.33
     rank 13: 11261.37
     rank 14: 11261.55
     rank 15: 11261.33
     rank 16: 11261.59
     rank 17: 11261.60
     rank 18: 11261.61
     rank 19: 11261.64
  forward-compute:
     rank  0: 1911.72
     rank  1: 1914.80
     rank  2: 1917.98
     rank  3: 1913.84
     rank  4: 2388.42
     rank  5: 2389.90
     rank  6: 2388.07
     rank  7: 2385.65
     rank  8: 3471.39
     rank  9: 3475.77
     rank 10: 3491.71
     rank 11: 3473.88
     rank 12: 4140.55
     rank 13: 4140.03
     rank 14: 4142.17
     rank 15: 4139.12
     rank 16: 4274.18
     rank 17: 4274.25
     rank 18: 4274.57
     rank 19: 4273.62
  backward-compute:
     rank  0: 1949.52
     rank  1: 1949.51
     rank  2: 1949.80
     rank  3: 1951.35
     rank  4: 2530.98
     rank  5: 2535.14
     rank  6: 2559.73
     rank  7: 2538.66
     rank  8: 4168.04
     rank  9: 4166.50
     rank 10: 4158.57
     rank 11: 4161.22
     rank 12: 4953.52
     rank 13: 4952.24
     rank 14: 4953.37
     rank 15: 4954.19
     rank 16: 5123.94
     rank 17: 5130.11
     rank 18: 5131.13
     rank 19: 5126.25
  pure-backward-compute:
     rank  0: 1948.67
     rank  1: 1948.62
     rank  2: 1948.98
     rank  3: 1950.38
     rank  4: 2529.83
     rank  5: 2533.90
     rank  6: 2558.30
     rank  7: 2537.14
     rank  8: 4166.20
     rank  9: 4164.99
     rank 10: 4157.16
     rank 11: 4159.61
     rank 12: 4952.52
     rank 13: 4950.82
     rank 14: 4952.27
     rank 15: 4953.24
     rank 16: 5121.41
     rank 17: 5128.48
     rank 18: 5129.37
     rank 19: 5123.85
  batch-generator:
     rank  0: 78.98
     rank  1: 85.90
     rank  2: 93.29
     rank  3: 87.64
     rank  4: 72.84
     rank  5: 78.31
     rank  6: 85.61
     rank  7: 80.65
     rank  8: 79.05
     rank  9: 85.91
     rank 10: 109.91
     rank 11: 88.74
     rank 12: 57.54
     rank 13: 60.16
     rank 14: 61.67
     rank 15: 58.44
     rank 16: 50.85
     rank 17: 53.49
     rank 18: 54.73
     rank 19: 55.90
  forward-recv:
     rank  4: 126.44
     rank  5: 126.55
     rank  6: 124.26
     rank  7: 124.98
     rank  8: 309.26
     rank  9: 308.19
     rank 10: 306.45
     rank 11: 308.49
     rank 12: 528.78
     rank 13: 528.16
     rank 14: 527.72
     rank 15: 528.34
     rank 16: 788.10
     rank 17: 787.71
     rank 18: 787.84
     rank 19: 788.19
  forward-send:
     rank  0: 318.22
     rank  1: 314.83
     rank  2: 311.39
     rank  3: 315.81
     rank  4: 222.65
     rank  5: 219.61
     rank  6: 218.36
     rank  7: 222.09
     rank  8: 64.59
     rank  9: 63.02
     rank 10: 63.39
     rank 11: 64.76
     rank 12: 10.52
     rank 13: 9.96
     rank 14: 10.14
     rank 15: 10.56
  backward-recv:
     rank  0: 1905.55
     rank  1: 1905.84
     rank  2: 1905.82
     rank  3: 1905.80
     rank  4: 1316.58
     rank  5: 1316.32
     rank  6: 1312.83
     rank  7: 1314.04
     rank  8: 657.59
     rank  9: 658.44
     rank 10: 661.19
     rank 11: 658.86
     rank 12: 279.97
     rank 13: 279.91
     rank 14: 280.07
     rank 15: 280.94
  backward-send:
     rank  4: 4.50
     rank  5: 4.58
     rank  6: 4.32
     rank  7: 5.54
     rank  8: 31.91
     rank  9: 31.69
     rank 10: 29.20
     rank 11: 31.08
     rank 12: 21.93
     rank 13: 21.53
     rank 14: 21.60
     rank 15: 21.15
     rank 16: 10.36
     rank 17: 10.47
     rank 18: 10.39
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 5151.18
     rank  1: 5151.44
     rank  2: 5153.50
     rank  3: 5151.94
     rank  4: 4503.99
     rank  5: 4502.09
     rank  6: 4484.08
     rank  7: 4501.45
     rank  8: 2066.61
     rank  9: 2068.25
     rank 10: 2075.78
     rank 11: 2075.90
     rank 12: 567.79
     rank 13: 569.28
     rank 14: 567.53
     rank 15: 567.91
  backward-send-forward-recv:
     rank  4: 30.26
     rank  5: 29.96
     rank  6: 30.31
     rank  7: 30.60
     rank  8: 191.29
     rank  9: 189.95
     rank 10: 175.26
     rank 11: 189.39
     rank 12: 187.26
     rank 13: 188.59
     rank 14: 187.65
     rank 15: 188.76
     rank 16: 168.56
     rank 17: 169.83
     rank 18: 168.49
     rank 19: 169.14
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.08
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.97
     rank  1: 0.97
     rank  2: 0.97
     rank  3: 1.07
     rank  4: 1.83
     rank  5: 1.96
     rank  6: 2.16
     rank  7: 1.77
     rank  8: 2.23
     rank  9: 2.13
     rank 10: 2.35
     rank 11: 2.24
     rank 12: 3.21
     rank 13: 3.38
     rank 14: 3.38
     rank 15: 3.23
     rank 16: 3.34
     rank 17: 3.31
     rank 18: 3.35
     rank 19: 3.37
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.23
     rank  3: 0.23
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.08
     rank  7: 0.04
     rank  8: 0.09
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.08
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.94
     rank  1: 15.88
     rank  2: 15.99
     rank  3: 16.11
     rank  4: 34.08
     rank  5: 34.41
     rank  6: 34.72
     rank  7: 33.79
     rank  8: 43.44
     rank  9: 43.40
     rank 10: 43.42
     rank 11: 43.48
     rank 12: 64.35
     rank 13: 64.50
     rank 14: 64.59
     rank 15: 64.47
     rank 16: 67.75
     rank 17: 67.63
     rank 18: 67.72
     rank 19: 67.70
  optimizer:
     rank  0: 17.09
     rank  1: 17.03
     rank  2: 17.15
     rank  3: 17.26
     rank  4: 35.24
     rank  5: 35.56
     rank  6: 35.74
     rank  7: 34.95
     rank  8: 44.60
     rank  9: 44.55
     rank 10: 44.58
     rank 11: 44.64
     rank 12: 65.50
     rank 13: 65.65
     rank 14: 65.74
     rank 15: 65.62
     rank 16: 68.90
     rank 17: 68.79
     rank 18: 68.87
     rank 19: 68.85
 [2024-12-05 23:07:55] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 11382.8 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 2.804387E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11276.32
     rank  1: 11276.32
     rank  2: 11276.38
     rank  3: 11276.42
     rank  4: 11278.22
     rank  5: 11278.29
     rank  6: 11278.24
     rank  7: 11278.43
     rank  8: 11279.16
     rank  9: 11279.18
     rank 10: 11279.18
     rank 11: 11279.24
     rank 12: 11281.03
     rank 13: 11281.14
     rank 14: 11281.13
     rank 15: 11281.19
     rank 16: 11281.32
     rank 17: 11281.29
     rank 18: 11281.32
     rank 19: 11281.34
  forward-compute:
     rank  0: 2115.05
     rank  1: 2114.96
     rank  2: 2119.39
     rank  3: 2121.24
     rank  4: 2382.03
     rank  5: 2380.48
     rank  6: 2379.13
     rank  7: 2382.22
     rank  8: 3483.74
     rank  9: 3482.33
     rank 10: 3499.84
     rank 11: 3492.46
     rank 12: 4149.40
     rank 13: 4147.60
     rank 14: 4148.60
     rank 15: 4150.56
     rank 16: 4284.56
     rank 17: 4285.26
     rank 18: 4284.41
     rank 19: 4283.00
  backward-compute:
     rank  0: 1943.33
     rank  1: 1943.28
     rank  2: 1944.38
     rank  3: 1943.32
     rank  4: 2526.56
     rank  5: 2527.99
     rank  6: 2551.38
     rank  7: 2538.82
     rank  8: 4185.33
     rank  9: 4186.09
     rank 10: 4177.25
     rank 11: 4180.34
     rank 12: 4960.53
     rank 13: 4961.57
     rank 14: 4961.20
     rank 15: 4960.77
     rank 16: 5138.24
     rank 17: 5145.66
     rank 18: 5146.33
     rank 19: 5143.84
  pure-backward-compute:
     rank  0: 1942.50
     rank  1: 1942.45
     rank  2: 1943.61
     rank  3: 1942.55
     rank  4: 2525.48
     rank  5: 2526.82
     rank  6: 2549.92
     rank  7: 2537.01
     rank  8: 4183.79
     rank  9: 4184.02
     rank 10: 4176.02
     rank 11: 4179.07
     rank 12: 4959.42
     rank 13: 4960.14
     rank 14: 4960.07
     rank 15: 4959.71
     rank 16: 5136.04
     rank 17: 5144.14
     rank 18: 5144.85
     rank 19: 5141.49
  batch-generator:
     rank  0: 73.81
     rank  1: 78.61
     rank  2: 86.30
     rank  3: 87.09
     rank  4: 69.08
     rank  5: 71.81
     rank  6: 79.50
     rank  7: 83.56
     rank  8: 73.75
     rank  9: 77.54
     rank 10: 102.14
     rank 11: 91.44
     rank 12: 50.98
     rank 13: 53.52
     rank 14: 57.59
     rank 15: 57.50
     rank 16: 51.33
     rank 17: 56.15
     rank 18: 55.86
     rank 19: 56.52
  forward-recv:
     rank  4: 122.10
     rank  5: 122.54
     rank  6: 121.22
     rank  7: 120.89
     rank  8: 301.09
     rank  9: 300.49
     rank 10: 297.78
     rank 11: 297.43
     rank 12: 519.35
     rank 13: 519.43
     rank 14: 518.46
     rank 15: 518.00
     rank 16: 778.05
     rank 17: 778.14
     rank 18: 778.14
     rank 19: 777.74
  forward-send:
     rank  0: 313.97
     rank  1: 313.78
     rank  2: 308.94
     rank  3: 307.48
     rank  4: 222.98
     rank  5: 223.00
     rank  6: 219.70
     rank  7: 217.47
     rank  8: 62.26
     rank  9: 63.01
     rank 10: 62.49
     rank 11: 60.72
     rank 12: 10.36
     rank 13: 10.58
     rank 14: 10.58
     rank 15: 9.95
  backward-recv:
     rank  0: 1920.04
     rank  1: 1920.06
     rank  2: 1919.52
     rank  3: 1920.25
     rank  4: 1325.20
     rank  5: 1325.25
     rank  6: 1321.31
     rank  7: 1322.53
     rank  8: 660.52
     rank  9: 660.61
     rank 10: 663.67
     rank 11: 662.48
     rank 12: 280.04
     rank 13: 279.89
     rank 14: 280.61
     rank 15: 280.52
  backward-send:
     rank  4: 4.49
     rank  5: 4.60
     rank  6: 3.60
     rank  7: 5.35
     rank  8: 31.66
     rank  9: 31.74
     rank 10: 28.64
     rank 11: 30.81
     rank 12: 22.66
     rank 13: 22.09
     rank 14: 21.59
     rank 15: 22.35
     rank 16: 10.30
     rank 17: 10.07
     rank 18: 10.21
     rank 19: 10.45
  forward-send-backward-recv:
     rank  0: 4963.45
     rank  1: 4963.81
     rank  2: 4965.23
     rank  3: 4965.64
     rank  4: 4529.34
     rank  5: 4529.53
     rank  6: 4513.77
     rank  7: 4522.83
     rank  8: 2060.17
     rank  9: 2063.24
     rank 10: 2072.13
     rank 11: 2069.86
     rank 12: 577.20
     rank 13: 577.00
     rank 14: 578.08
     rank 15: 577.99
  backward-send-forward-recv:
     rank  4: 29.59
     rank  5: 29.13
     rank  6: 29.73
     rank  7: 30.22
     rank  8: 192.13
     rank  9: 192.70
     rank 10: 179.00
     rank 11: 188.52
     rank 12: 186.38
     rank 13: 187.67
     rank 14: 187.69
     rank 15: 186.78
     rank 16: 168.35
     rank 17: 167.25
     rank 18: 167.70
     rank 19: 168.41
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.06
     rank  7: 0.08
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.92
     rank  1: 0.91
     rank  2: 0.97
     rank  3: 0.91
     rank  4: 1.80
     rank  5: 1.83
     rank  6: 1.80
     rank  7: 1.94
     rank  8: 2.24
     rank  9: 2.18
     rank 10: 2.21
     rank 11: 2.17
     rank 12: 3.21
     rank 13: 3.40
     rank 14: 3.28
     rank 15: 3.22
     rank 16: 3.34
     rank 17: 3.31
     rank 18: 3.33
     rank 19: 3.37
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.22
     rank  3: 0.21
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.07
     rank  8: 0.09
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.07
     rank 15: 0.08
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.12
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.49
     rank  1: 15.92
     rank  2: 15.91
     rank  3: 15.85
     rank  4: 33.86
     rank  5: 34.33
     rank  6: 34.25
     rank  7: 34.00
     rank  8: 43.61
     rank  9: 43.36
     rank 10: 43.38
     rank 11: 43.38
     rank 12: 64.42
     rank 13: 64.51
     rank 14: 64.55
     rank 15: 64.55
     rank 16: 67.78
     rank 17: 67.65
     rank 18: 67.71
     rank 19: 67.74
  optimizer:
     rank  0: 16.36
     rank  1: 16.79
     rank  2: 16.77
     rank  3: 16.71
     rank  4: 34.73
     rank  5: 35.20
     rank  6: 35.12
     rank  7: 34.86
     rank  8: 44.48
     rank  9: 44.23
     rank 10: 44.25
     rank 11: 44.25
     rank 12: 65.28
     rank 13: 65.38
     rank 14: 65.41
     rank 15: 65.41
     rank 16: 68.64
     rank 17: 68.51
     rank 18: 68.57
     rank 19: 68.60
 [2024-12-05 23:08:06] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 11362.9 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 6.040645E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11256.72
     rank  1: 11256.71
     rank  2: 11256.81
     rank  3: 11256.72
     rank  4: 11258.65
     rank  5: 11258.63
     rank  6: 11258.63
     rank  7: 11258.60
     rank  8: 11259.47
     rank  9: 11259.52
     rank 10: 11259.50
     rank 11: 11259.45
     rank 12: 11261.39
     rank 13: 11261.47
     rank 14: 11261.54
     rank 15: 11261.39
     rank 16: 11261.69
     rank 17: 11261.69
     rank 18: 11261.77
     rank 19: 11261.70
  forward-compute:
     rank  0: 1906.15
     rank  1: 1903.85
     rank  2: 1910.04
     rank  3: 1914.24
     rank  4: 2386.10
     rank  5: 2381.43
     rank  6: 2383.05
     rank  7: 2387.79
     rank  8: 3494.67
     rank  9: 3491.94
     rank 10: 3513.06
     rank 11: 3504.23
     rank 12: 4151.63
     rank 13: 4151.33
     rank 14: 4150.40
     rank 15: 4155.73
     rank 16: 4250.16
     rank 17: 4250.72
     rank 18: 4251.37
     rank 19: 4251.08
  backward-compute:
     rank  0: 1935.18
     rank  1: 1934.63
     rank  2: 1936.09
     rank  3: 1934.71
     rank  4: 2532.97
     rank  5: 2534.00
     rank  6: 2560.73
     rank  7: 2544.60
     rank  8: 4230.27
     rank  9: 4228.90
     rank 10: 4219.76
     rank 11: 4225.89
     rank 12: 4971.22
     rank 13: 4970.85
     rank 14: 4971.94
     rank 15: 4971.48
     rank 16: 5153.64
     rank 17: 5159.25
     rank 18: 5158.74
     rank 19: 5154.19
  pure-backward-compute:
     rank  0: 1934.36
     rank  1: 1933.77
     rank  2: 1935.13
     rank  3: 1933.92
     rank  4: 2531.91
     rank  5: 2532.73
     rank  6: 2559.25
     rank  7: 2542.70
     rank  8: 4228.65
     rank  9: 4227.30
     rank 10: 4218.43
     rank 11: 4224.63
     rank 12: 4970.17
     rank 13: 4969.58
     rank 14: 4970.33
     rank 15: 4970.48
     rank 16: 5151.05
     rank 17: 5157.53
     rank 18: 5157.21
     rank 19: 5152.16
  batch-generator:
     rank  0: 71.38
     rank  1: 74.05
     rank  2: 82.63
     rank  3: 85.40
     rank  4: 68.66
     rank  5: 68.58
     rank  6: 79.28
     rank  7: 85.44
     rank  8: 71.61
     rank  9: 73.55
     rank 10: 102.36
     rank 11: 90.06
     rank 12: 49.87
     rank 13: 53.14
     rank 14: 54.76
     rank 15: 58.03
     rank 16: 58.03
     rank 17: 58.96
     rank 18: 60.10
     rank 19: 61.47
  forward-recv:
     rank  4: 122.24
     rank  5: 122.45
     rank  6: 121.07
     rank  7: 121.30
     rank  8: 299.49
     rank  9: 300.64
     rank 10: 296.73
     rank 11: 295.20
     rank 12: 519.27
     rank 13: 519.14
     rank 14: 517.95
     rank 15: 517.32
     rank 16: 777.60
     rank 17: 777.76
     rank 18: 777.78
     rank 19: 777.14
  forward-send:
     rank  0: 315.40
     rank  1: 317.42
     rank  2: 310.53
     rank  3: 307.21
     rank  4: 221.91
     rank  5: 224.31
     rank  6: 218.81
     rank  7: 214.69
     rank  8: 62.85
     rank  9: 63.58
     rank 10: 62.92
     rank 11: 60.80
     rank 12: 10.30
     rank 13: 10.58
     rank 14: 10.53
     rank 15: 9.80
  backward-recv:
     rank  0: 1918.03
     rank  1: 1917.85
     rank  2: 1917.27
     rank  3: 1918.58
     rank  4: 1317.91
     rank  5: 1316.95
     rank  6: 1314.08
     rank  7: 1314.61
     rank  8: 650.58
     rank  9: 651.13
     rank 10: 654.21
     rank 11: 652.32
     rank 12: 279.53
     rank 13: 279.94
     rank 14: 279.69
     rank 15: 279.38
  backward-send:
     rank  4: 4.50
     rank  5: 4.43
     rank  6: 3.81
     rank  7: 5.32
     rank  8: 31.77
     rank  9: 31.67
     rank 10: 29.12
     rank 11: 31.11
     rank 12: 22.48
     rank 13: 22.68
     rank 14: 23.04
     rank 15: 23.02
     rank 16: 10.41
     rank 17: 10.31
     rank 18: 10.28
     rank 19: 10.51
  forward-send-backward-recv:
     rank  0: 5160.68
     rank  1: 5161.67
     rank  2: 5163.47
     rank  3: 5163.44
     rank  4: 4507.65
     rank  5: 4509.16
     rank  6: 4488.35
     rank  7: 4502.08
     rank  8: 1999.92
     rank  9: 2002.98
     rank 10: 2011.74
     rank 11: 2007.04
     rank 12: 543.49
     rank 13: 543.54
     rank 14: 544.11
     rank 15: 543.43
  backward-send-forward-recv:
     rank  4: 30.22
     rank  5: 30.33
     rank  6: 30.69
     rank  7: 31.02
     rank  8: 189.84
     rank  9: 190.82
     rank 10: 174.19
     rank 11: 186.80
     rank 12: 187.96
     rank 13: 188.62
     rank 14: 189.14
     rank 15: 186.47
     rank 16: 168.32
     rank 17: 167.75
     rank 18: 167.76
     rank 19: 168.21
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.08
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.93
     rank  1: 0.94
     rank  2: 0.96
     rank  3: 0.95
     rank  4: 1.91
     rank  5: 1.81
     rank  6: 1.84
     rank  7: 1.79
     rank  8: 2.28
     rank  9: 2.15
     rank 10: 2.16
     rank 11: 2.15
     rank 12: 3.18
     rank 13: 3.24
     rank 14: 3.35
     rank 15: 3.17
     rank 16: 3.31
     rank 17: 3.32
     rank 18: 3.84
     rank 19: 3.33
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.24
     rank  2: 0.22
     rank  3: 0.21
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.08
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.22
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.63
     rank  1: 15.55
     rank  2: 15.73
     rank  3: 15.81
     rank  4: 33.91
     rank  5: 34.22
     rank  6: 34.27
     rank  7: 33.90
     rank  8: 43.40
     rank  9: 43.42
     rank 10: 43.20
     rank 11: 43.36
     rank 12: 64.45
     rank 13: 64.41
     rank 14: 64.60
     rank 15: 64.49
     rank 16: 67.70
     rank 17: 67.70
     rank 18: 67.79
     rank 19: 67.70
  optimizer:
     rank  0: 16.94
     rank  1: 16.95
     rank  2: 17.04
     rank  3: 17.12
     rank  4: 35.22
     rank  5: 35.55
     rank  6: 35.58
     rank  7: 35.17
     rank  8: 44.70
     rank  9: 44.75
     rank 10: 44.51
     rank 11: 44.67
     rank 12: 65.76
     rank 13: 65.71
     rank 14: 65.90
     rank 15: 65.79
     rank 16: 69.00
     rank 17: 69.00
     rank 18: 68.97
     rank 19: 69.00
 [2024-12-05 23:08:17] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 11375.4 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.979146E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11270.93
     rank  1: 11270.99
     rank  2: 11271.01
     rank  3: 11270.93
     rank  4: 11272.81
     rank  5: 11272.77
     rank  6: 11272.89
     rank  7: 11272.83
     rank  8: 11273.70
     rank  9: 11273.68
     rank 10: 11273.74
     rank 11: 11273.79
     rank 12: 11275.64
     rank 13: 11275.62
     rank 14: 11275.71
     rank 15: 11275.69
     rank 16: 11275.91
     rank 17: 11275.89
     rank 18: 11275.91
     rank 19: 11275.92
  forward-compute:
     rank  0: 1912.41
     rank  1: 1912.86
     rank  2: 1918.14
     rank  3: 1915.88
     rank  4: 2409.89
     rank  5: 2410.00
     rank  6: 2409.37
     rank  7: 2407.29
     rank  8: 3490.68
     rank  9: 3489.34
     rank 10: 3509.64
     rank 11: 3493.23
     rank 12: 4163.78
     rank 13: 4160.88
     rank 14: 4163.16
     rank 15: 4168.13
     rank 16: 4277.74
     rank 17: 4277.57
     rank 18: 4275.99
     rank 19: 4275.71
  backward-compute:
     rank  0: 1937.78
     rank  1: 1938.59
     rank  2: 1936.35
     rank  3: 1938.53
     rank  4: 2542.10
     rank  5: 2543.69
     rank  6: 2569.06
     rank  7: 2551.33
     rank  8: 4214.21
     rank  9: 4211.49
     rank 10: 4205.01
     rank 11: 4212.50
     rank 12: 4960.34
     rank 13: 4960.56
     rank 14: 4959.53
     rank 15: 4959.59
     rank 16: 5133.21
     rank 17: 5136.81
     rank 18: 5137.84
     rank 19: 5134.91
  pure-backward-compute:
     rank  0: 1936.95
     rank  1: 1937.73
     rank  2: 1935.57
     rank  3: 1937.79
     rank  4: 2540.85
     rank  5: 2542.53
     rank  6: 2567.55
     rank  7: 2549.41
     rank  8: 4212.67
     rank  9: 4209.95
     rank 10: 4203.58
     rank 11: 4211.25
     rank 12: 4959.22
     rank 13: 4959.34
     rank 14: 4958.31
     rank 15: 4958.69
     rank 16: 5131.17
     rank 17: 5135.23
     rank 18: 5135.64
     rank 19: 5132.74
  batch-generator:
     rank  0: 74.51
     rank  1: 80.12
     rank  2: 88.20
     rank  3: 85.68
     rank  4: 77.13
     rank  5: 79.83
     rank  6: 88.86
     rank  7: 86.25
     rank  8: 71.55
     rank  9: 75.17
     rank 10: 102.82
     rank 11: 83.30
     rank 12: 56.89
     rank 13: 57.33
     rank 14: 59.99
     rank 15: 63.34
     rank 16: 49.84
     rank 17: 52.20
     rank 18: 51.43
     rank 19: 53.59
  forward-recv:
     rank  4: 124.14
     rank  5: 123.97
     rank  6: 121.92
     rank  7: 122.79
     rank  8: 305.10
     rank  9: 304.94
     rank 10: 301.01
     rank 11: 303.81
     rank 12: 523.23
     rank 13: 523.31
     rank 14: 522.02
     rank 15: 522.56
     rank 16: 785.30
     rank 17: 785.17
     rank 18: 785.34
     rank 19: 785.25
  forward-send:
     rank  0: 317.15
     rank  1: 316.37
     rank  2: 310.62
     rank  3: 313.44
     rank  4: 226.07
     rank  5: 226.26
     rank  6: 222.24
     rank  7: 224.25
     rank  8: 66.25
     rank  9: 66.48
     rank 10: 66.40
     rank 11: 66.00
     rank 12: 10.49
     rank 13: 10.41
     rank 14: 10.78
     rank 15: 10.37
  backward-recv:
     rank  0: 1906.19
     rank  1: 1906.37
     rank  2: 1905.72
     rank  3: 1906.24
     rank  4: 1311.32
     rank  5: 1310.44
     rank  6: 1307.43
     rank  7: 1308.86
     rank  8: 643.23
     rank  9: 643.35
     rank 10: 646.84
     rank 11: 644.46
     rank 12: 275.90
     rank 13: 276.39
     rank 14: 275.69
     rank 15: 275.62
  backward-send:
     rank  4: 4.44
     rank  5: 4.57
     rank  6: 4.38
     rank  7: 5.20
     rank  8: 31.62
     rank  9: 31.52
     rank 10: 28.61
     rank 11: 30.88
     rank 12: 21.78
     rank 13: 20.96
     rank 14: 21.87
     rank 15: 21.16
     rank 16: 10.54
     rank 17: 10.33
     rank 18: 10.49
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 5176.98
     rank  1: 5175.96
     rank  2: 5181.16
     rank  3: 5178.50
     rank  4: 4489.67
     rank  5: 4490.92
     rank  6: 4470.31
     rank  7: 4485.99
     rank  8: 2032.56
     rank  9: 2037.13
     rank 10: 2043.23
     rank 11: 2037.54
     rank 12: 557.35
     rank 13: 559.14
     rank 14: 559.50
     rank 15: 557.92
  backward-send-forward-recv:
     rank  4: 30.02
     rank  5: 28.65
     rank  6: 30.21
     rank  7: 30.52
     rank  8: 190.34
     rank  9: 191.43
     rank 10: 175.27
     rank 11: 189.36
     rank 12: 186.24
     rank 13: 188.17
     rank 14: 187.18
     rank 15: 183.90
     rank 16: 172.33
     rank 17: 173.41
     rank 18: 173.60
     rank 19: 173.88
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.09
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.94
     rank  1: 0.93
     rank  2: 0.97
     rank  3: 0.92
     rank  4: 1.78
     rank  5: 1.74
     rank  6: 1.93
     rank  7: 1.77
     rank  8: 2.14
     rank  9: 2.14
     rank 10: 2.15
     rank 11: 2.34
     rank 12: 3.25
     rank 13: 3.25
     rank 14: 3.33
     rank 15: 3.25
     rank 16: 3.30
     rank 17: 3.30
     rank 18: 3.35
     rank 19: 3.31
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.25
     rank  2: 0.21
     rank  3: 0.21
     rank  4: 0.05
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.10
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.82
     rank  1: 15.64
     rank  2: 15.79
     rank  3: 15.57
     rank  4: 33.91
     rank  5: 34.16
     rank  6: 34.42
     rank  7: 33.84
     rank  8: 43.35
     rank  9: 43.39
     rank 10: 43.29
     rank 11: 43.48
     rank 12: 64.34
     rank 13: 64.40
     rank 14: 64.67
     rank 15: 64.50
     rank 16: 67.71
     rank 17: 67.68
     rank 18: 67.77
     rank 19: 67.66
  optimizer:
     rank  0: 16.71
     rank  1: 16.53
     rank  2: 16.67
     rank  3: 16.57
     rank  4: 34.80
     rank  5: 35.04
     rank  6: 35.30
     rank  7: 34.71
     rank  8: 44.24
     rank  9: 44.28
     rank 10: 44.18
     rank 11: 44.37
     rank 12: 65.22
     rank 13: 65.28
     rank 14: 65.56
     rank 15: 65.38
     rank 16: 68.60
     rank 17: 68.57
     rank 18: 68.66
     rank 19: 68.54
 [2024-12-05 23:08:29] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 11398.0 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.602343E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11292.77
     rank  1: 11292.81
     rank  2: 11292.95
     rank  3: 11292.89
     rank  4: 11294.75
     rank  5: 11294.71
     rank  6: 11294.71
     rank  7: 11294.71
     rank  8: 11295.57
     rank  9: 11295.58
     rank 10: 11295.65
     rank 11: 11295.57
     rank 12: 11297.52
     rank 13: 11297.58
     rank 14: 11297.65
     rank 15: 11297.53
     rank 16: 11297.82
     rank 17: 11297.80
     rank 18: 11298.00
     rank 19: 11297.85
  forward-compute:
     rank  0: 1895.20
     rank  1: 1901.82
     rank  2: 1902.32
     rank  3: 1896.12
     rank  4: 2393.63
     rank  5: 2399.09
     rank  6: 2393.98
     rank  7: 2388.19
     rank  8: 3488.38
     rank  9: 3490.11
     rank 10: 3507.15
     rank 11: 3491.32
     rank 12: 4168.63
     rank 13: 4170.39
     rank 14: 4167.32
     rank 15: 4170.99
     rank 16: 4282.80
     rank 17: 4283.11
     rank 18: 4284.26
     rank 19: 4282.02
  backward-compute:
     rank  0: 1940.73
     rank  1: 1940.43
     rank  2: 1941.64
     rank  3: 1940.62
     rank  4: 2542.36
     rank  5: 2543.27
     rank  6: 2573.17
     rank  7: 2554.33
     rank  8: 4222.32
     rank  9: 4220.97
     rank 10: 4208.55
     rank 11: 4218.43
     rank 12: 4960.25
     rank 13: 4959.53
     rank 14: 4961.94
     rank 15: 4960.89
     rank 16: 5146.38
     rank 17: 5151.33
     rank 18: 5152.21
     rank 19: 5148.64
  pure-backward-compute:
     rank  0: 1939.92
     rank  1: 1939.59
     rank  2: 1940.87
     rank  3: 1939.86
     rank  4: 2541.25
     rank  5: 2542.05
     rank  6: 2571.81
     rank  7: 2552.33
     rank  8: 4220.62
     rank  9: 4219.44
     rank 10: 4207.33
     rank 11: 4217.25
     rank 12: 4959.10
     rank 13: 4958.26
     rank 14: 4960.20
     rank 15: 4959.92
     rank 16: 5144.23
     rank 17: 5149.80
     rank 18: 5150.57
     rank 19: 5146.58
  batch-generator:
     rank  0: 71.34
     rank  1: 83.03
     rank  2: 86.19
     rank  3: 78.95
     rank  4: 71.96
     rank  5: 80.74
     rank  6: 86.46
     rank  7: 79.35
     rank  8: 70.45
     rank  9: 77.42
     rank 10: 101.93
     rank 11: 82.68
     rank 12: 52.75
     rank 13: 59.81
     rank 14: 58.51
     rank 15: 58.62
     rank 16: 51.37
     rank 17: 53.89
     rank 18: 56.19
     rank 19: 55.73
  forward-recv:
     rank  4: 122.47
     rank  5: 122.36
     rank  6: 120.76
     rank  7: 121.99
     rank  8: 300.37
     rank  9: 297.87
     rank 10: 296.66
     rank 11: 299.26
     rank 12: 520.51
     rank 13: 519.02
     rank 14: 519.03
     rank 15: 519.94
     rank 16: 782.73
     rank 17: 782.05
     rank 18: 782.48
     rank 19: 782.68
  forward-send:
     rank  0: 320.88
     rank  1: 314.00
     rank  2: 312.83
     rank  3: 319.53
     rank  4: 228.63
     rank  5: 222.75
     rank  6: 223.21
     rank  7: 228.24
     rank  8: 67.80
     rank  9: 65.24
     rank 10: 66.45
     rank 11: 67.88
     rank 12: 10.49
     rank 13: 9.82
     rank 14: 10.30
     rank 15: 10.50
  backward-recv:
     rank  0: 1929.42
     rank  1: 1929.91
     rank  2: 1929.29
     rank  3: 1929.74
     rank  4: 1327.94
     rank  5: 1327.47
     rank  6: 1324.00
     rank  7: 1325.75
     rank  8: 646.40
     rank  9: 646.99
     rank 10: 650.11
     rank 11: 647.45
     rank 12: 279.03
     rank 13: 278.62
     rank 14: 277.92
     rank 15: 278.92
  backward-send:
     rank  4: 4.56
     rank  5: 4.80
     rank  6: 3.74
     rank  7: 5.22
     rank  8: 31.66
     rank  9: 31.54
     rank 10: 28.76
     rank 11: 31.17
     rank 12: 22.35
     rank 13: 22.31
     rank 14: 22.86
     rank 15: 22.19
     rank 16: 10.39
     rank 17: 10.30
     rank 18: 9.98
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 5185.52
     rank  1: 5185.45
     rank  2: 5188.02
     rank  3: 5188.09
     rank  4: 4509.10
     rank  5: 4511.31
     rank  6: 4487.23
     rank  7: 4503.54
     rank  8: 2048.11
     rank  9: 2052.72
     rank 10: 2064.85
     rank 11: 2056.95
     rank 12: 564.03
     rank 13: 564.87
     rank 14: 562.71
     rank 15: 564.40
  backward-send-forward-recv:
     rank  4: 30.53
     rank  5: 29.56
     rank  6: 30.58
     rank  7: 30.83
     rank  8: 188.84
     rank  9: 192.25
     rank 10: 175.53
     rank 11: 187.35
     rank 12: 186.26
     rank 13: 185.81
     rank 14: 188.49
     rank 15: 183.94
     rank 16: 167.68
     rank 17: 169.20
     rank 18: 166.94
     rank 19: 168.47
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.07
     rank  5: 0.09
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.08
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.92
     rank  1: 0.96
     rank  2: 0.95
     rank  3: 0.94
     rank  4: 1.84
     rank  5: 1.78
     rank  6: 1.77
     rank  7: 1.80
     rank  8: 2.16
     rank  9: 2.13
     rank 10: 2.15
     rank 11: 2.16
     rank 12: 3.24
     rank 13: 3.36
     rank 14: 3.40
     rank 15: 3.23
     rank 16: 3.35
     rank 17: 3.44
     rank 18: 3.48
     rank 19: 3.42
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.23
     rank  2: 0.22
     rank  3: 0.21
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.08
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.19
     rank 19: 0.08
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.67
     rank  1: 15.90
     rank  2: 15.61
     rank  3: 15.69
     rank  4: 34.00
     rank  5: 34.28
     rank  6: 34.16
     rank  7: 33.84
     rank  8: 43.34
     rank  9: 43.22
     rank 10: 43.20
     rank 11: 43.38
     rank 12: 64.41
     rank 13: 64.49
     rank 14: 64.64
     rank 15: 64.48
     rank 16: 67.81
     rank 17: 67.78
     rank 18: 68.04
     rank 19: 67.78
  optimizer:
     rank  0: 16.58
     rank  1: 16.81
     rank  2: 16.53
     rank  3: 16.60
     rank  4: 34.90
     rank  5: 35.19
     rank  6: 35.07
     rank  7: 34.75
     rank  8: 44.25
     rank  9: 44.13
     rank 10: 44.11
     rank 11: 44.28
     rank 12: 65.32
     rank 13: 65.40
     rank 14: 65.54
     rank 15: 65.38
     rank 16: 68.72
     rank 17: 68.69
     rank 18: 68.92
     rank 19: 68.68
 [2024-12-05 23:08:40] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 11384.0 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 9.760590E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11276.45
     rank  1: 11276.46
     rank  2: 11278.66
     rank  3: 11276.46
     rank  4: 11278.45
     rank  5: 11278.35
     rank  6: 11280.43
     rank  7: 11278.36
     rank  8: 11279.21
     rank  9: 11279.24
     rank 10: 11281.31
     rank 11: 11279.29
     rank 12: 11281.17
     rank 13: 11281.18
     rank 14: 11283.33
     rank 15: 11281.21
     rank 16: 11281.40
     rank 17: 11281.38
     rank 18: 11284.23
     rank 19: 11281.40
  forward-compute:
     rank  0: 1900.12
     rank  1: 1904.36
     rank  2: 1905.19
     rank  3: 1902.44
     rank  4: 2409.94
     rank  5: 2413.64
     rank  6: 2407.25
     rank  7: 2405.26
     rank  8: 3491.57
     rank  9: 3492.90
     rank 10: 3510.17
     rank 11: 3495.90
     rank 12: 4157.41
     rank 13: 4157.95
     rank 14: 4158.11
     rank 15: 4159.50
     rank 16: 4280.01
     rank 17: 4282.96
     rank 18: 4278.85
     rank 19: 4278.82
  backward-compute:
     rank  0: 1941.68
     rank  1: 1942.00
     rank  2: 1942.28
     rank  3: 1941.79
     rank  4: 2549.75
     rank  5: 2550.85
     rank  6: 2578.80
     rank  7: 2561.93
     rank  8: 4210.37
     rank  9: 4209.05
     rank 10: 4201.49
     rank 11: 4206.08
     rank 12: 4966.02
     rank 13: 4967.32
     rank 14: 4964.76
     rank 15: 4965.80
     rank 16: 5134.79
     rank 17: 5139.31
     rank 18: 5140.99
     rank 19: 5137.18
  pure-backward-compute:
     rank  0: 1940.85
     rank  1: 1941.19
     rank  2: 1941.52
     rank  3: 1941.03
     rank  4: 2548.61
     rank  5: 2549.69
     rank  6: 2577.32
     rank  7: 2559.94
     rank  8: 4209.02
     rank  9: 4207.52
     rank 10: 4200.21
     rank 11: 4204.87
     rank 12: 4964.95
     rank 13: 4966.28
     rank 14: 4963.59
     rank 15: 4964.84
     rank 16: 5132.76
     rank 17: 5137.78
     rank 18: 5138.48
     rank 19: 5135.21
  batch-generator:
     rank  0: 72.21
     rank  1: 80.99
     rank  2: 84.84
     rank  3: 81.76
     rank  4: 71.93
     rank  5: 79.33
     rank  6: 82.23
     rank  7: 79.67
     rank  8: 71.36
     rank  9: 77.46
     rank 10: 102.41
     rank 11: 84.83
     rank 12: 52.95
     rank 13: 58.14
     rank 14: 58.94
     rank 15: 58.80
     rank 16: 50.92
     rank 17: 56.52
     rank 18: 53.50
     rank 19: 54.88
  forward-recv:
     rank  4: 121.48
     rank  5: 121.08
     rank  6: 120.10
     rank  7: 121.10
     rank  8: 305.28
     rank  9: 303.77
     rank 10: 302.37
     rank 11: 304.35
     rank 12: 523.57
     rank 13: 522.47
     rank 14: 522.21
     rank 15: 523.04
     rank 16: 785.46
     rank 17: 785.20
     rank 18: 785.41
     rank 19: 785.45
  forward-send:
     rank  0: 326.58
     rank  1: 321.81
     rank  2: 320.63
     rank  3: 323.77
     rank  4: 225.01
     rank  5: 221.62
     rank  6: 221.57
     rank  7: 223.92
     rank  8: 67.01
     rank  9: 65.72
     rank 10: 66.54
     rank 11: 66.95
     rank 12: 10.56
     rank 13: 10.33
     rank 14: 10.54
     rank 15: 10.51
  backward-recv:
     rank  0: 1912.85
     rank  1: 1912.79
     rank  2: 1912.41
     rank  3: 1912.55
     rank  4: 1318.32
     rank  5: 1316.96
     rank  6: 1314.31
     rank  7: 1315.18
     rank  8: 653.69
     rank  9: 654.66
     rank 10: 656.96
     rank 11: 655.36
     rank 12: 280.29
     rank 13: 279.76
     rank 14: 279.55
     rank 15: 280.06
  backward-send:
     rank  4: 4.40
     rank  5: 4.73
     rank  6: 3.60
     rank  7: 5.37
     rank  8: 31.66
     rank  9: 31.68
     rank 10: 28.91
     rank 11: 31.14
     rank 12: 21.94
     rank 13: 21.96
     rank 14: 22.08
     rank 15: 22.13
     rank 16: 10.36
     rank 17: 10.43
     rank 18: 10.16
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 5173.77
     rank  1: 5173.68
     rank  2: 5176.66
     rank  3: 5176.42
     rank  4: 4483.09
     rank  5: 4485.26
     rank  6: 4462.76
     rank  7: 4476.79
     rank  8: 2026.26
     rank  9: 2030.77
     rank 10: 2038.17
     rank 11: 2035.05
     rank 12: 558.47
     rank 13: 557.81
     rank 14: 558.75
     rank 15: 558.44
  backward-send-forward-recv:
     rank  4: 30.54
     rank  5: 29.31
     rank  6: 31.32
     rank  7: 30.99
     rank  8: 190.06
     rank  9: 191.36
     rank 10: 174.41
     rank 11: 186.43
     rank 12: 187.24
     rank 13: 187.39
     rank 14: 187.79
     rank 15: 186.21
     rank 16: 169.65
     rank 17: 167.57
     rank 18: 169.02
     rank 19: 170.42
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.08
     rank  5: 0.09
     rank  6: 0.06
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.16
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.96
     rank  1: 0.93
     rank  2: 0.96
     rank  3: 0.91
     rank  4: 1.87
     rank  5: 1.83
     rank  6: 1.81
     rank  7: 1.79
     rank  8: 2.16
     rank  9: 2.17
     rank 10: 2.17
     rank 11: 2.18
     rank 12: 3.22
     rank 13: 3.27
     rank 14: 3.36
     rank 15: 3.19
     rank 16: 3.31
     rank 17: 3.29
     rank 18: 4.00
     rank 19: 3.31
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.21
     rank  3: 0.21
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.08
     rank 13: 0.08
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.09
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.95
     rank  1: 15.55
     rank  2: 16.01
     rank  3: 15.70
     rank  4: 33.89
     rank  5: 34.31
     rank  6: 34.24
     rank  7: 33.83
     rank  8: 43.27
     rank  9: 43.39
     rank 10: 43.28
     rank 11: 43.38
     rank 12: 64.41
     rank 13: 64.39
     rank 14: 64.62
     rank 15: 64.52
     rank 16: 67.65
     rank 17: 67.66
     rank 18: 67.90
     rank 19: 67.63
  optimizer:
     rank  0: 16.83
     rank  1: 16.44
     rank  2: 16.89
     rank  3: 16.58
     rank  4: 34.76
     rank  5: 35.19
     rank  6: 35.11
     rank  7: 34.71
     rank  8: 44.15
     rank  9: 44.27
     rank 10: 44.16
     rank 11: 44.26
     rank 12: 65.29
     rank 13: 65.27
     rank 14: 65.50
     rank 15: 65.39
     rank 16: 68.53
     rank 17: 68.54
     rank 18: 68.76
     rank 19: 68.51
 [2024-12-05 23:08:51] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 11377.7 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.430572E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11273.15
     rank  1: 11273.17
     rank  2: 11273.22
     rank  3: 11273.15
     rank  4: 11275.07
     rank  5: 11275.02
     rank  6: 11275.06
     rank  7: 11275.06
     rank  8: 11275.96
     rank  9: 11275.86
     rank 10: 11275.95
     rank 11: 11275.97
     rank 12: 11277.87
     rank 13: 11277.83
     rank 14: 11277.93
     rank 15: 11277.89
     rank 16: 11278.14
     rank 17: 11278.13
     rank 18: 11277.07
     rank 19: 11278.15
  forward-compute:
     rank  0: 1904.09
     rank  1: 1903.19
     rank  2: 1909.14
     rank  3: 1912.33
     rank  4: 2403.69
     rank  5: 2401.83
     rank  6: 2401.39
     rank  7: 2407.90
     rank  8: 3507.66
     rank  9: 3505.88
     rank 10: 3526.12
     rank 11: 3517.53
     rank 12: 4150.82
     rank 13: 4149.71
     rank 14: 4148.81
     rank 15: 4154.58
     rank 16: 4276.60
     rank 17: 4277.97
     rank 18: 4276.41
     rank 19: 4276.32
  backward-compute:
     rank  0: 1945.01
     rank  1: 1945.56
     rank  2: 1944.88
     rank  3: 1947.21
     rank  4: 2548.54
     rank  5: 2550.42
     rank  6: 2576.85
     rank  7: 2558.06
     rank  8: 4242.72
     rank  9: 4241.71
     rank 10: 4229.70
     rank 11: 4239.39
     rank 12: 4959.69
     rank 13: 4960.58
     rank 14: 4960.67
     rank 15: 4959.29
     rank 16: 5142.93
     rank 17: 5146.31
     rank 18: 5146.71
     rank 19: 5144.18
  pure-backward-compute:
     rank  0: 1944.22
     rank  1: 1944.73
     rank  2: 1944.13
     rank  3: 1946.35
     rank  4: 2547.44
     rank  5: 2549.34
     rank  6: 2575.37
     rank  7: 2556.86
     rank  8: 4241.26
     rank  9: 4240.11
     rank 10: 4228.47
     rank 11: 4238.11
     rank 12: 4958.71
     rank 13: 4959.51
     rank 14: 4958.78
     rank 15: 4958.22
     rank 16: 5140.87
     rank 17: 5144.77
     rank 18: 5144.77
     rank 19: 5142.17
  batch-generator:
     rank  0: 71.29
     rank  1: 76.21
     rank  2: 83.83
     rank  3: 86.40
     rank  4: 69.65
     rank  5: 72.10
     rank  6: 81.12
     rank  7: 84.79
     rank  8: 71.19
     rank  9: 74.54
     rank 10: 102.02
     rank 11: 89.93
     rank 12: 51.26
     rank 13: 54.67
     rank 14: 55.08
     rank 15: 59.26
     rank 16: 50.43
     rank 17: 54.18
     rank 18: 53.17
     rank 19: 54.93
  forward-recv:
     rank  4: 122.32
     rank  5: 122.37
     rank  6: 120.65
     rank  7: 121.27
     rank  8: 301.46
     rank  9: 301.40
     rank 10: 297.92
     rank 11: 296.34
     rank 12: 521.38
     rank 13: 521.42
     rank 14: 520.39
     rank 15: 519.64
     rank 16: 778.56
     rank 17: 778.63
     rank 18: 777.60
     rank 19: 778.06
  forward-send:
     rank  0: 311.75
     rank  1: 312.17
     rank  2: 305.88
     rank  3: 302.98
     rank  4: 218.37
     rank  5: 219.13
     rank  6: 214.74
     rank  7: 210.96
     rank  8: 61.53
     rank  9: 62.21
     rank 10: 61.56
     rank 11: 59.46
     rank 12: 10.43
     rank 13: 10.62
     rank 14: 10.55
     rank 15: 9.85
  backward-recv:
     rank  0: 1909.00
     rank  1: 1909.13
     rank  2: 1909.17
     rank  3: 1908.68
     rank  4: 1318.07
     rank  5: 1318.12
     rank  6: 1314.24
     rank  7: 1316.78
     rank  8: 655.36
     rank  9: 655.59
     rank 10: 658.45
     rank 11: 656.76
     rank 12: 283.33
     rank 13: 283.75
     rank 14: 283.24
     rank 15: 283.57
  backward-send:
     rank  4: 4.76
     rank  5: 4.73
     rank  6: 4.59
     rank  7: 4.85
     rank  8: 31.84
     rank  9: 31.75
     rank 10: 29.23
     rank 11: 31.13
     rank 12: 21.27
     rank 13: 21.25
     rank 14: 21.56
     rank 15: 21.31
     rank 16: 10.46
     rank 17: 10.31
     rank 18: 10.23
     rank 19: 10.49
  forward-send-backward-recv:
     rank  0: 5182.71
     rank  1: 5182.49
     rank  2: 5185.95
     rank  3: 5183.49
     rank  4: 4493.94
     rank  5: 4494.09
     rank  6: 4473.13
     rank  7: 4488.72
     rank  8: 1981.28
     rank  9: 1985.39
     rank 10: 1997.20
     rank 11: 1988.99
     rank 12: 566.35
     rank 13: 566.81
     rank 14: 566.40
     rank 15: 566.76
  backward-send-forward-recv:
     rank  4: 30.32
     rank  5: 29.70
     rank  6: 31.11
     rank  7: 30.21
     rank  8: 192.12
     rank  9: 192.80
     rank 10: 176.97
     rank 11: 189.10
     rank 12: 187.89
     rank 13: 188.19
     rank 14: 189.45
     rank 15: 186.75
     rank 16: 168.95
     rank 17: 168.42
     rank 18: 168.91
     rank 19: 169.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.10
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.93
     rank  1: 0.92
     rank  2: 0.96
     rank  3: 0.91
     rank  4: 1.80
     rank  5: 1.75
     rank  6: 1.78
     rank  7: 1.76
     rank  8: 2.26
     rank  9: 2.12
     rank 10: 2.18
     rank 11: 2.18
     rank 12: 3.25
     rank 13: 3.24
     rank 14: 3.28
     rank 15: 3.25
     rank 16: 3.30
     rank 17: 3.31
     rank 18: 3.33
     rank 19: 3.35
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.23
     rank  2: 0.22
     rank  3: 0.21
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.07
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.67
     rank  1: 15.69
     rank  2: 15.86
     rank  3: 15.67
     rank  4: 33.80
     rank  5: 34.20
     rank  6: 34.17
     rank  7: 33.78
     rank  8: 43.48
     rank  9: 43.26
     rank 10: 43.30
     rank 11: 43.47
     rank 12: 64.40
     rank 13: 64.37
     rank 14: 64.55
     rank 15: 64.50
     rank 16: 67.72
     rank 17: 67.61
     rank 18: 67.75
     rank 19: 67.69
  optimizer:
     rank  0: 16.48
     rank  1: 16.50
     rank  2: 16.67
     rank  3: 16.48
     rank  4: 34.61
     rank  5: 35.01
     rank  6: 34.98
     rank  7: 34.59
     rank  8: 44.29
     rank  9: 44.07
     rank 10: 44.11
     rank 11: 44.28
     rank 12: 65.21
     rank 13: 65.19
     rank 14: 65.36
     rank 15: 65.31
     rank 16: 68.53
     rank 17: 68.42
     rank 18: 68.56
     rank 19: 68.50
 [2024-12-05 23:09:03] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 11376.3 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.216852E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11271.57
     rank  1: 11271.63
     rank  2: 11271.61
     rank  3: 11271.59
     rank  4: 11273.53
     rank  5: 11273.41
     rank  6: 11273.55
     rank  7: 11273.48
     rank  8: 11274.42
     rank  9: 11274.31
     rank 10: 11274.41
     rank 11: 11274.37
     rank 12: 11276.33
     rank 13: 11276.25
     rank 14: 11276.45
     rank 15: 11276.30
     rank 16: 11276.58
     rank 17: 11276.59
     rank 18: 11276.60
     rank 19: 11276.59
  forward-compute:
     rank  0: 1923.65
     rank  1: 1923.63
     rank  2: 1927.57
     rank  3: 1929.89
     rank  4: 2411.91
     rank  5: 2410.29
     rank  6: 2407.66
     rank  7: 2413.59
     rank  8: 3514.86
     rank  9: 3513.54
     rank 10: 3531.82
     rank 11: 3522.20
     rank 12: 4159.90
     rank 13: 4159.17
     rank 14: 4160.12
     rank 15: 4163.41
     rank 16: 4281.01
     rank 17: 4282.20
     rank 18: 4281.45
     rank 19: 4281.21
  backward-compute:
     rank  0: 1942.41
     rank  1: 1942.64
     rank  2: 1942.51
     rank  3: 1944.61
     rank  4: 2549.21
     rank  5: 2551.97
     rank  6: 2577.55
     rank  7: 2559.28
     rank  8: 4227.27
     rank  9: 4226.75
     rank 10: 4216.06
     rank 11: 4224.51
     rank 12: 4961.89
     rank 13: 4962.60
     rank 14: 4962.66
     rank 15: 4961.64
     rank 16: 5136.96
     rank 17: 5142.55
     rank 18: 5142.01
     rank 19: 5139.81
  pure-backward-compute:
     rank  0: 1941.61
     rank  1: 1941.79
     rank  2: 1941.78
     rank  3: 1943.86
     rank  4: 2548.10
     rank  5: 2550.85
     rank  6: 2576.07
     rank  7: 2557.99
     rank  8: 4225.76
     rank  9: 4225.21
     rank 10: 4214.86
     rank 11: 4223.28
     rank 12: 4960.77
     rank 13: 4961.46
     rank 14: 4960.74
     rank 15: 4960.62
     rank 16: 5134.87
     rank 17: 5140.89
     rank 18: 5140.36
     rank 19: 5137.70
  batch-generator:
     rank  0: 72.04
     rank  1: 76.91
     rank  2: 83.62
     rank  3: 84.94
     rank  4: 71.92
     rank  5: 74.32
     rank  6: 81.30
     rank  7: 83.49
     rank  8: 72.29
     rank  9: 75.56
     rank 10: 101.13
     rank 11: 88.06
     rank 12: 51.12
     rank 13: 55.14
     rank 14: 55.88
     rank 15: 58.70
     rank 16: 49.87
     rank 17: 53.64
     rank 18: 53.31
     rank 19: 55.10
  forward-recv:
     rank  4: 122.32
     rank  5: 122.78
     rank  6: 121.85
     rank  7: 122.45
     rank  8: 303.46
     rank  9: 303.46
     rank 10: 301.61
     rank 11: 300.09
     rank 12: 522.58
     rank 13: 522.43
     rank 14: 521.68
     rank 15: 521.33
     rank 16: 779.47
     rank 17: 779.69
     rank 18: 779.67
     rank 19: 779.18
  forward-send:
     rank  0: 306.05
     rank  1: 305.68
     rank  2: 301.33
     rank  3: 299.49
     rank  4: 218.28
     rank  5: 218.66
     rank  6: 215.31
     rank  7: 212.63
     rank  8: 58.51
     rank  9: 59.49
     rank 10: 58.78
     rank 11: 57.22
     rank 12: 10.23
     rank 13: 10.60
     rank 14: 10.47
     rank 15: 9.92
  backward-recv:
     rank  0: 1898.52
     rank  1: 1898.66
     rank  2: 1898.76
     rank  3: 1898.18
     rank  4: 1306.71
     rank  5: 1306.53
     rank  6: 1303.04
     rank  7: 1305.61
     rank  8: 643.80
     rank  9: 643.97
     rank 10: 647.87
     rank 11: 645.18
     rank 12: 280.90
     rank 13: 281.60
     rank 14: 280.82
     rank 15: 281.35
  backward-send:
     rank  4: 4.74
     rank  5: 4.65
     rank  6: 4.58
     rank  7: 4.81
     rank  8: 31.63
     rank  9: 31.53
     rank 10: 28.90
     rank 11: 31.06
     rank 12: 21.54
     rank 13: 20.79
     rank 14: 21.72
     rank 15: 21.03
     rank 16: 10.40
     rank 17: 10.19
     rank 18: 10.47
     rank 19: 10.47
  forward-send-backward-recv:
     rank  0: 5180.55
     rank  1: 5180.39
     rank  2: 5183.50
     rank  3: 5181.10
     rank  4: 4495.19
     rank  5: 4495.28
     rank  6: 4474.55
     rank  7: 4489.39
     rank  8: 2004.41
     rank  9: 2006.26
     rank 10: 2016.20
     rank 11: 2009.73
     rank 12: 554.09
     rank 13: 554.92
     rank 14: 551.68
     rank 15: 553.97
  backward-send-forward-recv:
     rank  4: 30.57
     rank  5: 30.12
     rank  6: 31.34
     rank  7: 30.31
     rank  8: 190.22
     rank  9: 190.83
     rank 10: 175.14
     rank 11: 187.91
     rank 12: 188.21
     rank 13: 187.94
     rank 14: 189.08
     rank 15: 186.77
     rank 16: 169.75
     rank 17: 168.85
     rank 18: 169.43
     rank 19: 169.36
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.08
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.08
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.91
     rank  1: 0.95
     rank  2: 0.95
     rank  3: 0.95
     rank  4: 1.81
     rank  5: 1.72
     rank  6: 1.86
     rank  7: 1.80
     rank  8: 2.16
     rank  9: 2.14
     rank 10: 2.21
     rank 11: 2.12
     rank 12: 3.22
     rank 13: 3.15
     rank 14: 3.32
     rank 15: 3.23
     rank 16: 3.31
     rank 17: 3.32
     rank 18: 3.38
     rank 19: 3.33
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.23
     rank  2: 0.22
     rank  3: 0.21
     rank  4: 0.05
     rank  5: 0.03
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.07
     rank 13: 0.06
     rank 14: 0.10
     rank 15: 0.07
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.79
     rank  1: 15.62
     rank  2: 15.58
     rank  3: 15.51
     rank  4: 33.93
     rank  5: 34.08
     rank  6: 34.30
     rank  7: 33.92
     rank  8: 43.37
     rank  9: 43.38
     rank 10: 43.21
     rank 11: 43.34
     rank 12: 64.37
     rank 13: 64.37
     rank 14: 64.79
     rank 15: 64.78
     rank 16: 67.75
     rank 17: 67.68
     rank 18: 67.77
     rank 19: 67.68
  optimizer:
     rank  0: 16.66
     rank  1: 16.64
     rank  2: 16.68
     rank  3: 16.50
     rank  4: 34.80
     rank  5: 34.95
     rank  6: 35.17
     rank  7: 34.79
     rank  8: 44.24
     rank  9: 44.25
     rank 10: 44.08
     rank 11: 44.22
     rank 12: 65.26
     rank 13: 65.24
     rank 14: 65.69
     rank 15: 65.66
     rank 16: 68.62
     rank 17: 68.55
     rank 18: 68.64
     rank 19: 68.55
 [2024-12-05 23:09:14] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 11380.1 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.629150E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 11273.40
     rank  1: 11273.37
     rank  2: 11273.36
     rank  3: 11273.36
     rank  4: 11275.21
     rank  5: 11275.17
     rank  6: 11275.18
     rank  7: 11275.19
     rank  8: 11276.19
     rank  9: 11276.11
     rank 10: 11276.08
     rank 11: 11276.09
     rank 12: 11278.08
     rank 13: 11278.08
     rank 14: 11278.08
     rank 15: 11278.09
     rank 16: 11278.48
     rank 17: 11278.31
     rank 18: 11278.33
     rank 19: 11278.32
  forward-compute:
     rank  0: 1913.01
     rank  1: 1913.89
     rank  2: 1918.55
     rank  3: 1919.95
     rank  4: 2409.22
     rank  5: 2409.34
     rank  6: 2406.70
     rank  7: 2411.39
     rank  8: 3536.42
     rank  9: 3535.62
     rank 10: 3555.99
     rank 11: 3543.78
     rank 12: 4158.56
     rank 13: 4158.74
     rank 14: 4158.66
     rank 15: 4162.75
     rank 16: 4278.42
     rank 17: 4279.49
     rank 18: 4277.99
     rank 19: 4278.44
  backward-compute:
     rank  0: 1949.21
     rank  1: 1949.63
     rank  2: 1949.15
     rank  3: 1951.38
     rank  4: 2555.57
     rank  5: 2557.26
     rank  6: 2583.64
     rank  7: 2565.75
     rank  8: 4235.17
     rank  9: 4235.36
     rank 10: 4226.75
     rank 11: 4233.12
     rank 12: 4961.20
     rank 13: 4961.57
     rank 14: 4961.14
     rank 15: 4961.42
     rank 16: 5135.15
     rank 17: 5138.63
     rank 18: 5138.60
     rank 19: 5136.28
  pure-backward-compute:
     rank  0: 1948.39
     rank  1: 1948.80
     rank  2: 1948.42
     rank  3: 1950.64
     rank  4: 2554.44
     rank  5: 2556.15
     rank  6: 2582.07
     rank  7: 2564.47
     rank  8: 4233.54
     rank  9: 4233.80
     rank 10: 4225.45
     rank 11: 4231.93
     rank 12: 4959.97
     rank 13: 4960.52
     rank 14: 4959.85
     rank 15: 4960.43
     rank 16: 5132.83
     rank 17: 5137.13
     rank 18: 5136.99
     rank 19: 5134.21
  batch-generator:
     rank  0: 73.35
     rank  1: 79.35
     rank  2: 86.78
     rank  3: 87.25
     rank  4: 71.66
     rank  5: 76.17
     rank  6: 84.18
     rank  7: 84.49
     rank  8: 73.24
     rank  9: 77.19
     rank 10: 105.05
     rank 11: 89.64
     rank 12: 50.85
     rank 13: 55.62
     rank 14: 56.03
     rank 15: 58.43
     rank 16: 51.43
     rank 17: 54.47
     rank 18: 53.55
     rank 19: 56.46
  forward-recv:
     rank  4: 124.22
     rank  5: 124.47
     rank  6: 123.30
     rank  7: 123.69
     rank  8: 308.60
     rank  9: 308.47
     rank 10: 305.19
     rank 11: 305.19
     rank 12: 527.11
     rank 13: 527.13
     rank 14: 525.88
     rank 15: 525.85
     rank 16: 784.77
     rank 17: 784.83
     rank 18: 784.87
     rank 19: 784.45
  forward-send:
     rank  0: 316.91
     rank  1: 316.01
     rank  2: 311.10
     rank  3: 309.93
     rank  4: 218.55
     rank  5: 218.43
     rank  6: 215.02
     rank  7: 213.29
     rank  8: 53.06
     rank  9: 53.53
     rank 10: 53.08
     rank 11: 51.76
     rank 12: 10.37
     rank 13: 10.54
     rank 14: 10.63
     rank 15: 10.07
  backward-recv:
     rank  0: 1906.31
     rank  1: 1906.08
     rank  2: 1906.51
     rank  3: 1906.01
     rank  4: 1302.62
     rank  5: 1302.74
     rank  6: 1297.61
     rank  7: 1301.52
     rank  8: 643.30
     rank  9: 643.18
     rank 10: 646.24
     rank 11: 644.41
     rank 12: 277.03
     rank 13: 277.19
     rank 14: 276.92
     rank 15: 277.31
  backward-send:
     rank  4: 4.63
     rank  5: 4.30
     rank  6: 5.41
     rank  7: 4.36
     rank  8: 31.67
     rank  9: 31.65
     rank 10: 29.02
     rank 11: 31.10
     rank 12: 21.51
     rank 13: 20.74
     rank 14: 21.13
     rank 15: 21.31
     rank 16: 10.46
     rank 17: 10.15
     rank 18: 10.10
     rank 19: 10.35
  forward-send-backward-recv:
     rank  0: 5167.21
     rank  1: 5166.93
     rank  2: 5169.60
     rank  3: 5167.97
     rank  4: 4493.82
     rank  5: 4493.87
     rank  6: 4473.51
     rank  7: 4487.87
     rank  8: 1973.83
     rank  9: 1976.68
     rank 10: 1985.38
     rank 11: 1980.27
     rank 12: 555.55
     rank 13: 556.86
     rank 14: 556.46
     rank 15: 555.30
  backward-send-forward-recv:
     rank  4: 29.81
     rank  5: 28.87
     rank  6: 30.57
     rank  7: 30.00
     rank  8: 189.46
     rank  9: 190.29
     rank 10: 173.61
     rank 11: 187.27
     rank 12: 187.20
     rank 13: 186.24
     rank 14: 187.69
     rank 15: 185.16
     rank 16: 168.11
     rank 17: 167.84
     rank 18: 168.71
     rank 19: 168.36
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.93
     rank  1: 0.93
     rank  2: 0.94
     rank  3: 0.92
     rank  4: 1.73
     rank  5: 1.76
     rank  6: 1.76
     rank  7: 1.74
     rank  8: 2.28
     rank  9: 2.19
     rank 10: 2.13
     rank 11: 2.13
     rank 12: 3.19
     rank 13: 3.24
     rank 14: 3.31
     rank 15: 3.18
     rank 16: 3.42
     rank 17: 3.32
     rank 18: 3.34
     rank 19: 3.32
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.24
     rank  2: 0.21
     rank  3: 0.21
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.14
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.07
     rank 14: 0.08
     rank 15: 0.07
     rank 16: 0.17
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.69
     rank  1: 15.75
     rank  2: 15.56
     rank  3: 15.56
     rank  4: 33.76
     rank  5: 34.23
     rank  6: 34.11
     rank  7: 33.76
     rank  8: 43.43
     rank  9: 43.34
     rank 10: 43.09
     rank 11: 43.34
     rank 12: 64.41
     rank 13: 64.36
     rank 14: 64.56
     rank 15: 64.49
     rank 16: 67.90
     rank 17: 67.65
     rank 18: 67.77
     rank 19: 67.68
  optimizer:
     rank  0: 16.59
     rank  1: 16.64
     rank  2: 16.46
     rank  3: 16.46
     rank  4: 34.67
     rank  5: 35.12
     rank  6: 35.02
     rank  7: 34.67
     rank  8: 44.32
     rank  9: 44.25
     rank 10: 43.98
     rank 11: 44.23
     rank 12: 65.30
     rank 13: 65.25
     rank 14: 65.45
     rank 15: 65.39
     rank 16: 68.78
     rank 17: 68.54
     rank 18: 68.66
     rank 19: 68.56
