examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-03 23:07:36,663] torch.distributed.run: [WARNING] 
[2024-12-03 23:07:36,663] torch.distributed.run: [WARNING] *****************************************
[2024-12-03 23:07:36,663] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-03 23:07:36,663] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3][rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (33558528 elements):
	language_model.output_layer.weight
	language_model.decoder.final_layernorm.weight
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])

111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False111111---name:decoder.final_layernorm.weight param:torch.Size([4096])

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
111111---name:output_layer.weight param:torch.Size([8192, 4096])
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
111111---name:decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024])
111111---name:decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096])
111111---name:decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096])
111111---name:decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096])
111111---name:decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584])
111111---name:decoder.final_layernorm.weight param:torch.Size([4096])
111111---name:output_layer.weight param:torch.Size([8192, 4096])
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 134.13
     rank  1: 140.78
     rank  2: 137.20
     rank  3: 136.81
     rank  4: 64.40
     rank  5: 57.61
     rank  6: 56.35
     rank  7: 61.70
     rank  8: 45.86
     rank  9: 48.86
     rank 10: 52.13
     rank 11: 52.49
     rank 12: 48.62
     rank 13: 49.15
     rank 14: 50.43
     rank 15: 51.40
     rank 16: 50.51
     rank 17: 56.10
     rank 18: 46.77
     rank 19: 59.19
  train/valid/test-data-iterators-setup:
     rank  0: 990.82
     rank  1: 991.42
     rank  2: 990.95
     rank  3: 990.70
     rank  4: 1103.15
     rank  5: 1103.20
     rank  6: 1103.17
     rank  7: 1103.67
     rank  8: 1255.88
     rank  9: 1255.93
     rank 10: 1256.03
     rank 11: 1256.47
     rank 12: 1256.00
     rank 13: 1256.07
     rank 14: 1256.09
     rank 15: 1256.26
     rank 16: 1256.13
     rank 17: 1256.21
     rank 18: 1256.23
     rank 19: 1256.24
 [2024-12-03 23:08:43] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 22318.5 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4926.0 | max reserved: 4926.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4850.0 | max reserved: 4850.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4926.0 | max reserved: 4926.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 2261.5654296875 | max allocated: 4165.67578125 | reserved: 4926.0 | max reserved: 4926.0
times across ranks (ms):
  forward-backward:
     rank  0: 22203.29
     rank  1: 22203.38
     rank  2: 22203.30
     rank  3: 22203.35
     rank  4: 22203.49
     rank  5: 22203.48
     rank  6: 22203.47
     rank  7: 22205.78
     rank  8: 22245.74
     rank  9: 22237.94
     rank 10: 22241.67
     rank 11: 22246.66
     rank 12: 22253.16
     rank 13: 22246.33
     rank 14: 22246.34
     rank 15: 22241.69
     rank 16: 22202.75
     rank 17: 22202.70
     rank 18: 22202.92
     rank 19: 22202.76
  forward-compute:
     rank  0: 5232.83
     rank  1: 5241.12
     rank  2: 5264.63
     rank  3: 5251.20
     rank  4: 5465.55
     rank  5: 5478.16
     rank  6: 5495.14
     rank  7: 5492.46
     rank  8: 5405.27
     rank  9: 5420.62
     rank 10: 5417.41
     rank 11: 5410.36
     rank 12: 4969.18
     rank 13: 4976.92
     rank 14: 4979.25
     rank 15: 4977.23
     rank 16: 5095.33
     rank 17: 5098.49
     rank 18: 5102.06
     rank 19: 5104.00
  backward-compute:
     rank  0: 1967.51
     rank  1: 1973.60
     rank  2: 1966.56
     rank  3: 1975.62
     rank  4: 3105.63
     rank  5: 3115.88
     rank  6: 3106.56
     rank  7: 3104.50
     rank  8: 2993.18
     rank  9: 2993.88
     rank 10: 2993.62
     rank 11: 2996.59
     rank 12: 3002.59
     rank 13: 3004.83
     rank 14: 3006.55
     rank 15: 3006.14
     rank 16: 3190.17
     rank 17: 3187.92
     rank 18: 3190.06
     rank 19: 3189.87
  pure-backward-compute:
     rank  0: 1966.47
     rank  1: 1972.46
     rank  2: 1965.62
     rank  3: 1974.68
     rank  4: 3104.51
     rank  5: 3114.93
     rank  6: 3104.88
     rank  7: 3103.19
     rank  8: 2992.04
     rank  9: 2992.66
     rank 10: 2992.20
     rank 11: 2995.35
     rank 12: 3001.35
     rank 13: 3003.40
     rank 14: 3005.34
     rank 15: 3004.58
     rank 16: 3187.69
     rank 17: 3185.79
     rank 18: 3187.71
     rank 19: 3187.79
  batch-generator:
     rank  0: 1141.13
     rank  1: 1149.69
     rank  2: 1178.08
     rank  3: 1167.50
     rank  4: 1115.73
     rank  5: 1132.70
     rank  6: 1157.84
     rank  7: 1149.81
     rank  8: 1283.33
     rank  9: 1302.48
     rank 10: 1301.33
     rank 11: 1291.35
     rank 12: 1050.59
     rank 13: 1059.29
     rank 14: 1061.09
     rank 15: 1063.31
     rank 16: 1154.79
     rank 17: 1157.66
     rank 18: 1166.46
     rank 19: 1167.13
  forward-recv:
     rank  4: 4270.28
     rank  5: 4266.64
     rank  6: 4250.21
     rank  7: 4245.16
     rank  8: 6963.96
     rank  9: 6958.37
     rank 10: 6959.13
     rank 11: 6965.14
     rank 12: 9785.41
     rank 13: 9781.86
     rank 14: 9783.84
     rank 15: 9783.96
     rank 16: 12168.59
     rank 17: 12167.85
     rank 18: 12161.85
     rank 19: 12163.63
  forward-send:
     rank  0: 7858.98
     rank  1: 7845.39
     rank  2: 7826.26
     rank  3: 7830.33
     rank  4: 4882.59
     rank  5: 4872.82
     rank  6: 4869.63
     rank  7: 4878.04
     rank  8: 2246.40
     rank  9: 2241.39
     rank 10: 2238.03
     rank 11: 2240.10
     rank 12: 34.79
     rank 13: 33.88
     rank 14: 27.75
     rank 15: 29.54
  backward-recv:
     rank  0: 1285.87
     rank  1: 1284.65
     rank  2: 1285.81
     rank  3: 1284.94
     rank  4: 581.88
     rank  5: 582.26
     rank  6: 581.68
     rank  7: 582.03
     rank  8: 382.12
     rank  9: 383.26
     rank 10: 383.10
     rank 11: 382.31
     rank 12: 192.50
     rank 13: 191.99
     rank 14: 192.80
     rank 15: 193.25
  backward-send:
     rank  4: 4.28
     rank  5: 3.92
     rank  6: 4.24
     rank  7: 4.14
     rank  8: 31.31
     rank  9: 30.60
     rank 10: 31.09
     rank 11: 31.06
     rank 12: 20.38
     rank 13: 20.83
     rank 14: 20.74
     rank 15: 20.21
     rank 16: 10.60
     rank 17: 10.40
     rank 18: 10.51
     rank 19: 10.33
  forward-send-backward-recv:
     rank  0: 5750.82
     rank  1: 5751.79
     rank  2: 5752.24
     rank  3: 5754.75
     rank  4: 2980.11
     rank  5: 2973.76
     rank  6: 2979.67
     rank  7: 2980.46
     rank  8: 2794.82
     rank  9: 2794.27
     rank 10: 2795.07
     rank 11: 2791.26
     rank 12: 2634.39
     rank 13: 2632.00
     rank 14: 2633.66
     rank 15: 2632.71
  backward-send-forward-recv:
     rank  4: 742.46
     rank  5: 741.06
     rank  6: 744.21
     rank  7: 745.14
     rank  8: 1025.75
     rank  9: 1019.55
     rank 10: 1025.25
     rank 11: 1026.57
     rank 12: 1007.70
     rank 13: 1004.50
     rank 14: 1005.10
     rank 15: 1005.38
     rank 16: 985.05
     rank 17: 984.98
     rank 18: 985.12
     rank 19: 982.64
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.04
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.15
     rank  5: 0.14
     rank  6: 0.13
     rank  7: 0.15
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.09
     rank 19: 0.07
  all-grads-sync:
     rank  0: 45.45
     rank  1: 42.44
     rank  2: 45.81
     rank  3: 45.85
     rank  4: 92.69
     rank  5: 89.84
     rank  6: 88.43
     rank  7: 92.74
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.76
     rank 17: 0.63
     rank 18: 1.10
     rank 19: 1.09
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.23
     rank  2: 0.24
     rank  3: 0.37
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.05
     rank 12: 0.04
     rank 13: 0.08
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 25.04
     rank  1: 25.23
     rank  2: 23.40
     rank  3: 25.44
     rank  4: 61.38
     rank  5: 61.42
     rank  6: 61.43
     rank  7: 61.42
     rank  8: 0.16
     rank  9: 0.08
     rank 10: 0.13
     rank 11: 0.18
     rank 12: 0.16
     rank 13: 0.20
     rank 14: 0.12
     rank 15: 0.08
     rank 16: 4.37
     rank 17: 4.36
     rank 18: 4.99
     rank 19: 4.99
  optimizer:
     rank  0: 26.57
     rank  1: 26.75
     rank  2: 24.93
     rank  3: 26.88
     rank  4: 62.90
     rank  5: 63.00
     rank  6: 62.97
     rank  7: 62.99
     rank  8: 1.69
     rank  9: 1.48
     rank 10: 1.65
     rank 11: 1.71
     rank 12: 1.67
     rank 13: 1.73
     rank 14: 1.63
     rank 15: 1.58
     rank 16: 5.90
     rank 17: 5.89
     rank 18: 6.52
     rank 19: 6.54
 [2024-12-03 23:08:51] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7594.3 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7554.25
     rank  1: 7554.10
     rank  2: 7554.24
     rank  3: 7554.13
     rank  4: 7554.03
     rank  5: 7553.89
     rank  6: 7553.93
     rank  7: 7553.88
     rank  8: 7553.53
     rank  9: 7553.44
     rank 10: 7553.52
     rank 11: 7553.61
     rank 12: 7553.59
     rank 13: 7553.48
     rank 14: 7553.61
     rank 15: 7553.72
     rank 16: 7553.90
     rank 17: 7553.66
     rank 18: 7553.83
     rank 19: 7553.71
  forward-compute:
     rank  0: 964.89
     rank  1: 971.65
     rank  2: 969.26
     rank  3: 972.23
     rank  4: 2975.45
     rank  5: 2978.00
     rank  6: 2974.32
     rank  7: 2979.88
     rank  8: 2781.37
     rank  9: 2786.22
     rank 10: 2783.31
     rank 11: 2787.74
     rank 12: 2777.21
     rank 13: 2776.66
     rank 14: 2778.74
     rank 15: 2781.90
     rank 16: 2905.04
     rank 17: 2906.10
     rank 18: 2905.41
     rank 19: 2910.97
  backward-compute:
     rank  0: 1037.52
     rank  1: 1036.32
     rank  2: 1033.18
     rank  3: 1034.68
     rank  4: 3051.76
     rank  5: 3055.59
     rank  6: 3052.14
     rank  7: 3054.38
     rank  8: 2972.34
     rank  9: 2969.80
     rank 10: 2971.56
     rank 11: 2975.67
     rank 12: 2995.30
     rank 13: 2998.88
     rank 14: 2998.86
     rank 15: 2998.11
     rank 16: 3176.27
     rank 17: 3175.28
     rank 18: 3178.69
     rank 19: 3178.10
  pure-backward-compute:
     rank  0: 1036.63
     rank  1: 1035.48
     rank  2: 1032.49
     rank  3: 1034.04
     rank  4: 3050.83
     rank  5: 3054.82
     rank  6: 3051.17
     rank  7: 3053.45
     rank  8: 2970.99
     rank  9: 2968.52
     rank 10: 2970.71
     rank 11: 2974.55
     rank 12: 2994.20
     rank 13: 2997.82
     rank 14: 2997.77
     rank 15: 2996.98
     rank 16: 3174.06
     rank 17: 3173.16
     rank 18: 3176.98
     rank 19: 3176.49
  batch-generator:
     rank  0: 60.72
     rank  1: 64.96
     rank  2: 64.12
     rank  3: 67.55
     rank  4: 59.29
     rank  5: 63.29
     rank  6: 65.33
     rank  7: 67.99
     rank  8: 58.78
     rank  9: 65.92
     rank 10: 61.25
     rank 11: 65.00
     rank 12: 51.31
     rank 13: 50.40
     rank 14: 53.55
     rank 15: 59.21
     rank 16: 58.87
     rank 17: 60.90
     rank 18: 60.29
     rank 19: 65.82
  forward-recv:
     rank  4: 61.94
     rank  5: 60.57
     rank  6: 61.77
     rank  7: 61.08
     rank  8: 276.75
     rank  9: 274.78
     rank 10: 277.22
     rank 11: 276.67
     rank 12: 452.79
     rank 13: 452.76
     rank 14: 452.54
     rank 15: 452.46
     rank 16: 626.55
     rank 17: 626.68
     rank 18: 626.41
     rank 19: 625.98
  forward-send:
     rank  0: 414.60
     rank  1: 411.21
     rank  2: 413.88
     rank  3: 410.94
     rank  4: 31.96
     rank  5: 30.49
     rank  6: 31.57
     rank  7: 29.36
     rank  8: 21.45
     rank  9: 21.52
     rank 10: 20.72
     rank 11: 19.54
     rank 12: 10.52
     rank 13: 10.48
     rank 14: 10.28
     rank 15: 9.67
  backward-recv:
     rank  0: 1308.54
     rank  1: 1308.02
     rank  2: 1309.06
     rank  3: 1309.04
     rank  4: 601.88
     rank  5: 603.19
     rank  6: 602.68
     rank  7: 602.86
     rank  8: 395.93
     rank  9: 395.77
     rank 10: 395.82
     rank 11: 394.80
     rank 12: 202.61
     rank 13: 202.13
     rank 14: 202.84
     rank 15: 203.51
  backward-send:
     rank  4: 4.71
     rank  5: 3.90
     rank  6: 4.19
     rank  7: 4.34
     rank  8: 31.18
     rank  9: 31.08
     rank 10: 31.02
     rank 11: 31.32
     rank 12: 20.80
     rank 13: 20.93
     rank 14: 20.67
     rank 15: 19.65
     rank 16: 10.53
     rank 17: 10.18
     rank 18: 10.63
     rank 19: 10.43
  forward-send-backward-recv:
     rank  0: 3814.87
     rank  1: 3814.81
     rank  2: 3815.02
     rank  3: 3815.83
     rank  4: 726.46
     rank  5: 726.14
     rank  6: 726.43
     rank  7: 724.73
     rank  8: 647.28
     rank  9: 649.55
     rank 10: 649.89
     rank 11: 645.45
     rank 12: 471.47
     rank 13: 468.84
     rank 14: 471.39
     rank 15: 470.42
  backward-send-forward-recv:
     rank  4: 20.84
     rank  5: 19.42
     rank  6: 21.17
     rank  7: 20.70
     rank  8: 155.61
     rank  9: 152.70
     rank 10: 155.11
     rank 11: 153.26
     rank 12: 154.80
     rank 13: 155.56
     rank 14: 152.79
     rank 15: 151.56
     rank 16: 169.67
     rank 17: 169.48
     rank 18: 169.39
     rank 19: 165.29
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.10
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.57
     rank  2: 0.75
     rank  3: 0.60
     rank  4: 0.43
     rank  5: 0.52
     rank  6: 0.42
     rank  7: 0.44
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.64
     rank 17: 0.41
     rank 18: 0.50
     rank 19: 0.47
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.16
     rank  2: 0.19
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.57
     rank  1: 8.49
     rank  2: 8.67
     rank  3: 8.59
     rank  4: 5.25
     rank  5: 5.35
     rank  6: 5.25
     rank  7: 5.27
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.10
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 3.79
     rank 17: 3.70
     rank 18: 3.83
     rank 19: 3.73
  optimizer:
     rank  0: 9.30
     rank  1: 9.23
     rank  2: 9.41
     rank  3: 9.34
     rank  4: 5.99
     rank  5: 6.09
     rank  6: 5.99
     rank  7: 6.01
     rank  8: 0.78
     rank  9: 0.78
     rank 10: 0.77
     rank 11: 0.84
     rank 12: 0.78
     rank 13: 0.78
     rank 14: 0.78
     rank 15: 0.78
     rank 16: 4.53
     rank 17: 4.44
     rank 18: 4.57
     rank 19: 4.47
 [2024-12-03 23:08:58] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7580.1 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 6.965162E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7530.82
     rank  1: 7530.79
     rank  2: 7530.83
     rank  3: 7530.82
     rank  4: 7530.59
     rank  5: 7530.56
     rank  6: 7530.59
     rank  7: 7530.59
     rank  8: 7530.08
     rank  9: 7530.10
     rank 10: 7530.12
     rank 11: 7530.11
     rank 12: 7530.12
     rank 13: 7530.17
     rank 14: 7530.15
     rank 15: 7530.16
     rank 16: 7530.37
     rank 17: 7530.37
     rank 18: 7530.38
     rank 19: 7530.40
  forward-compute:
     rank  0: 1147.24
     rank  1: 1153.95
     rank  2: 1150.24
     rank  3: 1149.61
     rank  4: 2974.77
     rank  5: 2979.63
     rank  6: 2974.60
     rank  7: 2975.25
     rank  8: 2781.62
     rank  9: 2789.73
     rank 10: 2783.27
     rank 11: 2782.99
     rank 12: 2767.12
     rank 13: 2766.56
     rank 14: 2770.36
     rank 15: 2774.30
     rank 16: 2898.12
     rank 17: 2898.95
     rank 18: 2899.23
     rank 19: 2902.19
  backward-compute:
     rank  0: 1028.28
     rank  1: 1029.88
     rank  2: 1027.03
     rank  3: 1027.05
     rank  4: 3051.99
     rank  5: 3058.52
     rank  6: 3052.58
     rank  7: 3052.17
     rank  8: 2979.43
     rank  9: 2976.85
     rank 10: 2979.06
     rank 11: 2983.96
     rank 12: 2993.42
     rank 13: 2996.66
     rank 14: 2995.65
     rank 15: 2996.21
     rank 16: 3174.67
     rank 17: 3173.80
     rank 18: 3176.97
     rank 19: 3176.25
  pure-backward-compute:
     rank  0: 1027.60
     rank  1: 1029.11
     rank  2: 1026.37
     rank  3: 1026.42
     rank  4: 3050.94
     rank  5: 3057.78
     rank  6: 3051.60
     rank  7: 3051.17
     rank  8: 2978.16
     rank  9: 2975.61
     rank 10: 2978.09
     rank 11: 2983.04
     rank 12: 2992.32
     rank 13: 2995.55
     rank 14: 2994.58
     rank 15: 2995.15
     rank 16: 3172.54
     rank 17: 3171.74
     rank 18: 3175.36
     rank 19: 3174.54
  batch-generator:
     rank  0: 60.31
     rank  1: 66.74
     rank  2: 65.55
     rank  3: 64.28
     rank  4: 62.26
     rank  5: 67.76
     rank  6: 67.32
     rank  7: 66.89
     rank  8: 52.59
     rank  9: 62.13
     rank 10: 56.12
     rank 11: 56.26
     rank 12: 50.17
     rank 13: 49.09
     rank 14: 54.87
     rank 15: 60.03
     rank 16: 52.69
     rank 17: 55.92
     rank 18: 56.15
     rank 19: 59.33
  forward-recv:
     rank  4: 63.54
     rank  5: 62.28
     rank  6: 63.44
     rank  7: 62.97
     rank  8: 279.15
     rank  9: 276.25
     rank 10: 280.96
     rank 11: 279.64
     rank 12: 455.61
     rank 13: 455.28
     rank 14: 454.04
     rank 15: 454.81
     rank 16: 623.16
     rank 17: 623.47
     rank 18: 623.40
     rank 19: 623.63
  forward-send:
     rank  0: 403.06
     rank  1: 398.35
     rank  2: 402.46
     rank  3: 402.94
     rank  4: 31.12
     rank  5: 28.51
     rank  6: 31.09
     rank  7: 32.25
     rank  8: 21.10
     rank  9: 21.20
     rank 10: 19.54
     rank 11: 20.48
     rank 12: 10.16
     rank 13: 10.46
     rank 14: 10.39
     rank 15: 10.58
  backward-recv:
     rank  0: 1299.14
     rank  1: 1299.04
     rank  2: 1299.27
     rank  3: 1299.15
     rank  4: 593.41
     rank  5: 594.07
     rank  6: 593.99
     rank  7: 594.32
     rank  8: 388.35
     rank  9: 388.74
     rank 10: 387.71
     rank 11: 387.82
     rank 12: 192.23
     rank 13: 192.03
     rank 14: 192.47
     rank 15: 192.54
  backward-send:
     rank  4: 4.68
     rank  5: 4.00
     rank  6: 4.27
     rank  7: 4.19
     rank  8: 31.11
     rank  9: 30.78
     rank 10: 31.45
     rank 11: 30.79
     rank 12: 20.88
     rank 13: 20.91
     rank 14: 20.66
     rank 15: 19.88
     rank 16: 10.56
     rank 17: 10.47
     rank 18: 10.36
     rank 19: 9.73
  forward-send-backward-recv:
     rank  0: 3639.57
     rank  1: 3638.12
     rank  2: 3638.50
     rank  3: 3640.68
     rank  4: 720.86
     rank  5: 717.75
     rank  6: 720.79
     rank  7: 720.27
     rank  8: 620.76
     rank  9: 619.56
     rank 10: 621.50
     rank 11: 616.94
     rank 12: 478.95
     rank 13: 476.18
     rank 14: 478.37
     rank 15: 478.18
  backward-send-forward-recv:
     rank  4: 20.90
     rank  5: 18.69
     rank  6: 20.02
     rank  7: 20.98
     rank  8: 168.23
     rank  9: 164.14
     rank 10: 167.94
     rank 11: 169.04
     rank 12: 155.53
     rank 13: 155.95
     rank 14: 153.87
     rank 15: 149.37
     rank 16: 169.33
     rank 17: 169.28
     rank 18: 168.30
     rank 19: 165.78
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.55
     rank  2: 0.56
     rank  3: 0.55
     rank  4: 0.43
     rank  5: 0.44
     rank  6: 0.46
     rank  7: 0.46
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.44
     rank 17: 0.42
     rank 18: 0.55
     rank 19: 0.55
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.45
     rank  1: 8.52
     rank  2: 8.44
     rank  3: 8.51
     rank  4: 5.26
     rank  5: 5.29
     rank  6: 5.31
     rank  7: 5.36
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.06
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 3.71
     rank 17: 3.70
     rank 18: 3.74
     rank 19: 3.77
  optimizer:
     rank  0: 9.05
     rank  1: 9.14
     rank  2: 9.06
     rank  3: 9.13
     rank  4: 5.86
     rank  5: 5.89
     rank  6: 5.92
     rank  7: 5.96
     rank  8: 0.63
     rank  9: 0.67
     rank 10: 0.64
     rank 11: 0.63
     rank 12: 0.63
     rank 13: 0.66
     rank 14: 0.67
     rank 15: 0.67
     rank 16: 4.31
     rank 17: 4.31
     rank 18: 4.35
     rank 19: 4.38
 [2024-12-03 23:09:06] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7565.6 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 5.775586E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7533.03
     rank  1: 7533.04
     rank  2: 7533.06
     rank  3: 7533.05
     rank  4: 7532.79
     rank  5: 7532.80
     rank  6: 7532.84
     rank  7: 7532.83
     rank  8: 7532.33
     rank  9: 7532.34
     rank 10: 7532.39
     rank 11: 7532.40
     rank 12: 7532.35
     rank 13: 7532.35
     rank 14: 7532.44
     rank 15: 7532.43
     rank 16: 7532.62
     rank 17: 7532.61
     rank 18: 7532.61
     rank 19: 7532.61
  forward-compute:
     rank  0: 935.63
     rank  1: 942.17
     rank  2: 938.21
     rank  3: 939.26
     rank  4: 2977.66
     rank  5: 2980.65
     rank  6: 2976.88
     rank  7: 2980.02
     rank  8: 2779.79
     rank  9: 2786.95
     rank 10: 2782.50
     rank 11: 2782.82
     rank 12: 2778.48
     rank 13: 2779.56
     rank 14: 2781.01
     rank 15: 2785.39
     rank 16: 2894.48
     rank 17: 2894.61
     rank 18: 2896.76
     rank 19: 2901.24
  backward-compute:
     rank  0: 1001.99
     rank  1: 1000.99
     rank  2: 1000.00
     rank  3: 1001.07
     rank  4: 3050.34
     rank  5: 3058.05
     rank  6: 3052.63
     rank  7: 3052.30
     rank  8: 2985.35
     rank  9: 2981.28
     rank 10: 2983.30
     rank 11: 2988.33
     rank 12: 2998.02
     rank 13: 3000.49
     rank 14: 3000.47
     rank 15: 3000.48
     rank 16: 3176.14
     rank 17: 3175.39
     rank 18: 3178.37
     rank 19: 3177.58
  pure-backward-compute:
     rank  0: 1001.31
     rank  1: 1000.27
     rank  2: 999.33
     rank  3: 1000.44
     rank  4: 3049.45
     rank  5: 3057.22
     rank  6: 3051.65
     rank  7: 3051.38
     rank  8: 2983.84
     rank  9: 2979.91
     rank 10: 2982.38
     rank 11: 2987.30
     rank 12: 2996.91
     rank 13: 2999.46
     rank 14: 2999.56
     rank 15: 2999.50
     rank 16: 3173.97
     rank 17: 3173.25
     rank 18: 3176.77
     rank 19: 3176.06
  batch-generator:
     rank  0: 53.44
     rank  1: 60.65
     rank  2: 57.13
     rank  3: 58.69
     rank  4: 58.32
     rank  5: 67.08
     rank  6: 65.56
     rank  7: 68.02
     rank  8: 52.58
     rank  9: 63.76
     rank 10: 58.36
     rank 11: 58.28
     rank 12: 50.30
     rank 13: 49.86
     rank 14: 53.62
     rank 15: 60.40
     rank 16: 51.95
     rank 17: 54.13
     rank 18: 56.25
     rank 19: 61.03
  forward-recv:
     rank  4: 60.49
     rank  5: 58.59
     rank  6: 60.66
     rank  7: 59.97
     rank  8: 275.38
     rank  9: 273.97
     rank 10: 275.44
     rank 11: 275.34
     rank 12: 451.89
     rank 13: 451.26
     rank 14: 452.09
     rank 15: 452.74
     rank 16: 620.28
     rank 17: 620.31
     rank 18: 620.17
     rank 19: 619.70
  forward-send:
     rank  0: 413.88
     rank  1: 409.38
     rank  2: 413.65
     rank  3: 412.44
     rank  4: 31.36
     rank  5: 29.13
     rank  6: 31.40
     rank  7: 31.11
     rank  8: 21.04
     rank  9: 20.56
     rank 10: 21.02
     rank 11: 21.00
     rank 12: 10.53
     rank 13: 10.52
     rank 14: 10.28
     rank 15: 9.78
  backward-recv:
     rank  0: 1308.41
     rank  1: 1308.65
     rank  2: 1308.57
     rank  3: 1308.61
     rank  4: 597.75
     rank  5: 597.39
     rank  6: 597.57
     rank  7: 597.52
     rank  8: 388.27
     rank  9: 389.70
     rank 10: 389.53
     rank 11: 388.67
     rank 12: 195.35
     rank 13: 195.23
     rank 14: 195.73
     rank 15: 196.11
  backward-send:
     rank  4: 4.26
     rank  5: 4.23
     rank  6: 4.35
     rank  7: 4.13
     rank  8: 29.72
     rank  9: 30.46
     rank 10: 30.59
     rank 11: 30.73
     rank 12: 20.73
     rank 13: 20.99
     rank 14: 20.68
     rank 15: 20.07
     rank 16: 10.27
     rank 17: 10.55
     rank 18: 10.48
     rank 19: 10.49
  forward-send-backward-recv:
     rank  0: 3859.61
     rank  1: 3860.35
     rank  2: 3858.70
     rank  3: 3860.33
     rank  4: 714.98
     rank  5: 711.43
     rank  6: 714.05
     rank  7: 714.38
     rank  8: 623.82
     rank  9: 624.76
     rank 10: 624.70
     rank 11: 619.45
     rank 12: 459.55
     rank 13: 456.59
     rank 14: 458.78
     rank 15: 458.49
  backward-send-forward-recv:
     rank  4: 20.89
     rank  5: 20.26
     rank  6: 20.46
     rank  7: 20.33
     rank  8: 163.11
     rank  9: 158.18
     rank 10: 161.85
     rank 11: 162.30
     rank 12: 155.38
     rank 13: 156.16
     rank 14: 153.49
     rank 15: 148.97
     rank 16: 170.39
     rank 17: 171.09
     rank 18: 168.61
     rank 19: 165.47
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.55
     rank  1: 0.60
     rank  2: 0.55
     rank  3: 0.55
     rank  4: 0.43
     rank  5: 0.44
     rank  6: 0.42
     rank  7: 0.45
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.44
     rank 17: 0.44
     rank 18: 0.42
     rank 19: 0.42
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.58
     rank  2: 8.54
     rank  3: 8.53
     rank  4: 5.25
     rank  5: 5.26
     rank  6: 5.25
     rank  7: 5.28
     rank  8: 0.06
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 3.67
     rank 17: 3.67
     rank 18: 3.70
     rank 19: 3.67
  optimizer:
     rank  0: 9.14
     rank  1: 9.20
     rank  2: 9.17
     rank  3: 9.16
     rank  4: 5.88
     rank  5: 5.89
     rank  6: 5.88
     rank  7: 5.91
     rank  8: 0.69
     rank  9: 0.70
     rank 10: 0.69
     rank 11: 0.69
     rank 12: 0.66
     rank 13: 0.66
     rank 14: 0.66
     rank 15: 0.66
     rank 16: 4.30
     rank 17: 4.30
     rank 18: 4.32
     rank 19: 4.29
 [2024-12-03 23:09:13] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7569.3 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 4.508308E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7535.72
     rank  1: 7535.91
     rank  2: 7535.69
     rank  3: 7535.72
     rank  4: 7535.49
     rank  5: 7535.66
     rank  6: 7535.42
     rank  7: 7535.46
     rank  8: 7535.01
     rank  9: 7535.22
     rank 10: 7534.96
     rank 11: 7534.96
     rank 12: 7535.02
     rank 13: 7535.23
     rank 14: 7535.02
     rank 15: 7535.01
     rank 16: 7535.24
     rank 17: 7535.89
     rank 18: 7535.24
     rank 19: 7535.24
  forward-compute:
     rank  0: 935.00
     rank  1: 941.80
     rank  2: 937.88
     rank  3: 938.61
     rank  4: 2974.57
     rank  5: 2978.77
     rank  6: 2973.48
     rank  7: 2977.87
     rank  8: 2778.68
     rank  9: 2784.85
     rank 10: 2780.09
     rank 11: 2781.59
     rank 12: 2780.32
     rank 13: 2780.88
     rank 14: 2784.50
     rank 15: 2790.56
     rank 16: 2903.96
     rank 17: 2904.50
     rank 18: 2903.90
     rank 19: 2909.64
  backward-compute:
     rank  0: 979.29
     rank  1: 978.89
     rank  2: 976.79
     rank  3: 978.39
     rank  4: 3065.11
     rank  5: 3071.62
     rank  6: 3066.90
     rank  7: 3066.79
     rank  8: 2982.29
     rank  9: 2979.85
     rank 10: 2981.73
     rank 11: 2988.79
     rank 12: 2995.49
     rank 13: 2998.77
     rank 14: 2997.04
     rank 15: 2998.08
     rank 16: 3174.49
     rank 17: 3173.96
     rank 18: 3176.66
     rank 19: 3176.18
  pure-backward-compute:
     rank  0: 978.63
     rank  1: 978.22
     rank  2: 976.09
     rank  3: 977.75
     rank  4: 3064.23
     rank  5: 3070.85
     rank  6: 3066.02
     rank  7: 3066.02
     rank  8: 2980.94
     rank  9: 2978.53
     rank 10: 2980.93
     rank 11: 2987.95
     rank 12: 2994.39
     rank 13: 2997.68
     rank 14: 2996.05
     rank 15: 2997.28
     rank 16: 3172.42
     rank 17: 3171.82
     rank 18: 3174.87
     rank 19: 3174.62
  batch-generator:
     rank  0: 58.87
     rank  1: 64.23
     rank  2: 61.50
     rank  3: 62.52
     rank  4: 60.93
     rank  5: 67.03
     rank  6: 64.52
     rank  7: 63.46
     rank  8: 51.50
     rank  9: 60.59
     rank 10: 55.30
     rank 11: 55.74
     rank 12: 50.05
     rank 13: 49.86
     rank 14: 54.64
     rank 15: 62.48
     rank 16: 50.48
     rank 17: 55.03
     rank 18: 55.49
     rank 19: 60.31
  forward-recv:
     rank  4: 63.35
     rank  5: 61.54
     rank  6: 63.48
     rank  7: 62.93
     rank  8: 280.96
     rank  9: 278.87
     rank 10: 282.67
     rank 11: 281.51
     rank 12: 455.24
     rank 13: 454.85
     rank 14: 453.94
     rank 15: 455.50
     rank 16: 622.44
     rank 17: 622.57
     rank 18: 622.61
     rank 19: 622.13
  forward-send:
     rank  0: 417.71
     rank  1: 413.06
     rank  2: 417.18
     rank  3: 416.29
     rank  4: 31.26
     rank  5: 28.80
     rank  6: 31.27
     rank  7: 30.91
     rank  8: 21.09
     rank  9: 21.00
     rank 10: 19.67
     rank 11: 20.58
     rank 12: 10.45
     rank 13: 10.53
     rank 14: 10.41
     rank 15: 9.89
  backward-recv:
     rank  0: 1317.54
     rank  1: 1318.14
     rank  2: 1318.19
     rank  3: 1318.09
     rank  4: 595.51
     rank  5: 595.40
     rank  6: 594.82
     rank  7: 595.38
     rank  8: 385.89
     rank  9: 385.79
     rank 10: 386.75
     rank 11: 385.31
     rank 12: 192.82
     rank 13: 192.66
     rank 14: 193.00
     rank 15: 193.31
  backward-send:
     rank  4: 4.26
     rank  5: 4.42
     rank  6: 4.32
     rank  7: 4.48
     rank  8: 31.28
     rank  9: 31.43
     rank 10: 30.48
     rank 11: 31.20
     rank 12: 20.66
     rank 13: 20.94
     rank 14: 20.75
     rank 15: 19.97
     rank 16: 10.53
     rank 17: 10.51
     rank 18: 10.49
     rank 19: 10.16
  forward-send-backward-recv:
     rank  0: 3872.17
     rank  1: 3871.80
     rank  2: 3871.24
     rank  3: 3872.29
     rank  4: 710.41
     rank  5: 707.30
     rank  6: 709.98
     rank  7: 709.36
     rank  8: 625.13
     rank  9: 626.46
     rank 10: 626.38
     rank 11: 619.48
     rank 12: 466.87
     rank 13: 464.27
     rank 14: 467.20
     rank 15: 464.98
  backward-send-forward-recv:
     rank  4: 21.38
     rank  5: 19.83
     rank  6: 21.32
     rank  7: 20.08
     rank  8: 169.83
     rank  9: 166.06
     rank 10: 169.42
     rank 11: 169.28
     rank 12: 156.12
     rank 13: 155.86
     rank 14: 153.81
     rank 15: 147.68
     rank 16: 169.14
     rank 17: 169.23
     rank 18: 168.79
     rank 19: 164.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.06
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.15
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.53
     rank  1: 0.55
     rank  2: 0.63
     rank  3: 0.59
     rank  4: 0.45
     rank  5: 0.42
     rank  6: 0.42
     rank  7: 0.42
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.46
     rank 17: 0.95
     rank 18: 0.52
     rank 19: 0.51
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.06
     rank 18: 0.03
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.45
     rank  1: 8.56
     rank  2: 8.51
     rank  3: 8.43
     rank  4: 5.33
     rank  5: 5.27
     rank  6: 5.26
     rank  7: 5.27
     rank  8: 0.04
     rank  9: 0.08
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 3.67
     rank 17: 4.26
     rank 18: 3.74
     rank 19: 3.73
  optimizer:
     rank  0: 9.59
     rank  1: 9.70
     rank  2: 9.65
     rank  3: 9.56
     rank  4: 6.47
     rank  5: 6.40
     rank  6: 6.39
     rank  7: 6.41
     rank  8: 1.17
     rank  9: 1.21
     rank 10: 1.16
     rank 11: 1.16
     rank 12: 1.17
     rank 13: 1.17
     rank 14: 1.20
     rank 15: 1.20
     rank 16: 4.80
     rank 17: 5.41
     rank 18: 4.87
     rank 19: 4.86
 [2024-12-03 23:09:21] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7601.0 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 3.330555E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7562.84
     rank  1: 7562.96
     rank  2: 7562.84
     rank  3: 7562.82
     rank  4: 7562.60
     rank  5: 7562.71
     rank  6: 7562.60
     rank  7: 7562.60
     rank  8: 7562.16
     rank  9: 7562.22
     rank 10: 7562.15
     rank 11: 7562.13
     rank 12: 7562.18
     rank 13: 7562.25
     rank 14: 7562.19
     rank 15: 7562.19
     rank 16: 7562.42
     rank 17: 7562.49
     rank 18: 7562.39
     rank 19: 7562.39
  forward-compute:
     rank  0: 1029.84
     rank  1: 1034.52
     rank  2: 1031.03
     rank  3: 1031.51
     rank  4: 2978.44
     rank  5: 2981.22
     rank  6: 2978.38
     rank  7: 2981.14
     rank  8: 2794.21
     rank  9: 2797.43
     rank 10: 2798.00
     rank 11: 2798.02
     rank 12: 2796.36
     rank 13: 2797.84
     rank 14: 2802.10
     rank 15: 2805.79
     rank 16: 2910.99
     rank 17: 2910.30
     rank 18: 2913.67
     rank 19: 2917.18
  backward-compute:
     rank  0: 980.38
     rank  1: 981.68
     rank  2: 980.97
     rank  3: 981.54
     rank  4: 3055.25
     rank  5: 3059.08
     rank  6: 3057.13
     rank  7: 3057.03
     rank  8: 2990.05
     rank  9: 2988.37
     rank 10: 2991.70
     rank 11: 2997.47
     rank 12: 3003.97
     rank 13: 3005.91
     rank 14: 3005.97
     rank 15: 3005.15
     rank 16: 3186.33
     rank 17: 3185.94
     rank 18: 3189.65
     rank 19: 3188.81
  pure-backward-compute:
     rank  0: 979.64
     rank  1: 981.05
     rank  2: 980.32
     rank  3: 980.87
     rank  4: 3054.31
     rank  5: 3058.23
     rank  6: 3056.31
     rank  7: 3056.25
     rank  8: 2988.75
     rank  9: 2987.26
     rank 10: 2990.88
     rank 11: 2996.64
     rank 12: 3002.60
     rank 13: 3004.85
     rank 14: 3005.20
     rank 15: 3004.36
     rank 16: 3184.19
     rank 17: 3182.82
     rank 18: 3188.11
     rank 19: 3187.19
  batch-generator:
     rank  0: 59.94
     rank  1: 63.48
     rank  2: 61.49
     rank  3: 63.41
     rank  4: 61.61
     rank  5: 69.14
     rank  6: 64.89
     rank  7: 65.78
     rank  8: 58.98
     rank  9: 63.67
     rank 10: 63.05
     rank 11: 61.97
     rank 12: 59.83
     rank 13: 58.40
     rank 14: 63.40
     rank 15: 69.33
     rank 16: 53.31
     rank 17: 55.62
     rank 18: 57.88
     rank 19: 61.59
  forward-recv:
     rank  4: 62.50
     rank  5: 61.08
     rank  6: 62.74
     rank  7: 62.27
     rank  8: 279.21
     rank  9: 277.88
     rank 10: 279.84
     rank 11: 279.42
     rank 12: 454.67
     rank 13: 454.36
     rank 14: 454.56
     rank 15: 455.41
     rank 16: 622.38
     rank 17: 622.41
     rank 18: 622.20
     rank 19: 621.86
  forward-send:
     rank  0: 380.93
     rank  1: 376.98
     rank  2: 380.45
     rank  3: 380.04
     rank  4: 31.39
     rank  5: 29.59
     rank  6: 31.22
     rank  7: 31.16
     rank  8: 21.12
     rank  9: 20.97
     rank 10: 20.51
     rank 11: 20.90
     rank 12: 10.58
     rank 13: 10.54
     rank 14: 10.21
     rank 15: 9.84
  backward-recv:
     rank  0: 1323.06
     rank  1: 1322.95
     rank  2: 1323.25
     rank  3: 1322.66
     rank  4: 597.04
     rank  5: 596.53
     rank  6: 596.77
     rank  7: 596.81
     rank  8: 388.16
     rank  9: 389.05
     rank 10: 388.20
     rank 11: 387.56
     rank 12: 193.39
     rank 13: 193.51
     rank 14: 193.98
     rank 15: 194.46
  backward-send:
     rank  4: 4.21
     rank  5: 4.30
     rank  6: 4.34
     rank  7: 4.34
     rank  8: 31.49
     rank  9: 30.76
     rank 10: 31.33
     rank 11: 31.15
     rank 12: 20.90
     rank 13: 20.93
     rank 14: 20.57
     rank 15: 19.92
     rank 16: 10.32
     rank 17: 10.64
     rank 18: 10.51
     rank 19: 10.45
  forward-send-backward-recv:
     rank  0: 3834.71
     rank  1: 3835.08
     rank  2: 3833.52
     rank  3: 3835.20
     rank  4: 735.44
     rank  5: 735.76
     rank  6: 734.33
     rank  7: 734.65
     rank  8: 637.86
     rank  9: 639.09
     rank 10: 637.80
     rank 11: 631.88
     rank 12: 460.59
     rank 13: 459.44
     rank 14: 460.89
     rank 15: 461.27
  backward-send-forward-recv:
     rank  4: 21.54
     rank  5: 20.16
     rank  6: 20.78
     rank  7: 20.58
     rank  8: 151.98
     rank  9: 150.63
     rank 10: 149.20
     rank 11: 150.16
     rank 12: 157.38
     rank 13: 156.60
     rank 14: 153.08
     rank 15: 149.12
     rank 16: 169.41
     rank 17: 169.66
     rank 18: 166.98
     rank 19: 164.41
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.53
     rank  1: 0.54
     rank  2: 0.56
     rank  3: 0.55
     rank  4: 0.41
     rank  5: 0.48
     rank  6: 0.41
     rank  7: 0.43
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.44
     rank 17: 0.61
     rank 18: 0.43
     rank 19: 0.44
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.16
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.24
     rank  1: 8.51
     rank  2: 8.30
     rank  3: 8.54
     rank  4: 5.27
     rank  5: 5.35
     rank  6: 5.26
     rank  7: 5.27
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 3.76
     rank 17: 3.84
     rank 18: 3.68
     rank 19: 3.72
  optimizer:
     rank  0: 8.88
     rank  1: 9.12
     rank  2: 8.95
     rank  3: 9.16
     rank  4: 5.89
     rank  5: 5.97
     rank  6: 5.88
     rank  7: 5.89
     rank  8: 0.68
     rank  9: 0.67
     rank 10: 0.65
     rank 11: 0.65
     rank 12: 0.65
     rank 13: 0.65
     rank 14: 0.65
     rank 15: 0.65
     rank 16: 4.38
     rank 17: 4.46
     rank 18: 4.30
     rank 19: 4.35
 [2024-12-03 23:09:29] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7563.4 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 2.708305E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7525.43
     rank  1: 7525.49
     rank  2: 7525.51
     rank  3: 7525.53
     rank  4: 7525.24
     rank  5: 7525.26
     rank  6: 7525.26
     rank  7: 7525.28
     rank  8: 7524.76
     rank  9: 7524.77
     rank 10: 7524.83
     rank 11: 7524.79
     rank 12: 7524.84
     rank 13: 7524.81
     rank 14: 7524.88
     rank 15: 7524.86
     rank 16: 7525.05
     rank 17: 7525.09
     rank 18: 7525.08
     rank 19: 7525.07
  forward-compute:
     rank  0: 984.89
     rank  1: 989.02
     rank  2: 983.54
     rank  3: 985.59
     rank  4: 2961.73
     rank  5: 2965.53
     rank  6: 2959.39
     rank  7: 2963.77
     rank  8: 2784.65
     rank  9: 2788.16
     rank 10: 2788.48
     rank 11: 2789.32
     rank 12: 2790.94
     rank 13: 2792.01
     rank 14: 2796.46
     rank 15: 2800.14
     rank 16: 2895.61
     rank 17: 2895.41
     rank 18: 2897.86
     rank 19: 2901.82
  backward-compute:
     rank  0: 1029.96
     rank  1: 1031.09
     rank  2: 1029.68
     rank  3: 1030.69
     rank  4: 3038.56
     rank  5: 3039.91
     rank  6: 3040.31
     rank  7: 3041.68
     rank  8: 2984.77
     rank  9: 2983.27
     rank 10: 2985.93
     rank 11: 2990.75
     rank 12: 3006.19
     rank 13: 3008.98
     rank 14: 3008.52
     rank 15: 3008.16
     rank 16: 3171.20
     rank 17: 3171.34
     rank 18: 3173.97
     rank 19: 3173.07
  pure-backward-compute:
     rank  0: 1029.21
     rank  1: 1030.42
     rank  2: 1029.04
     rank  3: 1030.04
     rank  4: 3037.78
     rank  5: 3039.18
     rank  6: 3039.44
     rank  7: 3040.89
     rank  8: 2983.73
     rank  9: 2982.14
     rank 10: 2985.12
     rank 11: 2989.93
     rank 12: 3004.86
     rank 13: 3007.69
     rank 14: 3007.68
     rank 15: 3007.31
     rank 16: 3169.05
     rank 17: 3168.77
     rank 18: 3172.49
     rank 19: 3171.60
  batch-generator:
     rank  0: 56.68
     rank  1: 62.16
     rank  2: 56.67
     rank  3: 59.51
     rank  4: 55.34
     rank  5: 61.05
     rank  6: 57.65
     rank  7: 59.13
     rank  8: 53.29
     rank  9: 58.47
     rank 10: 58.09
     rank 11: 58.75
     rank 12: 52.85
     rank 13: 53.18
     rank 14: 58.77
     rank 15: 64.46
     rank 16: 52.07
     rank 17: 54.24
     rank 18: 56.32
     rank 19: 60.57
  forward-recv:
     rank  4: 61.61
     rank  5: 59.86
     rank  6: 62.80
     rank  7: 61.31
     rank  8: 277.54
     rank  9: 275.79
     rank 10: 279.05
     rank 11: 278.25
     rank 12: 452.19
     rank 13: 451.75
     rank 14: 451.43
     rank 15: 452.69
     rank 16: 624.31
     rank 17: 624.37
     rank 18: 624.31
     rank 19: 623.89
  forward-send:
     rank  0: 417.52
     rank  1: 412.77
     rank  2: 418.31
     rank  3: 416.27
     rank  4: 31.28
     rank  5: 28.75
     rank  6: 31.31
     rank  7: 31.05
     rank  8: 21.27
     rank  9: 20.85
     rank 10: 20.03
     rank 11: 20.75
     rank 12: 10.52
     rank 13: 10.57
     rank 14: 10.37
     rank 15: 9.92
  backward-recv:
     rank  0: 1299.94
     rank  1: 1299.76
     rank  2: 1300.22
     rank  3: 1300.09
     rank  4: 594.14
     rank  5: 593.30
     rank  6: 593.78
     rank  7: 593.25
     rank  8: 388.92
     rank  9: 389.15
     rank 10: 389.09
     rank 11: 388.81
     rank 12: 192.64
     rank 13: 192.44
     rank 14: 192.81
     rank 15: 193.51
  backward-send:
     rank  4: 4.21
     rank  5: 4.41
     rank  6: 4.34
     rank  7: 4.29
     rank  8: 31.81
     rank  9: 30.80
     rank 10: 30.89
     rank 11: 30.40
     rank 12: 20.68
     rank 13: 20.87
     rank 14: 20.70
     rank 15: 19.88
     rank 16: 10.54
     rank 17: 10.49
     rank 18: 10.41
     rank 19: 10.29
  forward-send-backward-recv:
     rank  0: 3779.42
     rank  1: 3781.34
     rank  2: 3780.42
     rank  3: 3781.41
     rank  4: 744.69
     rank  5: 746.96
     rank  6: 743.78
     rank  7: 743.01
     rank  8: 628.74
     rank  9: 629.20
     rank 10: 628.11
     rank 11: 624.07
     rank 12: 440.09
     rank 13: 437.60
     rank 14: 439.56
     rank 15: 439.18
  backward-send-forward-recv:
     rank  4: 20.87
     rank  5: 20.07
     rank  6: 21.31
     rank  7: 20.48
     rank  8: 148.92
     rank  9: 148.30
     rank 10: 146.31
     rank 11: 146.03
     rank 12: 157.45
     rank 13: 156.89
     rank 14: 153.58
     rank 15: 149.57
     rank 16: 170.20
     rank 17: 170.33
     rank 18: 168.12
     rank 19: 165.45
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.53
     rank  1: 0.54
     rank  2: 0.54
     rank  3: 0.54
     rank  4: 0.41
     rank  5: 0.42
     rank  6: 0.42
     rank  7: 0.42
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.03
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.42
     rank 17: 0.45
     rank 18: 0.46
     rank 19: 0.47
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.48
     rank  1: 8.49
     rank  2: 8.40
     rank  3: 8.51
     rank  4: 5.25
     rank  5: 5.26
     rank  6: 5.26
     rank  7: 5.25
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 3.66
     rank 17: 3.69
     rank 18: 3.69
     rank 19: 3.67
  optimizer:
     rank  0: 9.09
     rank  1: 9.10
     rank  2: 9.00
     rank  3: 9.12
     rank  4: 5.86
     rank  5: 5.87
     rank  6: 5.87
     rank  7: 5.86
     rank  8: 0.65
     rank  9: 0.65
     rank 10: 0.65
     rank 11: 0.64
     rank 12: 0.64
     rank 13: 0.64
     rank 14: 0.64
     rank 15: 0.64
     rank 16: 4.27
     rank 17: 4.30
     rank 18: 4.30
     rank 19: 4.28
 [2024-12-03 23:09:36] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7574.5 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 2.032074E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7542.72
     rank  1: 7542.49
     rank  2: 7542.50
     rank  3: 7542.48
     rank  4: 7542.48
     rank  5: 7542.26
     rank  6: 7542.26
     rank  7: 7542.25
     rank  8: 7541.97
     rank  9: 7541.78
     rank 10: 7541.82
     rank 11: 7541.84
     rank 12: 7542.03
     rank 13: 7541.82
     rank 14: 7541.85
     rank 15: 7541.86
     rank 16: 7542.71
     rank 17: 7542.10
     rank 18: 7542.05
     rank 19: 7542.05
  forward-compute:
     rank  0: 927.59
     rank  1: 931.69
     rank  2: 928.60
     rank  3: 929.44
     rank  4: 2976.49
     rank  5: 2979.38
     rank  6: 2975.56
     rank  7: 2977.72
     rank  8: 2788.81
     rank  9: 2791.82
     rank 10: 2791.01
     rank 11: 2793.84
     rank 12: 2780.93
     rank 13: 2781.53
     rank 14: 2785.41
     rank 15: 2789.47
     rank 16: 2903.67
     rank 17: 2905.00
     rank 18: 2907.90
     rank 19: 2909.96
  backward-compute:
     rank  0: 1028.57
     rank  1: 1030.09
     rank  2: 1027.32
     rank  3: 1028.97
     rank  4: 3038.08
     rank  5: 3041.44
     rank  6: 3039.59
     rank  7: 3040.85
     rank  8: 2993.30
     rank  9: 2991.37
     rank 10: 2994.30
     rank 11: 2999.05
     rank 12: 3006.71
     rank 13: 3010.90
     rank 14: 3011.25
     rank 15: 3010.52
     rank 16: 3171.10
     rank 17: 3170.57
     rank 18: 3173.37
     rank 19: 3172.61
  pure-backward-compute:
     rank  0: 1027.93
     rank  1: 1029.36
     rank  2: 1026.66
     rank  3: 1028.28
     rank  4: 3037.29
     rank  5: 3040.69
     rank  6: 3038.60
     rank  7: 3039.86
     rank  8: 2992.25
     rank  9: 2990.31
     rank 10: 2993.43
     rank 11: 2998.25
     rank 12: 3005.39
     rank 13: 3009.81
     rank 14: 3010.46
     rank 15: 3009.72
     rank 16: 3169.02
     rank 17: 3168.00
     rank 18: 3171.89
     rank 19: 3170.89
  batch-generator:
     rank  0: 54.63
     rank  1: 61.89
     rank  2: 58.16
     rank  3: 60.55
     rank  4: 56.24
     rank  5: 60.94
     rank  6: 61.99
     rank  7: 67.18
     rank  8: 50.91
     rank  9: 57.02
     rank 10: 55.66
     rank 11: 57.25
     rank 12: 52.91
     rank 13: 53.24
     rank 14: 57.84
     rank 15: 64.01
     rank 16: 52.09
     rank 17: 55.64
     rank 18: 58.30
     rank 19: 61.51
  forward-recv:
     rank  4: 61.51
     rank  5: 59.76
     rank  6: 61.55
     rank  7: 60.97
     rank  8: 277.35
     rank  9: 275.96
     rank 10: 277.80
     rank 11: 277.44
     rank 12: 451.30
     rank 13: 451.12
     rank 14: 451.20
     rank 15: 452.04
     rank 16: 620.82
     rank 17: 620.86
     rank 18: 620.77
     rank 19: 620.42
  forward-send:
     rank  0: 407.51
     rank  1: 403.57
     rank  2: 406.99
     rank  3: 406.09
     rank  4: 31.49
     rank  5: 29.66
     rank  6: 31.42
     rank  7: 31.18
     rank  8: 21.07
     rank  9: 20.90
     rank 10: 20.56
     rank 11: 20.82
     rank 12: 10.64
     rank 13: 10.64
     rank 14: 10.39
     rank 15: 9.98
  backward-recv:
     rank  0: 1300.00
     rank  1: 1299.77
     rank  2: 1301.27
     rank  3: 1299.82
     rank  4: 595.66
     rank  5: 596.01
     rank  6: 595.64
     rank  7: 596.10
     rank  8: 384.52
     rank  9: 384.83
     rank 10: 384.65
     rank 11: 383.69
     rank 12: 192.31
     rank 13: 192.23
     rank 14: 192.42
     rank 15: 192.88
  backward-send:
     rank  4: 4.39
     rank  5: 4.29
     rank  6: 4.28
     rank  7: 4.31
     rank  8: 31.31
     rank  9: 31.29
     rank 10: 31.11
     rank 11: 31.34
     rank 12: 20.74
     rank 13: 21.00
     rank 14: 20.61
     rank 15: 19.85
     rank 16: 10.58
     rank 17: 10.51
     rank 18: 10.10
     rank 19: 10.08
  forward-send-backward-recv:
     rank  0: 3864.75
     rank  1: 3865.29
     rank  2: 3864.03
     rank  3: 3866.00
     rank  4: 734.51
     rank  5: 734.32
     rank  6: 733.36
     rank  7: 732.85
     rank  8: 623.65
     rank  9: 624.37
     rank 10: 624.34
     rank 11: 619.05
     rank 12: 454.61
     rank 13: 451.06
     rank 14: 452.01
     rank 15: 451.88
  backward-send-forward-recv:
     rank  4: 20.19
     rank  5: 19.78
     rank  6: 20.51
     rank  7: 20.42
     rank  8: 151.31
     rank  9: 149.65
     rank 10: 149.55
     rank 11: 148.34
     rank 12: 157.17
     rank 13: 156.56
     rank 14: 153.93
     rank 15: 149.79
     rank 16: 170.06
     rank 17: 168.98
     rank 18: 166.07
     rank 19: 164.95
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.16
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.52
     rank  1: 0.54
     rank  2: 0.54
     rank  3: 0.54
     rank  4: 0.42
     rank  5: 0.45
     rank  6: 0.45
     rank  7: 0.44
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.92
     rank 17: 0.46
     rank 18: 0.44
     rank 19: 0.43
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.52
     rank  1: 8.32
     rank  2: 8.41
     rank  3: 8.30
     rank  4: 5.25
     rank  5: 5.28
     rank  6: 5.31
     rank  7: 5.27
     rank  8: 0.03
     rank  9: 0.06
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 4.17
     rank 17: 3.79
     rank 18: 3.71
     rank 19: 3.75
  optimizer:
     rank  0: 9.52
     rank  1: 9.32
     rank  2: 9.41
     rank  3: 9.30
     rank  4: 6.26
     rank  5: 6.28
     rank  6: 6.32
     rank  7: 6.27
     rank  8: 1.03
     rank  9: 1.07
     rank 10: 1.07
     rank 11: 1.07
     rank 12: 1.06
     rank 13: 1.03
     rank 14: 1.03
     rank 15: 1.03
     rank 16: 5.17
     rank 17: 4.79
     rank 18: 4.71
     rank 19: 4.75
 [2024-12-03 23:09:44] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7579.8 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.780944E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7541.07
     rank  1: 7540.84
     rank  2: 7540.79
     rank  3: 7540.80
     rank  4: 7540.84
     rank  5: 7540.58
     rank  6: 7540.56
     rank  7: 7540.56
     rank  8: 7540.35
     rank  9: 7540.09
     rank 10: 7540.06
     rank 11: 7540.07
     rank 12: 7540.42
     rank 13: 7540.17
     rank 14: 7540.15
     rank 15: 7540.14
     rank 16: 7541.09
     rank 17: 7540.41
     rank 18: 7540.38
     rank 19: 7540.37
  forward-compute:
     rank  0: 941.88
     rank  1: 944.40
     rank  2: 942.45
     rank  3: 944.09
     rank  4: 2968.84
     rank  5: 2971.34
     rank  6: 2967.30
     rank  7: 2971.96
     rank  8: 2789.93
     rank  9: 2793.34
     rank 10: 2792.97
     rank 11: 2792.00
     rank 12: 2778.03
     rank 13: 2778.11
     rank 14: 2782.23
     rank 15: 2785.04
     rank 16: 2898.44
     rank 17: 2900.04
     rank 18: 2901.90
     rank 19: 2907.44
  backward-compute:
     rank  0: 1064.68
     rank  1: 1067.79
     rank  2: 1064.53
     rank  3: 1064.96
     rank  4: 3047.56
     rank  5: 3051.25
     rank  6: 3050.06
     rank  7: 3048.18
     rank  8: 3004.74
     rank  9: 3001.79
     rank 10: 3004.07
     rank 11: 3010.35
     rank 12: 2997.07
     rank 13: 3000.99
     rank 14: 3000.56
     rank 15: 3002.02
     rank 16: 3176.75
     rank 17: 3176.03
     rank 18: 3179.31
     rank 19: 3178.19
  pure-backward-compute:
     rank  0: 1063.98
     rank  1: 1066.88
     rank  2: 1063.86
     rank  3: 1064.30
     rank  4: 3046.85
     rank  5: 3050.41
     rank  6: 3049.27
     rank  7: 3047.46
     rank  8: 3003.66
     rank  9: 3000.78
     rank 10: 3003.28
     rank 11: 3009.55
     rank 12: 2995.80
     rank 13: 3000.01
     rank 14: 2999.62
     rank 15: 3000.99
     rank 16: 3174.50
     rank 17: 3173.72
     rank 18: 3177.67
     rank 19: 3176.67
  batch-generator:
     rank  0: 56.54
     rank  1: 68.62
     rank  2: 63.73
     rank  3: 65.71
     rank  4: 52.91
     rank  5: 61.13
     rank  6: 55.56
     rank  7: 57.12
     rank  8: 50.16
     rank  9: 57.12
     rank 10: 56.12
     rank 11: 53.69
     rank 12: 52.61
     rank 13: 52.80
     rank 14: 57.63
     rank 15: 62.41
     rank 16: 54.08
     rank 17: 57.93
     rank 18: 59.54
     rank 19: 65.37
  forward-recv:
     rank  4: 61.47
     rank  5: 59.60
     rank  6: 61.75
     rank  7: 60.70
     rank  8: 276.89
     rank  9: 274.54
     rank 10: 278.35
     rank 11: 277.18
     rank 12: 451.58
     rank 13: 451.45
     rank 14: 450.86
     rank 15: 452.34
     rank 16: 623.56
     rank 17: 623.61
     rank 18: 623.37
     rank 19: 622.92
  forward-send:
     rank  0: 417.03
     rank  1: 412.31
     rank  2: 416.83
     rank  3: 415.38
     rank  4: 31.29
     rank  5: 28.71
     rank  6: 31.35
     rank  7: 30.94
     rank  8: 21.12
     rank  9: 21.20
     rank 10: 19.83
     rank 11: 20.72
     rank 12: 10.56
     rank 13: 10.53
     rank 14: 10.20
     rank 15: 9.71
  backward-recv:
     rank  0: 1292.44
     rank  1: 1292.29
     rank  2: 1292.60
     rank  3: 1292.82
     rank  4: 598.10
     rank  5: 597.45
     rank  6: 597.22
     rank  7: 597.67
     rank  8: 384.14
     rank  9: 385.01
     rank 10: 385.08
     rank 11: 383.75
     rank 12: 192.24
     rank 13: 192.10
     rank 14: 192.46
     rank 15: 192.88
  backward-send:
     rank  4: 4.03
     rank  5: 4.35
     rank  6: 4.30
     rank  7: 4.47
     rank  8: 31.28
     rank  9: 30.96
     rank 10: 30.68
     rank 11: 31.37
     rank 12: 20.51
     rank 13: 20.94
     rank 14: 20.71
     rank 15: 20.25
     rank 16: 10.56
     rank 17: 10.60
     rank 18: 10.33
     rank 19: 10.30
  forward-send-backward-recv:
     rank  0: 3810.09
     rank  1: 3811.25
     rank  2: 3809.76
     rank  3: 3811.12
     rank  4: 735.43
     rank  5: 735.90
     rank  6: 734.27
     rank  7: 735.50
     rank  8: 611.48
     rank  9: 613.71
     rank 10: 612.93
     rank 11: 607.88
     rank 12: 469.55
     rank 13: 466.19
     rank 14: 468.22
     rank 15: 466.09
  backward-send-forward-recv:
     rank  4: 21.18
     rank  5: 20.80
     rank  6: 21.24
     rank  7: 20.23
     rank  8: 156.27
     rank  9: 155.11
     rank 10: 153.95
     rank 11: 155.07
     rank 12: 157.02
     rank 13: 156.92
     rank 14: 154.15
     rank 15: 150.85
     rank 16: 169.61
     rank 17: 168.98
     rank 18: 166.73
     rank 19: 162.56
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.17
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.54
     rank  1: 0.62
     rank  2: 0.54
     rank  3: 0.56
     rank  4: 0.42
     rank  5: 0.42
     rank  6: 0.43
     rank  7: 0.43
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.56
     rank 17: 1.67
     rank 18: 0.49
     rank 19: 0.48
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.17
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.52
     rank  2: 8.28
     rank  3: 8.52
     rank  4: 5.27
     rank  5: 5.27
     rank  6: 5.26
     rank  7: 5.28
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.07
     rank 14: 0.06
     rank 15: 0.07
     rank 16: 4.58
     rank 17: 3.74
     rank 18: 3.71
     rank 19: 3.68
  optimizer:
     rank  0: 9.75
     rank  1: 9.76
     rank  2: 9.51
     rank  3: 9.76
     rank  4: 6.51
     rank  5: 6.51
     rank  6: 6.50
     rank  7: 6.52
     rank  8: 1.27
     rank  9: 1.27
     rank 10: 1.27
     rank 11: 1.27
     rank 12: 1.27
     rank 13: 1.30
     rank 14: 1.30
     rank 15: 1.30
     rank 16: 5.80
     rank 17: 4.99
     rank 18: 4.93
     rank 19: 4.92
 [2024-12-03 23:09:51] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7576.1 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.399025E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7544.57
     rank  1: 7544.58
     rank  2: 7544.63
     rank  3: 7544.56
     rank  4: 7544.39
     rank  5: 7544.36
     rank  6: 7544.38
     rank  7: 7544.35
     rank  8: 7543.92
     rank  9: 7543.85
     rank 10: 7543.93
     rank 11: 7543.88
     rank 12: 7543.95
     rank 13: 7543.89
     rank 14: 7543.99
     rank 15: 7543.93
     rank 16: 7544.14
     rank 17: 7544.14
     rank 18: 7544.23
     rank 19: 7544.14
  forward-compute:
     rank  0: 941.07
     rank  1: 943.52
     rank  2: 942.22
     rank  3: 943.53
     rank  4: 2966.52
     rank  5: 2969.27
     rank  6: 2965.66
     rank  7: 2969.27
     rank  8: 2789.89
     rank  9: 2790.98
     rank 10: 2794.51
     rank 11: 2793.19
     rank 12: 2775.66
     rank 13: 2776.87
     rank 14: 2780.99
     rank 15: 2784.31
     rank 16: 2906.24
     rank 17: 2906.91
     rank 18: 2909.42
     rank 19: 2913.57
  backward-compute:
     rank  0: 1035.14
     rank  1: 1038.78
     rank  2: 1035.10
     rank  3: 1035.38
     rank  4: 3070.97
     rank  5: 3073.19
     rank  6: 3075.17
     rank  7: 3072.29
     rank  8: 3004.33
     rank  9: 3002.31
     rank 10: 3004.64
     rank 11: 3011.54
     rank 12: 2994.37
     rank 13: 2996.14
     rank 14: 2996.24
     rank 15: 2997.13
     rank 16: 3178.41
     rank 17: 3177.40
     rank 18: 3181.09
     rank 19: 3180.12
  pure-backward-compute:
     rank  0: 1034.33
     rank  1: 1037.97
     rank  2: 1034.42
     rank  3: 1034.75
     rank  4: 3070.22
     rank  5: 3072.46
     rank  6: 3074.33
     rank  7: 3071.59
     rank  8: 3003.29
     rank  9: 3001.22
     rank 10: 3003.86
     rank 11: 3010.76
     rank 12: 2992.98
     rank 13: 2995.13
     rank 14: 2995.39
     rank 15: 2996.29
     rank 16: 3176.02
     rank 17: 3175.11
     rank 18: 3179.58
     rank 19: 3178.60
  batch-generator:
     rank  0: 60.22
     rank  1: 66.68
     rank  2: 65.42
     rank  3: 66.12
     rank  4: 53.62
     rank  5: 59.70
     rank  6: 56.71
     rank  7: 58.36
     rank  8: 50.08
     rank  9: 56.23
     rank 10: 58.09
     rank 11: 54.60
     rank 12: 51.37
     rank 13: 52.17
     rank 14: 56.98
     rank 15: 62.53
     rank 16: 53.69
     rank 17: 57.01
     rank 18: 58.98
     rank 19: 63.27
  forward-recv:
     rank  4: 62.21
     rank  5: 60.49
     rank  6: 62.32
     rank  7: 61.70
     rank  8: 277.33
     rank  9: 276.24
     rank 10: 277.96
     rank 11: 277.02
     rank 12: 454.35
     rank 13: 454.13
     rank 14: 453.72
     rank 15: 454.91
     rank 16: 622.13
     rank 17: 622.08
     rank 18: 622.11
     rank 19: 621.78
  forward-send:
     rank  0: 413.62
     rank  1: 410.12
     rank  2: 413.41
     rank  3: 412.30
     rank  4: 31.53
     rank  5: 29.85
     rank  6: 31.46
     rank  7: 31.07
     rank  8: 20.92
     rank  9: 20.71
     rank 10: 20.36
     rank 11: 20.98
     rank 12: 10.47
     rank 13: 10.53
     rank 14: 10.50
     rank 15: 10.03
  backward-recv:
     rank  0: 1307.68
     rank  1: 1307.74
     rank  2: 1308.36
     rank  3: 1308.66
     rank  4: 589.75
     rank  5: 589.76
     rank  6: 588.91
     rank  7: 589.57
     rank  8: 385.23
     rank  9: 385.91
     rank 10: 386.22
     rank 11: 384.60
     rank 12: 193.62
     rank 13: 193.49
     rank 14: 193.90
     rank 15: 193.97
  backward-send:
     rank  4: 3.98
     rank  5: 4.07
     rank  6: 4.18
     rank  7: 4.48
     rank  8: 31.36
     rank  9: 30.92
     rank 10: 30.54
     rank 11: 31.36
     rank 12: 20.71
     rank 13: 20.96
     rank 14: 20.73
     rank 15: 20.08
     rank 16: 10.56
     rank 17: 10.52
     rank 18: 10.35
     rank 19: 9.94
  forward-send-backward-recv:
     rank  0: 3832.09
     rank  1: 3832.35
     rank  2: 3831.33
     rank  3: 3833.29
     rank  4: 728.95
     rank  5: 730.78
     rank  6: 726.28
     rank  7: 728.71
     rank  8: 625.79
     rank  9: 627.92
     rank 10: 626.59
     rank 11: 620.84
     rank 12: 480.23
     rank 13: 477.45
     rank 14: 479.04
     rank 15: 477.94
  backward-send-forward-recv:
     rank  4: 21.71
     rank  5: 20.51
     rank  6: 21.53
     rank  7: 21.00
     rank  8: 148.11
     rank  9: 146.93
     rank 10: 143.71
     rank 11: 145.60
     rank 12: 155.89
     rank 13: 156.36
     rank 14: 153.16
     rank 15: 149.35
     rank 16: 169.47
     rank 17: 170.06
     rank 18: 167.02
     rank 19: 164.10
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.57
     rank  1: 0.55
     rank  2: 0.58
     rank  3: 0.54
     rank  4: 0.46
     rank  5: 0.45
     rank  6: 0.43
     rank  7: 0.47
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.60
     rank 17: 0.45
     rank 18: 0.50
     rank 19: 0.46
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.15
     rank  2: 0.18
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.43
     rank  1: 8.48
     rank  2: 8.61
     rank  3: 8.39
     rank  4: 5.36
     rank  5: 5.27
     rank  6: 5.26
     rank  7: 5.32
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 3.72
     rank 17: 3.73
     rank 18: 3.79
     rank 19: 3.78
  optimizer:
     rank  0: 9.06
     rank  1: 9.11
     rank  2: 9.24
     rank  3: 9.03
     rank  4: 5.99
     rank  5: 5.91
     rank  6: 5.89
     rank  7: 5.95
     rank  8: 0.67
     rank  9: 0.67
     rank 10: 0.66
     rank 11: 0.66
     rank 12: 0.66
     rank 13: 0.67
     rank 14: 0.67
     rank 15: 0.67
     rank 16: 4.35
     rank 17: 4.36
     rank 18: 4.43
     rank 19: 4.42
