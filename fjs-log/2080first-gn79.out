examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
2
[2024-12-02 15:47:28,022] torch.distributed.run: [WARNING] 
[2024-12-02 15:47:28,022] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 15:47:28,022] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 15:47:28,022] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 9---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 10---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 11---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 8---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 9---Pipeline Parallel Group GPUs: [2, 2, 2, 2, 2]

---Rank 10---Pipeline Parallel Group GPUs: [2, 2, 2, 2, 2]
---Rank 11---Pipeline Parallel Group GPUs: [2, 2, 2, 2, 2]
---Rank 8---Pipeline Parallel Group GPUs: [2, 2, 2, 2, 2]
[rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 708943872
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 708943872
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 708943872
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 708943872
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
before training log report memory flag:Truebefore training log report memory flag:True
before training log report memory flag:True

before training log report memory flag:True
[Rank 11] (after 1 iterations) memory (MB) | allocated: 2789.658203125 | max allocated: 12276.26025390625 | reserved: 13388.0 | max reserved: 13388.0
[Rank 10] (after 1 iterations) memory (MB) | allocated: 2789.658203125 | max allocated: 12276.26025390625 | reserved: 13364.0 | max reserved: 13364.0
[Rank 9] (after 1 iterations) memory (MB) | allocated: 2789.658203125 | max allocated: 12276.26025390625 | reserved: 13404.0 | max reserved: 13404.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 2789.658203125 | max allocated: 12276.26025390625 | reserved: 13412.0 | max reserved: 13412.0
after training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:Falsebefore training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False

before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:Falsebefore training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:Falsebefore training log report memory flag:False

before training log report memory flag:Falsebefore training log report memory flag:False

after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
before training log report memory flag:False
after training log report memory flag:Falseafter training log report memory flag:False

after training log report memory flag:False
after training log report memory flag:False
