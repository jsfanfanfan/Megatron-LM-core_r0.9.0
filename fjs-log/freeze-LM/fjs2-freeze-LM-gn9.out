examples/multimodal/pretrain-freeze-llm-hete-2080tifirst.sh: line 4: activate: No such file or directory
4
[2024-12-05 17:19:34,867] torch.distributed.run: [WARNING] 
[2024-12-05 17:19:34,867] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 17:19:34,867] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 17:19:34,867] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 147.22
     rank  1: 134.73
     rank  2: 141.40
     rank  3: 141.03
     rank  4: 56.69
     rank  5: 37.97
     rank  6: 39.62
     rank  7: 36.78
     rank  8: 36.26
     rank  9: 50.18
     rank 10: 57.86
     rank 11: 40.82
     rank 12: 57.80
     rank 13: 58.92
     rank 14: 57.76
     rank 15: 59.71
     rank 16: 55.26
     rank 17: 53.00
     rank 18: 51.08
     rank 19: 52.42
  train/valid/test-data-iterators-setup:
     rank  0: 1239.97
     rank  1: 1239.95
     rank  2: 1240.11
     rank  3: 1239.98
     rank  4: 1239.70
     rank  5: 1240.33
     rank  6: 1239.92
     rank  7: 1239.80
     rank  8: 1239.77
     rank  9: 1239.89
     rank 10: 1239.74
     rank 11: 1239.77
     rank 12: 1239.80
     rank 13: 1239.63
     rank 14: 1240.51
     rank 15: 1239.64
     rank 16: 1239.95
     rank 17: 1240.02
     rank 18: 1240.13
     rank 19: 1240.03
 [2024-12-05 17:20:22] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 20432.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.010201E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2524.0 | max reserved: 2524.0[Rank 18] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2524.0 | max reserved: 2524.0

[Rank 16] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
times across ranks (ms):
  forward-backward:
     rank  0: 20333.70
     rank  1: 20333.38
     rank  2: 20333.66
     rank  3: 20333.70
     rank  4: 20375.21
     rank  5: 20370.88
     rank  6: 20375.33
     rank  7: 20370.67
     rank  8: 20376.88
     rank  9: 20374.01
     rank 10: 20378.73
     rank 11: 20372.98
     rank 12: 20369.05
     rank 13: 20368.63
     rank 14: 20373.52
     rank 15: 20370.62
     rank 16: 20331.10
     rank 17: 20330.92
     rank 18: 20330.83
     rank 19: 20331.12
  forward-compute:
     rank  0: 5406.62
     rank  1: 5413.52
     rank  2: 5400.79
     rank  3: 5403.44
     rank  4: 5841.25
     rank  5: 5847.65
     rank  6: 5843.96
     rank  7: 5845.05
     rank  8: 5957.90
     rank  9: 5963.18
     rank 10: 5965.85
     rank 11: 5965.26
     rank 12: 4714.03
     rank 13: 4714.08
     rank 14: 4717.06
     rank 15: 4712.17
     rank 16: 4709.02
     rank 17: 4714.87
     rank 18: 4715.70
     rank 19: 4715.99
  backward-compute:
     rank  0: 3724.00
     rank  1: 3726.08
     rank  2: 3727.15
     rank  3: 3728.99
     rank  4: 4129.02
     rank  5: 4133.67
     rank  6: 4130.54
     rank  7: 4129.75
     rank  8: 4090.26
     rank  9: 4097.98
     rank 10: 4097.12
     rank 11: 4088.07
     rank 12: 3375.81
     rank 13: 3379.94
     rank 14: 3372.98
     rank 15: 3373.31
     rank 16: 3810.20
     rank 17: 3809.47
     rank 18: 3812.93
     rank 19: 3808.87
  pure-backward-compute:
     rank  0: 3722.41
     rank  1: 3724.94
     rank  2: 3725.87
     rank  3: 3727.48
     rank  4: 4127.72
     rank  5: 4132.29
     rank  6: 4129.24
     rank  7: 4128.55
     rank  8: 4089.10
     rank  9: 4096.33
     rank 10: 4095.75
     rank 11: 4086.25
     rank 12: 3372.53
     rank 13: 3378.00
     rank 14: 3370.96
     rank 15: 3370.83
     rank 16: 3807.19
     rank 17: 3807.49
     rank 18: 3811.04
     rank 19: 3805.72
  batch-generator:
     rank  0: 1264.90
     rank  1: 1267.13
     rank  2: 1272.31
     rank  3: 1281.38
     rank  4: 1235.29
     rank  5: 1246.00
     rank  6: 1245.32
     rank  7: 1246.44
     rank  8: 1064.20
     rank  9: 1074.40
     rank 10: 1077.09
     rank 11: 1076.91
     rank 12: 1360.75
     rank 13: 1362.85
     rank 14: 1367.64
     rank 15: 1366.96
     rank 16: 1051.80
     rank 17: 1061.28
     rank 18: 1063.54
     rank 19: 1068.33
  forward-recv:
     rank  4: 2847.67
     rank  5: 2844.22
     rank  6: 2852.93
     rank  7: 2849.53
     rank  8: 5131.79
     rank  9: 5124.74
     rank 10: 5134.59
     rank 11: 5132.58
     rank 12: 7519.06
     rank 13: 7517.15
     rank 14: 7516.19
     rank 15: 7514.61
     rank 16: 9124.29
     rank 17: 9128.25
     rank 18: 9116.43
     rank 19: 9122.57
  forward-send:
     rank  0: 5903.76
     rank  1: 5896.60
     rank  2: 5904.17
     rank  3: 5902.75
     rank  4: 3547.23
     rank  5: 3543.61
     rank  6: 3540.25
     rank  7: 3542.65
     rank  8: 1397.71
     rank  9: 1400.96
     rank 10: 1387.83
     rank 11: 1390.39
     rank 12: 25.77
     rank 13: 30.45
     rank 14: 19.15
     rank 15: 24.60
  backward-recv:
     rank  0: 1275.84
     rank  1: 1275.89
     rank  2: 1276.20
     rank  3: 1275.30
     rank  4: 628.98
     rank  5: 629.14
     rank  6: 629.77
     rank  7: 630.13
     rank  8: 390.30
     rank  9: 390.15
     rank 10: 388.13
     rank 11: 390.04
     rank 12: 247.59
     rank 13: 247.84
     rank 14: 249.31
     rank 15: 248.22
  backward-send:
     rank  4: 43.84
     rank  5: 43.44
     rank  6: 42.88
     rank  7: 42.68
     rank  8: 31.34
     rank  9: 31.08
     rank 10: 31.34
     rank 11: 31.01
     rank 12: 21.05
     rank 13: 20.83
     rank 14: 19.19
     rank 15: 21.12
     rank 16: 10.77
     rank 17: 10.27
     rank 18: 10.74
     rank 19: 10.76
  forward-send-backward-recv:
     rank  0: 3936.86
     rank  1: 3938.94
     rank  2: 3941.05
     rank  3: 3939.94
     rank  4: 2016.42
     rank  5: 2013.07
     rank  6: 2014.90
     rank  7: 2014.49
     rank  8: 1754.72
     rank  9: 1749.60
     rank 10: 1752.04
     rank 11: 1752.91
     rank 12: 1663.67
     rank 13: 1660.06
     rank 14: 1660.99
     rank 15: 1664.23
  backward-send-forward-recv:
     rank  4: 1032.67
     rank  5: 1032.85
     rank  6: 1031.67
     rank  7: 1032.12
     rank  8: 1064.14
     rank  9: 1060.56
     rank 10: 1061.64
     rank 11: 1062.10
     rank 12: 1985.35
     rank 13: 1984.01
     rank 14: 1992.91
     rank 15: 1993.58
     rank 16: 1679.18
     rank 17: 1672.96
     rank 18: 1680.94
     rank 19: 1675.13
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.09
     rank  3: 0.08
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.09
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.23
     rank  1: 0.15
     rank  2: 0.23
     rank  3: 0.20
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.08
  all-grads-sync:
     rank  0: 60.32
     rank  1: 61.95
     rank  2: 60.38
     rank  3: 58.29
     rank  4: 0.01
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.07
     rank 13: 0.02
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  optimizer-copy-to-main-grad:
     rank  0: 0.34
     rank  1: 0.34
     rank  2: 0.34
     rank  3: 0.33
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.06
     rank 11: 0.08
     rank 12: 0.05
     rank 13: 0.09
     rank 14: 0.05
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.06
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.07
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 47.98
     rank  1: 47.85
     rank  2: 48.40
     rank  3: 48.59
     rank  4: 0.09
     rank  5: 0.13
     rank  6: 0.13
     rank  7: 0.10
     rank  8: 0.09
     rank  9: 0.25
     rank 10: 0.11
     rank 11: 0.53
     rank 12: 0.26
     rank 13: 0.37
     rank 14: 0.37
     rank 15: 0.28
     rank 16: 0.11
     rank 17: 0.08
     rank 18: 0.09
     rank 19: 0.10
  optimizer:
     rank  0: 50.50
     rank  1: 50.37
     rank  2: 50.91
     rank  3: 51.10
     rank  4: 2.58
     rank  5: 2.63
     rank  6: 2.65
     rank  7: 2.60
     rank  8: 2.59
     rank  9: 2.83
     rank 10: 2.60
     rank 11: 3.00
     rank 12: 2.77
     rank 13: 2.88
     rank 14: 2.90
     rank 15: 2.78
     rank 16: 2.63
     rank 17: 2.60
     rank 18: 2.60
     rank 19: 2.61
 [2024-12-05 17:20:32] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 10048.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.002605E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9994.09
     rank  1: 9994.01
     rank  2: 9994.08
     rank  3: 9993.93
     rank  4: 9992.56
     rank  5: 9992.54
     rank  6: 9992.67
     rank  7: 9992.49
     rank  8: 9992.65
     rank  9: 9992.59
     rank 10: 9992.67
     rank 11: 9992.65
     rank 12: 9992.72
     rank 13: 9992.79
     rank 14: 9992.83
     rank 15: 9992.71
     rank 16: 9992.57
     rank 17: 9992.53
     rank 18: 9992.61
     rank 19: 9992.54
  forward-compute:
     rank  0: 2866.62
     rank  1: 2869.96
     rank  2: 2860.76
     rank  3: 2860.83
     rank  4: 3810.62
     rank  5: 3815.71
     rank  6: 3813.18
     rank  7: 3814.08
     rank  8: 3821.64
     rank  9: 3825.14
     rank 10: 3824.85
     rank 11: 3824.30
     rank 12: 3329.87
     rank 13: 3335.40
     rank 14: 3325.32
     rank 15: 3322.63
     rank 16: 3560.86
     rank 17: 3560.44
     rank 18: 3574.95
     rank 19: 3564.18
  backward-compute:
     rank  0: 2704.72
     rank  1: 2704.88
     rank  2: 2703.28
     rank  3: 2707.81
     rank  4: 4120.55
     rank  5: 4125.08
     rank  6: 4122.13
     rank  7: 4119.21
     rank  8: 4091.11
     rank  9: 4100.75
     rank 10: 4101.53
     rank 11: 4090.19
     rank 12: 3382.37
     rank 13: 3380.22
     rank 14: 3383.10
     rank 15: 3380.07
     rank 16: 3806.00
     rank 17: 3807.51
     rank 18: 3809.38
     rank 19: 3804.33
  pure-backward-compute:
     rank  0: 2703.70
     rank  1: 2703.82
     rank  2: 2702.22
     rank  3: 2706.69
     rank  4: 4119.46
     rank  5: 4124.05
     rank  6: 4120.61
     rank  7: 4118.07
     rank  8: 4090.15
     rank  9: 4099.86
     rank 10: 4100.56
     rank 11: 4088.80
     rank 12: 3380.99
     rank 13: 3378.58
     rank 14: 3381.66
     rank 15: 3378.62
     rank 16: 3803.58
     rank 17: 3805.38
     rank 18: 3807.46
     rank 19: 3801.87
  batch-generator:
     rank  0: 78.72
     rank  1: 88.01
     rank  2: 87.82
     rank  3: 85.30
     rank  4: 53.42
     rank  5: 59.68
     rank  6: 57.90
     rank  7: 59.20
     rank  8: 51.14
     rank  9: 57.74
     rank 10: 56.17
     rank 11: 55.72
     rank 12: 67.22
     rank 13: 78.57
     rank 14: 77.06
     rank 15: 71.43
     rank 16: 71.46
     rank 17: 73.54
     rank 18: 90.08
     rank 19: 81.81
  forward-recv:
     rank  4: 224.30
     rank  5: 221.86
     rank  6: 223.30
     rank  7: 223.74
     rank  8: 463.58
     rank  9: 463.59
     rank 10: 464.07
     rank 11: 462.97
     rank 12: 732.57
     rank 13: 731.71
     rank 14: 733.72
     rank 15: 733.57
     rank 16: 909.75
     rank 17: 909.72
     rank 18: 908.49
     rank 19: 909.36
  forward-send:
     rank  0: 242.71
     rank  1: 237.77
     rank  2: 242.34
     rank  3: 244.34
     rank  4: 31.56
     rank  5: 29.61
     rank  6: 30.83
     rank  7: 32.39
     rank  8: 21.23
     rank  9: 20.01
     rank 10: 20.48
     rank 11: 20.45
     rank 12: 10.59
     rank 13: 10.74
     rank 14: 9.84
     rank 15: 10.50
  backward-recv:
     rank  0: 1276.45
     rank  1: 1277.15
     rank  2: 1277.30
     rank  3: 1276.22
     rank  4: 625.54
     rank  5: 625.00
     rank  6: 626.34
     rank  7: 625.84
     rank  8: 388.15
     rank  9: 389.08
     rank 10: 385.97
     rank 11: 387.49
     rank 12: 249.59
     rank 13: 248.82
     rank 14: 251.02
     rank 15: 249.76
  backward-send:
     rank  4: 44.07
     rank  5: 44.15
     rank  6: 43.68
     rank  7: 43.12
     rank  8: 31.30
     rank  9: 30.70
     rank 10: 31.30
     rank 11: 31.01
     rank 12: 20.85
     rank 13: 21.22
     rank 14: 19.13
     rank 15: 20.75
     rank 16: 10.76
     rank 17: 10.24
     rank 18: 10.70
     rank 19: 10.72
  forward-send-backward-recv:
     rank  0: 2885.24
     rank  1: 2886.31
     rank  2: 2889.41
     rank  3: 2885.22
     rank  4: 802.61
     rank  5: 800.22
     rank  6: 801.55
     rank  7: 802.77
     rank  8: 558.17
     rank  9: 551.61
     rank 10: 551.84
     rank 11: 556.33
     rank 12: 492.44
     rank 13: 493.01
     rank 14: 489.10
     rank 15: 493.67
  backward-send-forward-recv:
     rank  4: 151.86
     rank  5: 151.01
     rank  6: 151.23
     rank  7: 148.96
     rank  8: 166.31
     rank  9: 163.20
     rank 10: 164.03
     rank 11: 166.18
     rank 12: 1062.32
     rank 13: 1056.94
     rank 14: 1065.58
     rank 15: 1068.03
     rank 16: 770.13
     rank 17: 771.93
     rank 18: 756.81
     rank 19: 768.86
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.06
     rank  2: 0.10
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.07
  all-grads-sync:
     rank  0: 1.09
     rank  1: 0.99
     rank  2: 1.16
     rank  3: 0.97
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.26
     rank  3: 0.25
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.11
     rank  1: 16.33
     rank  2: 16.59
     rank  3: 16.09
     rank  4: 0.03
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.09
     rank 13: 0.09
     rank 14: 0.16
     rank 15: 0.10
     rank 16: 0.07
     rank 17: 0.08
     rank 18: 0.09
     rank 19: 0.04
  optimizer:
     rank  0: 17.03
     rank  1: 17.27
     rank  2: 17.52
     rank  3: 17.02
     rank  4: 0.96
     rank  5: 1.00
     rank  6: 1.00
     rank  7: 1.00
     rank  8: 0.96
     rank  9: 1.00
     rank 10: 1.00
     rank 11: 1.03
     rank 12: 1.03
     rank 13: 1.03
     rank 14: 1.09
     rank 15: 1.03
     rank 16: 1.01
     rank 17: 1.02
     rank 18: 1.02
     rank 19: 0.98
 [2024-12-05 17:20:42] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 10065.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.094562E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10017.99
     rank  1: 10017.93
     rank  2: 10017.95
     rank  3: 10017.98
     rank  4: 10016.56
     rank  5: 10016.45
     rank  6: 10016.48
     rank  7: 10016.51
     rank  8: 10016.62
     rank  9: 10016.52
     rank 10: 10016.56
     rank 11: 10017.24
     rank 12: 10016.75
     rank 13: 10016.61
     rank 14: 10016.66
     rank 15: 10016.67
     rank 16: 10016.45
     rank 17: 10016.52
     rank 18: 10016.39
     rank 19: 10016.47
  forward-compute:
     rank  0: 2874.89
     rank  1: 2880.10
     rank  2: 2870.60
     rank  3: 2871.42
     rank  4: 3808.67
     rank  5: 3814.96
     rank  6: 3811.78
     rank  7: 3810.43
     rank  8: 3838.53
     rank  9: 3842.36
     rank 10: 3840.66
     rank 11: 3840.20
     rank 12: 3340.98
     rank 13: 3346.75
     rank 14: 3338.71
     rank 15: 3335.30
     rank 16: 3569.71
     rank 17: 3578.51
     rank 18: 3583.36
     rank 19: 3575.39
  backward-compute:
     rank  0: 2703.31
     rank  1: 2705.37
     rank  2: 2703.97
     rank  3: 2706.16
     rank  4: 4136.17
     rank  5: 4141.38
     rank  6: 4137.20
     rank  7: 4135.26
     rank  8: 4087.22
     rank  9: 4096.80
     rank 10: 4098.59
     rank 11: 4086.64
     rank 12: 3393.91
     rank 13: 3398.32
     rank 14: 3396.21
     rank 15: 3395.34
     rank 16: 3812.74
     rank 17: 3811.41
     rank 18: 3815.06
     rank 19: 3810.08
  pure-backward-compute:
     rank  0: 2702.29
     rank  1: 2704.33
     rank  2: 2702.90
     rank  3: 2705.05
     rank  4: 4135.15
     rank  5: 4140.25
     rank  6: 4135.94
     rank  7: 4134.17
     rank  8: 4086.00
     rank  9: 4095.82
     rank 10: 4097.46
     rank 11: 4085.45
     rank 12: 3392.25
     rank 13: 3397.11
     rank 14: 3393.12
     rank 15: 3393.46
     rank 16: 3810.00
     rank 17: 3809.76
     rank 18: 3813.31
     rank 19: 3807.22
  batch-generator:
     rank  0: 74.15
     rank  1: 85.30
     rank  2: 85.13
     rank  3: 84.07
     rank  4: 55.67
     rank  5: 62.62
     rank  6: 60.54
     rank  7: 59.70
     rank  8: 63.31
     rank  9: 67.97
     rank 10: 66.00
     rank 11: 64.93
     rank 12: 76.65
     rank 13: 86.49
     rank 14: 90.62
     rank 15: 81.67
     rank 16: 70.07
     rank 17: 81.06
     rank 18: 88.33
     rank 19: 83.07
  forward-recv:
     rank  4: 224.22
     rank  5: 220.97
     rank  6: 222.92
     rank  7: 223.19
     rank  8: 467.11
     rank  9: 467.07
     rank 10: 467.65
     rank 11: 467.12
     rank 12: 741.64
     rank 13: 741.34
     rank 14: 743.29
     rank 15: 742.18
     rank 16: 913.39
     rank 17: 913.12
     rank 18: 912.07
     rank 19: 913.32
  forward-send:
     rank  0: 253.50
     rank  1: 246.84
     rank  2: 252.01
     rank  3: 253.27
     rank  4: 34.53
     rank  5: 32.13
     rank  6: 33.23
     rank  7: 34.52
     rank  8: 25.77
     rank  9: 24.49
     rank 10: 24.75
     rank 11: 25.68
     rank 12: 10.70
     rank 13: 10.49
     rank 14: 9.74
     rank 15: 10.68
  backward-recv:
     rank  0: 1279.14
     rank  1: 1278.71
     rank  2: 1278.74
     rank  3: 1278.68
     rank  4: 631.56
     rank  5: 631.54
     rank  6: 632.87
     rank  7: 632.41
     rank  8: 393.11
     rank  9: 393.79
     rank 10: 390.69
     rank 11: 392.83
     rank 12: 242.96
     rank 13: 243.05
     rank 14: 244.61
     rank 15: 243.10
  backward-send:
     rank  4: 43.76
     rank  5: 43.20
     rank  6: 42.74
     rank  7: 42.90
     rank  8: 31.30
     rank  9: 30.73
     rank 10: 31.22
     rank 11: 31.02
     rank 12: 21.03
     rank 13: 20.93
     rank 14: 19.03
     rank 15: 20.97
     rank 16: 10.83
     rank 17: 10.35
     rank 18: 10.69
     rank 19: 10.75
  forward-send-backward-recv:
     rank  0: 2887.58
     rank  1: 2887.25
     rank  2: 2891.46
     rank  3: 2889.07
     rank  4: 801.45
     rank  5: 798.63
     rank  6: 800.39
     rank  7: 801.55
     rank  8: 551.88
     rank  9: 546.00
     rank 10: 547.07
     rank 11: 550.24
     rank 12: 487.12
     rank 13: 484.83
     rank 14: 478.99
     rank 15: 485.70
  backward-send-forward-recv:
     rank  4: 155.06
     rank  5: 154.65
     rank  6: 155.39
     rank  7: 154.87
     rank  8: 170.48
     rank  9: 167.03
     rank 10: 167.57
     rank 11: 170.48
     rank 12: 1065.08
     rank 13: 1060.15
     rank 14: 1064.77
     rank 15: 1069.29
     rank 16: 775.10
     rank 17: 770.04
     rank 18: 763.22
     rank 19: 771.88
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.07
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.07
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.98
     rank  1: 0.98
     rank  2: 1.02
     rank  3: 0.99
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.05
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.26
     rank  2: 0.25
     rank  3: 0.25
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.06
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.03
     rank  1: 16.47
     rank  2: 16.39
     rank  3: 16.36
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.05
     rank 12: 0.13
     rank 13: 0.08
     rank 14: 0.10
     rank 15: 0.08
     rank 16: 0.07
     rank 17: 0.09
     rank 18: 0.04
     rank 19: 0.09
  optimizer:
     rank  0: 17.17
     rank  1: 17.49
     rank  2: 17.41
     rank  3: 17.39
     rank  4: 1.05
     rank  5: 1.05
     rank  6: 1.08
     rank  7: 1.06
     rank  8: 1.08
     rank  9: 1.09
     rank 10: 1.08
     rank 11: 1.07
     rank 12: 1.15
     rank 13: 1.09
     rank 14: 1.12
     rank 15: 1.10
     rank 16: 1.10
     rank 17: 1.11
     rank 18: 1.06
     rank 19: 1.10
 [2024-12-05 17:20:52] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 10285.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.957464E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10231.12
     rank  1: 10231.15
     rank  2: 10231.07
     rank  3: 10231.08
     rank  4: 10229.59
     rank  5: 10229.61
     rank  6: 10229.64
     rank  7: 10229.57
     rank  8: 10229.69
     rank  9: 10229.71
     rank 10: 10229.70
     rank 11: 10229.72
     rank 12: 10229.74
     rank 13: 10229.80
     rank 14: 10230.79
     rank 15: 10229.80
     rank 16: 10229.66
     rank 17: 10229.70
     rank 18: 10229.60
     rank 19: 10229.65
  forward-compute:
     rank  0: 3076.66
     rank  1: 3084.21
     rank  2: 3071.21
     rank  3: 3072.06
     rank  4: 3810.32
     rank  5: 3819.56
     rank  6: 3814.45
     rank  7: 3810.45
     rank  8: 3821.95
     rank  9: 3827.39
     rank 10: 3824.84
     rank 11: 3823.55
     rank 12: 3357.44
     rank 13: 3363.46
     rank 14: 3354.98
     rank 15: 3349.98
     rank 16: 3573.75
     rank 17: 3579.64
     rank 18: 3587.92
     rank 19: 3578.06
  backward-compute:
     rank  0: 2699.40
     rank  1: 2701.29
     rank  2: 2702.07
     rank  3: 2699.01
     rank  4: 4122.38
     rank  5: 4127.87
     rank  6: 4125.28
     rank  7: 4122.33
     rank  8: 4070.80
     rank  9: 4080.18
     rank 10: 4079.70
     rank 11: 4070.20
     rank 12: 3396.66
     rank 13: 3397.49
     rank 14: 3396.13
     rank 15: 3393.00
     rank 16: 3814.58
     rank 17: 3814.00
     rank 18: 3817.11
     rank 19: 3812.03
  pure-backward-compute:
     rank  0: 2698.40
     rank  1: 2700.21
     rank  2: 2701.01
     rank  3: 2697.96
     rank  4: 4121.51
     rank  5: 4127.03
     rank  6: 4124.19
     rank  7: 4121.46
     rank  8: 4069.86
     rank  9: 4079.42
     rank 10: 4078.89
     rank 11: 4068.78
     rank 12: 3395.09
     rank 13: 3396.32
     rank 14: 3394.14
     rank 15: 3391.31
     rank 16: 3811.92
     rank 17: 3812.42
     rank 18: 3815.33
     rank 19: 3809.66
  batch-generator:
     rank  0: 71.66
     rank  1: 85.68
     rank  2: 80.92
     rank  3: 79.33
     rank  4: 50.93
     rank  5: 63.05
     rank  6: 58.67
     rank  7: 55.10
     rank  8: 51.72
     rank  9: 60.22
     rank 10: 57.91
     rank 11: 57.07
     rank 12: 74.35
     rank 13: 83.21
     rank 14: 86.66
     rank 15: 77.08
     rank 16: 68.04
     rank 17: 75.77
     rank 18: 86.47
     rank 19: 79.39
  forward-recv:
     rank  4: 449.16
     rank  5: 443.87
     rank  6: 447.95
     rank  7: 448.44
     rank  8: 688.97
     rank  9: 687.21
     rank 10: 688.93
     rank 11: 688.64
     rank 12: 957.92
     rank 13: 957.60
     rank 14: 959.47
     rank 15: 958.51
     rank 16: 1144.62
     rank 17: 1144.44
     rank 18: 1143.03
     rank 19: 1144.36
  forward-send:
     rank  0: 260.68
     rank  1: 251.36
     rank  2: 260.04
     rank  3: 261.22
     rank  4: 37.71
     rank  5: 34.05
     rank  6: 36.96
     rank  7: 37.93
     rank  8: 22.18
     rank  9: 21.15
     rank 10: 21.80
     rank 11: 22.37
     rank 12: 10.62
     rank 13: 10.50
     rank 14: 9.43
     rank 15: 10.57
  backward-recv:
     rank  0: 1284.37
     rank  1: 1283.79
     rank  2: 1283.92
     rank  3: 1284.35
     rank  4: 630.57
     rank  5: 630.39
     rank  6: 631.99
     rank  7: 630.63
     rank  8: 397.48
     rank  9: 398.65
     rank 10: 397.39
     rank 11: 396.66
     rank 12: 245.99
     rank 13: 245.69
     rank 14: 245.96
     rank 15: 246.05
  backward-send:
     rank  4: 44.11
     rank  5: 43.13
     rank  6: 42.81
     rank  7: 44.02
     rank  8: 31.46
     rank  9: 30.11
     rank 10: 31.41
     rank 11: 31.11
     rank 12: 21.55
     rank 13: 21.46
     rank 14: 21.64
     rank 15: 21.01
     rank 16: 10.78
     rank 17: 10.21
     rank 18: 10.61
     rank 19: 10.64
  forward-send-backward-recv:
     rank  0: 2891.80
     rank  1: 2891.99
     rank  2: 2893.91
     rank  3: 2895.04
     rank  4: 802.73
     rank  5: 799.59
     rank  6: 800.21
     rank  7: 801.74
     rank  8: 560.12
     rank  9: 554.67
     rank 10: 555.70
     rank 11: 557.63
     rank 12: 494.52
     rank 13: 495.36
     rank 14: 493.29
     rank 15: 496.92
  backward-send-forward-recv:
     rank  4: 151.61
     rank  5: 151.14
     rank  6: 149.92
     rank  7: 153.32
     rank  8: 185.81
     rank  9: 182.43
     rank 10: 182.55
     rank 11: 185.04
     rank 12: 1032.74
     rank 13: 1027.62
     rank 14: 1033.96
     rank 15: 1039.09
     rank 16: 750.75
     rank 17: 748.24
     rank 18: 738.18
     rank 19: 748.95
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.08
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.07
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.97
     rank  1: 0.98
     rank  2: 0.89
     rank  3: 0.92
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.25
     rank  3: 0.25
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.08
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.87
     rank  1: 16.39
     rank  2: 16.09
     rank  3: 15.90
     rank  4: 0.06
     rank  5: 0.07
     rank  6: 0.07
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.08
     rank 12: 0.05
     rank 13: 0.07
     rank 14: 0.55
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.08
  optimizer:
     rank  0: 17.51
     rank  1: 18.05
     rank  2: 17.76
     rank  3: 17.57
     rank  4: 1.71
     rank  5: 1.72
     rank  6: 1.72
     rank  7: 1.68
     rank  8: 1.69
     rank  9: 1.69
     rank 10: 1.69
     rank 11: 1.73
     rank 12: 1.70
     rank 13: 1.73
     rank 14: 2.21
     rank 15: 1.71
     rank 16: 1.70
     rank 17: 0.97
     rank 18: 1.70
     rank 19: 1.73
 [2024-12-05 17:21:02] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 10069.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.482935E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10015.08
     rank  1: 10015.08
     rank  2: 10015.03
     rank  3: 10015.04
     rank  4: 10013.56
     rank  5: 10013.58
     rank  6: 10013.56
     rank  7: 10013.52
     rank  8: 10013.68
     rank  9: 10013.66
     rank 10: 10013.64
     rank 11: 10013.77
     rank 12: 10013.79
     rank 13: 10013.75
     rank 14: 10013.87
     rank 15: 10013.87
     rank 16: 10013.61
     rank 17: 10013.55
     rank 18: 10013.55
     rank 19: 10013.67
  forward-compute:
     rank  0: 2876.36
     rank  1: 2878.70
     rank  2: 2871.47
     rank  3: 2871.41
     rank  4: 3820.03
     rank  5: 3823.35
     rank  6: 3823.64
     rank  7: 3824.06
     rank  8: 3818.79
     rank  9: 3823.50
     rank 10: 3820.91
     rank 11: 3819.55
     rank 12: 3380.14
     rank 13: 3386.64
     rank 14: 3379.06
     rank 15: 3373.19
     rank 16: 3579.56
     rank 17: 3584.79
     rank 18: 3592.50
     rank 19: 3583.60
  backward-compute:
     rank  0: 2700.49
     rank  1: 2702.41
     rank  2: 2701.99
     rank  3: 2704.94
     rank  4: 4117.40
     rank  5: 4123.87
     rank  6: 4118.89
     rank  7: 4115.36
     rank  8: 4077.77
     rank  9: 4087.49
     rank 10: 4089.66
     rank 11: 4075.82
     rank 12: 3408.89
     rank 13: 3409.82
     rank 14: 3408.14
     rank 15: 3406.28
     rank 16: 3818.23
     rank 17: 3818.17
     rank 18: 3820.92
     rank 19: 3815.90
  pure-backward-compute:
     rank  0: 2699.49
     rank  1: 2701.33
     rank  2: 2700.94
     rank  3: 2703.86
     rank  4: 4116.34
     rank  5: 4122.97
     rank  6: 4117.73
     rank  7: 4114.42
     rank  8: 4076.87
     rank  9: 4086.62
     rank 10: 4088.84
     rank 11: 4074.62
     rank 12: 3407.29
     rank 13: 3408.60
     rank 14: 3406.51
     rank 15: 3404.17
     rank 16: 3815.80
     rank 17: 3816.39
     rank 18: 3819.22
     rank 19: 3813.26
  batch-generator:
     rank  0: 70.93
     rank  1: 79.74
     rank  2: 81.08
     rank  3: 79.02
     rank  4: 49.71
     rank  5: 55.99
     rank  6: 56.76
     rank  7: 57.26
     rank  8: 50.05
     rank  9: 57.50
     rank 10: 55.81
     rank 11: 54.38
     rank 12: 72.35
     rank 13: 83.39
     rank 14: 86.90
     rank 15: 78.49
     rank 16: 70.48
     rank 17: 76.89
     rank 18: 86.86
     rank 19: 80.53
  forward-recv:
     rank  4: 224.24
     rank  5: 222.67
     rank  6: 223.31
     rank  7: 223.89
     rank  8: 465.34
     rank  9: 466.09
     rank 10: 465.33
     rank 11: 465.20
     rank 12: 734.39
     rank 13: 733.83
     rank 14: 735.14
     rank 15: 734.83
     rank 16: 911.98
     rank 17: 911.99
     rank 18: 911.42
     rank 19: 911.65
  forward-send:
     rank  0: 253.12
     rank  1: 249.24
     rank  2: 251.98
     rank  3: 254.23
     rank  4: 31.75
     rank  5: 29.98
     rank  6: 30.61
     rank  7: 31.61
     rank  8: 21.56
     rank  9: 20.02
     rank 10: 21.11
     rank 11: 21.59
     rank 12: 10.82
     rank 13: 10.78
     rank 14: 10.40
     rank 15: 10.74
  backward-recv:
     rank  0: 1290.23
     rank  1: 1290.60
     rank  2: 1290.21
     rank  3: 1289.19
     rank  4: 641.51
     rank  5: 641.21
     rank  6: 642.10
     rank  7: 642.92
     rank  8: 402.83
     rank  9: 403.01
     rank 10: 400.49
     rank 11: 402.25
     rank 12: 250.57
     rank 13: 250.76
     rank 14: 252.51
     rank 15: 250.19
  backward-send:
     rank  4: 44.41
     rank  5: 44.04
     rank  6: 44.01
     rank  7: 43.07
     rank  8: 31.22
     rank  9: 30.56
     rank 10: 31.22
     rank 11: 31.39
     rank 12: 21.01
     rank 13: 20.77
     rank 14: 19.13
     rank 15: 21.02
     rank 16: 10.84
     rank 17: 10.37
     rank 18: 10.63
     rank 19: 10.56
  forward-send-backward-recv:
     rank  0: 2876.06
     rank  1: 2876.04
     rank  2: 2878.73
     rank  3: 2875.82
     rank  4: 801.06
     rank  5: 797.25
     rank  6: 799.25
     rank  7: 801.22
     rank  8: 560.14
     rank  9: 554.45
     rank 10: 554.89
     rank 11: 558.33
     rank 12: 487.95
     rank 13: 488.51
     rank 14: 485.63
     rank 15: 489.38
  backward-send-forward-recv:
     rank  4: 152.14
     rank  5: 151.99
     rank  6: 151.60
     rank  7: 150.09
     rank  8: 184.65
     rank  9: 180.15
     rank 10: 181.92
     rank 11: 185.27
     rank 12: 1007.57
     rank 13: 1001.88
     rank 14: 1008.56
     rank 15: 1013.28
     rank 16: 755.96
     rank 17: 753.42
     rank 18: 743.61
     rank 19: 754.01
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.08
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.09
  all-grads-sync:
     rank  0: 0.92
     rank  1: 0.97
     rank  2: 0.92
     rank  3: 0.92
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.25
     rank  3: 0.24
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.79
     rank  1: 16.07
     rank  2: 15.97
     rank  3: 15.91
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.12
     rank 12: 0.09
     rank 13: 0.11
     rank 14: 0.16
     rank 15: 0.10
     rank 16: 0.04
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.05
  optimizer:
     rank  0: 16.72
     rank  1: 17.01
     rank  2: 16.83
     rank  3: 16.77
     rank  4: 0.89
     rank  5: 0.89
     rank  6: 0.93
     rank  7: 0.89
     rank  8: 0.89
     rank  9: 0.90
     rank 10: 0.89
     rank 11: 0.98
     rank 12: 0.94
     rank 13: 0.96
     rank 14: 1.04
     rank 15: 0.96
     rank 16: 0.90
     rank 17: 0.93
     rank 18: 0.93
     rank 19: 0.91
 [2024-12-05 17:21:12] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 10052.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.391147E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9998.68
     rank  1: 9998.68
     rank  2: 9998.76
     rank  3: 9998.69
     rank  4: 9997.15
     rank  5: 9997.19
     rank  6: 9997.34
     rank  7: 9997.18
     rank  8: 9997.23
     rank  9: 9997.26
     rank 10: 9997.35
     rank 11: 9997.28
     rank 12: 9997.30
     rank 13: 9997.35
     rank 14: 9997.48
     rank 15: 9997.37
     rank 16: 9997.25
     rank 17: 9997.20
     rank 18: 9997.17
     rank 19: 9997.22
  forward-compute:
     rank  0: 2899.69
     rank  1: 2902.76
     rank  2: 2893.10
     rank  3: 2894.78
     rank  4: 3805.40
     rank  5: 3809.40
     rank  6: 3807.25
     rank  7: 3807.98
     rank  8: 3818.90
     rank  9: 3823.40
     rank 10: 3820.62
     rank 11: 3820.03
     rank 12: 3372.10
     rank 13: 3377.94
     rank 14: 3369.28
     rank 15: 3366.13
     rank 16: 3583.70
     rank 17: 3589.51
     rank 18: 3600.70
     rank 19: 3586.74
  backward-compute:
     rank  0: 2706.86
     rank  1: 2707.22
     rank  2: 2706.36
     rank  3: 2709.26
     rank  4: 4120.70
     rank  5: 4127.30
     rank  6: 4123.51
     rank  7: 4119.55
     rank  8: 4069.96
     rank  9: 4078.47
     rank 10: 4080.72
     rank 11: 4068.38
     rank 12: 3410.97
     rank 13: 3411.84
     rank 14: 3412.43
     rank 15: 3406.71
     rank 16: 3819.88
     rank 17: 3819.94
     rank 18: 3822.89
     rank 19: 3818.11
  pure-backward-compute:
     rank  0: 2705.84
     rank  1: 2706.17
     rank  2: 2705.31
     rank  3: 2708.18
     rank  4: 4119.81
     rank  5: 4126.25
     rank  6: 4122.09
     rank  7: 4118.44
     rank  8: 4069.01
     rank  9: 4077.58
     rank 10: 4079.83
     rank 11: 4067.25
     rank 12: 3409.36
     rank 13: 3410.68
     rank 14: 3411.02
     rank 15: 3404.81
     rank 16: 3817.51
     rank 17: 3818.02
     rank 18: 3821.24
     rank 19: 3815.73
  batch-generator:
     rank  0: 69.87
     rank  1: 79.83
     rank  2: 79.03
     rank  3: 77.84
     rank  4: 56.98
     rank  5: 62.55
     rank  6: 60.74
     rank  7: 62.46
     rank  8: 50.77
     rank  9: 58.24
     rank 10: 55.86
     rank 11: 54.91
     rank 12: 71.71
     rank 13: 81.75
     rank 14: 84.93
     rank 15: 77.29
     rank 16: 67.48
     rank 17: 75.53
     rank 18: 89.19
     rank 19: 77.77
  forward-recv:
     rank  4: 224.43
     rank  5: 221.19
     rank  6: 223.04
     rank  7: 223.84
     rank  8: 463.19
     rank  9: 464.35
     rank 10: 464.19
     rank 11: 463.40
     rank 12: 730.41
     rank 13: 729.13
     rank 14: 731.69
     rank 15: 730.42
     rank 16: 911.79
     rank 17: 911.58
     rank 18: 910.19
     rank 19: 911.62
  forward-send:
     rank  0: 240.73
     rank  1: 236.40
     rank  2: 241.46
     rank  3: 241.94
     rank  4: 31.48
     rank  5: 30.18
     rank  6: 31.30
     rank  7: 31.53
     rank  8: 22.32
     rank  9: 20.50
     rank 10: 21.59
     rank 11: 22.02
     rank 12: 10.64
     rank 13: 10.42
     rank 14: 9.39
     rank 15: 10.64
  backward-recv:
     rank  0: 1280.76
     rank  1: 1281.17
     rank  2: 1281.09
     rank  3: 1280.20
     rank  4: 638.76
     rank  5: 638.37
     rank  6: 638.71
     rank  7: 639.51
     rank  8: 400.53
     rank  9: 401.08
     rank 10: 398.49
     rank 11: 399.97
     rank 12: 248.28
     rank 13: 248.04
     rank 14: 249.69
     rank 15: 248.56
  backward-send:
     rank  4: 45.58
     rank  5: 45.13
     rank  6: 45.47
     rank  7: 44.12
     rank  8: 31.09
     rank  9: 30.28
     rank 10: 31.03
     rank 11: 31.09
     rank 12: 21.03
     rank 13: 21.07
     rank 14: 19.43
     rank 15: 21.00
     rank 16: 10.79
     rank 17: 10.25
     rank 18: 10.66
     rank 19: 10.78
  forward-send-backward-recv:
     rank  0: 2851.63
     rank  1: 2853.02
     rank  2: 2855.71
     rank  3: 2852.69
     rank  4: 800.54
     rank  5: 796.97
     rank  6: 799.07
     rank  7: 800.78
     rank  8: 561.24
     rank  9: 556.19
     rank 10: 557.08
     rank 11: 559.78
     rank 12: 490.45
     rank 13: 491.72
     rank 14: 488.82
     rank 15: 493.06
  backward-send-forward-recv:
     rank  4: 149.78
     rank  5: 150.33
     rank  6: 150.03
     rank  7: 148.46
     rank  8: 181.20
     rank  9: 176.10
     rank 10: 176.83
     rank 11: 181.48
     rank 12: 1002.00
     rank 13: 998.03
     rank 14: 1004.10
     rank 15: 1006.96
     rank 16: 735.85
     rank 17: 732.70
     rank 18: 720.30
     rank 19: 734.77
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.06
     rank  2: 0.09
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.96
     rank  1: 0.94
     rank  2: 0.97
     rank  3: 0.94
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.25
     rank  3: 0.24
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.85
     rank  1: 16.04
     rank  2: 16.51
     rank  3: 15.94
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.08
     rank  7: 0.06
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.10
     rank 14: 0.05
     rank 15: 0.11
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 16.72
     rank  1: 16.92
     rank  2: 17.39
     rank  3: 16.82
     rank  4: 0.91
     rank  5: 0.91
     rank  6: 0.95
     rank  7: 0.94
     rank  8: 0.91
     rank  9: 0.95
     rank 10: 0.94
     rank 11: 0.92
     rank 12: 0.96
     rank 13: 0.98
     rank 14: 0.95
     rank 15: 0.99
     rank 16: 0.95
     rank 17: 0.95
     rank 18: 0.93
     rank 19: 0.92
 [2024-12-05 17:21:22] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 10052.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.530118E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9998.89
     rank  1: 9998.84
     rank  2: 9998.91
     rank  3: 9998.88
     rank  4: 9997.45
     rank  5: 9997.38
     rank  6: 9997.48
     rank  7: 9997.38
     rank  8: 9997.50
     rank  9: 9997.47
     rank 10: 9997.50
     rank 11: 9997.45
     rank 12: 9997.59
     rank 13: 9997.53
     rank 14: 9997.71
     rank 15: 9997.59
     rank 16: 9997.44
     rank 17: 9997.39
     rank 18: 9997.36
     rank 19: 9997.54
  forward-compute:
     rank  0: 2935.98
     rank  1: 2949.14
     rank  2: 2929.93
     rank  3: 2930.50
     rank  4: 3796.96
     rank  5: 3810.81
     rank  6: 3800.18
     rank  7: 3799.25
     rank  8: 3822.56
     rank  9: 3828.99
     rank 10: 3824.21
     rank 11: 3822.42
     rank 12: 3379.00
     rank 13: 3387.16
     rank 14: 3375.97
     rank 15: 3372.78
     rank 16: 3592.92
     rank 17: 3598.76
     rank 18: 3609.67
     rank 19: 3595.19
  backward-compute:
     rank  0: 2712.95
     rank  1: 2713.24
     rank  2: 2712.85
     rank  3: 2716.59
     rank  4: 4114.06
     rank  5: 4120.55
     rank  6: 4116.98
     rank  7: 4112.84
     rank  8: 4079.53
     rank  9: 4091.37
     rank 10: 4090.73
     rank 11: 4079.34
     rank 12: 3413.72
     rank 13: 3414.29
     rank 14: 3414.71
     rank 15: 3409.70
     rank 16: 3820.88
     rank 17: 3820.51
     rank 18: 3823.70
     rank 19: 3818.76
  pure-backward-compute:
     rank  0: 2711.95
     rank  1: 2712.20
     rank  2: 2711.70
     rank  3: 2715.48
     rank  4: 4112.82
     rank  5: 4119.51
     rank  6: 4115.73
     rank  7: 4111.78
     rank  8: 4078.56
     rank  9: 4090.59
     rank 10: 4089.91
     rank 11: 4078.08
     rank 12: 3412.13
     rank 13: 3413.24
     rank 14: 3413.23
     rank 15: 3407.87
     rank 16: 3818.52
     rank 17: 3818.61
     rank 18: 3822.04
     rank 19: 3816.43
  batch-generator:
     rank  0: 69.11
     rank  1: 88.04
     rank  2: 79.34
     rank  3: 76.98
     rank  4: 53.57
     rank  5: 69.40
     rank  6: 59.28
     rank  7: 58.95
     rank  8: 51.21
     rank  9: 62.06
     rank 10: 57.38
     rank 11: 55.60
     rank 12: 69.73
     rank 13: 81.63
     rank 14: 82.68
     rank 15: 74.85
     rank 16: 67.30
     rank 17: 75.28
     rank 18: 88.82
     rank 19: 76.67
  forward-recv:
     rank  4: 221.35
     rank  5: 212.77
     rank  6: 220.50
     rank  7: 220.82
     rank  8: 461.25
     rank  9: 458.33
     rank 10: 461.16
     rank 11: 460.53
     rank 12: 730.81
     rank 13: 729.85
     rank 14: 732.41
     rank 15: 731.34
     rank 16: 912.27
     rank 17: 912.07
     rank 18: 910.52
     rank 19: 912.08
  forward-send:
     rank  0: 228.64
     rank  1: 213.87
     rank  2: 227.97
     rank  3: 229.89
     rank  4: 33.29
     rank  5: 27.71
     rank  6: 32.17
     rank  7: 33.53
     rank  8: 21.45
     rank  9: 19.94
     rank 10: 20.93
     rank 11: 21.53
     rank 12: 10.59
     rank 13: 10.44
     rank 14: 9.21
     rank 15: 10.58
  backward-recv:
     rank  0: 1284.47
     rank  1: 1284.83
     rank  2: 1284.40
     rank  3: 1282.87
     rank  4: 639.27
     rank  5: 639.29
     rank  6: 640.49
     rank  7: 640.88
     rank  8: 400.29
     rank  9: 400.96
     rank 10: 398.42
     rank 11: 399.35
     rank 12: 248.85
     rank 13: 248.62
     rank 14: 250.15
     rank 15: 249.13
  backward-send:
     rank  4: 45.93
     rank  5: 45.36
     rank  6: 45.18
     rank  7: 44.29
     rank  8: 31.34
     rank  9: 30.36
     rank 10: 31.23
     rank 11: 31.35
     rank 12: 21.17
     rank 13: 21.06
     rank 14: 19.71
     rank 15: 20.94
     rank 16: 10.79
     rank 17: 10.25
     rank 18: 10.69
     rank 19: 10.71
  forward-send-backward-recv:
     rank  0: 2818.83
     rank  1: 2819.95
     rank  2: 2821.97
     rank  3: 2818.41
     rank  4: 814.86
     rank  5: 811.42
     rank  6: 812.95
     rank  7: 815.43
     rank  8: 565.42
     rank  9: 559.45
     rank 10: 562.10
     rank 11: 564.20
     rank 12: 489.15
     rank 13: 490.62
     rank 14: 488.26
     rank 15: 492.67
  backward-send-forward-recv:
     rank  4: 150.57
     rank  5: 150.82
     rank  6: 150.30
     rank  7: 149.18
     rank  8: 165.41
     rank  9: 161.31
     rank 10: 162.39
     rank 11: 165.54
     rank 12: 991.32
     rank 13: 984.71
     rank 14: 993.10
     rank 15: 996.00
     rank 16: 724.74
     rank 17: 721.61
     rank 18: 709.73
     rank 19: 724.70
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.06
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.08
  all-grads-sync:
     rank  0: 0.91
     rank  1: 0.95
     rank  2: 1.00
     rank  3: 0.94
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.28
     rank  1: 0.28
     rank  2: 0.26
     rank  3: 0.26
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.01
     rank  1: 16.29
     rank  2: 16.20
     rank  3: 16.01
     rank  4: 0.03
     rank  5: 0.06
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.05
     rank 13: 0.06
     rank 14: 0.10
     rank 15: 0.05
     rank 16: 0.05
     rank 17: 0.08
     rank 18: 0.04
     rank 19: 0.08
  optimizer:
     rank  0: 16.90
     rank  1: 17.17
     rank  2: 17.08
     rank  3: 16.89
     rank  4: 0.91
     rank  5: 0.94
     rank  6: 0.93
     rank  7: 0.92
     rank  8: 0.91
     rank  9: 0.92
     rank 10: 0.92
     rank 11: 0.94
     rank 12: 0.93
     rank 13: 0.94
     rank 14: 0.98
     rank 15: 0.93
     rank 16: 0.93
     rank 17: 0.96
     rank 18: 0.92
     rank 19: 0.96
 [2024-12-05 17:21:32] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 10057.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.867164E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10005.16
     rank  1: 10005.16
     rank  2: 10005.16
     rank  3: 10005.16
     rank  4: 10003.68
     rank  5: 10003.68
     rank  6: 10003.72
     rank  7: 10003.68
     rank  8: 10003.77
     rank  9: 10003.74
     rank 10: 10003.74
     rank 11: 10003.76
     rank 12: 10003.81
     rank 13: 10003.79
     rank 14: 10003.83
     rank 15: 10003.84
     rank 16: 10003.74
     rank 17: 10003.69
     rank 18: 10003.67
     rank 19: 10003.74
  forward-compute:
     rank  0: 2931.58
     rank  1: 2945.35
     rank  2: 2925.07
     rank  3: 2926.42
     rank  4: 3796.85
     rank  5: 3812.67
     rank  6: 3800.89
     rank  7: 3798.29
     rank  8: 3824.63
     rank  9: 3831.60
     rank 10: 3825.94
     rank 11: 3825.87
     rank 12: 3398.43
     rank 13: 3405.54
     rank 14: 3396.50
     rank 15: 3392.15
     rank 16: 3588.86
     rank 17: 3595.48
     rank 18: 3606.71
     rank 19: 3592.99
  backward-compute:
     rank  0: 2708.16
     rank  1: 2709.83
     rank  2: 2709.02
     rank  3: 2711.46
     rank  4: 4115.67
     rank  5: 4120.54
     rank  6: 4116.50
     rank  7: 4114.25
     rank  8: 4094.56
     rank  9: 4104.02
     rank 10: 4105.74
     rank 11: 4093.98
     rank 12: 3429.64
     rank 13: 3430.89
     rank 14: 3431.42
     rank 15: 3425.57
     rank 16: 3830.68
     rank 17: 3830.70
     rank 18: 3833.74
     rank 19: 3828.69
  pure-backward-compute:
     rank  0: 2707.14
     rank  1: 2708.75
     rank  2: 2707.80
     rank  3: 2710.39
     rank  4: 4114.68
     rank  5: 4119.63
     rank  6: 4115.44
     rank  7: 4113.27
     rank  8: 4093.64
     rank  9: 4103.23
     rank 10: 4104.92
     rank 11: 4092.92
     rank 12: 3428.13
     rank 13: 3429.62
     rank 14: 3430.06
     rank 15: 3423.90
     rank 16: 3828.36
     rank 17: 3828.90
     rank 18: 3832.07
     rank 19: 3826.40
  batch-generator:
     rank  0: 67.02
     rank  1: 87.38
     rank  2: 77.16
     rank  3: 75.21
     rank  4: 51.31
     rank  5: 69.56
     rank  6: 57.89
     rank  7: 56.90
     rank  8: 52.01
     rank  9: 62.52
     rank 10: 56.87
     rank 11: 56.79
     rank 12: 70.07
     rank 13: 80.93
     rank 14: 83.93
     rank 15: 74.94
     rank 16: 67.38
     rank 17: 76.50
     rank 18: 89.96
     rank 19: 78.94
  forward-recv:
     rank  4: 217.37
     rank  5: 208.51
     rank  6: 216.68
     rank  7: 217.31
     rank  8: 455.73
     rank  9: 452.90
     rank 10: 455.96
     rank 11: 456.06
     rank 12: 727.87
     rank 13: 726.65
     rank 14: 729.15
     rank 15: 728.01
     rank 16: 905.54
     rank 17: 905.39
     rank 18: 904.00
     rank 19: 905.28
  forward-send:
     rank  0: 225.73
     rank  1: 210.29
     rank  2: 225.51
     rank  3: 226.66
     rank  4: 34.08
     rank  5: 28.15
     rank  6: 32.98
     rank  7: 33.87
     rank  8: 22.40
     rank  9: 20.46
     rank 10: 21.68
     rank 11: 21.83
     rank 12: 10.61
     rank 13: 10.49
     rank 14: 9.39
     rank 15: 10.59
  backward-recv:
     rank  0: 1285.28
     rank  1: 1284.64
     rank  2: 1285.55
     rank  3: 1283.38
     rank  4: 644.84
     rank  5: 644.23
     rank  6: 645.06
     rank  7: 646.18
     rank  8: 403.31
     rank  9: 404.06
     rank 10: 400.94
     rank 11: 402.74
     rank 12: 247.31
     rank 13: 246.82
     rank 14: 248.74
     rank 15: 247.49
  backward-send:
     rank  4: 43.56
     rank  5: 43.31
     rank  6: 43.55
     rank  7: 41.90
     rank  8: 31.37
     rank  9: 30.57
     rank 10: 31.30
     rank 11: 30.96
     rank 12: 20.94
     rank 13: 21.01
     rank 14: 19.09
     rank 15: 20.79
     rank 16: 10.86
     rank 17: 10.40
     rank 18: 10.64
     rank 19: 10.57
  forward-send-backward-recv:
     rank  0: 2837.01
     rank  1: 2836.36
     rank  2: 2838.76
     rank  3: 2837.60
     rank  4: 819.88
     rank  5: 817.25
     rank  6: 818.80
     rank  7: 820.05
     rank  8: 561.33
     rank  9: 555.95
     rank 10: 557.05
     rank 11: 559.89
     rank 12: 483.84
     rank 13: 485.28
     rank 14: 482.39
     rank 15: 487.68
  backward-send-forward-recv:
     rank  4: 150.99
     rank  5: 150.39
     rank  6: 150.30
     rank  7: 150.79
     rank  8: 160.03
     rank  9: 156.87
     rank 10: 158.25
     rank 11: 160.34
     rank 12: 972.62
     rank 13: 967.15
     rank 14: 973.77
     rank 15: 977.90
     rank 16: 731.49
     rank 17: 727.66
     rank 18: 715.01
     rank 19: 729.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.91
     rank  1: 0.92
     rank  2: 0.95
     rank  3: 0.92
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.05
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.24
     rank  2: 0.26
     rank  3: 0.24
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.03
     rank  1: 15.92
     rank  2: 16.25
     rank  3: 16.15
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.06
     rank 14: 0.15
     rank 15: 0.09
     rank 16: 0.04
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 16.85
     rank  1: 16.72
     rank  2: 17.07
     rank  3: 16.97
     rank  4: 0.85
     rank  5: 0.85
     rank  6: 0.87
     rank  7: 0.85
     rank  8: 0.88
     rank  9: 0.85
     rank 10: 0.85
     rank 11: 0.86
     rank 12: 0.86
     rank 13: 0.88
     rank 14: 1.01
     rank 15: 0.90
     rank 16: 0.86
     rank 17: 0.87
     rank 18: 0.86
     rank 19: 0.86
 [2024-12-05 17:21:42] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10049.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.808594E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9997.04
     rank  1: 9996.98
     rank  2: 9997.06
     rank  3: 9997.03
     rank  4: 9995.51
     rank  5: 9995.54
     rank  6: 9995.55
     rank  7: 9995.50
     rank  8: 9995.59
     rank  9: 9995.58
     rank 10: 9995.61
     rank 11: 9995.62
     rank 12: 9995.63
     rank 13: 9995.65
     rank 14: 9995.80
     rank 15: 9995.66
     rank 16: 9995.57
     rank 17: 9995.51
     rank 18: 9995.52
     rank 19: 9995.57
  forward-compute:
     rank  0: 2939.80
     rank  1: 2952.97
     rank  2: 2934.73
     rank  3: 2934.47
     rank  4: 3789.71
     rank  5: 3805.14
     rank  6: 3796.57
     rank  7: 3791.65
     rank  8: 3823.28
     rank  9: 3829.58
     rank 10: 3824.77
     rank 11: 3824.06
     rank 12: 3386.63
     rank 13: 3394.15
     rank 14: 3384.67
     rank 15: 3380.64
     rank 16: 3586.14
     rank 17: 3592.29
     rank 18: 3603.22
     rank 19: 3589.89
  backward-compute:
     rank  0: 2717.00
     rank  1: 2719.45
     rank  2: 2718.90
     rank  3: 2719.15
     rank  4: 4113.73
     rank  5: 4117.88
     rank  6: 4113.99
     rank  7: 4111.89
     rank  8: 4094.34
     rank  9: 4105.39
     rank 10: 4106.48
     rank 11: 4093.94
     rank 12: 3424.69
     rank 13: 3425.52
     rank 14: 3426.34
     rank 15: 3420.44
     rank 16: 3831.43
     rank 17: 3831.44
     rank 18: 3833.93
     rank 19: 3828.99
  pure-backward-compute:
     rank  0: 2715.86
     rank  1: 2718.38
     rank  2: 2717.75
     rank  3: 2718.07
     rank  4: 4112.72
     rank  5: 4117.09
     rank  6: 4113.13
     rank  7: 4110.88
     rank  8: 4093.46
     rank  9: 4104.55
     rank 10: 4105.75
     rank 11: 4092.87
     rank 12: 3423.23
     rank 13: 3424.43
     rank 14: 3425.03
     rank 15: 3418.96
     rank 16: 3829.08
     rank 17: 3829.58
     rank 18: 3832.30
     rank 19: 3826.73
  batch-generator:
     rank  0: 70.78
     rank  1: 88.89
     rank  2: 79.99
     rank  3: 76.97
     rank  4: 51.20
     rank  5: 68.73
     rank  6: 60.19
     rank  7: 57.14
     rank  8: 51.55
     rank  9: 61.37
     rank 10: 56.78
     rank 11: 55.94
     rank 12: 68.68
     rank 13: 79.89
     rank 14: 82.42
     rank 15: 73.85
     rank 16: 67.42
     rank 17: 75.77
     rank 18: 88.99
     rank 19: 78.11
  forward-recv:
     rank  4: 221.22
     rank  5: 213.01
     rank  6: 220.31
     rank  7: 220.94
     rank  8: 461.44
     rank  9: 458.77
     rank 10: 461.30
     rank 11: 461.48
     rank 12: 733.05
     rank 13: 731.90
     rank 14: 734.11
     rank 15: 733.34
     rank 16: 913.01
     rank 17: 912.83
     rank 18: 911.72
     rank 19: 912.75
  forward-send:
     rank  0: 229.95
     rank  1: 215.34
     rank  2: 228.69
     rank  3: 231.17
     rank  4: 38.57
     rank  5: 32.86
     rank  6: 36.76
     rank  7: 38.65
     rank  8: 23.29
     rank  9: 21.42
     rank 10: 22.47
     rank 11: 23.09
     rank 12: 10.64
     rank 13: 10.49
     rank 14: 9.62
     rank 15: 10.61
  backward-recv:
     rank  0: 1282.89
     rank  1: 1282.51
     rank  2: 1283.46
     rank  3: 1282.57
     rank  4: 639.62
     rank  5: 639.11
     rank  6: 640.41
     rank  7: 640.94
     rank  8: 396.96
     rank  9: 397.77
     rank 10: 395.07
     rank 11: 396.02
     rank 12: 248.71
     rank 13: 248.88
     rank 14: 250.20
     rank 15: 248.92
  backward-send:
     rank  4: 43.54
     rank  5: 42.69
     rank  6: 42.69
     rank  7: 42.49
     rank  8: 31.35
     rank  9: 30.31
     rank 10: 31.20
     rank 11: 31.35
     rank 12: 21.06
     rank 13: 21.03
     rank 14: 19.41
     rank 15: 20.92
     rank 16: 10.84
     rank 17: 10.39
     rank 18: 10.65
     rank 19: 10.74
  forward-send-backward-recv:
     rank  0: 2809.42
     rank  1: 2808.68
     rank  2: 2810.03
     rank  3: 2809.59
     rank  4: 817.58
     rank  5: 815.60
     rank  6: 816.03
     rank  7: 817.50
     rank  8: 563.20
     rank  9: 557.18
     rank 10: 558.07
     rank 11: 561.82
     rank 12: 484.65
     rank 13: 485.96
     rank 14: 483.06
     rank 15: 487.90
  backward-send-forward-recv:
     rank  4: 150.28
     rank  5: 149.30
     rank  6: 148.66
     rank  7: 149.91
     rank  8: 150.54
     rank  9: 147.45
     rank 10: 149.12
     rank 11: 151.02
     rank 12: 972.19
     rank 13: 966.22
     rank 14: 973.40
     rank 15: 977.41
     rank 16: 715.28
     rank 17: 711.82
     rank 18: 699.66
     rank 19: 714.09
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.06
     rank  2: 0.08
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.96
     rank  1: 0.92
     rank  2: 0.99
     rank  3: 0.95
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.24
     rank  2: 0.26
     rank  3: 0.26
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.81
     rank  1: 16.24
     rank  2: 16.11
     rank  3: 15.93
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.08
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.04
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.04
  optimizer:
     rank  0: 16.70
     rank  1: 17.12
     rank  2: 16.98
     rank  3: 16.81
     rank  4: 0.91
     rank  5: 0.90
     rank  6: 0.93
     rank  7: 0.91
     rank  8: 0.91
     rank  9: 0.91
     rank 10: 0.90
     rank 11: 0.90
     rank 12: 0.91
     rank 13: 0.95
     rank 14: 0.92
     rank 15: 0.92
     rank 16: 0.92
     rank 17: 0.95
     rank 18: 0.95
     rank 19: 0.92
 [2024-12-05 17:21:53] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 10068.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.761298E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10014.62
     rank  1: 10014.47
     rank  2: 10014.46
     rank  3: 10014.47
     rank  4: 10013.05
     rank  5: 10013.02
     rank  6: 10013.04
     rank  7: 10013.02
     rank  8: 10013.11
     rank  9: 10013.08
     rank 10: 10013.09
     rank 11: 10013.11
     rank 12: 10013.19
     rank 13: 10013.17
     rank 14: 10013.18
     rank 15: 10013.19
     rank 16: 10013.06
     rank 17: 10013.03
     rank 18: 10013.01
     rank 19: 10013.06
  forward-compute:
     rank  0: 2947.08
     rank  1: 2955.91
     rank  2: 2942.14
     rank  3: 2942.37
     rank  4: 3802.60
     rank  5: 3819.64
     rank  6: 3814.34
     rank  7: 3809.80
     rank  8: 3826.91
     rank  9: 3832.44
     rank 10: 3828.91
     rank 11: 3828.53
     rank 12: 3399.34
     rank 13: 3406.27
     rank 14: 3396.53
     rank 15: 3393.02
     rank 16: 3591.49
     rank 17: 3598.26
     rank 18: 3608.95
     rank 19: 3595.64
  backward-compute:
     rank  0: 2717.82
     rank  1: 2726.88
     rank  2: 2726.70
     rank  3: 2724.08
     rank  4: 4115.34
     rank  5: 4119.27
     rank  6: 4115.02
     rank  7: 4114.47
     rank  8: 4083.00
     rank  9: 4092.75
     rank 10: 4092.75
     rank 11: 4081.51
     rank 12: 3425.56
     rank 13: 3426.67
     rank 14: 3427.31
     rank 15: 3421.19
     rank 16: 3837.31
     rank 17: 3836.24
     rank 18: 3839.90
     rank 19: 3834.52
  pure-backward-compute:
     rank  0: 2716.64
     rank  1: 2725.81
     rank  2: 2725.55
     rank  3: 2723.06
     rank  4: 4114.26
     rank  5: 4118.52
     rank  6: 4114.03
     rank  7: 4113.41
     rank  8: 4082.13
     rank  9: 4091.99
     rank 10: 4091.97
     rank 11: 4080.43
     rank 12: 3424.17
     rank 13: 3425.61
     rank 14: 3425.98
     rank 15: 3419.73
     rank 16: 3834.75
     rank 17: 3834.65
     rank 18: 3838.24
     rank 19: 3832.26
  batch-generator:
     rank  0: 74.69
     rank  1: 87.03
     rank  2: 83.36
     rank  3: 80.15
     rank  4: 51.32
     rank  5: 70.45
     rank  6: 64.85
     rank  7: 61.65
     rank  8: 52.29
     rank  9: 60.31
     rank 10: 56.46
     rank 11: 56.39
     rank 12: 69.16
     rank 13: 79.95
     rank 14: 82.01
     rank 15: 74.05
     rank 16: 67.01
     rank 17: 76.03
     rank 18: 89.08
     rank 19: 78.31
  forward-recv:
     rank  4: 224.23
     rank  5: 218.89
     rank  6: 223.30
     rank  7: 223.80
     rank  8: 463.58
     rank  9: 461.77
     rank 10: 463.45
     rank 11: 463.46
     rank 12: 735.05
     rank 13: 734.46
     rank 14: 736.45
     rank 15: 735.45
     rank 16: 914.78
     rank 17: 914.57
     rank 18: 913.44
     rank 19: 914.60
  forward-send:
     rank  0: 228.75
     rank  1: 218.52
     rank  2: 227.49
     rank  3: 229.59
     rank  4: 35.86
     rank  5: 31.66
     rank  6: 34.06
     rank  7: 36.02
     rank  8: 22.39
     rank  9: 21.09
     rank 10: 21.60
     rank 11: 22.30
     rank 12: 10.66
     rank 13: 10.51
     rank 14: 9.48
     rank 15: 10.66
  backward-recv:
     rank  0: 1285.08
     rank  1: 1285.11
     rank  2: 1284.62
     rank  3: 1284.71
     rank  4: 637.47
     rank  5: 637.77
     rank  6: 639.28
     rank  7: 638.79
     rank  8: 402.18
     rank  9: 402.34
     rank 10: 399.77
     rank 11: 401.11
     rank 12: 248.28
     rank 13: 248.52
     rank 14: 249.68
     rank 15: 248.45
  backward-send:
     rank  4: 49.05
     rank  5: 48.17
     rank  6: 47.14
     rank  7: 48.17
     rank  8: 31.10
     rank  9: 30.68
     rank 10: 31.33
     rank 11: 31.31
     rank 12: 21.05
     rank 13: 21.04
     rank 14: 19.29
     rank 15: 20.90
     rank 16: 10.81
     rank 17: 10.35
     rank 18: 10.66
     rank 19: 10.66
  forward-send-backward-recv:
     rank  0: 2817.71
     rank  1: 2810.55
     rank  2: 2812.54
     rank  3: 2813.90
     rank  4: 812.60
     rank  5: 810.25
     rank  6: 811.66
     rank  7: 812.11
     rank  8: 565.88
     rank  9: 559.77
     rank 10: 561.12
     rank 11: 564.38
     rank 12: 488.67
     rank 13: 489.78
     rank 14: 487.22
     rank 15: 492.09
  backward-send-forward-recv:
     rank  4: 153.92
     rank  5: 147.16
     rank  6: 147.16
     rank  7: 148.35
     rank  8: 165.44
     rank  9: 163.01
     rank 10: 164.93
     rank 11: 166.09
     rank 12: 970.32
     rank 13: 964.38
     rank 14: 971.99
     rank 15: 975.73
     rank 16: 720.27
     rank 17: 716.97
     rank 18: 704.37
     rank 19: 718.75
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.96
     rank  1: 0.94
     rank  2: 0.91
     rank  3: 0.91
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.24
     rank  2: 0.24
     rank  3: 0.24
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.79
     rank  1: 16.23
     rank  2: 16.14
     rank  3: 16.16
     rank  4: 0.06
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.06
     rank 14: 0.08
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.07
  optimizer:
     rank  0: 16.61
     rank  1: 17.07
     rank  2: 16.98
     rank  3: 17.00
     rank  4: 0.89
     rank  5: 0.86
     rank  6: 0.89
     rank  7: 0.86
     rank  8: 0.86
     rank  9: 0.86
     rank 10: 0.86
     rank 11: 0.89
     rank 12: 0.87
     rank 13: 0.89
     rank 14: 0.92
     rank 15: 0.88
     rank 16: 0.91
     rank 17: 0.89
     rank 18: 0.88
     rank 19: 0.91
