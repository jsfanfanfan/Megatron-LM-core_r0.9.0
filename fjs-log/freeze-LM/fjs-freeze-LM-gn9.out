examples/multimodal/pretrain-freeze-llm-hete-2080tifirst.sh: line 4: activate: No such file or directory
4
[2024-12-05 17:27:54,070] torch.distributed.run: [WARNING] 
[2024-12-05 17:27:54,070] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 17:27:54,070] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 17:27:54,070] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]


---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 138.47
     rank  1: 174.67
     rank  2: 142.73
     rank  3: 137.13
     rank  4: 71.74
     rank  5: 69.32
     rank  6: 76.87
     rank  7: 79.66
     rank  8: 74.00
     rank  9: 35.74
     rank 10: 37.13
     rank 11: 47.64
     rank 12: 70.90
     rank 13: 69.82
     rank 14: 68.73
     rank 15: 68.87
     rank 16: 50.75
     rank 17: 53.72
     rank 18: 50.82
     rank 19: 53.74
  train/valid/test-data-iterators-setup:
     rank  0: 1315.73
     rank  1: 1316.59
     rank  2: 1354.29
     rank  3: 1315.54
     rank  4: 1353.46
     rank  5: 1353.58
     rank  6: 1315.78
     rank  7: 1354.32
     rank  8: 1353.57
     rank  9: 1353.61
     rank 10: 1353.88
     rank 11: 1353.60
     rank 12: 1353.65
     rank 13: 1353.61
     rank 14: 1353.60
     rank 15: 1353.60
     rank 16: 1353.72
     rank 17: 1353.75
     rank 18: 1353.85
     rank 19: 1353.74
 [2024-12-05 17:28:40] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 20163.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.152428E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2492.0 | max reserved: 2492.0[Rank 18] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2524.0 | max reserved: 2524.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2492.0 | max reserved: 2492.0

[Rank 16] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
times across ranks (ms):
  forward-backward:
     rank  0: 20059.91
     rank  1: 20058.21
     rank  2: 20059.10
     rank  3: 20059.14
     rank  4: 20097.31
     rank  5: 20095.72
     rank  6: 20097.55
     rank  7: 20097.69
     rank  8: 20097.55
     rank  9: 20101.62
     rank 10: 20101.74
     rank 11: 20097.23
     rank 12: 20102.95
     rank 13: 20108.25
     rank 14: 20097.18
     rank 15: 20095.92
     rank 16: 20056.58
     rank 17: 20056.64
     rank 18: 20056.54
     rank 19: 20056.65
  forward-compute:
     rank  0: 5251.88
     rank  1: 5287.18
     rank  2: 5292.54
     rank  3: 5279.06
     rank  4: 4656.43
     rank  5: 4695.45
     rank  6: 4713.08
     rank  7: 4686.82
     rank  8: 5749.86
     rank  9: 5774.61
     rank 10: 5777.69
     rank 11: 5755.15
     rank 12: 5789.00
     rank 13: 5797.14
     rank 14: 5799.99
     rank 15: 5790.53
     rank 16: 4774.88
     rank 17: 4782.55
     rank 18: 4799.21
     rank 19: 4785.47
  backward-compute:
     rank  0: 3464.17
     rank  1: 3468.43
     rank  2: 3479.98
     rank  3: 3469.05
     rank  4: 3416.77
     rank  5: 3415.96
     rank  6: 3410.23
     rank  7: 3416.24
     rank  8: 4090.66
     rank  9: 4090.57
     rank 10: 4089.95
     rank 11: 4088.57
     rank 12: 4063.21
     rank 13: 4065.67
     rank 14: 4076.07
     rank 15: 4070.44
     rank 16: 3847.94
     rank 17: 3845.71
     rank 18: 3849.01
     rank 19: 3846.60
  pure-backward-compute:
     rank  0: 3462.49
     rank  1: 3467.23
     rank  2: 3478.74
     rank  3: 3467.88
     rank  4: 3414.93
     rank  5: 3414.26
     rank  6: 3408.71
     rank  7: 3414.57
     rank  8: 4089.22
     rank  9: 4089.18
     rank 10: 4088.45
     rank 11: 4087.37
     rank 12: 4061.83
     rank 13: 4064.79
     rank 14: 4074.63
     rank 15: 4069.06
     rank 16: 3845.40
     rank 17: 3843.36
     rank 18: 3846.18
     rank 19: 3844.36
  batch-generator:
     rank  0: 1295.36
     rank  1: 1323.93
     rank  2: 1347.83
     rank  3: 1314.31
     rank  4: 1252.56
     rank  5: 1307.40
     rank  6: 1322.82
     rank  7: 1290.12
     rank  8: 1013.50
     rank  9: 1038.70
     rank 10: 1043.69
     rank 11: 1019.78
     rank 12: 912.26
     rank 13: 923.52
     rank 14: 930.16
     rank 15: 918.52
     rank 16: 1069.98
     rank 17: 1076.25
     rank 18: 1096.28
     rank 19: 1083.84
  forward-recv:
     rank  4: 2880.63
     rank  5: 2874.43
     rank  6: 2863.85
     rank  7: 2858.33
     rank  8: 4402.26
     rank  9: 4385.90
     rank 10: 4388.55
     rank 11: 4401.96
     rank 12: 6610.12
     rank 13: 6604.52
     rank 14: 6606.64
     rank 15: 6609.69
     rank 16: 8842.20
     rank 17: 8840.32
     rank 18: 8834.36
     rank 19: 8842.20
  forward-send:
     rank  0: 5633.68
     rank  1: 5599.85
     rank  2: 5590.82
     rank  3: 5607.17
     rank  4: 4064.07
     rank  5: 4037.67
     rank  6: 4038.41
     rank  7: 4060.85
     rank  8: 2026.84
     rank  9: 2019.53
     rank 10: 2016.31
     rank 11: 2027.16
     rank 12: 31.21
     rank 13: 29.13
     rank 14: 23.01
     rank 15: 30.72
  backward-recv:
     rank  0: 1420.71
     rank  1: 1420.29
     rank  2: 1414.25
     rank  3: 1419.57
     rank  4: 849.16
     rank  5: 849.07
     rank  6: 853.56
     rank  7: 849.82
     rank  8: 462.40
     rank  9: 463.32
     rank 10: 462.59
     rank 11: 463.12
     rank 12: 212.30
     rank 13: 211.80
     rank 14: 212.53
     rank 15: 212.67
  backward-send:
     rank  4: 42.06
     rank  5: 41.95
     rank  6: 38.52
     rank  7: 41.78
     rank  8: 34.16
     rank  9: 33.35
     rank 10: 33.89
     rank 11: 33.79
     rank 12: 20.95
     rank 13: 20.66
     rank 14: 20.84
     rank 15: 20.61
     rank 16: 10.75
     rank 17: 10.00
     rank 18: 10.76
     rank 19: 10.63
  forward-send-backward-recv:
     rank  0: 4200.01
     rank  1: 4193.40
     rank  2: 4192.78
     rank  3: 4199.72
     rank  4: 2845.97
     rank  5: 2845.80
     rank  6: 2851.21
     rank  7: 2844.41
     rank  8: 1823.91
     rank  9: 1822.68
     rank 10: 1824.13
     rank 11: 1823.91
     rank 12: 1613.46
     rank 13: 1613.15
     rank 14: 1604.06
     rank 15: 1607.27
  backward-send-forward-recv:
     rank  4: 1071.28
     rank  5: 1065.68
     rank  6: 1057.48
     rank  7: 1066.49
     rank  8: 1011.11
     rank  9: 1011.79
     rank 10: 1009.83
     rank 11: 1010.04
     rank 12: 992.36
     rank 13: 991.58
     rank 14: 992.03
     rank 15: 993.46
     rank 16: 1595.88
     rank 17: 1593.33
     rank 18: 1578.21
     rank 19: 1588.24
  layernorm-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.20
     rank  1: 0.11
     rank  2: 0.20
     rank  3: 0.15
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.07
  all-grads-sync:
     rank  0: 56.81
     rank  1: 61.12
     rank  2: 64.35
     rank  3: 58.09
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.56
     rank  1: 0.34
     rank  2: 0.31
     rank  3: 0.30
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.05
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 49.82
     rank  1: 49.68
     rank  2: 49.48
     rank  3: 44.36
     rank  4: 0.15
     rank  5: 0.11
     rank  6: 0.18
     rank  7: 0.10
     rank  8: 0.15
     rank  9: 0.10
     rank 10: 0.08
     rank 11: 0.08
     rank 12: 0.18
     rank 13: 0.09
     rank 14: 0.10
     rank 15: 0.12
     rank 16: 0.11
     rank 17: 0.09
     rank 18: 0.08
     rank 19: 0.09
  optimizer:
     rank  0: 52.50
     rank  1: 52.36
     rank  2: 52.16
     rank  3: 47.03
     rank  4: 2.79
     rank  5: 2.75
     rank  6: 2.82
     rank  7: 2.74
     rank  8: 2.80
     rank  9: 2.75
     rank 10: 2.71
     rank 11: 2.70
     rank 12: 2.86
     rank 13: 2.77
     rank 14: 2.75
     rank 15: 2.77
     rank 16: 2.80
     rank 17: 2.77
     rank 18: 2.76
     rank 19: 2.77
 [2024-12-05 17:28:50] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 10127.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.087465E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10075.18
     rank  1: 10075.17
     rank  2: 10075.32
     rank  3: 10075.14
     rank  4: 10073.82
     rank  5: 10073.80
     rank  6: 10073.90
     rank  7: 10073.79
     rank  8: 10073.75
     rank  9: 10073.69
     rank 10: 10073.83
     rank 11: 10073.69
     rank 12: 10073.77
     rank 13: 10073.77
     rank 14: 10073.92
     rank 15: 10073.82
     rank 16: 10073.72
     rank 17: 10073.68
     rank 18: 10073.73
     rank 19: 10073.81
  forward-compute:
     rank  0: 2633.12
     rank  1: 2635.14
     rank  2: 2642.75
     rank  3: 2629.71
     rank  4: 3399.80
     rank  5: 3401.05
     rank  6: 3424.24
     rank  7: 3398.12
     rank  8: 3797.25
     rank  9: 3800.18
     rank 10: 3802.68
     rank 11: 3802.51
     rank 12: 3812.97
     rank 13: 3811.88
     rank 14: 3814.83
     rank 15: 3814.31
     rank 16: 3630.19
     rank 17: 3635.50
     rank 18: 3649.33
     rank 19: 3637.22
  backward-compute:
     rank  0: 2447.53
     rank  1: 2448.75
     rank  2: 2461.54
     rank  3: 2450.91
     rank  4: 3401.43
     rank  5: 3403.77
     rank  6: 3393.50
     rank  7: 3401.94
     rank  8: 4102.45
     rank  9: 4101.26
     rank 10: 4104.00
     rank 11: 4101.68
     rank 12: 4068.49
     rank 13: 4070.56
     rank 14: 4080.73
     rank 15: 4072.52
     rank 16: 3838.60
     rank 17: 3838.06
     rank 18: 3840.29
     rank 19: 3838.46
  pure-backward-compute:
     rank  0: 2446.45
     rank  1: 2447.72
     rank  2: 2459.85
     rank  3: 2449.89
     rank  4: 3399.72
     rank  5: 3402.37
     rank  6: 3391.98
     rank  7: 3400.15
     rank  8: 4100.99
     rank  9: 4100.21
     rank 10: 4102.77
     rank 11: 4100.70
     rank 12: 4067.25
     rank 13: 4069.70
     rank 14: 4079.74
     rank 15: 4071.58
     rank 16: 3836.17
     rank 17: 3836.10
     rank 18: 3837.50
     rank 19: 3836.69
  batch-generator:
     rank  0: 91.65
     rank  1: 99.86
     rank  2: 117.62
     rank  3: 97.74
     rank  4: 63.32
     rank  5: 70.18
     rank  6: 100.76
     rank  7: 74.16
     rank  8: 53.31
     rank  9: 61.45
     rank 10: 63.98
     rank 11: 62.05
     rank 12: 58.95
     rank 13: 60.36
     rank 14: 62.41
     rank 15: 59.86
     rank 16: 70.28
     rank 17: 75.44
     rank 18: 93.74
     rank 19: 81.79
  forward-recv:
     rank  4: 201.87
     rank  5: 201.13
     rank  6: 191.13
     rank  7: 201.63
     rank  8: 416.51
     rank  9: 415.59
     rank 10: 413.12
     rank 11: 416.35
     rank 12: 652.83
     rank 13: 652.72
     rank 14: 652.28
     rank 15: 652.90
     rank 16: 894.75
     rank 17: 894.40
     rank 18: 894.26
     rank 19: 894.73
  forward-send:
     rank  0: 256.13
     rank  1: 251.28
     rank  2: 238.97
     rank  3: 255.06
     rank  4: 82.59
     rank  5: 81.09
     rank  6: 78.86
     rank  7: 83.38
     rank  8: 23.75
     rank  9: 23.04
     rank 10: 22.35
     rank 11: 23.59
     rank 12: 13.41
     rank 13: 13.15
     rank 14: 12.96
     rank 15: 13.26
  backward-recv:
     rank  0: 1420.01
     rank  1: 1418.87
     rank  2: 1414.88
     rank  3: 1419.20
     rank  4: 841.94
     rank  5: 842.04
     rank  6: 847.02
     rank  7: 842.33
     rank  8: 461.05
     rank  9: 461.48
     rank 10: 460.94
     rank 11: 461.06
     rank 12: 211.42
     rank 13: 210.90
     rank 14: 211.66
     rank 15: 211.97
  backward-send:
     rank  4: 42.10
     rank  5: 41.79
     rank  6: 38.47
     rank  7: 41.89
     rank  8: 32.37
     rank  9: 31.38
     rank 10: 32.61
     rank 11: 31.69
     rank 12: 20.77
     rank 13: 20.67
     rank 14: 21.03
     rank 15: 20.18
     rank 16: 10.78
     rank 17: 10.08
     rank 18: 10.73
     rank 19: 10.35
  forward-send-backward-recv:
     rank  0: 3298.57
     rank  1: 3301.08
     rank  2: 3293.45
     rank  3: 3302.36
     rank  4: 1773.08
     rank  5: 1770.13
     rank  6: 1778.86
     rank  7: 1771.60
     rank  8: 662.01
     rank  9: 661.36
     rank 10: 660.70
     rank 11: 660.66
     rank 12: 472.25
     rank 13: 472.98
     rank 14: 464.80
     rank 15: 470.08
  backward-send-forward-recv:
     rank  4: 168.02
     rank  5: 169.01
     rank  6: 157.50
     rank  7: 167.76
     rank  8: 190.80
     rank  9: 189.66
     rank 10: 190.46
     rank 11: 188.19
     rank 12: 162.30
     rank 13: 162.61
     rank 14: 160.65
     rank 15: 162.72
     rank 16: 778.81
     rank 17: 776.46
     rank 18: 758.60
     rank 19: 773.80
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.09
     rank  3: 0.06
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.11
  all-grads-sync:
     rank  0: 0.99
     rank  1: 1.02
     rank  2: 1.16
     rank  3: 0.94
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.05
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  optimizer-copy-to-main-grad:
     rank  0: 0.28
     rank  1: 0.28
     rank  2: 0.37
     rank  3: 0.22
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.32
     rank  1: 16.54
     rank  2: 16.74
     rank  3: 16.06
     rank  4: 0.06
     rank  5: 0.08
     rank  6: 0.09
     rank  7: 0.08
     rank  8: 0.07
     rank  9: 0.07
     rank 10: 0.08
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.06
     rank 16: 0.07
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.09
  optimizer:
     rank  0: 17.31
     rank  1: 17.53
     rank  2: 18.37
     rank  3: 17.04
     rank  4: 1.04
     rank  5: 1.06
     rank  6: 1.07
     rank  7: 1.06
     rank  8: 1.04
     rank  9: 1.04
     rank 10: 1.04
     rank 11: 1.01
     rank 12: 1.01
     rank 13: 1.01
     rank 14: 1.02
     rank 15: 1.04
     rank 16: 1.06
     rank 17: 1.06
     rank 18: 1.05
     rank 19: 1.07
 [2024-12-05 17:29:00] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 10127.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.972503E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10078.70
     rank  1: 10078.74
     rank  2: 10078.86
     rank  3: 10078.73
     rank  4: 10077.43
     rank  5: 10077.43
     rank  6: 10077.46
     rank  7: 10077.40
     rank  8: 10077.31
     rank  9: 10077.28
     rank 10: 10077.38
     rank 11: 10077.25
     rank 12: 10077.31
     rank 13: 10077.39
     rank 14: 10077.42
     rank 15: 10077.42
     rank 16: 10077.25
     rank 17: 10077.25
     rank 18: 10077.25
     rank 19: 10077.26
  forward-compute:
     rank  0: 2615.44
     rank  1: 2618.60
     rank  2: 2617.34
     rank  3: 2612.59
     rank  4: 3415.63
     rank  5: 3420.58
     rank  6: 3434.71
     rank  7: 3415.94
     rank  8: 3800.32
     rank  9: 3802.19
     rank 10: 3802.63
     rank 11: 3801.56
     rank 12: 3804.08
     rank 13: 3801.53
     rank 14: 3804.29
     rank 15: 3805.22
     rank 16: 3639.25
     rank 17: 3642.64
     rank 18: 3657.33
     rank 19: 3645.62
  backward-compute:
     rank  0: 2443.59
     rank  1: 2444.58
     rank  2: 2455.76
     rank  3: 2446.46
     rank  4: 3420.15
     rank  5: 3419.11
     rank  6: 3410.45
     rank  7: 3414.05
     rank  8: 4124.88
     rank  9: 4122.86
     rank 10: 4125.03
     rank 11: 4124.18
     rank 12: 4075.20
     rank 13: 4077.30
     rank 14: 4089.37
     rank 15: 4079.65
     rank 16: 3849.81
     rank 17: 3849.47
     rank 18: 3850.85
     rank 19: 3850.03
  pure-backward-compute:
     rank  0: 2442.46
     rank  1: 2443.57
     rank  2: 2454.39
     rank  3: 2445.42
     rank  4: 3418.55
     rank  5: 3417.58
     rank  6: 3408.85
     rank  7: 3412.45
     rank  8: 4123.75
     rank  9: 4121.44
     rank 10: 4123.48
     rank 11: 4123.05
     rank 12: 4074.28
     rank 13: 4076.40
     rank 14: 4088.52
     rank 15: 4078.70
     rank 16: 3847.34
     rank 17: 3847.62
     rank 18: 3848.25
     rank 19: 3848.22
  batch-generator:
     rank  0: 80.98
     rank  1: 90.08
     rank  2: 106.48
     rank  3: 91.46
     rank  4: 66.52
     rank  5: 76.51
     rank  6: 98.31
     rank  7: 76.65
     rank  8: 50.61
     rank  9: 57.10
     rank 10: 57.11
     rank 11: 54.06
     rank 12: 51.04
     rank 13: 53.09
     rank 14: 55.34
     rank 15: 54.22
     rank 16: 70.73
     rank 17: 73.69
     rank 18: 93.24
     rank 19: 81.53
  forward-recv:
     rank  4: 199.51
     rank  5: 197.68
     rank  6: 192.14
     rank  7: 198.87
     rank  8: 415.87
     rank  9: 413.51
     rank 10: 413.91
     rank 11: 415.65
     rank 12: 652.17
     rank 13: 651.70
     rank 14: 652.65
     rank 15: 652.26
     rank 16: 891.73
     rank 17: 891.80
     rank 18: 891.05
     rank 19: 891.40
  forward-send:
     rank  0: 273.40
     rank  1: 268.94
     rank  2: 264.97
     rank  3: 273.03
     rank  4: 77.66
     rank  5: 75.05
     rank  6: 75.76
     rank  7: 77.24
     rank  8: 21.25
     rank  9: 20.97
     rank 10: 20.54
     rank 11: 20.66
     rank 12: 11.36
     rank 13: 11.53
     rank 14: 10.62
     rank 15: 11.13
  backward-recv:
     rank  0: 1422.49
     rank  1: 1422.91
     rank  2: 1418.80
     rank  3: 1421.52
     rank  4: 840.94
     rank  5: 841.05
     rank  6: 842.84
     rank  7: 842.48
     rank  8: 458.98
     rank  9: 459.08
     rank 10: 459.02
     rank 11: 458.73
     rank 12: 213.19
     rank 13: 212.86
     rank 14: 213.49
     rank 15: 212.82
  backward-send:
     rank  4: 41.99
     rank  5: 41.87
     rank  6: 41.29
     rank  7: 41.18
     rank  8: 33.45
     rank  9: 32.89
     rank 10: 33.31
     rank 11: 33.01
     rank 12: 20.87
     rank 13: 20.55
     rank 14: 20.77
     rank 15: 21.04
     rank 16: 10.72
     rank 17: 10.05
     rank 18: 10.70
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 3303.50
     rank  1: 3304.25
     rank  2: 3297.15
     rank  3: 3306.98
     rank  4: 1746.85
     rank  5: 1748.25
     rank  6: 1755.15
     rank  7: 1751.81
     rank  8: 646.37
     rank  9: 646.99
     rank 10: 646.13
     rank 11: 645.59
     rank 12: 474.47
     rank 13: 474.80
     rank 14: 464.73
     rank 15: 471.73
  backward-send-forward-recv:
     rank  4: 169.93
     rank  5: 169.23
     rank  6: 159.16
     rank  7: 169.76
     rank  8: 188.50
     rank  9: 188.70
     rank 10: 188.76
     rank 11: 188.87
     rank 12: 169.77
     rank 13: 171.08
     rank 14: 168.78
     rank 15: 170.43
     rank 16: 767.42
     rank 17: 766.69
     rank 18: 749.04
     rank 19: 763.38
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.06
     rank  2: 0.07
     rank  3: 0.08
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.02
     rank  1: 0.99
     rank  2: 1.06
     rank  3: 0.99
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.25
     rank  2: 0.40
     rank  3: 0.24
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.36
     rank  1: 16.09
     rank  2: 16.46
     rank  3: 16.29
     rank  4: 0.26
     rank  5: 0.10
     rank  6: 0.07
     rank  7: 0.09
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.05
  optimizer:
     rank  0: 17.52
     rank  1: 17.25
     rank  2: 17.61
     rank  3: 17.45
     rank  4: 1.41
     rank  5: 1.25
     rank  6: 1.22
     rank  7: 1.24
     rank  8: 1.18
     rank  9: 1.21
     rank 10: 1.21
     rank 11: 1.18
     rank 12: 1.17
     rank 13: 1.22
     rank 14: 1.21
     rank 15: 1.20
     rank 16: 1.22
     rank 17: 1.22
     rank 18: 1.20
     rank 19: 1.20
 [2024-12-05 17:29:11] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 10373.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.737872E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10318.97
     rank  1: 10318.94
     rank  2: 10318.96
     rank  3: 10318.83
     rank  4: 10317.55
     rank  5: 10317.58
     rank  6: 10317.51
     rank  7: 10317.53
     rank  8: 10317.48
     rank  9: 10317.42
     rank 10: 10317.42
     rank 11: 10317.42
     rank 12: 10317.60
     rank 13: 10317.52
     rank 14: 10317.49
     rank 15: 10317.54
     rank 16: 10317.43
     rank 17: 10317.42
     rank 18: 10317.43
     rank 19: 10317.42
  forward-compute:
     rank  0: 2815.50
     rank  1: 2815.44
     rank  2: 2816.38
     rank  3: 2811.02
     rank  4: 3425.15
     rank  5: 3426.23
     rank  6: 3443.46
     rank  7: 3421.60
     rank  8: 3809.32
     rank  9: 3810.10
     rank 10: 3811.15
     rank 11: 3809.80
     rank 12: 3816.24
     rank 13: 3815.11
     rank 14: 3816.65
     rank 15: 3817.58
     rank 16: 3639.64
     rank 17: 3642.81
     rank 18: 3656.12
     rank 19: 3646.25
  backward-compute:
     rank  0: 2453.80
     rank  1: 2455.79
     rank  2: 2471.24
     rank  3: 2456.80
     rank  4: 3412.61
     rank  5: 3412.31
     rank  6: 3406.92
     rank  7: 3406.57
     rank  8: 4129.89
     rank  9: 4130.51
     rank 10: 4130.10
     rank 11: 4130.81
     rank 12: 4071.66
     rank 13: 4072.49
     rank 14: 4082.54
     rank 15: 4074.88
     rank 16: 3848.43
     rank 17: 3847.72
     rank 18: 3849.05
     rank 19: 3851.82
  pure-backward-compute:
     rank  0: 2452.49
     rank  1: 2454.69
     rank  2: 2470.07
     rank  3: 2455.78
     rank  4: 3411.24
     rank  5: 3410.66
     rank  6: 3405.39
     rank  7: 3404.69
     rank  8: 4128.79
     rank  9: 4129.45
     rank 10: 4129.10
     rank 11: 4129.80
     rank 12: 4070.45
     rank 13: 4071.56
     rank 14: 4081.73
     rank 15: 4073.96
     rank 16: 3846.05
     rank 17: 3846.00
     rank 18: 3846.55
     rank 19: 3849.71
  batch-generator:
     rank  0: 79.45
     rank  1: 85.30
     rank  2: 95.91
     rank  3: 85.89
     rank  4: 68.89
     rank  5: 74.57
     rank  6: 99.14
     rank  7: 75.72
     rank  8: 54.54
     rank  9: 59.59
     rank 10: 60.42
     rank 11: 57.08
     rank 12: 57.26
     rank 13: 59.44
     rank 14: 60.23
     rank 15: 59.03
     rank 16: 70.85
     rank 17: 73.76
     rank 18: 91.61
     rank 19: 81.85
  forward-recv:
     rank  4: 430.72
     rank  5: 429.99
     rank  6: 424.86
     rank  7: 429.96
     rank  8: 644.73
     rank  9: 644.20
     rank 10: 643.15
     rank 11: 644.51
     rank 12: 885.46
     rank 13: 885.72
     rank 14: 885.89
     rank 15: 885.53
     rank 16: 1123.02
     rank 17: 1122.79
     rank 18: 1122.75
     rank 19: 1123.00
  forward-send:
     rank  0: 272.77
     rank  1: 271.71
     rank  2: 266.35
     rank  3: 274.35
     rank  4: 78.45
     rank  5: 78.07
     rank  6: 77.97
     rank  7: 79.74
     rank  8: 20.86
     rank  9: 20.78
     rank 10: 20.85
     rank 11: 21.05
     rank 12: 10.88
     rank 13: 10.53
     rank 14: 10.44
     rank 15: 10.78
  backward-recv:
     rank  0: 1423.05
     rank  1: 1422.37
     rank  2: 1415.91
     rank  3: 1422.72
     rank  4: 847.80
     rank  5: 848.50
     rank  6: 852.00
     rank  7: 848.13
     rank  8: 459.10
     rank  9: 458.19
     rank 10: 459.56
     rank 11: 458.61
     rank 12: 212.96
     rank 13: 213.04
     rank 14: 213.73
     rank 15: 213.58
  backward-send:
     rank  4: 42.07
     rank  5: 41.80
     rank  6: 38.11
     rank  7: 41.98
     rank  8: 33.00
     rank  9: 32.19
     rank 10: 32.55
     rank 11: 33.04
     rank 12: 20.98
     rank 13: 20.55
     rank 14: 20.82
     rank 15: 20.65
     rank 16: 10.66
     rank 17: 9.92
     rank 18: 10.69
     rank 19: 10.44
  forward-send-backward-recv:
     rank  0: 3333.30
     rank  1: 3334.17
     rank  2: 3325.48
     rank  3: 3336.97
     rank  4: 1748.81
     rank  5: 1748.13
     rank  6: 1753.52
     rank  7: 1753.69
     rank  8: 643.77
     rank  9: 643.27
     rank 10: 642.95
     rank 11: 642.12
     rank 12: 470.97
     rank 13: 473.94
     rank 14: 465.39
     rank 15: 470.37
  backward-send-forward-recv:
     rank  4: 169.44
     rank  5: 169.05
     rank  6: 156.53
     rank  7: 171.06
     rank  8: 190.20
     rank  9: 189.74
     rank 10: 189.76
     rank 11: 189.99
     rank 12: 168.44
     rank 13: 168.64
     rank 14: 167.21
     rank 15: 169.06
     rank 16: 775.44
     rank 17: 775.30
     rank 18: 758.38
     rank 19: 767.97
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.08
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.02
     rank  1: 0.99
     rank  2: 0.97
     rank  3: 0.91
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.39
     rank  3: 0.22
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.07
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-inner-step:
     rank  0: 16.33
     rank  1: 16.49
     rank  2: 16.20
     rank  3: 15.93
     rank  4: 0.04
     rank  5: 0.09
     rank  6: 0.08
     rank  7: 0.09
     rank  8: 0.07
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.03
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.08
     rank 19: 0.08
  optimizer:
     rank  0: 17.35
     rank  1: 17.49
     rank  2: 17.22
     rank  3: 16.93
     rank  4: 1.04
     rank  5: 1.09
     rank  6: 1.07
     rank  7: 1.09
     rank  8: 1.06
     rank  9: 1.03
     rank 10: 1.03
     rank 11: 1.04
     rank 12: 1.06
     rank 13: 1.02
     rank 14: 1.06
     rank 15: 1.06
     rank 16: 1.04
     rank 17: 1.05
     rank 18: 1.08
     rank 19: 1.08
 [2024-12-05 17:29:21] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 10147.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.485784E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10091.55
     rank  1: 10091.50
     rank  2: 10091.44
     rank  3: 10091.41
     rank  4: 10090.17
     rank  5: 10090.08
     rank  6: 10090.05
     rank  7: 10090.06
     rank  8: 10089.99
     rank  9: 10089.95
     rank 10: 10089.94
     rank 11: 10089.93
     rank 12: 10090.04
     rank 13: 10090.02
     rank 14: 10089.98
     rank 15: 10089.98
     rank 16: 10089.95
     rank 17: 10089.92
     rank 18: 10090.01
     rank 19: 10089.90
  forward-compute:
     rank  0: 2613.96
     rank  1: 2614.05
     rank  2: 2624.45
     rank  3: 2610.19
     rank  4: 3425.86
     rank  5: 3425.17
     rank  6: 3450.73
     rank  7: 3423.83
     rank  8: 3804.34
     rank  9: 3804.58
     rank 10: 3810.73
     rank 11: 3805.28
     rank 12: 3812.92
     rank 13: 3811.34
     rank 14: 3813.64
     rank 15: 3813.18
     rank 16: 3634.16
     rank 17: 3636.50
     rank 18: 3652.23
     rank 19: 3641.79
  backward-compute:
     rank  0: 2453.96
     rank  1: 2454.56
     rank  2: 2467.61
     rank  3: 2456.16
     rank  4: 3422.59
     rank  5: 3424.33
     rank  6: 3416.32
     rank  7: 3417.61
     rank  8: 4141.46
     rank  9: 4140.51
     rank 10: 4141.12
     rank 11: 4140.58
     rank 12: 4067.90
     rank 13: 4068.46
     rank 14: 4079.78
     rank 15: 4071.66
     rank 16: 3853.32
     rank 17: 3852.69
     rank 18: 3854.52
     rank 19: 3853.99
  pure-backward-compute:
     rank  0: 2452.69
     rank  1: 2453.29
     rank  2: 2466.52
     rank  3: 2455.12
     rank  4: 3421.00
     rank  5: 3422.82
     rank  6: 3414.58
     rank  7: 3415.78
     rank  8: 4140.39
     rank  9: 4139.41
     rank 10: 4140.03
     rank 11: 4139.59
     rank 12: 4066.84
     rank 13: 4067.58
     rank 14: 4079.10
     rank 15: 4070.92
     rank 16: 3850.93
     rank 17: 3850.96
     rank 18: 3851.84
     rank 19: 3852.24
  batch-generator:
     rank  0: 90.33
     rank  1: 94.72
     rank  2: 115.34
     rank  3: 96.17
     rank  4: 65.82
     rank  5: 71.02
     rank  6: 104.04
     rank  7: 77.84
     rank  8: 50.06
     rank  9: 54.83
     rank 10: 60.28
     rank 11: 53.17
     rank 12: 54.86
     rank 13: 56.93
     rank 14: 58.46
     rank 15: 55.88
     rank 16: 70.59
     rank 17: 72.65
     rank 18: 93.11
     rank 19: 83.00
  forward-recv:
     rank  4: 200.25
     rank  5: 199.80
     rank  6: 189.48
     rank  7: 199.58
     rank  8: 415.16
     rank  9: 415.25
     rank 10: 411.19
     rank 11: 414.92
     rank 12: 653.90
     rank 13: 653.95
     rank 14: 653.17
     rank 15: 654.24
     rank 16: 894.55
     rank 17: 894.41
     rank 18: 894.11
     rank 19: 894.43
  forward-send:
     rank  0: 266.79
     rank  1: 265.55
     rank  2: 250.56
     rank  3: 267.30
     rank  4: 76.27
     rank  5: 76.84
     rank  6: 72.07
     rank  7: 77.91
     rank  8: 22.36
     rank  9: 21.89
     rank 10: 20.52
     rank 11: 22.13
     rank 12: 11.62
     rank 13: 11.36
     rank 14: 10.99
     rank 15: 11.34
  backward-recv:
     rank  0: 1422.34
     rank  1: 1420.31
     rank  2: 1419.84
     rank  3: 1422.99
     rank  4: 844.50
     rank  5: 843.56
     rank  6: 848.12
     rank  7: 845.09
     rank  8: 462.62
     rank  9: 463.37
     rank 10: 462.58
     rank 11: 462.31
     rank 12: 215.38
     rank 13: 215.01
     rank 14: 215.81
     rank 15: 215.33
  backward-send:
     rank  4: 42.34
     rank  5: 42.14
     rank  6: 39.14
     rank  7: 41.90
     rank  8: 32.38
     rank  9: 31.32
     rank 10: 32.39
     rank 11: 32.29
     rank 12: 20.91
     rank 13: 20.87
     rank 14: 20.69
     rank 15: 20.88
     rank 16: 10.60
     rank 17: 9.98
     rank 18: 10.58
     rank 19: 10.30
  forward-send-backward-recv:
     rank  0: 3312.75
     rank  1: 3313.62
     rank  2: 3307.76
     rank  3: 3317.27
     rank  4: 1744.81
     rank  5: 1743.67
     rank  6: 1749.39
     rank  7: 1748.63
     rank  8: 636.54
     rank  9: 636.37
     rank 10: 636.25
     rank 11: 636.28
     rank 12: 471.77
     rank 13: 475.11
     rank 14: 466.02
     rank 15: 471.59
  backward-send-forward-recv:
     rank  4: 169.08
     rank  5: 170.21
     rank  6: 158.94
     rank  7: 169.51
     rank  8: 187.60
     rank  9: 187.18
     rank 10: 187.01
     rank 11: 187.29
     rank 12: 173.93
     rank 13: 174.69
     rank 14: 173.78
     rank 15: 175.77
     rank 16: 775.26
     rank 17: 775.86
     rank 18: 756.68
     rank 19: 769.66
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.09
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.08
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.08
     rank  1: 1.01
     rank  2: 0.99
     rank  3: 0.99
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.39
     rank  1: 0.25
     rank  2: 0.26
     rank  3: 0.23
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.81
     rank  1: 16.41
     rank  2: 16.47
     rank  3: 16.06
     rank  4: 0.27
     rank  5: 0.09
     rank  6: 0.09
     rank  7: 0.09
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.04
     rank 18: 0.08
     rank 19: 0.04
  optimizer:
     rank  0: 17.91
     rank  1: 17.51
     rank  2: 17.59
     rank  3: 17.16
     rank  4: 1.37
     rank  5: 1.19
     rank  6: 1.19
     rank  7: 1.18
     rank  8: 1.13
     rank  9: 1.13
     rank 10: 1.13
     rank 11: 1.16
     rank 12: 1.13
     rank 13: 1.12
     rank 14: 1.12
     rank 15: 1.13
     rank 16: 1.13
     rank 17: 1.14
     rank 18: 1.18
     rank 19: 1.14
 [2024-12-05 17:29:31] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 10154.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.482191E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10096.89
     rank  1: 10096.50
     rank  2: 10096.54
     rank  3: 10096.54
     rank  4: 10095.28
     rank  5: 10095.13
     rank  6: 10095.23
     rank  7: 10095.20
     rank  8: 10095.21
     rank  9: 10094.97
     rank 10: 10095.08
     rank 11: 10095.05
     rank 12: 10095.26
     rank 13: 10095.06
     rank 14: 10095.16
     rank 15: 10095.16
     rank 16: 10095.04
     rank 17: 10095.00
     rank 18: 10095.06
     rank 19: 10095.01
  forward-compute:
     rank  0: 2595.21
     rank  1: 2593.75
     rank  2: 2607.69
     rank  3: 2591.04
     rank  4: 3419.84
     rank  5: 3420.65
     rank  6: 3450.25
     rank  7: 3419.48
     rank  8: 3804.75
     rank  9: 3806.59
     rank 10: 3814.27
     rank 11: 3807.65
     rank 12: 3809.32
     rank 13: 3808.56
     rank 14: 3810.98
     rank 15: 3811.49
     rank 16: 3649.03
     rank 17: 3651.44
     rank 18: 3666.75
     rank 19: 3655.35
  backward-compute:
     rank  0: 2449.56
     rank  1: 2449.90
     rank  2: 2464.78
     rank  3: 2452.91
     rank  4: 3418.30
     rank  5: 3421.92
     rank  6: 3411.89
     rank  7: 3416.89
     rank  8: 4125.33
     rank  9: 4124.70
     rank 10: 4125.28
     rank 11: 4126.32
     rank 12: 4073.85
     rank 13: 4073.94
     rank 14: 4085.60
     rank 15: 4076.84
     rank 16: 3860.63
     rank 17: 3860.05
     rank 18: 3862.08
     rank 19: 3861.47
  pure-backward-compute:
     rank  0: 2448.44
     rank  1: 2448.84
     rank  2: 2463.73
     rank  3: 2451.88
     rank  4: 3416.56
     rank  5: 3420.47
     rank  6: 3410.32
     rank  7: 3415.17
     rank  8: 4123.95
     rank  9: 4123.68
     rank 10: 4124.04
     rank 11: 4125.34
     rank 12: 4072.89
     rank 13: 4073.08
     rank 14: 4084.93
     rank 15: 4076.04
     rank 16: 3858.30
     rank 17: 3858.39
     rank 18: 3859.40
     rank 19: 3859.62
  batch-generator:
     rank  0: 81.08
     rank  1: 84.86
     rank  2: 106.77
     rank  3: 86.81
     rank  4: 66.19
     rank  5: 72.26
     rank  6: 109.58
     rank  7: 77.50
     rank  8: 52.22
     rank  9: 58.41
     rank 10: 65.80
     rank 11: 57.58
     rank 12: 50.26
     rank 13: 53.93
     rank 14: 55.85
     rank 15: 54.63
     rank 16: 69.29
     rank 17: 71.36
     rank 18: 91.61
     rank 19: 80.27
  forward-recv:
     rank  4: 198.84
     rank  5: 198.55
     rank  6: 186.40
     rank  7: 198.22
     rank  8: 414.33
     rank  9: 414.43
     rank 10: 408.73
     rank 11: 414.13
     rank 12: 651.62
     rank 13: 651.66
     rank 14: 650.52
     rank 15: 651.76
     rank 16: 891.75
     rank 17: 891.59
     rank 18: 891.19
     rank 19: 891.69
  forward-send:
     rank  0: 266.38
     rank  1: 265.84
     rank  2: 247.74
     rank  3: 267.27
     rank  4: 73.02
     rank  5: 73.37
     rank  6: 66.26
     rank  7: 73.76
     rank  8: 22.28
     rank  9: 22.09
     rank 10: 20.18
     rank 11: 22.13
     rank 12: 11.74
     rank 13: 11.61
     rank 14: 11.10
     rank 15: 11.60
  backward-recv:
     rank  0: 1425.26
     rank  1: 1426.18
     rank  2: 1421.29
     rank  3: 1425.99
     rank  4: 847.24
     rank  5: 846.74
     rank  6: 851.58
     rank  7: 847.47
     rank  8: 463.60
     rank  9: 464.29
     rank 10: 463.27
     rank 11: 464.55
     rank 12: 215.76
     rank 13: 215.70
     rank 14: 216.06
     rank 15: 215.70
  backward-send:
     rank  4: 42.38
     rank  5: 42.06
     rank  6: 39.14
     rank  7: 41.96
     rank  8: 32.30
     rank  9: 30.98
     rank 10: 32.59
     rank 11: 31.13
     rank 12: 20.90
     rank 13: 20.59
     rank 14: 20.84
     rank 15: 20.68
     rank 16: 10.75
     rank 17: 10.15
     rank 18: 10.76
     rank 19: 10.39
  forward-send-backward-recv:
     rank  0: 3338.52
     rank  1: 3340.00
     rank  2: 3329.31
     rank  3: 3340.34
     rank  4: 1762.22
     rank  5: 1758.24
     rank  6: 1766.16
     rank  7: 1762.89
     rank  8: 651.92
     rank  9: 651.50
     rank 10: 651.07
     rank 11: 650.18
     rank 12: 474.67
     rank 13: 476.80
     rank 14: 467.34
     rank 15: 473.44
  backward-send-forward-recv:
     rank  4: 169.64
     rank  5: 168.79
     rank  6: 157.79
     rank  7: 169.06
     rank  8: 191.10
     rank  9: 189.38
     rank 10: 189.58
     rank 11: 189.07
     rank 12: 176.61
     rank 13: 176.34
     rank 14: 175.79
     rank 15: 176.44
     rank 16: 760.39
     rank 17: 760.98
     rank 18: 741.80
     rank 19: 755.66
  layernorm-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.10
     rank  1: 0.07
     rank  2: 0.06
     rank  3: 0.08
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.36
     rank  1: 0.97
     rank  2: 0.95
     rank  3: 0.97
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.50
     rank  1: 0.26
     rank  2: 0.27
     rank  3: 0.25
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.06
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 17.47
     rank  1: 16.36
     rank  2: 16.15
     rank  3: 16.26
     rank  4: 0.09
     rank  5: 0.09
     rank  6: 0.10
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.07
     rank 10: 0.06
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.07
  optimizer:
     rank  0: 18.77
     rank  1: 17.66
     rank  2: 17.45
     rank  3: 17.56
     rank  4: 1.38
     rank  5: 1.39
     rank  6: 1.40
     rank  7: 1.35
     rank  8: 1.34
     rank  9: 1.37
     rank 10: 1.36
     rank 11: 1.34
     rank 12: 1.33
     rank 13: 1.34
     rank 14: 1.33
     rank 15: 1.33
     rank 16: 1.38
     rank 17: 1.37
     rank 18: 1.35
     rank 19: 1.37
 [2024-12-05 17:29:41] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 10138.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.891548E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10080.97
     rank  1: 10080.65
     rank  2: 10080.65
     rank  3: 10080.61
     rank  4: 10079.48
     rank  5: 10079.27
     rank  6: 10079.36
     rank  7: 10079.31
     rank  8: 10079.33
     rank  9: 10079.14
     rank 10: 10079.21
     rank 11: 10079.13
     rank 12: 10079.42
     rank 13: 10079.23
     rank 14: 10079.32
     rank 15: 10079.26
     rank 16: 10079.30
     rank 17: 10079.15
     rank 18: 10079.31
     rank 19: 10079.18
  forward-compute:
     rank  0: 2606.58
     rank  1: 2605.39
     rank  2: 2615.18
     rank  3: 2602.69
     rank  4: 3425.52
     rank  5: 3427.22
     rank  6: 3451.02
     rank  7: 3425.53
     rank  8: 3796.39
     rank  9: 3797.96
     rank 10: 3804.47
     rank 11: 3797.78
     rank 12: 3810.32
     rank 13: 3809.39
     rank 14: 3810.08
     rank 15: 3813.00
     rank 16: 3646.32
     rank 17: 3648.71
     rank 18: 3663.40
     rank 19: 3652.23
  backward-compute:
     rank  0: 2447.36
     rank  1: 2448.61
     rank  2: 2462.06
     rank  3: 2450.46
     rank  4: 3425.04
     rank  5: 3426.27
     rank  6: 3420.64
     rank  7: 3420.65
     rank  8: 4130.72
     rank  9: 4130.75
     rank 10: 4128.88
     rank 11: 4132.49
     rank 12: 4069.83
     rank 13: 4070.37
     rank 14: 4081.37
     rank 15: 4072.72
     rank 16: 3858.31
     rank 17: 3858.00
     rank 18: 3859.39
     rank 19: 3859.31
  pure-backward-compute:
     rank  0: 2446.14
     rank  1: 2447.40
     rank  2: 2460.91
     rank  3: 2449.45
     rank  4: 3423.34
     rank  5: 3424.89
     rank  6: 3418.95
     rank  7: 3419.08
     rank  8: 4129.57
     rank  9: 4129.79
     rank 10: 4127.90
     rank 11: 4131.40
     rank 12: 4068.93
     rank 13: 4069.50
     rank 14: 4080.55
     rank 15: 4071.94
     rank 16: 3855.96
     rank 17: 3856.44
     rank 18: 3856.94
     rank 19: 3857.47
  batch-generator:
     rank  0: 80.13
     rank  1: 90.71
     rank  2: 107.07
     rank  3: 88.94
     rank  4: 66.25
     rank  5: 73.04
     rank  6: 104.19
     rank  7: 77.20
     rank  8: 51.27
     rank  9: 57.11
     rank 10: 62.93
     rank 11: 55.18
     rank 12: 49.14
     rank 13: 52.90
     rank 14: 52.97
     rank 15: 53.89
     rank 16: 68.64
     rank 17: 70.53
     rank 18: 90.03
     rank 19: 79.16
  forward-recv:
     rank  4: 199.61
     rank  5: 199.38
     rank  6: 188.83
     rank  7: 199.04
     rank  8: 414.90
     rank  9: 414.66
     rank 10: 410.86
     rank 11: 414.85
     rank 12: 652.33
     rank 13: 652.42
     rank 14: 651.67
     rank 15: 652.41
     rank 16: 892.29
     rank 17: 892.08
     rank 18: 891.88
     rank 19: 892.24
  forward-send:
     rank  0: 261.75
     rank  1: 260.86
     rank  2: 247.50
     rank  3: 262.45
     rank  4: 75.22
     rank  5: 74.98
     rank  6: 70.93
     rank  7: 75.47
     rank  8: 21.56
     rank  9: 21.09
     rank 10: 19.82
     rank 11: 21.11
     rank 12: 11.08
     rank 13: 10.81
     rank 14: 10.52
     rank 15: 10.94
  backward-recv:
     rank  0: 1420.03
     rank  1: 1418.70
     rank  2: 1416.22
     rank  3: 1420.32
     rank  4: 842.53
     rank  5: 842.96
     rank  6: 846.65
     rank  7: 843.63
     rank  8: 461.12
     rank  9: 461.04
     rank 10: 461.55
     rank 11: 461.08
     rank 12: 210.89
     rank 13: 210.51
     rank 14: 211.33
     rank 15: 211.26
  backward-send:
     rank  4: 42.38
     rank  5: 42.00
     rank  6: 39.10
     rank  7: 41.87
     rank  8: 34.31
     rank  9: 33.94
     rank 10: 33.82
     rank 11: 34.10
     rank 12: 20.95
     rank 13: 20.50
     rank 14: 20.86
     rank 15: 20.54
     rank 16: 10.68
     rank 17: 9.97
     rank 18: 10.67
     rank 19: 10.47
  forward-send-backward-recv:
     rank  0: 3324.19
     rank  1: 3323.77
     rank  2: 3318.31
     rank  3: 3326.54
     rank  4: 1735.20
     rank  5: 1734.18
     rank  6: 1737.71
     rank  7: 1738.33
     rank  8: 641.49
     rank  9: 641.95
     rank 10: 642.85
     rank 11: 640.62
     rank 12: 473.37
     rank 13: 475.01
     rank 14: 466.40
     rank 15: 471.94
  backward-send-forward-recv:
     rank  4: 170.31
     rank  5: 169.23
     rank  6: 159.34
     rank  7: 169.76
     rank  8: 189.16
     rank  9: 188.08
     rank 10: 187.61
     rank 11: 188.22
     rank 12: 170.45
     rank 13: 170.27
     rank 14: 170.65
     rank 15: 169.67
     rank 16: 750.42
     rank 17: 751.11
     rank 18: 732.52
     rank 19: 745.99
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.11
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.10
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.32
     rank  1: 1.00
     rank  2: 1.06
     rank  3: 0.92
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.48
     rank  1: 0.25
     rank  2: 0.24
     rank  3: 0.22
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.03
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.69
     rank  1: 16.41
     rank  2: 15.92
     rank  3: 15.76
     rank  4: 0.10
     rank  5: 0.05
     rank  6: 0.06
     rank  7: 0.09
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.07
     rank 15: 0.07
     rank 16: 0.05
     rank 17: 0.04
     rank 18: 0.08
     rank 19: 0.05
  optimizer:
     rank  0: 17.83
     rank  1: 17.57
     rank  2: 17.08
     rank  3: 16.92
     rank  4: 1.26
     rank  5: 1.20
     rank  6: 1.21
     rank  7: 1.24
     rank  8: 1.21
     rank  9: 1.19
     rank 10: 1.20
     rank 11: 1.18
     rank 12: 1.18
     rank 13: 1.19
     rank 14: 1.22
     rank 15: 1.22
     rank 16: 1.20
     rank 17: 1.19
     rank 18: 1.24
     rank 19: 1.20
 [2024-12-05 17:29:51] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 10128.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.233137E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10079.95
     rank  1: 10079.75
     rank  2: 10079.79
     rank  3: 10079.75
     rank  4: 10078.41
     rank  5: 10078.45
     rank  6: 10078.43
     rank  7: 10078.38
     rank  8: 10078.30
     rank  9: 10078.26
     rank 10: 10078.30
     rank 11: 10078.35
     rank 12: 10078.38
     rank 13: 10078.40
     rank 14: 10078.39
     rank 15: 10078.36
     rank 16: 10078.28
     rank 17: 10078.25
     rank 18: 10078.28
     rank 19: 10078.25
  forward-compute:
     rank  0: 2596.33
     rank  1: 2595.03
     rank  2: 2604.13
     rank  3: 2592.07
     rank  4: 3427.06
     rank  5: 3425.50
     rank  6: 3455.72
     rank  7: 3425.35
     rank  8: 3792.05
     rank  9: 3792.23
     rank 10: 3798.60
     rank 11: 3793.41
     rank 12: 3808.65
     rank 13: 3807.14
     rank 14: 3808.44
     rank 15: 3809.58
     rank 16: 3645.21
     rank 17: 3647.06
     rank 18: 3662.79
     rank 19: 3653.23
  backward-compute:
     rank  0: 2443.87
     rank  1: 2444.62
     rank  2: 2460.68
     rank  3: 2446.56
     rank  4: 3421.32
     rank  5: 3421.32
     rank  6: 3415.13
     rank  7: 3417.60
     rank  8: 4116.56
     rank  9: 4116.05
     rank 10: 4115.73
     rank 11: 4116.66
     rank 12: 4069.62
     rank 13: 4070.18
     rank 14: 4082.40
     rank 15: 4074.52
     rank 16: 3862.86
     rank 17: 3862.11
     rank 18: 3863.63
     rank 19: 3862.86
  pure-backward-compute:
     rank  0: 2442.77
     rank  1: 2443.64
     rank  2: 2459.51
     rank  3: 2445.48
     rank  4: 3419.91
     rank  5: 3419.84
     rank  6: 3413.63
     rank  7: 3415.81
     rank  8: 4115.54
     rank  9: 4115.05
     rank 10: 4114.77
     rank 11: 4115.48
     rank 12: 4068.73
     rank 13: 4069.28
     rank 14: 4081.76
     rank 15: 4073.80
     rank 16: 3860.43
     rank 17: 3860.53
     rank 18: 3860.98
     rank 19: 3861.21
  batch-generator:
     rank  0: 83.36
     rank  1: 87.08
     rank  2: 104.19
     rank  3: 87.39
     rank  4: 64.25
     rank  5: 68.16
     rank  6: 105.78
     rank  7: 76.60
     rank  8: 50.25
     rank  9: 54.71
     rank 10: 60.11
     rank 11: 54.54
     rank 12: 49.57
     rank 13: 52.87
     rank 14: 53.42
     rank 15: 52.55
     rank 16: 69.42
     rank 17: 70.96
     rank 18: 91.53
     rank 19: 82.29
  forward-recv:
     rank  4: 196.92
     rank  5: 196.77
     rank  6: 185.66
     rank  7: 196.67
     rank  8: 411.73
     rank  9: 411.74
     rank 10: 408.06
     rank 11: 411.76
     rank 12: 648.40
     rank 13: 648.54
     rank 14: 647.81
     rank 15: 648.51
     rank 16: 888.06
     rank 17: 887.97
     rank 18: 887.74
     rank 19: 888.06
  forward-send:
     rank  0: 264.57
     rank  1: 264.64
     rank  2: 250.66
     rank  3: 265.70
     rank  4: 73.21
     rank  5: 73.56
     rank  6: 69.41
     rank  7: 73.90
     rank  8: 21.74
     rank  9: 21.59
     rank 10: 20.39
     rank 11: 21.38
     rank 12: 12.13
     rank 13: 11.99
     rank 14: 11.75
     rank 15: 11.95
  backward-recv:
     rank  0: 1422.92
     rank  1: 1423.85
     rank  2: 1419.07
     rank  3: 1423.43
     rank  4: 845.87
     rank  5: 845.87
     rank  6: 849.63
     rank  7: 846.62
     rank  8: 463.17
     rank  9: 463.45
     rank 10: 463.39
     rank 11: 462.89
     rank 12: 212.80
     rank 13: 212.66
     rank 14: 213.44
     rank 15: 212.95
  backward-send:
     rank  4: 42.05
     rank  5: 41.88
     rank  6: 38.32
     rank  7: 41.86
     rank  8: 35.36
     rank  9: 34.91
     rank 10: 34.69
     rank 11: 35.22
     rank 12: 21.11
     rank 13: 20.74
     rank 14: 20.64
     rank 15: 20.61
     rank 16: 10.77
     rank 17: 9.98
     rank 18: 10.71
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 3330.97
     rank  1: 3331.85
     rank  2: 3323.04
     rank  3: 3333.22
     rank  4: 1740.20
     rank  5: 1739.41
     rank  6: 1745.21
     rank  7: 1742.42
     rank  8: 657.79
     rank  9: 658.00
     rank 10: 658.06
     rank 11: 657.40
     rank 12: 476.09
     rank 13: 478.34
     rank 14: 468.52
     rank 15: 473.65
  backward-send-forward-recv:
     rank  4: 169.01
     rank  5: 170.20
     rank  6: 154.57
     rank  7: 168.93
     rank  8: 191.78
     rank  9: 191.54
     rank 10: 190.87
     rank 11: 190.95
     rank 12: 171.18
     rank 13: 171.15
     rank 14: 171.26
     rank 15: 172.24
     rank 16: 750.81
     rank 17: 752.15
     rank 18: 732.79
     rank 19: 745.10
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 1.17
     rank  1: 0.99
     rank  2: 1.00
     rank  3: 1.00
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.40
     rank  1: 0.27
     rank  2: 0.28
     rank  3: 0.24
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.35
     rank  1: 16.18
     rank  2: 16.20
     rank  3: 16.26
     rank  4: 0.04
     rank  5: 0.10
     rank  6: 0.09
     rank  7: 0.04
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.10
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 17.41
     rank  1: 17.22
     rank  2: 17.23
     rank  3: 17.29
     rank  4: 1.06
     rank  5: 1.13
     rank  6: 1.11
     rank  7: 1.07
     rank  8: 1.05
     rank  9: 1.05
     rank 10: 1.05
     rank 11: 1.13
     rank 12: 1.05
     rank 13: 1.09
     rank 14: 1.06
     rank 15: 1.05
     rank 16: 1.07
     rank 17: 1.08
     rank 18: 1.08
     rank 19: 1.07
 [2024-12-05 17:30:01] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10142.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.952036E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10085.55
     rank  1: 10085.40
     rank  2: 10085.35
     rank  3: 10085.33
     rank  4: 10084.12
     rank  5: 10084.08
     rank  6: 10084.00
     rank  7: 10084.00
     rank  8: 10083.99
     rank  9: 10084.63
     rank 10: 10083.86
     rank 11: 10083.83
     rank 12: 10084.09
     rank 13: 10084.03
     rank 14: 10083.94
     rank 15: 10083.93
     rank 16: 10083.93
     rank 17: 10083.88
     rank 18: 10083.93
     rank 19: 10083.88
  forward-compute:
     rank  0: 2602.73
     rank  1: 2596.72
     rank  2: 2609.98
     rank  3: 2595.46
     rank  4: 3426.38
     rank  5: 3422.65
     rank  6: 3451.05
     rank  7: 3421.14
     rank  8: 3794.51
     rank  9: 3791.43
     rank 10: 3799.64
     rank 11: 3795.06
     rank 12: 3811.11
     rank 13: 3810.29
     rank 14: 3812.26
     rank 15: 3812.47
     rank 16: 3647.34
     rank 17: 3649.05
     rank 18: 3665.07
     rank 19: 3654.25
  backward-compute:
     rank  0: 2449.40
     rank  1: 2451.38
     rank  2: 2466.51
     rank  3: 2452.22
     rank  4: 3424.13
     rank  5: 3423.07
     rank  6: 3414.46
     rank  7: 3420.95
     rank  8: 4110.68
     rank  9: 4110.86
     rank 10: 4110.54
     rank 11: 4110.74
     rank 12: 4067.79
     rank 13: 4067.95
     rank 14: 4079.93
     rank 15: 4071.79
     rank 16: 3863.88
     rank 17: 3863.05
     rank 18: 3864.90
     rank 19: 3864.22
  pure-backward-compute:
     rank  0: 2448.20
     rank  1: 2450.36
     rank  2: 2465.42
     rank  3: 2451.21
     rank  4: 3422.72
     rank  5: 3421.57
     rank  6: 3413.06
     rank  7: 3418.92
     rank  8: 4109.73
     rank  9: 4109.91
     rank 10: 4109.66
     rank 11: 4109.76
     rank 12: 4066.82
     rank 13: 4067.06
     rank 14: 4079.23
     rank 15: 4071.06
     rank 16: 3861.45
     rank 17: 3861.47
     rank 18: 3862.35
     rank 19: 3862.55
  batch-generator:
     rank  0: 89.82
     rank  1: 88.15
     rank  2: 110.40
     rank  3: 91.61
     rank  4: 64.65
     rank  5: 67.70
     rank  6: 103.61
     rank  7: 72.71
     rank  8: 49.46
     rank  9: 50.75
     rank 10: 58.11
     rank 11: 52.98
     rank 12: 52.16
     rank 13: 55.70
     rank 14: 56.97
     rank 15: 55.25
     rank 16: 68.92
     rank 17: 70.30
     rank 18: 91.26
     rank 19: 80.59
  forward-recv:
     rank  4: 203.13
     rank  5: 203.34
     rank  6: 193.17
     rank  7: 203.79
     rank  8: 417.52
     rank  9: 419.77
     rank 10: 414.21
     rank 11: 418.20
     rank 12: 655.58
     rank 13: 656.05
     rank 14: 654.94
     rank 15: 656.01
     rank 16: 894.97
     rank 17: 894.83
     rank 18: 894.56
     rank 19: 894.88
  forward-send:
     rank  0: 268.07
     rank  1: 273.07
     rank  2: 255.49
     rank  3: 272.20
     rank  4: 73.84
     rank  5: 77.93
     rank  6: 70.62
     rank  7: 76.69
     rank  8: 21.06
     rank  9: 21.42
     rank 10: 19.51
     rank 11: 21.17
     rank 12: 11.31
     rank 13: 11.14
     rank 14: 10.74
     rank 15: 11.07
  backward-recv:
     rank  0: 1418.61
     rank  1: 1419.30
     rank  2: 1415.14
     rank  3: 1419.43
     rank  4: 841.46
     rank  5: 841.28
     rank  6: 846.07
     rank  7: 841.85
     rank  8: 464.13
     rank  9: 464.60
     rank 10: 464.22
     rank 11: 464.58
     rank 12: 215.29
     rank 13: 215.47
     rank 14: 215.94
     rank 15: 215.70
  backward-send:
     rank  4: 42.09
     rank  5: 41.99
     rank  6: 38.26
     rank  7: 41.85
     rank  8: 33.16
     rank  9: 32.41
     rank 10: 33.11
     rank 11: 32.62
     rank 12: 21.01
     rank 13: 20.48
     rank 14: 20.75
     rank 15: 20.83
     rank 16: 10.71
     rank 17: 10.04
     rank 18: 10.71
     rank 19: 10.41
  forward-send-backward-recv:
     rank  0: 3322.74
     rank  1: 3324.06
     rank  2: 3317.92
     rank  3: 3327.15
     rank  4: 1740.05
     rank  5: 1741.03
     rank  6: 1748.32
     rank  7: 1742.69
     rank  8: 663.94
     rank  9: 664.28
     rank 10: 663.83
     rank 11: 663.25
     rank 12: 480.92
     rank 13: 483.15
     rank 14: 473.84
     rank 15: 479.35
  backward-send-forward-recv:
     rank  4: 169.07
     rank  5: 168.21
     rank  6: 157.08
     rank  7: 169.30
     rank  8: 189.61
     rank  9: 189.78
     rank 10: 189.51
     rank 11: 189.07
     rank 12: 164.36
     rank 13: 163.87
     rank 14: 163.62
     rank 15: 164.80
     rank 16: 749.32
     rank 17: 750.89
     rank 18: 730.96
     rank 19: 744.57
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.06
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.21
     rank  1: 0.97
     rank  2: 0.89
     rank  3: 0.90
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.41
     rank  1: 0.26
     rank  2: 0.25
     rank  3: 0.22
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.05
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.73
     rank  1: 16.41
     rank  2: 15.88
     rank  3: 15.75
     rank  4: 0.10
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.08
     rank  8: 0.04
     rank  9: 0.10
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 18.10
     rank  1: 17.79
     rank  2: 17.25
     rank  3: 17.14
     rank  4: 1.48
     rank  5: 1.44
     rank  6: 1.44
     rank  7: 1.47
     rank  8: 1.42
     rank  9: 1.59
     rank 10: 1.42
     rank 11: 1.42
     rank 12: 1.42
     rank 13: 1.43
     rank 14: 1.42
     rank 15: 1.41
     rank 16: 1.43
     rank 17: 1.43
     rank 18: 1.43
     rank 19: 1.42
 [2024-12-05 17:30:11] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 10141.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.648175E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10086.60
     rank  1: 10086.51
     rank  2: 10086.55
     rank  3: 10086.53
     rank  4: 10085.19
     rank  5: 10085.16
     rank  6: 10085.16
     rank  7: 10085.23
     rank  8: 10085.06
     rank  9: 10085.04
     rank 10: 10085.02
     rank 11: 10085.08
     rank 12: 10085.21
     rank 13: 10085.13
     rank 14: 10085.12
     rank 15: 10085.21
     rank 16: 10085.09
     rank 17: 10085.05
     rank 18: 10085.09
     rank 19: 10085.05
  forward-compute:
     rank  0: 2638.82
     rank  1: 2636.69
     rank  2: 2650.53
     rank  3: 2643.75
     rank  4: 3443.25
     rank  5: 3442.48
     rank  6: 3470.94
     rank  7: 3450.25
     rank  8: 3791.30
     rank  9: 3789.70
     rank 10: 3800.35
     rank 11: 3794.95
     rank 12: 3817.83
     rank 13: 3816.58
     rank 14: 3820.10
     rank 15: 3821.41
     rank 16: 3648.41
     rank 17: 3650.95
     rank 18: 3667.05
     rank 19: 3656.16
  backward-compute:
     rank  0: 2458.41
     rank  1: 2462.64
     rank  2: 2474.61
     rank  3: 2464.19
     rank  4: 3426.85
     rank  5: 3421.90
     rank  6: 3418.76
     rank  7: 3422.36
     rank  8: 4112.44
     rank  9: 4112.70
     rank 10: 4112.39
     rank 11: 4113.59
     rank 12: 4068.44
     rank 13: 4068.99
     rank 14: 4081.54
     rank 15: 4073.21
     rank 16: 3868.46
     rank 17: 3868.08
     rank 18: 3868.74
     rank 19: 3868.82
  pure-backward-compute:
     rank  0: 2457.00
     rank  1: 2461.64
     rank  2: 2473.54
     rank  3: 2463.19
     rank  4: 3425.50
     rank  5: 3420.47
     rank  6: 3417.17
     rank  7: 3420.70
     rank  8: 4111.43
     rank  9: 4111.08
     rank 10: 4111.49
     rank 11: 4112.65
     rank 12: 4067.38
     rank 13: 4068.14
     rank 14: 4080.85
     rank 15: 4072.40
     rank 16: 3865.90
     rank 17: 3866.41
     rank 18: 3866.26
     rank 19: 3867.17
  batch-generator:
     rank  0: 87.77
     rank  1: 91.67
     rank  2: 116.12
     rank  3: 102.54
     rank  4: 62.80
     rank  5: 67.86
     rank  6: 103.75
     rank  7: 81.43
     rank  8: 50.18
     rank  9: 51.50
     rank 10: 61.89
     rank 11: 56.84
     rank 12: 58.35
     rank 13: 60.24
     rank 14: 62.85
     rank 15: 62.26
     rank 16: 69.52
     rank 17: 71.81
     rank 18: 92.50
     rank 19: 81.82
  forward-recv:
     rank  4: 203.65
     rank  5: 203.11
     rank  6: 192.54
     rank  7: 197.73
     rank  8: 417.66
     rank  9: 417.49
     rank 10: 412.20
     rank 11: 416.11
     rank 12: 655.25
     rank 13: 655.79
     rank 14: 654.00
     rank 15: 654.78
     rank 16: 895.09
     rank 17: 894.98
     rank 18: 894.45
     rank 19: 895.17
  forward-send:
     rank  0: 268.80
     rank  1: 270.00
     rank  2: 251.77
     rank  3: 260.97
     rank  4: 71.24
     rank  5: 72.65
     rank  6: 64.72
     rank  7: 69.22
     rank  8: 22.68
     rank  9: 21.58
     rank 10: 20.29
     rank 11: 21.80
     rank 12: 10.81
     rank 13: 10.76
     rank 14: 10.08
     rank 15: 10.67
  backward-recv:
     rank  0: 1419.42
     rank  1: 1419.69
     rank  2: 1415.42
     rank  3: 1419.42
     rank  4: 840.46
     rank  5: 840.80
     rank  6: 845.04
     rank  7: 841.05
     rank  8: 464.77
     rank  9: 464.76
     rank 10: 464.88
     rank 11: 464.51
     rank 12: 214.28
     rank 13: 214.85
     rank 14: 215.27
     rank 15: 215.05
  backward-send:
     rank  4: 42.09
     rank  5: 41.98
     rank  6: 38.30
     rank  7: 41.84
     rank  8: 32.62
     rank  9: 32.32
     rank 10: 32.50
     rank 11: 32.35
     rank 12: 21.10
     rank 13: 20.47
     rank 14: 20.74
     rank 15: 20.38
     rank 16: 10.75
     rank 17: 10.17
     rank 18: 10.73
     rank 19: 10.40
  forward-send-backward-recv:
     rank  0: 3277.97
     rank  1: 3278.11
     rank  2: 3273.08
     rank  3: 3280.26
     rank  4: 1726.31
     rank  5: 1730.47
     rank  6: 1732.58
     rank  7: 1729.12
     rank  8: 663.76
     rank  9: 663.87
     rank 10: 662.85
     rank 11: 662.99
     rank 12: 478.53
     rank 13: 481.80
     rank 14: 471.86
     rank 15: 477.59
  backward-send-forward-recv:
     rank  4: 168.75
     rank  5: 168.33
     rank  6: 157.63
     rank  7: 168.47
     rank  8: 191.91
     rank  9: 193.45
     rank 10: 191.61
     rank 11: 191.11
     rank 12: 162.99
     rank 13: 162.59
     rank 14: 161.76
     rank 15: 161.81
     rank 16: 746.84
     rank 17: 747.44
     rank 18: 728.22
     rank 19: 741.18
  layernorm-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.10
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.08
     rank  1: 0.89
     rank  2: 0.98
     rank  3: 0.91
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.25
     rank  2: 0.26
     rank  3: 0.22
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.64
     rank  1: 16.25
     rank  2: 16.23
     rank  3: 16.09
     rank  4: 0.04
     rank  5: 0.10
     rank  6: 0.05
     rank  7: 0.10
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.07
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.08
     rank 19: 0.07
  optimizer:
     rank  0: 17.49
     rank  1: 17.12
     rank  2: 17.09
     rank  3: 16.95
     rank  4: 0.90
     rank  5: 0.95
     rank  6: 0.90
     rank  7: 0.95
     rank  8: 0.89
     rank  9: 0.92
     rank 10: 0.89
     rank 11: 0.88
     rank 12: 0.93
     rank 13: 0.90
     rank 14: 0.89
     rank 15: 0.89
     rank 16: 0.93
     rank 17: 0.90
     rank 18: 0.94
     rank 19: 0.93
