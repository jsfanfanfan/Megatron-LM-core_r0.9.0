examples/multimodal/pretrain-freeze-llm-hete-2080tifirst.sh: line 4: activate: No such file or directory
4
[2024-12-05 16:28:09,090] torch.distributed.run: [WARNING] 
[2024-12-05 16:28:09,090] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 16:28:09,090] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 16:28:09,090] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 114.91
     rank  1: 116.91
     rank  2: 116.85
     rank  3: 116.79
     rank  4: 30.71
     rank  5: 29.07
     rank  6: 39.72
     rank  7: 30.66
     rank  8: 27.99
     rank  9: 30.33
     rank 10: 29.13
     rank 11: 27.86
     rank 12: 71.44
     rank 13: 73.59
     rank 14: 86.13
     rank 15: 71.17
     rank 16: 64.57
     rank 17: 65.80
     rank 18: 65.41
     rank 19: 64.45
  train/valid/test-data-iterators-setup:
     rank  0: 1261.18
     rank  1: 1263.75
     rank  2: 1288.99
     rank  3: 1261.12
     rank  4: 1261.26
     rank  5: 1261.52
     rank  6: 1261.23
     rank  7: 1261.24
     rank  8: 1260.87
     rank  9: 1261.21
     rank 10: 1261.27
     rank 11: 1261.26
     rank 12: 1288.13
     rank 13: 1288.75
     rank 14: 1261.94
     rank 15: 1288.02
     rank 16: 1288.05
     rank 17: 1288.19
     rank 18: 1288.18
     rank 19: 1288.37
 [2024-12-05 16:29:02] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 25537.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.016058E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4500.0 | max reserved: 4500.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4456.0 | max reserved: 4456.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4496.0 | max reserved: 4496.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4488.0 | max reserved: 4488.0
times across ranks (ms):
  forward-backward:
     rank  0: 25445.21
     rank  1: 25447.05
     rank  2: 25447.54
     rank  3: 25444.88
     rank  4: 25483.67
     rank  5: 25483.47
     rank  6: 25477.75
     rank  7: 25476.39
     rank  8: 25474.40
     rank  9: 25478.51
     rank 10: 25482.45
     rank 11: 25479.40
     rank 12: 25479.51
     rank 13: 25479.91
     rank 14: 25477.41
     rank 15: 25481.52
     rank 16: 25440.36
     rank 17: 25440.56
     rank 18: 25440.18
     rank 19: 25440.09
  forward-compute:
     rank  0: 4471.18
     rank  1: 4472.60
     rank  2: 4482.86
     rank  3: 4479.28
     rank  4: 4957.56
     rank  5: 4960.04
     rank  6: 4973.13
     rank  7: 4968.78
     rank  8: 5057.47
     rank  9: 5056.92
     rank 10: 5069.63
     rank 11: 5069.28
     rank 12: 6926.97
     rank 13: 6932.15
     rank 14: 6950.73
     rank 15: 6939.21
     rank 16: 7878.12
     rank 17: 7885.36
     rank 18: 7884.07
     rank 19: 7894.85
  backward-compute:
     rank  0: 2940.33
     rank  1: 2941.60
     rank  2: 2946.63
     rank  3: 2950.80
     rank  4: 2989.26
     rank  5: 2988.44
     rank  6: 2988.21
     rank  7: 2988.18
     rank  8: 2973.85
     rank  9: 2975.56
     rank 10: 2992.08
     rank 11: 2976.83
     rank 12: 5823.73
     rank 13: 5820.44
     rank 14: 5814.08
     rank 15: 5823.36
     rank 16: 7054.13
     rank 17: 7051.95
     rank 18: 7053.62
     rank 19: 7050.87
  pure-backward-compute:
     rank  0: 2939.16
     rank  1: 2940.55
     rank  2: 2945.38
     rank  3: 2949.78
     rank  4: 2987.64
     rank  5: 2987.37
     rank  6: 2987.23
     rank  7: 2987.05
     rank  8: 2972.96
     rank  9: 2974.51
     rank 10: 2990.83
     rank 11: 2975.95
     rank 12: 5821.61
     rank 13: 5817.81
     rank 14: 5811.62
     rank 15: 5819.89
     rank 16: 7051.44
     rank 17: 7047.46
     rank 18: 7051.19
     rank 19: 7048.49
  batch-generator:
     rank  0: 1269.71
     rank  1: 1287.83
     rank  2: 1297.00
     rank  3: 1293.19
     rank  4: 1048.72
     rank  5: 1054.45
     rank  6: 1067.05
     rank  7: 1066.60
     rank  8: 1110.55
     rank  9: 1114.17
     rank 10: 1128.26
     rank 11: 1125.58
     rank 12: 1320.34
     rank 13: 1318.48
     rank 14: 1354.16
     rank 15: 1345.79
     rank 16: 1096.00
     rank 17: 1104.31
     rank 18: 1105.27
     rank 19: 1117.18
  forward-recv:
     rank  4: 2795.56
     rank  5: 2793.57
     rank  6: 2797.78
     rank  7: 2795.95
     rank  8: 5084.23
     rank  9: 5091.61
     rank 10: 5081.67
     rank 11: 5089.66
     rank 12: 7576.49
     rank 13: 7575.69
     rank 14: 7570.28
     rank 15: 7572.26
     rank 16: 9297.58
     rank 17: 9291.87
     rank 18: 9292.49
     rank 19: 9286.26
  forward-send:
     rank  0: 6271.63
     rank  1: 6269.89
     rank  2: 6259.73
     rank  3: 6261.08
     rank  4: 3889.15
     rank  5: 3889.36
     rank  6: 3875.47
     rank  7: 3879.10
     rank  8: 1587.07
     rank  9: 1580.01
     rank 10: 1575.33
     rank 11: 1571.57
     rank 12: 31.70
     rank 13: 25.79
     rank 14: 26.93
     rank 15: 20.78
  backward-recv:
     rank  0: 3063.34
     rank  1: 3062.67
     rank  2: 3062.73
     rank  3: 3062.38
     rank  4: 2043.87
     rank  5: 2044.70
     rank  6: 2044.66
     rank  7: 2046.01
     rank  8: 1351.80
     rank  9: 1352.35
     rank 10: 1350.00
     rank 11: 1350.85
     rank 12: 497.13
     rank 13: 497.45
     rank 14: 496.82
     rank 15: 497.46
  backward-send:
     rank  4: 25.27
     rank  5: 24.74
     rank  6: 24.93
     rank  7: 24.08
     rank  8: 31.10
     rank  9: 31.01
     rank 10: 31.40
     rank 11: 31.02
     rank 12: 21.11
     rank 13: 21.19
     rank 14: 19.83
     rank 15: 19.98
     rank 16: 10.89
     rank 17: 10.65
     rank 18: 10.49
     rank 19: 9.95
  forward-send-backward-recv:
     rank  0: 8611.77
     rank  1: 8612.55
     rank  2: 8605.47
     rank  3: 8606.34
     rank  4: 7782.76
     rank  5: 7782.93
     rank  6: 7784.87
     rank  7: 7785.45
     rank  8: 8678.24
     rank  9: 8679.99
     rank 10: 8663.64
     rank 11: 8679.15
     rank 12: 3801.64
     rank 13: 3805.76
     rank 14: 3812.19
     rank 15: 3803.71
  backward-send-forward-recv:
     rank  4: 772.11
     rank  5: 773.07
     rank  6: 770.50
     rank  7: 771.23
     rank  8: 299.92
     rank  9: 297.92
     rank 10: 299.73
     rank 11: 298.70
     rank 12: 185.22
     rank 13: 185.34
     rank 14: 170.87
     rank 15: 185.21
     rank 16: 249.72
     rank 17: 249.60
     rank 18: 250.57
     rank 19: 247.10
  layernorm-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.07
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.04
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.17
     rank  1: 0.15
     rank  2: 0.15
     rank  3: 0.13
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.08
     rank 16: 0.10
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.07
  all-grads-sync:
     rank  0: 61.08
     rank  1: 61.30
     rank  2: 63.83
     rank  3: 59.53
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.30
     rank  1: 0.32
     rank  2: 0.35
     rank  3: 0.32
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.05
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.06
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.04
     rank  3: 0.02
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 47.25
     rank  1: 49.10
     rank  2: 49.49
     rank  3: 49.25
     rank  4: 0.20
     rank  5: 0.17
     rank  6: 0.20
     rank  7: 0.17
     rank  8: 0.20
     rank  9: 0.20
     rank 10: 0.18
     rank 11: 0.11
     rank 12: 0.13
     rank 13: 0.23
     rank 14: 0.16
     rank 15: 0.58
     rank 16: 0.11
     rank 17: 0.09
     rank 18: 0.09
     rank 19: 0.09
  optimizer:
     rank  0: 48.95
     rank  1: 50.80
     rank  2: 51.19
     rank  3: 50.95
     rank  4: 1.90
     rank  5: 1.89
     rank  6: 1.91
     rank  7: 1.88
     rank  8: 1.90
     rank  9: 1.91
     rank 10: 1.89
     rank 11: 1.81
     rank 12: 1.83
     rank 13: 1.93
     rank 14: 1.82
     rank 15: 2.29
     rank 16: 1.82
     rank 17: 1.79
     rank 18: 1.79
     rank 19: 1.79
 [2024-12-05 16:29:18] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 15834.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.017110E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15777.81
     rank  1: 15777.76
     rank  2: 15777.88
     rank  3: 15777.72
     rank  4: 15776.29
     rank  5: 15776.28
     rank  6: 15776.28
     rank  7: 15776.25
     rank  8: 15776.43
     rank  9: 15776.36
     rank 10: 15776.45
     rank 11: 15776.38
     rank 12: 15776.56
     rank 13: 15776.46
     rank 14: 15776.74
     rank 15: 15776.16
     rank 16: 15776.25
     rank 17: 15776.33
     rank 18: 15776.37
     rank 19: 15776.24
  forward-compute:
     rank  0: 1856.35
     rank  1: 1864.66
     rank  2: 1868.85
     rank  3: 1861.04
     rank  4: 2873.27
     rank  5: 2882.42
     rank  6: 2884.75
     rank  7: 2876.57
     rank  8: 2764.55
     rank  9: 2769.51
     rank 10: 2772.88
     rank 11: 2767.75
     rank 12: 5599.91
     rank 13: 5600.96
     rank 14: 5621.05
     rank 15: 5600.45
     rank 16: 6700.49
     rank 17: 6703.81
     rank 18: 6703.40
     rank 19: 6704.73
  backward-compute:
     rank  0: 1742.87
     rank  1: 1745.59
     rank  2: 1740.75
     rank  3: 1749.15
     rank  4: 2974.18
     rank  5: 2975.12
     rank  6: 2974.65
     rank  7: 2973.79
     rank  8: 2955.15
     rank  9: 2956.18
     rank 10: 2975.31
     rank 11: 2958.70
     rank 12: 5841.63
     rank 13: 5842.37
     rank 14: 5834.18
     rank 15: 5840.29
     rank 16: 7075.27
     rank 17: 7073.48
     rank 18: 7075.78
     rank 19: 7072.14
  pure-backward-compute:
     rank  0: 1741.81
     rank  1: 1744.65
     rank  2: 1739.63
     rank  3: 1748.20
     rank  4: 2973.02
     rank  5: 2974.23
     rank  6: 2973.71
     rank  7: 2972.91
     rank  8: 2954.24
     rank  9: 2955.38
     rank 10: 2974.24
     rank 11: 2957.83
     rank 12: 5839.97
     rank 13: 5840.42
     rank 14: 5832.07
     rank 15: 5837.93
     rank 16: 7072.86
     rank 17: 7070.77
     rank 18: 7073.82
     rank 19: 7069.55
  batch-generator:
     rank  0: 83.82
     rank  1: 95.17
     rank  2: 100.41
     rank  3: 92.52
     rank  4: 58.72
     rank  5: 72.30
     rank  6: 75.47
     rank  7: 68.80
     rank  8: 52.37
     rank  9: 59.90
     rank 10: 65.53
     rank 11: 57.84
     rank 12: 72.85
     rank 13: 79.52
     rank 14: 107.17
     rank 15: 91.81
     rank 16: 72.54
     rank 17: 77.28
     rank 18: 79.92
     rank 19: 83.08
  forward-recv:
     rank  4: 132.94
     rank  5: 129.93
     rank  6: 131.89
     rank  7: 132.36
     rank  8: 327.56
     rank  9: 325.73
     rank 10: 323.03
     rank 11: 326.81
     rank 12: 507.21
     rank 13: 505.93
     rank 14: 503.03
     rank 15: 505.75
     rank 16: 859.74
     rank 17: 859.62
     rank 18: 858.07
     rank 19: 859.79
  forward-send:
     rank  0: 404.56
     rank  1: 396.85
     rank  2: 392.58
     rank  3: 400.38
     rank  4: 206.29
     rank  5: 201.97
     rank  6: 196.24
     rank  7: 205.23
     rank  8: 201.50
     rank  9: 199.99
     rank 10: 195.70
     rank 11: 201.31
     rank 12: 10.63
     rank 13: 10.32
     rank 14: 9.23
     rank 15: 10.88
  backward-recv:
     rank  0: 3060.72
     rank  1: 3060.80
     rank  2: 3061.44
     rank  3: 3059.96
     rank  4: 2049.71
     rank  5: 2049.96
     rank  6: 2050.24
     rank  7: 2050.36
     rank  8: 1354.66
     rank  9: 1355.13
     rank 10: 1353.39
     rank 11: 1353.73
     rank 12: 495.42
     rank 13: 495.38
     rank 14: 496.46
     rank 15: 495.61
  backward-send:
     rank  4: 23.74
     rank  5: 23.14
     rank  6: 23.36
     rank  7: 23.19
     rank  8: 31.20
     rank  9: 30.74
     rank 10: 31.37
     rank 11: 31.17
     rank 12: 21.26
     rank 13: 21.33
     rank 14: 20.51
     rank 15: 20.37
     rank 16: 10.84
     rank 17: 10.39
     rank 18: 10.75
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 8694.02
     rank  1: 8693.12
     rank  2: 8694.69
     rank  3: 8690.50
     rank  4: 7272.96
     rank  5: 7271.65
     rank  6: 7275.84
     rank  7: 7275.01
     rank  8: 7686.90
     rank  9: 7689.02
     rank 10: 7670.00
     rank 11: 7686.54
     rank 12: 2615.44
     rank 13: 2617.00
     rank 14: 2622.00
     rank 15: 2615.88
  backward-send-forward-recv:
     rank  4: 123.51
     rank  5: 123.59
     rank  6: 123.10
     rank  7: 122.95
     rank  8: 145.28
     rank  9: 143.63
     rank 10: 145.42
     rank 11: 143.96
     rank 12: 177.73
     rank 13: 177.52
     rank 14: 161.05
     rank 15: 175.75
     rank 16: 248.17
     rank 17: 246.88
     rank 18: 247.71
     rank 19: 246.58
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.09
     rank  1: 0.07
     rank  2: 0.09
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.09
     rank 18: 0.11
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.02
     rank  1: 1.13
     rank  2: 1.14
     rank  3: 0.98
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.06
     rank 15: 0.07
     rank 16: 0.02
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.25
     rank  2: 0.28
     rank  3: 0.25
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.06
     rank  1: 16.37
     rank  2: 16.62
     rank  3: 16.17
     rank  4: 0.08
     rank  5: 0.07
     rank  6: 0.04
     rank  7: 0.07
     rank  8: 0.06
     rank  9: 0.04
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.09
     rank 13: 0.06
     rank 14: 0.11
     rank 15: 0.13
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.09
     rank 19: 0.07
  optimizer:
     rank  0: 17.01
     rank  1: 17.32
     rank  2: 17.57
     rank  3: 17.11
     rank  4: 1.01
     rank  5: 1.01
     rank  6: 0.98
     rank  7: 1.01
     rank  8: 1.00
     rank  9: 0.97
     rank 10: 1.01
     rank 11: 1.01
     rank 12: 1.03
     rank 13: 0.99
     rank 14: 1.04
     rank 15: 1.07
     rank 16: 0.99
     rank 17: 1.03
     rank 18: 1.03
     rank 19: 1.01
 [2024-12-05 16:29:34] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 15862.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.133165E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15810.13
     rank  1: 15810.08
     rank  2: 15810.12
     rank  3: 15810.10
     rank  4: 15808.57
     rank  5: 15808.54
     rank  6: 15808.54
     rank  7: 15808.56
     rank  8: 15808.70
     rank  9: 15809.42
     rank 10: 15808.71
     rank 11: 15808.70
     rank 12: 15808.80
     rank 13: 15808.89
     rank 14: 15808.86
     rank 15: 15808.84
     rank 16: 15808.61
     rank 17: 15808.62
     rank 18: 15808.74
     rank 19: 15808.61
  forward-compute:
     rank  0: 2097.14
     rank  1: 2097.79
     rank  2: 2108.14
     rank  3: 2102.41
     rank  4: 2863.60
     rank  5: 2866.25
     rank  6: 2874.47
     rank  7: 2871.39
     rank  8: 2763.69
     rank  9: 2765.04
     rank 10: 2768.03
     rank 11: 2767.16
     rank 12: 5622.99
     rank 13: 5623.77
     rank 14: 5636.33
     rank 15: 5625.00
     rank 16: 6711.87
     rank 17: 6715.48
     rank 18: 6713.53
     rank 19: 6716.90
  backward-compute:
     rank  0: 1737.84
     rank  1: 1743.47
     rank  2: 1740.57
     rank  3: 1748.69
     rank  4: 2981.60
     rank  5: 2982.78
     rank  6: 2981.83
     rank  7: 2981.14
     rank  8: 2954.80
     rank  9: 2956.28
     rank 10: 2972.78
     rank 11: 2958.46
     rank 12: 5868.71
     rank 13: 5867.57
     rank 14: 5859.84
     rank 15: 5866.20
     rank 16: 7095.00
     rank 17: 7093.44
     rank 18: 7096.74
     rank 19: 7091.66
  pure-backward-compute:
     rank  0: 1736.81
     rank  1: 1742.55
     rank  2: 1739.50
     rank  3: 1747.65
     rank  4: 2980.52
     rank  5: 2981.96
     rank  6: 2981.05
     rank  7: 2980.12
     rank  8: 2953.94
     rank  9: 2955.50
     rank 10: 2971.91
     rank 11: 2957.69
     rank 12: 5867.24
     rank 13: 5865.92
     rank 14: 5857.97
     rank 15: 5864.10
     rank 16: 7092.59
     rank 17: 7091.05
     rank 18: 7094.74
     rank 19: 7089.24
  batch-generator:
     rank  0: 81.78
     rank  1: 85.19
     rank  2: 99.31
     rank  3: 92.07
     rank  4: 55.71
     rank  5: 60.36
     rank  6: 70.61
     rank  7: 68.71
     rank  8: 49.23
     rank  9: 53.66
     rank 10: 58.97
     rank 11: 54.68
     rank 12: 71.10
     rank 13: 77.51
     rank 14: 97.10
     rank 15: 88.86
     rank 16: 71.21
     rank 17: 75.92
     rank 18: 77.80
     rank 19: 82.59
  forward-recv:
     rank  4: 133.97
     rank  5: 134.31
     rank  6: 131.67
     rank  7: 133.12
     rank  8: 329.93
     rank  9: 330.75
     rank 10: 326.86
     rank 11: 329.74
     rank 12: 506.57
     rank 13: 506.46
     rank 14: 505.22
     rank 15: 506.21
     rank 16: 857.53
     rank 17: 857.41
     rank 18: 856.49
     rank 19: 857.00
  forward-send:
     rank  0: 390.50
     rank  1: 390.41
     rank  2: 379.75
     rank  3: 385.43
     rank  4: 205.55
     rank  5: 205.06
     rank  6: 198.61
     rank  7: 203.44
     rank  8: 200.18
     rank  9: 199.36
     rank 10: 197.11
     rank 11: 198.72
     rank 12: 10.79
     rank 13: 10.59
     rank 14: 10.17
     rank 15: 10.52
  backward-recv:
     rank  0: 3077.30
     rank  1: 3075.93
     rank  2: 3076.91
     rank  3: 3075.65
     rank  4: 2056.55
     rank  5: 2057.30
     rank  6: 2056.85
     rank  7: 2056.47
     rank  8: 1362.43
     rank  9: 1362.64
     rank 10: 1360.78
     rank 11: 1362.32
     rank 12: 497.96
     rank 13: 498.46
     rank 14: 498.58
     rank 15: 497.68
  backward-send:
     rank  4: 24.70
     rank  5: 23.06
     rank  6: 24.19
     rank  7: 24.03
     rank  8: 31.15
     rank  9: 30.98
     rank 10: 31.44
     rank 11: 31.09
     rank 12: 21.49
     rank 13: 21.36
     rank 14: 20.33
     rank 15: 21.43
     rank 16: 10.81
     rank 17: 10.57
     rank 18: 10.80
     rank 19: 10.62
  forward-send-backward-recv:
     rank  0: 8488.53
     rank  1: 8485.05
     rank  2: 8485.25
     rank  3: 8480.38
     rank  4: 7296.10
     rank  5: 7294.34
     rank  6: 7298.46
     rank  7: 7298.11
     rank  8: 7709.32
     rank  9: 7710.65
     rank 10: 7694.31
     rank 11: 7707.37
     rank 12: 2592.17
     rank 13: 2595.12
     rank 14: 2600.63
     rank 15: 2593.82
  backward-send-forward-recv:
     rank  4: 126.76
     rank  5: 125.68
     rank  6: 125.46
     rank  7: 122.57
     rank  8: 145.84
     rank  9: 144.04
     rank 10: 145.97
     rank 11: 145.03
     rank 12: 178.96
     rank 13: 178.11
     rank 14: 166.59
     rank 15: 176.17
     rank 16: 246.70
     rank 17: 245.43
     rank 18: 246.49
     rank 19: 245.14
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.10
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.05
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.08
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.08
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.12
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.00
     rank  1: 0.90
     rank  2: 0.99
     rank  3: 0.98
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.06
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.05
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.24
     rank  2: 0.26
     rank  3: 0.24
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.06
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.05
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.30
     rank  1: 16.24
     rank  2: 16.17
     rank  3: 16.09
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.03
     rank  9: 0.23
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.11
     rank 14: 0.12
     rank 15: 0.12
     rank 16: 0.08
     rank 17: 0.08
     rank 18: 0.10
     rank 19: 0.05
  optimizer:
     rank  0: 17.63
     rank  1: 17.58
     rank  2: 17.51
     rank  3: 17.43
     rank  4: 1.40
     rank  5: 1.37
     rank  6: 1.36
     rank  7: 1.40
     rank  8: 1.36
     rank  9: 1.61
     rank 10: 1.36
     rank 11: 1.37
     rank 12: 1.40
     rank 13: 1.44
     rank 14: 1.45
     rank 15: 1.45
     rank 16: 1.41
     rank 17: 1.42
     rank 18: 1.43
     rank 19: 1.38
 [2024-12-05 16:29:50] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 15889.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.919312E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15842.04
     rank  1: 15841.98
     rank  2: 15842.04
     rank  3: 15841.97
     rank  4: 15840.42
     rank  5: 15840.40
     rank  6: 15840.46
     rank  7: 15840.41
     rank  8: 15840.59
     rank  9: 15840.55
     rank 10: 15840.65
     rank 11: 15840.53
     rank 12: 15840.67
     rank 13: 15840.63
     rank 14: 15840.75
     rank 15: 15841.39
     rank 16: 15840.43
     rank 17: 15840.55
     rank 18: 15840.45
     rank 19: 15840.44
  forward-compute:
     rank  0: 1862.54
     rank  1: 1863.78
     rank  2: 1871.20
     rank  3: 1872.02
     rank  4: 2874.96
     rank  5: 2877.87
     rank  6: 2883.57
     rank  7: 2883.88
     rank  8: 2769.10
     rank  9: 2769.48
     rank 10: 2769.66
     rank 11: 2775.92
     rank 12: 5643.36
     rank 13: 5642.98
     rank 14: 5655.78
     rank 15: 5647.42
     rank 16: 6724.20
     rank 17: 6727.11
     rank 18: 6724.83
     rank 19: 6728.84
  backward-compute:
     rank  0: 1751.58
     rank  1: 1756.44
     rank  2: 1753.90
     rank  3: 1759.20
     rank  4: 2976.44
     rank  5: 2976.96
     rank  6: 2975.77
     rank  7: 2977.39
     rank  8: 2959.60
     rank  9: 2959.43
     rank 10: 2977.99
     rank 11: 2964.70
     rank 12: 5900.07
     rank 13: 5899.27
     rank 14: 5890.88
     rank 15: 5897.42
     rank 16: 7104.93
     rank 17: 7105.24
     rank 18: 7106.83
     rank 19: 7101.72
  pure-backward-compute:
     rank  0: 1750.54
     rank  1: 1755.50
     rank  2: 1752.75
     rank  3: 1758.11
     rank  4: 2975.38
     rank  5: 2976.11
     rank  6: 2974.93
     rank  7: 2976.31
     rank  8: 2958.75
     rank  9: 2958.36
     rank 10: 2976.74
     rank 11: 2963.97
     rank 12: 5898.56
     rank 13: 5897.38
     rank 14: 5888.79
     rank 15: 5895.32
     rank 16: 7102.62
     rank 17: 7102.93
     rank 18: 7104.83
     rank 19: 7099.32
  batch-generator:
     rank  0: 74.65
     rank  1: 78.85
     rank  2: 89.69
     rank  3: 88.90
     rank  4: 59.67
     rank  5: 66.08
     rank  6: 73.17
     rank  7: 75.12
     rank  8: 51.53
     rank  9: 57.52
     rank 10: 59.01
     rank 11: 61.86
     rank 12: 69.23
     rank 13: 74.13
     rank 14: 94.39
     rank 15: 90.29
     rank 16: 69.84
     rank 17: 74.03
     rank 18: 75.37
     rank 19: 80.84
  forward-recv:
     rank  4: 138.46
     rank  5: 138.50
     rank  6: 135.25
     rank  7: 134.92
     rank  8: 332.77
     rank  9: 332.98
     rank 10: 331.42
     rank 11: 331.07
     rank 12: 510.54
     rank 13: 510.67
     rank 14: 509.82
     rank 15: 510.05
     rank 16: 862.96
     rank 17: 862.81
     rank 18: 862.76
     rank 19: 862.49
  forward-send:
     rank  0: 381.15
     rank  1: 380.45
     rank  2: 372.73
     rank  3: 372.15
     rank  4: 212.55
     rank  5: 212.29
     rank  6: 209.22
     rank  7: 209.07
     rank  8: 203.55
     rank  9: 202.89
     rank 10: 202.31
     rank 11: 202.09
     rank 12: 10.91
     rank 13: 10.64
     rank 14: 10.78
     rank 15: 10.47
  backward-recv:
     rank  0: 3075.05
     rank  1: 3074.95
     rank  2: 3074.66
     rank  3: 3074.02
     rank  4: 2057.80
     rank  5: 2057.76
     rank  6: 2057.98
     rank  7: 2058.71
     rank  8: 1360.45
     rank  9: 1360.71
     rank 10: 1357.50
     rank 11: 1360.40
     rank 12: 497.96
     rank 13: 497.84
     rank 14: 500.10
     rank 15: 497.64
  backward-send:
     rank  4: 24.32
     rank  5: 23.79
     rank  6: 23.97
     rank  7: 23.32
     rank  8: 31.11
     rank  9: 30.93
     rank 10: 31.40
     rank 11: 30.81
     rank 12: 20.90
     rank 13: 20.81
     rank 14: 18.85
     rank 15: 20.97
     rank 16: 10.79
     rank 17: 10.42
     rank 18: 10.77
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 8752.08
     rank  1: 8749.36
     rank  2: 8747.95
     rank  3: 8747.25
     rank  4: 7310.86
     rank  5: 7309.06
     rank  6: 7313.36
     rank  7: 7311.79
     rank  8: 7725.30
     rank  9: 7728.10
     rank 10: 7711.70
     rank 11: 7721.59
     rank 12: 2567.83
     rank 13: 2569.77
     rank 14: 2576.83
     rank 15: 2567.44
  backward-send-forward-recv:
     rank  4: 123.17
     rank  5: 122.78
     rank  6: 122.08
     rank  7: 121.41
     rank  8: 145.26
     rank  9: 143.82
     rank 10: 144.36
     rank 11: 142.79
     rank 12: 176.79
     rank 13: 176.26
     rank 14: 163.41
     rank 15: 173.04
     rank 16: 247.72
     rank 17: 246.59
     rank 18: 247.64
     rank 19: 246.43
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.10
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.10
     rank  1: 0.07
     rank  2: 0.09
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.05
     rank  1: 1.01
     rank  2: 1.05
     rank  3: 1.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.18
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.25
     rank  3: 0.24
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.05
     rank  1: 0.02
     rank  2: 0.05
     rank  3: 0.03
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.06
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.26
     rank  1: 16.49
     rank  2: 16.36
     rank  3: 16.19
     rank  4: 0.07
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.07
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.09
     rank 13: 0.09
     rank 14: 0.09
     rank 15: 0.42
     rank 16: 0.07
     rank 17: 0.08
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 17.51
     rank  1: 17.75
     rank  2: 17.62
     rank  3: 17.44
     rank  4: 1.31
     rank  5: 1.31
     rank  6: 1.32
     rank  7: 1.32
     rank  8: 1.31
     rank  9: 1.31
     rank 10: 1.31
     rank 11: 1.28
     rank 12: 1.33
     rank 13: 1.33
     rank 14: 1.34
     rank 15: 1.71
     rank 16: 1.32
     rank 17: 1.33
     rank 18: 1.30
     rank 19: 1.28
 [2024-12-05 16:30:06] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 15925.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.346019E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15870.46
     rank  1: 15870.42
     rank  2: 15870.45
     rank  3: 15870.44
     rank  4: 15868.93
     rank  5: 15868.92
     rank  6: 15868.88
     rank  7: 15868.86
     rank  8: 15869.06
     rank  9: 15869.08
     rank 10: 15869.04
     rank 11: 15869.06
     rank 12: 15869.10
     rank 13: 15869.41
     rank 14: 15869.15
     rank 15: 15869.84
     rank 16: 15868.95
     rank 17: 15869.02
     rank 18: 15868.93
     rank 19: 15868.96
  forward-compute:
     rank  0: 1860.43
     rank  1: 1862.39
     rank  2: 1875.28
     rank  3: 1865.50
     rank  4: 2870.03
     rank  5: 2871.72
     rank  6: 2882.16
     rank  7: 2868.98
     rank  8: 2763.14
     rank  9: 2764.30
     rank 10: 2770.77
     rank 11: 2765.76
     rank 12: 5673.11
     rank 13: 5674.10
     rank 14: 5695.62
     rank 15: 5668.66
     rank 16: 6733.38
     rank 17: 6735.83
     rank 18: 6735.61
     rank 19: 6737.72
  backward-compute:
     rank  0: 1751.59
     rank  1: 1754.67
     rank  2: 1751.83
     rank  3: 1756.64
     rank  4: 2980.53
     rank  5: 2981.77
     rank  6: 2980.90
     rank  7: 2981.15
     rank  8: 2954.30
     rank  9: 2957.90
     rank 10: 2978.96
     rank 11: 2956.01
     rank 12: 5922.10
     rank 13: 5920.40
     rank 14: 5912.17
     rank 15: 5919.84
     rank 16: 7124.99
     rank 17: 7125.51
     rank 18: 7126.65
     rank 19: 7121.74
  pure-backward-compute:
     rank  0: 1750.54
     rank  1: 1753.71
     rank  2: 1750.73
     rank  3: 1755.39
     rank  4: 2979.67
     rank  5: 2980.83
     rank  6: 2980.09
     rank  7: 2980.02
     rank  8: 2953.47
     rank  9: 2957.02
     rank 10: 2977.90
     rank 11: 2955.25
     rank 12: 5920.63
     rank 13: 5918.93
     rank 14: 5910.92
     rank 15: 5917.24
     rank 16: 7122.71
     rank 17: 7123.10
     rank 18: 7124.72
     rank 19: 7119.23
  batch-generator:
     rank  0: 80.25
     rank  1: 85.17
     rank  2: 100.55
     rank  3: 89.79
     rank  4: 53.91
     rank  5: 59.32
     rank  6: 72.47
     rank  7: 61.86
     rank  8: 51.25
     rank  9: 55.08
     rank 10: 64.04
     rank 11: 56.26
     rank 12: 68.39
     rank 13: 75.14
     rank 14: 103.50
     rank 15: 84.10
     rank 16: 71.23
     rank 17: 74.89
     rank 18: 78.31
     rank 19: 82.03
  forward-recv:
     rank  4: 137.02
     rank  5: 137.45
     rank  6: 134.93
     rank  7: 136.50
     rank  8: 330.57
     rank  9: 331.21
     rank 10: 325.37
     rank 11: 330.47
     rank 12: 505.77
     rank 13: 505.66
     rank 14: 503.03
     rank 15: 506.28
     rank 16: 864.29
     rank 17: 864.00
     rank 18: 862.85
     rank 19: 864.00
  forward-send:
     rank  0: 399.62
     rank  1: 398.69
     rank  2: 385.68
     rank  3: 395.11
     rank  4: 211.31
     rank  5: 210.43
     rank  6: 201.15
     rank  7: 209.62
     rank  8: 207.06
     rank  9: 206.41
     rank 10: 202.66
     rank 11: 206.38
     rank 12: 10.69
     rank 13: 10.49
     rank 14: 9.61
     rank 15: 10.65
  backward-recv:
     rank  0: 3083.95
     rank  1: 3083.41
     rank  2: 3083.63
     rank  3: 3083.05
     rank  4: 2062.52
     rank  5: 2062.28
     rank  6: 2062.79
     rank  7: 2062.20
     rank  8: 1368.01
     rank  9: 1368.63
     rank 10: 1365.38
     rank 11: 1367.73
     rank 12: 499.15
     rank 13: 498.86
     rank 14: 500.62
     rank 15: 499.22
  backward-send:
     rank  4: 24.59
     rank  5: 24.02
     rank  6: 24.01
     rank  7: 24.07
     rank  8: 31.17
     rank  9: 30.41
     rank 10: 31.49
     rank 11: 30.87
     rank 12: 21.03
     rank 13: 21.04
     rank 14: 19.30
     rank 15: 20.76
     rank 16: 10.82
     rank 17: 10.46
     rank 18: 10.72
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 8755.43
     rank  1: 8754.50
     rank  2: 8754.07
     rank  3: 8753.78
     rank  4: 7338.42
     rank  5: 7337.59
     rank  6: 7341.51
     rank  7: 7341.73
     rank  8: 7757.51
     rank  9: 7758.43
     rank 10: 7736.24
     rank 11: 7758.21
     rank 12: 2551.81
     rank 13: 2553.59
     rank 14: 2560.97
     rank 15: 2553.44
  backward-send-forward-recv:
     rank  4: 125.84
     rank  5: 125.32
     rank  6: 124.93
     rank  7: 127.13
     rank  8: 145.15
     rank  9: 142.23
     rank 10: 145.37
     rank 11: 144.10
     rank 12: 176.46
     rank 13: 175.42
     rank 14: 157.54
     rank 15: 177.91
     rank 16: 248.48
     rank 17: 247.55
     rank 18: 248.18
     rank 19: 247.14
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.06
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 1.00
     rank  1: 0.96
     rank  2: 0.98
     rank  3: 0.97
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.09
     rank 14: 0.02
     rank 15: 0.22
     rank 16: 0.02
     rank 17: 0.06
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.24
     rank  2: 0.25
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.20
     rank  1: 16.28
     rank  2: 16.05
     rank  3: 15.70
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.03
     rank  9: 0.13
     rank 10: 0.06
     rank 11: 0.08
     rank 12: 0.04
     rank 13: 0.12
     rank 14: 0.10
     rank 15: 0.29
     rank 16: 0.04
     rank 17: 0.09
     rank 18: 0.07
     rank 19: 0.07
  optimizer:
     rank  0: 17.60
     rank  1: 17.69
     rank  2: 17.46
     rank  3: 17.10
     rank  4: 1.46
     rank  5: 1.43
     rank  6: 1.43
     rank  7: 1.47
     rank  8: 1.43
     rank  9: 1.53
     rank 10: 1.46
     rank 11: 1.47
     rank 12: 1.43
     rank 13: 1.52
     rank 14: 1.49
     rank 15: 1.72
     rank 16: 1.44
     rank 17: 1.49
     rank 18: 1.48
     rank 19: 1.47
 [2024-12-05 16:30:22] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 15930.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.276205E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15884.76
     rank  1: 15884.79
     rank  2: 15884.79
     rank  3: 15884.70
     rank  4: 15883.17
     rank  5: 15883.22
     rank  6: 15883.25
     rank  7: 15883.20
     rank  8: 15883.33
     rank  9: 15883.41
     rank 10: 15883.37
     rank 11: 15883.31
     rank 12: 15883.42
     rank 13: 15883.47
     rank 14: 15883.44
     rank 15: 15883.59
     rank 16: 15883.29
     rank 17: 15883.38
     rank 18: 15883.26
     rank 19: 15883.29
  forward-compute:
     rank  0: 1848.13
     rank  1: 1849.92
     rank  2: 1860.65
     rank  3: 1859.01
     rank  4: 2873.56
     rank  5: 2876.76
     rank  6: 2884.09
     rank  7: 2881.35
     rank  8: 2770.01
     rank  9: 2772.82
     rank 10: 2776.05
     rank 11: 2775.75
     rank 12: 5671.33
     rank 13: 5670.93
     rank 14: 5690.09
     rank 15: 5671.93
     rank 16: 6741.74
     rank 17: 6745.24
     rank 18: 6743.81
     rank 19: 6747.05
  backward-compute:
     rank  0: 1740.98
     rank  1: 1744.10
     rank  2: 1741.65
     rank  3: 1744.98
     rank  4: 2978.85
     rank  5: 2980.70
     rank  6: 2978.45
     rank  7: 2979.31
     rank  8: 2957.70
     rank  9: 2959.15
     rank 10: 2977.97
     rank 11: 2960.27
     rank 12: 5919.39
     rank 13: 5917.99
     rank 14: 5908.66
     rank 15: 5916.42
     rank 16: 7138.89
     rank 17: 7139.25
     rank 18: 7141.25
     rank 19: 7135.98
  pure-backward-compute:
     rank  0: 1739.98
     rank  1: 1743.15
     rank  2: 1740.63
     rank  3: 1744.08
     rank  4: 2977.84
     rank  5: 2979.93
     rank  6: 2977.71
     rank  7: 2978.14
     rank  8: 2956.83
     rank  9: 2958.44
     rank 10: 2976.96
     rank 11: 2959.52
     rank 12: 5917.94
     rank 13: 5916.23
     rank 14: 5907.13
     rank 15: 5914.00
     rank 16: 7136.62
     rank 17: 7137.23
     rank 18: 7139.34
     rank 19: 7133.63
  batch-generator:
     rank  0: 74.73
     rank  1: 79.01
     rank  2: 91.89
     rank  3: 88.93
     rank  4: 54.16
     rank  5: 60.73
     rank  6: 70.11
     rank  7: 70.44
     rank  8: 50.43
     rank  9: 55.70
     rank 10: 61.52
     rank 11: 58.50
     rank 12: 70.58
     rank 13: 75.22
     rank 14: 101.73
     rank 15: 89.34
     rank 16: 69.94
     rank 17: 74.48
     rank 18: 76.83
     rank 19: 81.62
  forward-recv:
     rank  4: 131.92
     rank  5: 131.90
     rank  6: 130.69
     rank  7: 129.12
     rank  8: 324.75
     rank  9: 323.73
     rank 10: 321.40
     rank 11: 323.15
     rank 12: 501.96
     rank 13: 503.69
     rank 14: 498.96
     rank 15: 500.69
     rank 16: 857.05
     rank 17: 856.76
     rank 18: 855.60
     rank 19: 855.95
  forward-send:
     rank  0: 393.53
     rank  1: 392.51
     rank  2: 381.99
     rank  3: 383.56
     rank  4: 205.68
     rank  5: 205.15
     rank  6: 196.90
     rank  7: 200.79
     rank  8: 203.14
     rank  9: 204.35
     rank 10: 198.35
     rank 11: 200.71
     rank 12: 10.81
     rank 13: 10.63
     rank 14: 9.60
     rank 15: 10.17
  backward-recv:
     rank  0: 3092.55
     rank  1: 3092.84
     rank  2: 3093.04
     rank  3: 3092.94
     rank  4: 2064.38
     rank  5: 2064.98
     rank  6: 2065.36
     rank  7: 2064.57
     rank  8: 1367.32
     rank  9: 1367.62
     rank 10: 1365.42
     rank 11: 1367.46
     rank 12: 498.68
     rank 13: 498.79
     rank 14: 499.49
     rank 15: 498.57
  backward-send:
     rank  4: 24.28
     rank  5: 23.92
     rank  6: 23.96
     rank  7: 24.26
     rank  8: 31.24
     rank  9: 31.04
     rank 10: 31.29
     rank 11: 30.70
     rank 12: 21.23
     rank 13: 21.16
     rank 14: 19.74
     rank 15: 21.14
     rank 16: 10.78
     rank 17: 10.48
     rank 18: 10.76
     rank 19: 10.51
  forward-send-backward-recv:
     rank  0: 8790.77
     rank  1: 8788.49
     rank  2: 8788.04
     rank  3: 8788.32
     rank  4: 7362.28
     rank  5: 7359.30
     rank  6: 7364.64
     rank  7: 7364.91
     rank  8: 7772.94
     rank  9: 7773.33
     rank 10: 7756.41
     rank 11: 7772.70
     rank 12: 2573.82
     rank 13: 2575.80
     rank 14: 2585.78
     rank 15: 2576.15
  backward-send-forward-recv:
     rank  4: 123.09
     rank  5: 122.64
     rank  6: 123.30
     rank  7: 122.46
     rank  8: 145.29
     rank  9: 142.44
     rank 10: 144.97
     rank 11: 144.35
     rank 12: 176.86
     rank 13: 175.26
     rank 14: 161.57
     rank 15: 176.43
     rank 16: 246.13
     rank 17: 245.06
     rank 18: 245.82
     rank 19: 244.77
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.07
     rank  2: 0.07
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.09
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.98
     rank  1: 0.99
     rank  2: 0.99
     rank  3: 0.91
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.26
     rank  1: 0.25
     rank  2: 0.27
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.02
     rank  1: 16.16
     rank  2: 16.25
     rank  3: 16.02
     rank  4: 0.06
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.08
     rank 10: 0.08
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.12
     rank 14: 0.08
     rank 15: 0.12
     rank 16: 0.07
     rank 17: 0.09
     rank 18: 0.05
     rank 19: 0.05
  optimizer:
     rank  0: 17.06
     rank  1: 17.20
     rank  2: 17.29
     rank  3: 17.06
     rank  4: 1.09
     rank  5: 1.06
     rank  6: 1.06
     rank  7: 1.10
     rank  8: 1.06
     rank  9: 1.11
     rank 10: 1.11
     rank 11: 1.06
     rank 12: 1.11
     rank 13: 1.14
     rank 14: 1.12
     rank 15: 1.14
     rank 16: 1.10
     rank 17: 1.12
     rank 18: 1.08
     rank 19: 1.08
 [2024-12-05 16:30:38] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 15981.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.854445E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15921.16
     rank  1: 15921.18
     rank  2: 15921.16
     rank  3: 15921.17
     rank  4: 15919.62
     rank  5: 15919.61
     rank  6: 15919.66
     rank  7: 15919.66
     rank  8: 15919.82
     rank  9: 15919.83
     rank 10: 15919.79
     rank 11: 15919.71
     rank 12: 15919.87
     rank 13: 15920.04
     rank 14: 15919.92
     rank 15: 15919.90
     rank 16: 15919.66
     rank 17: 15919.76
     rank 18: 15919.64
     rank 19: 15919.66
  forward-compute:
     rank  0: 1849.71
     rank  1: 1853.99
     rank  2: 1862.97
     rank  3: 1856.27
     rank  4: 2882.09
     rank  5: 2888.02
     rank  6: 2897.18
     rank  7: 2888.89
     rank  8: 2781.61
     rank  9: 2785.31
     rank 10: 2790.79
     rank 11: 2784.98
     rank 12: 5670.67
     rank 13: 5671.12
     rank 14: 5690.58
     rank 15: 5670.45
     rank 16: 6757.95
     rank 17: 6762.10
     rank 18: 6760.95
     rank 19: 6762.36
  backward-compute:
     rank  0: 1749.89
     rank  1: 1751.13
     rank  2: 1753.00
     rank  3: 1757.81
     rank  4: 2975.52
     rank  5: 2976.62
     rank  6: 2974.25
     rank  7: 2973.83
     rank  8: 2967.36
     rank  9: 2967.78
     rank 10: 2987.63
     rank 11: 2971.12
     rank 12: 5925.67
     rank 13: 5924.49
     rank 14: 5916.30
     rank 15: 5920.56
     rank 16: 7150.56
     rank 17: 7150.91
     rank 18: 7152.56
     rank 19: 7147.83
  pure-backward-compute:
     rank  0: 1748.86
     rank  1: 1750.14
     rank  2: 1751.96
     rank  3: 1756.84
     rank  4: 2974.20
     rank  5: 2975.69
     rank  6: 2973.52
     rank  7: 2972.99
     rank  8: 2966.53
     rank  9: 2967.07
     rank 10: 2986.71
     rank 11: 2970.40
     rank 12: 5923.88
     rank 13: 5922.26
     rank 14: 5914.62
     rank 15: 5917.95
     rank 16: 7148.24
     rank 17: 7148.70
     rank 18: 7150.51
     rank 19: 7145.31
  batch-generator:
     rank  0: 73.96
     rank  1: 81.23
     rank  2: 91.90
     rank  3: 84.07
     rank  4: 59.19
     rank  5: 64.32
     rank  6: 76.47
     rank  7: 69.56
     rank  8: 53.72
     rank  9: 59.84
     rank 10: 66.96
     rank 11: 57.80
     rank 12: 69.02
     rank 13: 75.65
     rank 14: 101.69
     rank 15: 86.65
     rank 16: 69.61
     rank 17: 74.99
     rank 18: 77.43
     rank 19: 80.85
  forward-recv:
     rank  4: 133.05
     rank  5: 131.71
     rank  6: 131.59
     rank  7: 130.48
     rank  8: 328.80
     rank  9: 328.60
     rank 10: 324.34
     rank 11: 327.65
     rank 12: 504.96
     rank 13: 504.73
     rank 14: 501.39
     rank 15: 504.89
     rank 16: 857.69
     rank 17: 857.38
     rank 18: 856.00
     rank 19: 857.73
  forward-send:
     rank  0: 390.52
     rank  1: 386.88
     rank  2: 377.91
     rank  3: 384.45
     rank  4: 204.21
     rank  5: 201.75
     rank  6: 193.66
     rank  7: 202.60
     rank  8: 199.74
     rank  9: 198.53
     rank 10: 194.07
     rank 11: 199.51
     rank 12: 10.56
     rank 13: 10.25
     rank 14: 9.18
     rank 15: 10.73
  backward-recv:
     rank  0: 3103.54
     rank  1: 3101.80
     rank  2: 3103.01
     rank  3: 3102.50
     rank  4: 2079.59
     rank  5: 2080.31
     rank  6: 2080.39
     rank  7: 2080.41
     rank  8: 1373.43
     rank  9: 1374.13
     rank 10: 1372.08
     rank 11: 1373.62
     rank 12: 500.69
     rank 13: 499.64
     rank 14: 501.47
     rank 15: 500.23
  backward-send:
     rank  4: 24.05
     rank  5: 23.01
     rank  6: 23.60
     rank  7: 23.38
     rank  8: 31.25
     rank  9: 30.73
     rank 10: 31.38
     rank 11: 30.88
     rank 12: 21.04
     rank 13: 21.23
     rank 14: 19.43
     rank 15: 21.00
     rank 16: 10.83
     rank 17: 10.41
     rank 18: 10.77
     rank 19: 10.52
  forward-send-backward-recv:
     rank  0: 8808.82
     rank  1: 8810.08
     rank  2: 8804.64
     rank  3: 8804.00
     rank  4: 7375.66
     rank  5: 7372.71
     rank  6: 7376.95
     rank  7: 7378.10
     rank  8: 7779.86
     rank  9: 7782.69
     rank 10: 7761.12
     rank 11: 7777.99
     rank 12: 2595.03
     rank 13: 2596.72
     rank 14: 2604.30
     rank 15: 2598.27
  backward-send-forward-recv:
     rank  4: 125.06
     rank  5: 125.52
     rank  6: 124.30
     rank  7: 123.80
     rank  8: 145.69
     rank  9: 142.90
     rank 10: 145.50
     rank 11: 144.93
     rank 12: 177.91
     rank 13: 177.52
     rank 14: 162.75
     rank 15: 176.23
     rank 16: 249.60
     rank 17: 247.74
     rank 18: 248.93
     rank 19: 247.82
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.07
     rank  2: 0.08
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.10
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.99
     rank  1: 0.99
     rank  2: 0.97
     rank  3: 1.00
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.07
     rank 14: 0.06
     rank 15: 0.04
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.24
     rank  2: 0.25
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.81
     rank  1: 16.06
     rank  2: 16.10
     rank  3: 16.32
     rank  4: 0.07
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.07
     rank  8: 0.07
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.03
     rank 12: 0.05
     rank 13: 0.12
     rank 14: 0.10
     rank 15: 0.15
     rank 16: 0.07
     rank 17: 0.09
     rank 18: 0.08
     rank 19: 0.04
  optimizer:
     rank  0: 16.79
     rank  1: 17.04
     rank  2: 17.08
     rank  3: 17.29
     rank  4: 1.03
     rank  5: 1.00
     rank  6: 1.00
     rank  7: 1.04
     rank  8: 1.03
     rank  9: 1.01
     rank 10: 1.00
     rank 11: 1.00
     rank 12: 1.02
     rank 13: 1.08
     rank 14: 1.07
     rank 15: 1.11
     rank 16: 1.04
     rank 17: 1.06
     rank 18: 1.04
     rank 19: 1.01
 [2024-12-05 16:30:54] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 15995.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.275778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15942.18
     rank  1: 15942.23
     rank  2: 15942.10
     rank  3: 15942.22
     rank  4: 15940.63
     rank  5: 15940.65
     rank  6: 15940.60
     rank  7: 15940.64
     rank  8: 15940.80
     rank  9: 15941.13
     rank 10: 15940.73
     rank 11: 15940.80
     rank 12: 15940.90
     rank 13: 15940.90
     rank 14: 15941.00
     rank 15: 15940.96
     rank 16: 15940.70
     rank 17: 15940.70
     rank 18: 15940.70
     rank 19: 15940.70
  forward-compute:
     rank  0: 1838.12
     rank  1: 1841.99
     rank  2: 1854.20
     rank  3: 1839.86
     rank  4: 2876.12
     rank  5: 2880.67
     rank  6: 2890.99
     rank  7: 2877.76
     rank  8: 2774.18
     rank  9: 2776.88
     rank 10: 2783.35
     rank 11: 2776.80
     rank 12: 5681.60
     rank 13: 5684.07
     rank 14: 5694.97
     rank 15: 5679.80
     rank 16: 6769.12
     rank 17: 6772.76
     rank 18: 6771.82
     rank 19: 6772.46
  backward-compute:
     rank  0: 1741.84
     rank  1: 1741.08
     rank  2: 1741.99
     rank  3: 1747.72
     rank  4: 2971.59
     rank  5: 2973.64
     rank  6: 2972.34
     rank  7: 2972.68
     rank  8: 2966.80
     rank  9: 2969.98
     rank 10: 2983.07
     rank 11: 2970.84
     rank 12: 5916.80
     rank 13: 5915.57
     rank 14: 5907.72
     rank 15: 5916.73
     rank 16: 7160.39
     rank 17: 7160.49
     rank 18: 7162.05
     rank 19: 7157.06
  pure-backward-compute:
     rank  0: 1740.84
     rank  1: 1740.14
     rank  2: 1740.98
     rank  3: 1746.75
     rank  4: 2970.62
     rank  5: 2972.61
     rank  6: 2971.41
     rank  7: 2971.77
     rank  8: 2965.65
     rank  9: 2968.99
     rank 10: 2982.26
     rank 11: 2970.10
     rank 12: 5915.32
     rank 13: 5913.95
     rank 14: 5905.77
     rank 15: 5912.11
     rank 16: 7158.12
     rank 17: 7158.50
     rank 18: 7160.19
     rank 19: 7154.69
  batch-generator:
     rank  0: 73.81
     rank  1: 80.66
     rank  2: 95.05
     rank  3: 79.91
     rank  4: 51.43
     rank  5: 61.41
     rank  6: 70.94
     rank  7: 59.16
     rank  8: 53.02
     rank  9: 58.96
     rank 10: 66.93
     rank 11: 57.36
     rank 12: 69.87
     rank 13: 78.64
     rank 14: 98.09
     rank 15: 86.93
     rank 16: 69.34
     rank 17: 74.02
     rank 18: 76.75
     rank 19: 79.02
  forward-recv:
     rank  4: 134.47
     rank  5: 133.25
     rank  6: 131.19
     rank  7: 134.09
     rank  8: 329.71
     rank  9: 329.15
     rank 10: 324.70
     rank 11: 329.26
     rank 12: 504.55
     rank 13: 503.89
     rank 14: 501.12
     rank 15: 504.26
     rank 16: 864.16
     rank 17: 863.82
     rank 18: 862.57
     rank 19: 864.48
  forward-send:
     rank  0: 403.88
     rank  1: 400.62
     rank  2: 388.65
     rank  3: 402.69
     rank  4: 210.10
     rank  5: 207.89
     rank  6: 199.19
     rank  7: 210.12
     rank  8: 204.58
     rank  9: 203.33
     rank 10: 198.94
     rank 11: 204.81
     rank 12: 10.44
     rank 13: 10.13
     rank 14: 9.13
     rank 15: 10.30
  backward-recv:
     rank  0: 3099.43
     rank  1: 3099.94
     rank  2: 3100.59
     rank  3: 3100.56
     rank  4: 2078.52
     rank  5: 2075.48
     rank  6: 2076.08
     rank  7: 2076.20
     rank  8: 1374.78
     rank  9: 1375.16
     rank 10: 1372.75
     rank 11: 1374.80
     rank 12: 503.43
     rank 13: 503.47
     rank 14: 505.43
     rank 15: 503.14
  backward-send:
     rank  4: 23.44
     rank  5: 24.72
     rank  6: 24.88
     rank  7: 24.77
     rank  8: 31.35
     rank  9: 30.80
     rank 10: 31.19
     rank 11: 31.05
     rank 12: 21.26
     rank 13: 21.09
     rank 14: 19.63
     rank 15: 21.17
     rank 16: 10.83
     rank 17: 10.46
     rank 18: 10.78
     rank 19: 10.51
  forward-send-backward-recv:
     rank  0: 8840.50
     rank  1: 8841.57
     rank  2: 8837.91
     rank  3: 8835.28
     rank  4: 7402.33
     rank  5: 7400.11
     rank  6: 7404.49
     rank  7: 7404.22
     rank  8: 7802.38
     rank  9: 7803.10
     rank 10: 7791.17
     rank 11: 7801.21
     rank 12: 2617.09
     rank 13: 2620.08
     rank 14: 2626.30
     rank 15: 2619.31
  backward-send-forward-recv:
     rank  4: 124.91
     rank  5: 125.05
     rank  6: 124.74
     rank  7: 123.79
     rank  8: 145.37
     rank  9: 144.05
     rank 10: 145.39
     rank 11: 144.29
     rank 12: 176.50
     rank 13: 174.61
     rank 14: 166.55
     rank 15: 172.49
     rank 16: 248.17
     rank 17: 246.99
     rank 18: 247.46
     rank 19: 247.23
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.07
     rank  2: 0.05
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.97
     rank  1: 0.99
     rank  2: 0.89
     rank  3: 0.98
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.06
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.24
     rank  1: 0.24
     rank  2: 0.24
     rank  3: 0.23
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.75
     rank  1: 16.35
     rank  2: 15.71
     rank  3: 15.70
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.08
     rank  9: 0.20
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.04
     rank 13: 0.10
     rank 14: 0.11
     rank 15: 0.11
     rank 16: 0.04
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.07
  optimizer:
     rank  0: 16.81
     rank  1: 17.42
     rank  2: 16.78
     rank  3: 16.75
     rank  4: 1.12
     rank  5: 1.10
     rank  6: 1.13
     rank  7: 1.09
     rank  8: 1.14
     rank  9: 1.26
     rank 10: 1.12
     rank 11: 1.12
     rank 12: 1.10
     rank 13: 1.16
     rank 14: 1.22
     rank 15: 1.22
     rank 16: 1.10
     rank 17: 1.12
     rank 18: 1.11
     rank 19: 1.14
 [2024-12-05 16:31:10] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 15992.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.044836E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15939.73
     rank  1: 15939.66
     rank  2: 15939.77
     rank  3: 15939.67
     rank  4: 15938.13
     rank  5: 15938.07
     rank  6: 15938.16
     rank  7: 15938.10
     rank  8: 15938.33
     rank  9: 15938.25
     rank 10: 15938.34
     rank 11: 15938.22
     rank 12: 15938.31
     rank 13: 15938.31
     rank 14: 15938.47
     rank 15: 15938.43
     rank 16: 15938.16
     rank 17: 15938.17
     rank 18: 15938.16
     rank 19: 15938.16
  forward-compute:
     rank  0: 1857.26
     rank  1: 1859.35
     rank  2: 1872.07
     rank  3: 1861.84
     rank  4: 2868.46
     rank  5: 2871.68
     rank  6: 2880.33
     rank  7: 2867.84
     rank  8: 2779.85
     rank  9: 2781.06
     rank 10: 2786.77
     rank 11: 2783.30
     rank 12: 5704.43
     rank 13: 5707.93
     rank 14: 5725.02
     rank 15: 5699.96
     rank 16: 6768.49
     rank 17: 6772.73
     rank 18: 6770.98
     rank 19: 6772.98
  backward-compute:
     rank  0: 1746.71
     rank  1: 1748.68
     rank  2: 1743.28
     rank  3: 1743.77
     rank  4: 2974.63
     rank  5: 2976.44
     rank  6: 2975.91
     rank  7: 2978.41
     rank  8: 2969.59
     rank  9: 2973.00
     rank 10: 2992.12
     rank 11: 2969.57
     rank 12: 5934.73
     rank 13: 5933.49
     rank 14: 5924.76
     rank 15: 5935.75
     rank 16: 7162.30
     rank 17: 7162.30
     rank 18: 7163.83
     rank 19: 7159.40
  pure-backward-compute:
     rank  0: 1745.67
     rank  1: 1747.70
     rank  2: 1742.25
     rank  3: 1742.82
     rank  4: 2973.81
     rank  5: 2975.57
     rank  6: 2974.67
     rank  7: 2976.76
     rank  8: 2968.73
     rank  9: 2971.95
     rank 10: 2990.95
     rank 11: 2968.88
     rank 12: 5933.34
     rank 13: 5932.15
     rank 14: 5923.31
     rank 15: 5929.75
     rank 16: 7159.87
     rank 17: 7160.25
     rank 18: 7161.96
     rank 19: 7156.76
  batch-generator:
     rank  0: 74.18
     rank  1: 78.94
     rank  2: 93.93
     rank  3: 83.15
     rank  4: 50.38
     rank  5: 54.95
     rank  6: 66.13
     rank  7: 55.28
     rank  8: 50.73
     rank  9: 56.21
     rank 10: 62.91
     rank 11: 55.89
     rank 12: 69.39
     rank 13: 77.73
     rank 14: 103.84
     rank 15: 82.82
     rank 16: 69.87
     rank 17: 75.38
     rank 18: 77.26
     rank 19: 81.15
  forward-recv:
     rank  4: 134.05
     rank  5: 133.80
     rank  6: 131.63
     rank  7: 133.26
     rank  8: 326.64
     rank  9: 326.69
     rank 10: 321.74
     rank 11: 326.18
     rank 12: 505.92
     rank 13: 505.97
     rank 14: 503.03
     rank 15: 505.71
     rank 16: 859.19
     rank 17: 859.05
     rank 18: 857.85
     rank 19: 858.98
  forward-send:
     rank  0: 410.46
     rank  1: 408.85
     rank  2: 396.30
     rank  3: 406.26
     rank  4: 208.34
     rank  5: 207.00
     rank  6: 198.13
     rank  7: 206.96
     rank  8: 201.36
     rank  9: 200.56
     rank 10: 196.50
     rank 11: 200.78
     rank 12: 10.75
     rank 13: 10.49
     rank 14: 9.65
     rank 15: 10.72
  backward-recv:
     rank  0: 3101.13
     rank  1: 3100.60
     rank  2: 3102.34
     rank  3: 3101.71
     rank  4: 2079.48
     rank  5: 2079.81
     rank  6: 2077.92
     rank  7: 2077.75
     rank  8: 1375.53
     rank  9: 1376.14
     rank 10: 1373.19
     rank 11: 1375.44
     rank 12: 501.36
     rank 13: 501.31
     rank 14: 502.89
     rank 15: 501.18
  backward-send:
     rank  4: 26.42
     rank  5: 25.24
     rank  6: 27.19
     rank  7: 26.88
     rank  8: 31.07
     rank  9: 30.56
     rank 10: 31.27
     rank 11: 30.54
     rank 12: 21.16
     rank 13: 21.08
     rank 14: 19.67
     rank 15: 21.10
     rank 16: 10.79
     rank 17: 10.47
     rank 18: 10.76
     rank 19: 10.51
  forward-send-backward-recv:
     rank  0: 8805.48
     rank  1: 8805.54
     rank  2: 8805.93
     rank  3: 8809.43
     rank  4: 7402.78
     rank  5: 7400.89
     rank  6: 7406.23
     rank  7: 7404.90
     rank  8: 7798.01
     rank  9: 7798.17
     rank 10: 7780.62
     rank 11: 7800.16
     rank 12: 2573.69
     rank 13: 2575.92
     rank 14: 2582.36
     rank 15: 2573.09
  backward-send-forward-recv:
     rank  4: 124.31
     rank  5: 123.53
     rank  6: 123.65
     rank  7: 123.99
     rank  8: 144.38
     rank  9: 142.60
     rank 10: 143.33
     rank 11: 143.37
     rank 12: 177.17
     rank 13: 173.68
     rank 14: 160.00
     rank 15: 174.50
     rank 16: 248.00
     rank 17: 246.16
     rank 18: 247.45
     rank 19: 246.30
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.07
     rank  2: 0.09
     rank  3: 0.07
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.99
     rank  1: 0.99
     rank  2: 1.00
     rank  3: 1.00
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.26
     rank  2: 0.25
     rank  3: 0.26
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 16.28
     rank  1: 16.04
     rank  2: 16.42
     rank  3: 16.31
     rank  4: 0.07
     rank  5: 0.07
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.08
     rank  9: 0.08
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.08
     rank 13: 0.09
     rank 14: 0.12
     rank 15: 0.10
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.04
     rank 19: 0.05
  optimizer:
     rank  0: 17.17
     rank  1: 17.06
     rank  2: 17.30
     rank  3: 17.19
     rank  4: 0.95
     rank  5: 0.94
     rank  6: 0.91
     rank  7: 0.95
     rank  8: 0.96
     rank  9: 0.95
     rank 10: 0.95
     rank 11: 0.92
     rank 12: 0.96
     rank 13: 0.98
     rank 14: 1.00
     rank 15: 0.99
     rank 16: 0.92
     rank 17: 0.95
     rank 18: 0.91
     rank 19: 0.93
 [2024-12-05 16:31:26] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 16000.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.769112E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 15947.51
     rank  1: 15947.52
     rank  2: 15947.52
     rank  3: 15947.41
     rank  4: 15945.96
     rank  5: 15945.98
     rank  6: 15945.94
     rank  7: 15945.90
     rank  8: 15946.08
     rank  9: 15946.21
     rank 10: 15946.15
     rank 11: 15946.04
     rank 12: 15946.16
     rank 13: 15946.15
     rank 14: 15946.24
     rank 15: 15946.32
     rank 16: 15945.96
     rank 17: 15945.98
     rank 18: 15945.98
     rank 19: 15945.96
  forward-compute:
     rank  0: 1864.97
     rank  1: 1869.14
     rank  2: 1875.81
     rank  3: 1869.02
     rank  4: 2874.54
     rank  5: 2880.60
     rank  6: 2884.21
     rank  7: 2874.72
     rank  8: 2780.56
     rank  9: 2782.58
     rank 10: 2786.56
     rank 11: 2783.22
     rank 12: 5684.82
     rank 13: 5689.37
     rank 14: 5705.99
     rank 15: 5682.20
     rank 16: 6766.95
     rank 17: 6772.09
     rank 18: 6770.05
     rank 19: 6772.13
  backward-compute:
     rank  0: 1744.18
     rank  1: 1745.91
     rank  2: 1741.72
     rank  3: 1743.54
     rank  4: 2975.54
     rank  5: 2976.97
     rank  6: 2976.82
     rank  7: 2977.59
     rank  8: 2968.37
     rank  9: 2971.57
     rank 10: 2989.94
     rank 11: 2970.23
     rank 12: 5927.95
     rank 13: 5928.22
     rank 14: 5919.37
     rank 15: 5925.89
     rank 16: 7168.02
     rank 17: 7167.17
     rank 18: 7169.44
     rank 19: 7164.47
  pure-backward-compute:
     rank  0: 1743.15
     rank  1: 1744.97
     rank  2: 1740.61
     rank  3: 1742.59
     rank  4: 2974.69
     rank  5: 2976.20
     rank  6: 2975.72
     rank  7: 2976.02
     rank  8: 2967.61
     rank  9: 2970.67
     rank 10: 2989.07
     rank 11: 2969.50
     rank 12: 5926.16
     rank 13: 5927.08
     rank 14: 5918.09
     rank 15: 5923.59
     rank 16: 7165.68
     rank 17: 7165.28
     rank 18: 7167.57
     rank 19: 7162.14
  batch-generator:
     rank  0: 74.48
     rank  1: 81.45
     rank  2: 90.63
     rank  3: 82.91
     rank  4: 52.99
     rank  5: 59.60
     rank  6: 66.43
     rank  7: 59.85
     rank  8: 54.38
     rank  9: 58.64
     rank 10: 64.55
     rank 11: 57.88
     rank 12: 70.86
     rank 13: 79.30
     rank 14: 104.07
     rank 15: 83.35
     rank 16: 69.41
     rank 17: 75.36
     rank 18: 77.03
     rank 19: 80.84
  forward-recv:
     rank  4: 136.97
     rank  5: 135.97
     rank  6: 135.37
     rank  7: 136.08
     rank  8: 331.26
     rank  9: 331.52
     rank 10: 327.14
     rank 11: 331.17
     rank 12: 511.47
     rank 13: 510.90
     rank 14: 509.27
     rank 15: 511.50
     rank 16: 863.91
     rank 17: 863.85
     rank 18: 862.61
     rank 19: 863.62
  forward-send:
     rank  0: 393.06
     rank  1: 389.50
     rank  2: 382.86
     rank  3: 389.48
     rank  4: 210.56
     rank  5: 208.38
     rank  6: 202.36
     rank  7: 209.45
     rank  8: 205.08
     rank  9: 203.62
     rank 10: 201.15
     rank 11: 204.54
     rank 12: 10.69
     rank 13: 10.45
     rank 14: 9.71
     rank 15: 10.61
  backward-recv:
     rank  0: 3101.58
     rank  1: 3101.92
     rank  2: 3102.26
     rank  3: 3102.06
     rank  4: 2078.88
     rank  5: 2078.58
     rank  6: 2077.32
     rank  7: 2077.98
     rank  8: 1368.33
     rank  9: 1368.90
     rank 10: 1366.23
     rank 11: 1368.06
     rank 12: 501.52
     rank 13: 500.87
     rank 14: 503.29
     rank 15: 501.35
  backward-send:
     rank  4: 23.35
     rank  5: 23.42
     rank  6: 23.87
     rank  7: 24.00
     rank  8: 31.20
     rank  9: 30.53
     rank 10: 31.28
     rank 11: 31.12
     rank 12: 21.08
     rank 13: 21.11
     rank 14: 19.48
     rank 15: 21.06
     rank 16: 10.83
     rank 17: 10.42
     rank 18: 10.78
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 8824.88
     rank  1: 8824.02
     rank  2: 8825.50
     rank  3: 8827.14
     rank  4: 7400.97
     rank  5: 7399.18
     rank  6: 7403.39
     rank  7: 7403.85
     rank  8: 7805.68
     rank  9: 7805.94
     rank 10: 7787.39
     rank 11: 7805.77
     rank 12: 2605.50
     rank 13: 2607.50
     rank 14: 2614.23
     rank 15: 2604.87
  backward-send-forward-recv:
     rank  4: 126.15
     rank  5: 124.85
     rank  6: 125.30
     rank  7: 124.93
     rank  8: 145.19
     rank  9: 143.49
     rank 10: 145.50
     rank 11: 144.40
     rank 12: 174.14
     rank 13: 171.33
     rank 14: 156.38
     rank 15: 175.75
     rank 16: 247.48
     rank 17: 244.96
     rank 18: 246.38
     rank 19: 245.60
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.03
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.10
     rank  2: 0.07
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.98
     rank  1: 1.01
     rank  2: 0.99
     rank  3: 0.95
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.25
     rank  1: 0.25
     rank  2: 0.25
     rank  3: 0.23
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 15.94
     rank  1: 16.15
     rank  2: 16.41
     rank  3: 15.93
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.14
     rank 10: 0.08
     rank 11: 0.03
     rank 12: 0.09
     rank 13: 0.05
     rank 14: 0.05
     rank 15: 0.10
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.04
  optimizer:
     rank  0: 16.88
     rank  1: 17.22
     rank  2: 17.36
     rank  3: 16.87
     rank  4: 0.97
     rank  5: 0.97
     rank  6: 0.97
     rank  7: 1.00
     rank  8: 0.98
     rank  9: 1.07
     rank 10: 1.02
     rank 11: 0.97
     rank 12: 1.03
     rank 13: 0.98
     rank 14: 0.99
     rank 15: 1.06
     rank 16: 0.98
     rank 17: 1.02
     rank 18: 1.01
     rank 19: 0.98
