examples/multimodal/pretrain-freeze-llm-hete-3090first.sh: line 4: activate: No such file or directory
4
[2024-12-05 19:06:14,397] torch.distributed.run: [WARNING] 
[2024-12-05 19:06:14,397] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 19:06:14,397] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 19:06:14,397] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]

---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]



---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 251695104

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 251695104name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 102.99
     rank  1: 102.41
     rank  2: 106.91
     rank  3: 151.44
     rank  4: 35.49
     rank  5: 57.00
     rank  6: 51.65
     rank  7: 59.41
     rank  8: 55.14
     rank  9: 59.92
     rank 10: 60.48
     rank 11: 59.25
     rank 12: 55.77
     rank 13: 59.34
     rank 14: 61.55
     rank 15: 58.56
     rank 16: 53.46
     rank 17: 53.50
     rank 18: 51.40
     rank 19: 51.80
  train/valid/test-data-iterators-setup:
     rank  0: 832.09
     rank  1: 831.96
     rank  2: 831.99
     rank  3: 832.01
     rank  4: 1095.04
     rank  5: 1095.09
     rank  6: 1095.03
     rank  7: 1095.22
     rank  8: 1344.93
     rank  9: 1345.36
     rank 10: 1095.15
     rank 11: 1344.87
     rank 12: 1404.69
     rank 13: 1404.62
     rank 14: 1346.04
     rank 15: 1404.61
     rank 16: 1404.59
     rank 17: 1404.76
     rank 18: 1404.85
     rank 19: 1404.56
 [2024-12-05 19:06:59] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21546.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.107800E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[Rank 18] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0

[Rank 16] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0[Rank 17] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0

[Rank 19] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
times across ranks (ms):
  forward-backward:
     rank  0: 21479.93
     rank  1: 21480.02
     rank  2: 21479.95
     rank  3: 21479.94
     rank  4: 21520.61
     rank  5: 21520.77
     rank  6: 21528.69
     rank  7: 21523.22
     rank  8: 21521.39
     rank  9: 21517.07
     rank 10: 21511.27
     rank 11: 21521.14
     rank 12: 21510.10
     rank 13: 21518.71
     rank 14: 21515.93
     rank 15: 21511.15
     rank 16: 21479.00
     rank 17: 21479.25
     rank 18: 21479.12
     rank 19: 21479.00
  forward-compute:
     rank  0: 7818.87
     rank  1: 7779.33
     rank  2: 7797.91
     rank  3: 7811.25
     rank  4: 6095.40
     rank  5: 6097.26
     rank  6: 6112.44
     rank  7: 6087.72
     rank  8: 4495.50
     rank  9: 4522.33
     rank 10: 4521.34
     rank 11: 4501.92
     rank 12: 4743.29
     rank 13: 4762.84
     rank 14: 4754.61
     rank 15: 4744.56
     rank 16: 4710.07
     rank 17: 4724.89
     rank 18: 4721.60
     rank 19: 4717.28
  backward-compute:
     rank  0: 4405.58
     rank  1: 4447.41
     rank  2: 4442.02
     rank  3: 4405.35
     rank  4: 4116.28
     rank  5: 4125.64
     rank  6: 4131.46
     rank  7: 4120.94
     rank  8: 3212.21
     rank  9: 3216.95
     rank 10: 3227.29
     rank 11: 3212.27
     rank 12: 3162.60
     rank 13: 3164.42
     rank 14: 3162.20
     rank 15: 3163.85
     rank 16: 3792.55
     rank 17: 3790.89
     rank 18: 3797.59
     rank 19: 3792.04
  pure-backward-compute:
     rank  0: 4404.62
     rank  1: 4446.62
     rank  2: 4441.15
     rank  3: 4404.37
     rank  4: 4114.94
     rank  5: 4124.52
     rank  6: 4130.16
     rank  7: 4119.12
     rank  8: 3210.22
     rank  9: 3215.23
     rank 10: 3225.52
     rank 11: 3209.78
     rank 12: 3160.10
     rank 13: 3161.98
     rank 14: 3160.18
     rank 15: 3160.62
     rank 16: 3789.83
     rank 17: 3787.54
     rank 18: 3795.80
     rank 19: 3790.22
  batch-generator:
     rank  0: 1078.09
     rank  1: 1046.25
     rank  2: 1066.91
     rank  3: 1080.34
     rank  4: 1192.53
     rank  5: 1196.38
     rank  6: 1211.04
     rank  7: 1186.39
     rank  8: 1357.34
     rank  9: 1391.45
     rank 10: 1385.22
     rank 11: 1373.23
     rank 12: 1631.26
     rank 13: 1646.18
     rank 14: 1664.04
     rank 15: 1628.30
     rank 16: 1075.86
     rank 17: 1093.51
     rank 18: 1095.53
     rank 19: 1089.26
  forward-recv:
     rank  4: 4308.15
     rank  5: 4319.04
     rank  6: 4306.14
     rank  7: 4320.77
     rank  8: 6843.99
     rank  9: 6843.11
     rank 10: 6831.40
     rank 11: 6842.90
     rank 12: 8457.97
     rank 13: 8452.61
     rank 14: 8458.07
     rank 15: 8455.83
     rank 16: 10317.36
     rank 17: 10307.31
     rank 18: 10313.12
     rank 19: 10315.77
  forward-send:
     rank  0: 5481.29
     rank  1: 5479.36
     rank  2: 5464.13
     rank  3: 5487.83
     rank  4: 3038.77
     rank  5: 3024.66
     rank  6: 3022.82
     rank  7: 3033.46
     rank  8: 1700.52
     rank  9: 1685.45
     rank 10: 1697.57
     rank 11: 1697.15
     rank 12: 34.24
     rank 13: 24.38
     rank 14: 30.15
     rank 15: 33.04
  backward-recv:
     rank  0: 931.49
     rank  1: 931.58
     rank  2: 931.86
     rank  3: 931.37
     rank  4: 505.54
     rank  5: 504.96
     rank  6: 503.06
     rank  7: 505.39
     rank  8: 465.51
     rank  9: 465.80
     rank 10: 465.75
     rank 11: 465.09
     rank 12: 257.81
     rank 13: 258.48
     rank 14: 259.35
     rank 15: 258.03
  backward-send:
     rank  4: 41.71
     rank  5: 40.99
     rank  6: 41.46
     rank  7: 41.56
     rank  8: 31.77
     rank  9: 31.41
     rank 10: 29.28
     rank 11: 31.94
     rank 12: 21.66
     rank 13: 21.39
     rank 14: 19.66
     rank 15: 20.80
     rank 16: 10.67
     rank 17: 10.69
     rank 18: 10.56
     rank 19: 9.95
  forward-send-backward-recv:
     rank  0: 2774.34
     rank  1: 2777.73
     rank  2: 2777.65
     rank  3: 2776.71
     rank  4: 2047.46
     rank  5: 2040.65
     rank  6: 2038.38
     rank  7: 2040.35
     rank  8: 1868.95
     rank  9: 1865.54
     rank 10: 1856.70
     rank 11: 1870.62
     rank 12: 1763.62
     rank 13: 1763.63
     rank 14: 1765.11
     rank 15: 1765.21
  backward-send-forward-recv:
     rank  4: 1042.56
     rank  5: 1041.90
     rank  6: 1043.33
     rank  7: 1043.73
     rank  8: 2310.76
     rank  9: 2299.99
     rank 10: 2300.16
     rank 11: 2306.32
     rank 12: 2276.19
     rank 13: 2271.54
     rank 14: 2268.38
     rank 15: 2276.72
     rank 16: 1683.16
     rank 17: 1679.25
     rank 18: 1673.66
     rank 19: 1680.36
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.06
     rank 12: 0.07
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.04
     rank 18: 0.05
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.14
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.14
     rank  4: 0.02
     rank  5: 0.03
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.06
  all-grads-sync:
     rank  0: 45.76
     rank  1: 46.99
     rank  2: 47.53
     rank  3: 47.23
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.05
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.23
     rank  2: 0.23
     rank  3: 0.23
     rank  4: 0.03
     rank  5: 0.06
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 13.37
     rank  1: 13.22
     rank  2: 13.79
     rank  3: 13.56
     rank  4: 0.11
     rank  5: 0.49
     rank  6: 0.19
     rank  7: 0.19
     rank  8: 0.26
     rank  9: 0.17
     rank 10: 0.12
     rank 11: 0.29
     rank 12: 0.26
     rank 13: 0.43
     rank 14: 0.13
     rank 15: 0.17
     rank 16: 0.12
     rank 17: 0.09
     rank 18: 0.08
     rank 19: 0.08
  optimizer:
     rank  0: 14.90
     rank  1: 14.79
     rank  2: 15.43
     rank  3: 15.09
     rank  4: 1.67
     rank  5: 2.06
     rank  6: 1.77
     rank  7: 1.79
     rank  8: 1.83
     rank  9: 1.69
     rank 10: 1.63
     rank 11: 1.86
     rank 12: 1.79
     rank 13: 2.00
     rank 14: 1.68
     rank 15: 1.71
     rank 16: 1.69
     rank 17: 1.66
     rank 18: 1.65
     rank 19: 1.65
 [2024-12-05 19:07:09] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 9901.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.055213E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9853.71
     rank  1: 9853.71
     rank  2: 9853.65
     rank  3: 9853.71
     rank  4: 9852.96
     rank  5: 9852.98
     rank  6: 9852.98
     rank  7: 9852.96
     rank  8: 9853.07
     rank  9: 9853.21
     rank 10: 9853.09
     rank 11: 9853.68
     rank 12: 9853.25
     rank 13: 9853.28
     rank 14: 9853.18
     rank 15: 9853.35
     rank 16: 9853.03
     rank 17: 9852.98
     rank 18: 9852.84
     rank 19: 9852.86
  forward-compute:
     rank  0: 3572.30
     rank  1: 3578.40
     rank  2: 3574.83
     rank  3: 3567.64
     rank  4: 3814.73
     rank  5: 3821.42
     rank  6: 3819.05
     rank  7: 3816.15
     rank  8: 3079.36
     rank  9: 3084.29
     rank 10: 3080.70
     rank 11: 3072.41
     rank 12: 3069.28
     rank 13: 3071.62
     rank 14: 3076.20
     rank 15: 3069.42
     rank 16: 3560.12
     rank 17: 3562.00
     rank 18: 3573.94
     rank 19: 3567.76
  backward-compute:
     rank  0: 3580.96
     rank  1: 3584.01
     rank  2: 3583.00
     rank  3: 3584.77
     rank  4: 4089.65
     rank  5: 4092.72
     rank  6: 4098.58
     rank  7: 4088.54
     rank  8: 3191.24
     rank  9: 3196.57
     rank 10: 3206.61
     rank 11: 3196.79
     rank 12: 3152.84
     rank 13: 3150.39
     rank 14: 3151.58
     rank 15: 3152.50
     rank 16: 3798.98
     rank 17: 3796.20
     rank 18: 3800.81
     rank 19: 3798.38
  pure-backward-compute:
     rank  0: 3580.21
     rank  1: 3583.36
     rank  2: 3582.30
     rank  3: 3583.94
     rank  4: 4088.63
     rank  5: 4091.81
     rank  6: 4097.70
     rank  7: 4086.86
     rank  8: 3189.46
     rank  9: 3195.34
     rank 10: 3204.81
     rank 11: 3194.88
     rank 12: 3150.38
     rank 13: 3148.53
     rank 14: 3150.22
     rank 15: 3151.25
     rank 16: 3796.47
     rank 17: 3793.47
     rank 18: 3799.20
     rank 19: 3796.73
  batch-generator:
     rank  0: 63.77
     rank  1: 71.41
     rank  2: 68.30
     rank  3: 68.89
     rank  4: 53.32
     rank  5: 60.89
     rank  6: 58.62
     rank  7: 56.86
     rank  8: 72.37
     rank  9: 82.50
     rank 10: 85.28
     rank 11: 79.10
     rank 12: 90.36
     rank 13: 96.42
     rank 14: 105.19
     rank 15: 98.70
     rank 16: 68.81
     rank 17: 73.21
     rank 18: 87.28
     rank 19: 81.98
  forward-recv:
     rank  4: 259.20
     rank  5: 256.76
     rank  6: 258.38
     rank  7: 258.97
     rank  8: 594.24
     rank  9: 593.32
     rank 10: 596.36
     rank 11: 594.04
     rank 12: 740.14
     rank 13: 739.73
     rank 14: 739.47
     rank 15: 741.14
     rank 16: 889.25
     rank 17: 889.18
     rank 18: 888.27
     rank 19: 889.04
  forward-send:
     rank  0: 132.50
     rank  1: 126.71
     rank  2: 130.12
     rank  3: 132.99
     rank  4: 34.26
     rank  5: 32.25
     rank  6: 33.47
     rank  7: 34.50
     rank  8: 21.02
     rank  9: 20.50
     rank 10: 19.30
     rank 11: 20.69
     rank 12: 10.70
     rank 13: 10.68
     rank 14: 10.00
     rank 15: 10.66
  backward-recv:
     rank  0: 938.73
     rank  1: 937.76
     rank  2: 938.58
     rank  3: 938.00
     rank  4: 515.07
     rank  5: 515.09
     rank  6: 511.94
     rank  7: 514.97
     rank  8: 471.55
     rank  9: 471.44
     rank 10: 471.10
     rank 11: 471.63
     rank 12: 261.53
     rank 13: 262.10
     rank 14: 263.45
     rank 15: 261.61
  backward-send:
     rank  4: 41.73
     rank  5: 40.66
     rank  6: 41.78
     rank  7: 41.69
     rank  8: 31.64
     rank  9: 31.42
     rank 10: 28.99
     rank 11: 31.59
     rank 12: 21.36
     rank 13: 21.21
     rank 14: 19.09
     rank 15: 21.13
     rank 16: 10.67
     rank 17: 10.71
     rank 18: 10.61
     rank 19: 9.90
  forward-send-backward-recv:
     rank  0: 1609.47
     rank  1: 1609.95
     rank  2: 1610.73
     rank  3: 1610.01
     rank  4: 729.75
     rank  5: 727.47
     rank  6: 726.13
     rank  7: 729.71
     rank  8: 629.33
     rank  9: 627.36
     rank 10: 619.56
     rank 11: 626.94
     rank 12: 627.24
     rank 13: 631.50
     rank 14: 631.86
     rank 15: 630.98
  backward-send-forward-recv:
     rank  4: 134.54
     rank  5: 132.42
     rank  6: 132.24
     rank  7: 133.73
     rank  8: 1334.23
     rank  9: 1331.42
     rank 10: 1331.77
     rank 11: 1340.36
     rank 12: 1257.01
     rank 13: 1255.68
     rank 14: 1252.44
     rank 15: 1257.32
     rank 16: 677.59
     rank 17: 677.90
     rank 18: 665.71
     rank 19: 673.56
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.13
     rank 12: 0.05
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.05
     rank 10: 0.03
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.67
     rank  1: 0.63
     rank  2: 0.63
     rank  3: 0.64
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.14
     rank 10: 0.02
     rank 11: 0.22
     rank 12: 0.03
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.05
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.27
     rank  1: 10.55
     rank  2: 10.54
     rank  3: 10.41
     rank  4: 0.07
     rank  5: 0.11
     rank  6: 0.08
     rank  7: 0.08
     rank  8: 0.14
     rank  9: 0.13
     rank 10: 0.10
     rank 11: 0.16
     rank 12: 0.11
     rank 13: 0.27
     rank 14: 0.20
     rank 15: 0.19
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 11.26
     rank  1: 11.53
     rank  2: 11.52
     rank  3: 11.40
     rank  4: 1.05
     rank  5: 1.10
     rank  6: 1.06
     rank  7: 1.06
     rank  8: 1.13
     rank  9: 1.12
     rank 10: 1.08
     rank 11: 1.17
     rank 12: 1.11
     rank 13: 1.28
     rank 14: 1.20
     rank 15: 1.19
     rank 16: 1.04
     rank 17: 1.08
     rank 18: 1.03
     rank 19: 1.03
 [2024-12-05 19:07:19] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 9892.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.169855E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9833.25
     rank  1: 9833.26
     rank  2: 9833.24
     rank  3: 9833.28
     rank  4: 9832.46
     rank  5: 9832.52
     rank  6: 9832.56
     rank  7: 9832.51
     rank  8: 9832.82
     rank  9: 9832.57
     rank 10: 9832.61
     rank 11: 9832.69
     rank 12: 9833.16
     rank 13: 9832.90
     rank 14: 9832.68
     rank 15: 9832.95
     rank 16: 9832.53
     rank 17: 9832.53
     rank 18: 9832.45
     rank 19: 9832.56
  forward-compute:
     rank  0: 3530.32
     rank  1: 3535.98
     rank  2: 3532.38
     rank  3: 3528.11
     rank  4: 3809.46
     rank  5: 3813.43
     rank  6: 3812.26
     rank  7: 3810.24
     rank  8: 3078.73
     rank  9: 3083.42
     rank 10: 3083.70
     rank 11: 3071.24
     rank 12: 3066.04
     rank 13: 3067.98
     rank 14: 3071.28
     rank 15: 3067.53
     rank 16: 3569.37
     rank 17: 3574.04
     rank 18: 3581.68
     rank 19: 3579.73
  backward-compute:
     rank  0: 3552.85
     rank  1: 3554.80
     rank  2: 3554.29
     rank  3: 3554.15
     rank  4: 4075.96
     rank  5: 4079.91
     rank  6: 4086.95
     rank  7: 4075.02
     rank  8: 3196.42
     rank  9: 3201.66
     rank 10: 3210.58
     rank 11: 3201.55
     rank 12: 3157.19
     rank 13: 3156.12
     rank 14: 3156.41
     rank 15: 3158.86
     rank 16: 3789.30
     rank 17: 3786.48
     rank 18: 3790.53
     rank 19: 3788.43
  pure-backward-compute:
     rank  0: 3552.15
     rank  1: 3553.98
     rank  2: 3553.55
     rank  3: 3552.82
     rank  4: 4074.94
     rank  5: 4078.71
     rank  6: 4085.95
     rank  7: 4073.72
     rank  8: 3194.72
     rank  9: 3200.13
     rank 10: 3209.19
     rank 11: 3199.73
     rank 12: 3154.18
     rank 13: 3154.33
     rank 14: 3155.06
     rank 15: 3157.19
     rank 16: 3786.86
     rank 17: 3783.95
     rank 18: 3788.85
     rank 19: 3786.83
  batch-generator:
     rank  0: 57.21
     rank  1: 62.62
     rank  2: 59.77
     rank  3: 60.58
     rank  4: 51.71
     rank  5: 58.19
     rank  6: 57.16
     rank  7: 55.16
     rank  8: 70.19
     rank  9: 80.16
     rank 10: 86.40
     rank 11: 71.89
     rank 12: 90.55
     rank 13: 95.41
     rank 14: 103.00
     rank 15: 96.39
     rank 16: 70.44
     rank 17: 77.99
     rank 18: 86.86
     rank 19: 85.99
  forward-recv:
     rank  4: 259.15
     rank  5: 257.00
     rank  6: 258.56
     rank  7: 258.87
     rank  8: 588.29
     rank  9: 587.77
     rank 10: 590.53
     rank 11: 589.19
     rank 12: 741.84
     rank 13: 741.63
     rank 14: 741.15
     rank 15: 741.82
     rank 16: 885.82
     rank 17: 885.59
     rank 18: 884.74
     rank 19: 885.40
  forward-send:
     rank  0: 103.39
     rank  1: 98.29
     rank  2: 101.55
     rank  3: 103.83
     rank  4: 35.12
     rank  5: 33.38
     rank  6: 34.23
     rank  7: 35.27
     rank  8: 21.45
     rank  9: 21.12
     rank 10: 19.94
     rank 11: 21.35
     rank 12: 10.72
     rank 13: 10.73
     rank 14: 10.10
     rank 15: 10.68
  backward-recv:
     rank  0: 938.46
     rank  1: 936.64
     rank  2: 937.64
     rank  3: 936.31
     rank  4: 514.21
     rank  5: 513.90
     rank  6: 511.45
     rank  7: 514.05
     rank  8: 471.12
     rank  9: 471.00
     rank 10: 470.93
     rank 11: 470.73
     rank 12: 263.68
     rank 13: 263.91
     rank 14: 263.44
     rank 15: 263.59
  backward-send:
     rank  4: 41.81
     rank  5: 41.04
     rank  6: 41.40
     rank  7: 41.07
     rank  8: 31.72
     rank  9: 31.54
     rank 10: 29.09
     rank 11: 31.70
     rank 12: 21.30
     rank 13: 21.05
     rank 14: 18.97
     rank 15: 21.11
     rank 16: 10.36
     rank 17: 10.46
     rank 18: 9.23
     rank 19: 9.96
  forward-send-backward-recv:
     rank  0: 1689.67
     rank  1: 1690.23
     rank  2: 1690.58
     rank  3: 1690.54
     rank  4: 728.59
     rank  5: 726.12
     rank  6: 723.75
     rank  7: 727.47
     rank  8: 646.64
     rank  9: 644.19
     rank 10: 636.76
     rank 11: 644.41
     rank 12: 641.01
     rank 13: 648.76
     rank 14: 650.59
     rank 15: 648.50
  backward-send-forward-recv:
     rank  4: 134.64
     rank  5: 133.62
     rank  6: 133.11
     rank  7: 134.69
     rank  8: 1299.31
     rank  9: 1295.68
     rank 10: 1293.66
     rank 11: 1305.12
     rank 12: 1215.18
     rank 13: 1214.95
     rank 14: 1212.51
     rank 15: 1215.19
     rank 16: 662.21
     rank 17: 659.98
     rank 18: 651.64
     rank 19: 655.49
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.07
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.08
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.06
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.10
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.66
     rank  2: 0.62
     rank  3: 0.62
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.08
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.08
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.20
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.06
     rank 13: 0.08
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.46
     rank  1: 10.35
     rank  2: 10.55
     rank  3: 10.48
     rank  4: 0.03
     rank  5: 0.06
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.12
     rank  9: 0.09
     rank 10: 0.10
     rank 11: 0.11
     rank 12: 0.41
     rank 13: 0.30
     rank 14: 0.10
     rank 15: 0.13
     rank 16: 0.08
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.08
  optimizer:
     rank  0: 11.48
     rank  1: 11.36
     rank  2: 11.57
     rank  3: 11.50
     rank  4: 1.05
     rank  5: 1.07
     rank  6: 1.08
     rank  7: 1.08
     rank  8: 1.13
     rank  9: 1.11
     rank 10: 1.12
     rank 11: 1.13
     rank 12: 1.43
     rank 13: 1.31
     rank 14: 1.11
     rank 15: 1.14
     rank 16: 1.09
     rank 17: 1.09
     rank 18: 1.08
     rank 19: 1.09
 [2024-12-05 19:07:29] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 9881.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.961879E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9832.37
     rank  1: 9832.33
     rank  2: 9832.33
     rank  3: 9832.33
     rank  4: 9831.55
     rank  5: 9831.53
     rank  6: 9831.54
     rank  7: 9831.57
     rank  8: 9831.73
     rank  9: 9831.59
     rank 10: 9831.84
     rank 11: 9832.31
     rank 12: 9831.82
     rank 13: 9832.01
     rank 14: 9831.70
     rank 15: 9832.03
     rank 16: 9831.59
     rank 17: 9831.59
     rank 18: 9831.50
     rank 19: 9831.49
  forward-compute:
     rank  0: 3623.78
     rank  1: 3629.98
     rank  2: 3625.60
     rank  3: 3621.59
     rank  4: 3810.33
     rank  5: 3817.26
     rank  6: 3812.28
     rank  7: 3811.58
     rank  8: 3089.82
     rank  9: 3099.62
     rank 10: 3099.11
     rank 11: 3086.59
     rank 12: 3068.02
     rank 13: 3072.68
     rank 14: 3077.59
     rank 15: 3067.97
     rank 16: 3575.92
     rank 17: 3579.55
     rank 18: 3588.50
     rank 19: 3586.08
  backward-compute:
     rank  0: 3580.42
     rank  1: 3583.16
     rank  2: 3581.60
     rank  3: 3582.03
     rank  4: 4076.79
     rank  5: 4083.12
     rank  6: 4089.21
     rank  7: 4078.78
     rank  8: 3207.22
     rank  9: 3212.73
     rank 10: 3224.82
     rank 11: 3210.12
     rank 12: 3154.58
     rank 13: 3153.75
     rank 14: 3153.54
     rank 15: 3156.28
     rank 16: 3800.39
     rank 17: 3797.82
     rank 18: 3802.36
     rank 19: 3799.96
  pure-backward-compute:
     rank  0: 3579.35
     rank  1: 3582.45
     rank  2: 3580.89
     rank  3: 3580.76
     rank  4: 4075.88
     rank  5: 4082.32
     rank  6: 4088.45
     rank  7: 4077.54
     rank  8: 3205.50
     rank  9: 3211.29
     rank 10: 3223.55
     rank 11: 3208.61
     rank 12: 3152.61
     rank 13: 3152.18
     rank 14: 3152.25
     rank 15: 3154.84
     rank 16: 3797.65
     rank 17: 3795.37
     rank 18: 3800.74
     rank 19: 3798.31
  batch-generator:
     rank  0: 66.74
     rank  1: 73.32
     rank  2: 70.38
     rank  3: 69.61
     rank  4: 51.79
     rank  5: 61.02
     rank  6: 55.82
     rank  7: 55.48
     rank  8: 72.82
     rank  9: 86.56
     rank 10: 92.03
     rank 11: 82.23
     rank 12: 83.53
     rank 13: 88.20
     rank 14: 100.88
     rank 15: 89.15
     rank 16: 69.60
     rank 17: 75.86
     rank 18: 86.70
     rank 19: 85.32
  forward-recv:
     rank  4: 258.21
     rank  5: 255.26
     rank  6: 257.82
     rank  7: 257.97
     rank  8: 587.26
     rank  9: 586.86
     rank 10: 589.60
     rank 11: 587.81
     rank 12: 737.41
     rank 13: 737.05
     rank 14: 736.93
     rank 15: 737.88
     rank 16: 886.15
     rank 17: 886.00
     rank 18: 884.75
     rank 19: 885.42
  forward-send:
     rank  0: 132.39
     rank  1: 126.09
     rank  2: 130.58
     rank  3: 132.74
     rank  4: 33.04
     rank  5: 30.97
     rank  6: 32.11
     rank  7: 32.99
     rank  8: 21.25
     rank  9: 20.74
     rank 10: 19.52
     rank 11: 21.77
     rank 12: 10.81
     rank 13: 10.62
     rank 14: 9.77
     rank 15: 10.37
  backward-recv:
     rank  0: 924.94
     rank  1: 925.58
     rank  2: 925.53
     rank  3: 924.60
     rank  4: 513.28
     rank  5: 512.88
     rank  6: 511.83
     rank  7: 512.46
     rank  8: 467.66
     rank  9: 467.50
     rank 10: 467.34
     rank 11: 467.33
     rank 12: 260.56
     rank 13: 261.08
     rank 14: 262.68
     rank 15: 261.33
  backward-send:
     rank  4: 41.78
     rank  5: 41.48
     rank  6: 41.68
     rank  7: 41.43
     rank  8: 32.10
     rank  9: 32.01
     rank 10: 30.65
     rank 11: 31.15
     rank 12: 21.45
     rank 13: 21.09
     rank 14: 19.32
     rank 15: 20.95
     rank 16: 10.71
     rank 17: 10.66
     rank 18: 10.61
     rank 19: 10.03
  forward-send-backward-recv:
     rank  0: 1549.58
     rank  1: 1550.15
     rank  2: 1551.61
     rank  3: 1552.15
     rank  4: 726.73
     rank  5: 723.15
     rank  6: 720.13
     rank  7: 724.86
     rank  8: 646.50
     rank  9: 644.83
     rank 10: 633.81
     rank 11: 645.59
     rank 12: 656.59
     rank 13: 659.80
     rank 14: 661.21
     rank 15: 660.47
  backward-send-forward-recv:
     rank  4: 134.13
     rank  5: 132.24
     rank  6: 133.28
     rank  7: 133.98
     rank  8: 1277.26
     rank  9: 1268.77
     rank 10: 1267.74
     rank 11: 1279.02
     rank 12: 1206.48
     rank 13: 1204.44
     rank 14: 1199.06
     rank 15: 1207.06
     rank 16: 638.48
     rank 17: 637.74
     rank 18: 628.47
     rank 19: 632.62
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.05
     rank 11: 0.05
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.69
     rank  1: 0.68
     rank  2: 0.67
     rank  3: 0.67
     rank  4: 0.04
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.13
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.19
     rank  2: 0.19
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.10
     rank 12: 0.02
     rank 13: 0.08
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.07
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.40
     rank  1: 10.49
     rank  2: 10.42
     rank  3: 10.29
     rank  4: 0.05
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.07
     rank  8: 0.11
     rank  9: 0.08
     rank 10: 0.11
     rank 11: 0.60
     rank 12: 0.19
     rank 13: 0.15
     rank 14: 0.08
     rank 15: 0.04
     rank 16: 0.09
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 12.07
     rank  1: 12.17
     rank  2: 12.10
     rank  3: 11.98
     rank  4: 1.72
     rank  5: 1.72
     rank  6: 1.71
     rank  7: 1.74
     rank  8: 1.79
     rank  9: 1.77
     rank 10: 1.79
     rank 11: 2.27
     rank 12: 1.86
     rank 13: 1.32
     rank 14: 1.76
     rank 15: 1.71
     rank 16: 1.77
     rank 17: 1.72
     rank 18: 1.73
     rank 19: 1.72
 [2024-12-05 19:07:39] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 9905.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.188199E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9856.76
     rank  1: 9855.55
     rank  2: 9855.51
     rank  3: 9855.55
     rank  4: 9854.85
     rank  5: 9854.78
     rank  6: 9854.74
     rank  7: 9854.82
     rank  8: 9854.93
     rank  9: 9855.14
     rank 10: 9854.85
     rank 11: 9854.96
     rank 12: 9855.03
     rank 13: 9854.99
     rank 14: 9854.97
     rank 15: 9854.93
     rank 16: 9854.90
     rank 17: 9854.86
     rank 18: 9854.74
     rank 19: 9854.74
  forward-compute:
     rank  0: 3647.95
     rank  1: 3655.56
     rank  2: 3652.50
     rank  3: 3646.02
     rank  4: 3810.94
     rank  5: 3816.48
     rank  6: 3813.34
     rank  7: 3812.80
     rank  8: 3092.88
     rank  9: 3099.11
     rank 10: 3096.88
     rank 11: 3087.35
     rank 12: 3052.75
     rank 13: 3055.22
     rank 14: 3059.08
     rank 15: 3053.17
     rank 16: 3571.63
     rank 17: 3577.04
     rank 18: 3585.40
     rank 19: 3580.89
  backward-compute:
     rank  0: 3597.85
     rank  1: 3597.22
     rank  2: 3596.35
     rank  3: 3599.58
     rank  4: 4074.58
     rank  5: 4079.46
     rank  6: 4086.15
     rank  7: 4074.60
     rank  8: 3211.91
     rank  9: 3217.98
     rank 10: 3227.25
     rank 11: 3217.55
     rank 12: 3162.66
     rank 13: 3162.20
     rank 14: 3162.15
     rank 15: 3163.76
     rank 16: 3804.60
     rank 17: 3802.25
     rank 18: 3806.61
     rank 19: 3803.86
  pure-backward-compute:
     rank  0: 3596.86
     rank  1: 3596.34
     rank  2: 3595.67
     rank  3: 3598.85
     rank  4: 4073.63
     rank  5: 4078.52
     rank  6: 4085.40
     rank  7: 4073.14
     rank  8: 3210.13
     rank  9: 3216.63
     rank 10: 3225.85
     rank 11: 3215.62
     rank 12: 3160.65
     rank 13: 3161.13
     rank 14: 3160.89
     rank 15: 3162.57
     rank 16: 3802.19
     rank 17: 3799.60
     rank 18: 3805.04
     rank 19: 3802.24
  batch-generator:
     rank  0: 79.55
     rank  1: 83.24
     rank  2: 81.01
     rank  3: 83.37
     rank  4: 55.28
     rank  5: 62.93
     rank  6: 59.39
     rank  7: 58.97
     rank  8: 70.05
     rank  9: 81.25
     rank 10: 88.16
     rank 11: 75.80
     rank 12: 79.74
     rank 13: 85.96
     rank 14: 94.91
     rank 15: 86.31
     rank 16: 67.98
     rank 17: 75.95
     rank 18: 86.96
     rank 19: 83.14
  forward-recv:
     rank  4: 272.56
     rank  5: 270.34
     rank  6: 271.93
     rank  7: 272.18
     rank  8: 602.82
     rank  9: 602.07
     rank 10: 604.78
     rank 11: 603.53
     rank 12: 753.55
     rank 13: 753.58
     rank 14: 753.62
     rank 15: 754.17
     rank 16: 901.93
     rank 17: 901.70
     rank 18: 900.62
     rank 19: 901.74
  forward-send:
     rank  0: 94.69
     rank  1: 88.90
     rank  2: 91.66
     rank  3: 94.92
     rank  4: 33.15
     rank  5: 31.09
     rank  6: 32.09
     rank  7: 33.18
     rank  8: 21.43
     rank  9: 21.07
     rank 10: 20.06
     rank 11: 21.39
     rank 12: 10.81
     rank 13: 10.60
     rank 14: 9.71
     rank 15: 10.56
  backward-recv:
     rank  0: 920.52
     rank  1: 920.80
     rank  2: 921.80
     rank  3: 921.01
     rank  4: 514.44
     rank  5: 514.28
     rank  6: 511.22
     rank  7: 512.99
     rank  8: 467.39
     rank  9: 468.14
     rank 10: 467.73
     rank 11: 466.73
     rank 12: 260.74
     rank 13: 261.39
     rank 14: 262.42
     rank 15: 260.66
  backward-send:
     rank  4: 41.52
     rank  5: 40.50
     rank  6: 41.67
     rank  7: 41.87
     rank  8: 31.67
     rank  9: 31.48
     rank 10: 29.03
     rank 11: 31.58
     rank 12: 21.52
     rank 13: 21.30
     rank 14: 19.84
     rank 15: 20.92
     rank 16: 10.69
     rank 17: 10.76
     rank 18: 10.52
     rank 19: 9.94
  forward-send-backward-recv:
     rank  0: 1570.94
     rank  1: 1576.26
     rank  2: 1576.88
     rank  3: 1575.51
     rank  4: 733.90
     rank  5: 731.46
     rank  6: 729.24
     rank  7: 733.53
     rank  8: 637.66
     rank  9: 635.47
     rank 10: 627.10
     rank 11: 636.22
     rank 12: 659.83
     rank 13: 664.95
     rank 14: 664.20
     rank 15: 663.78
  backward-send-forward-recv:
     rank  4: 134.87
     rank  5: 133.66
     rank  6: 134.28
     rank  7: 134.49
     rank  8: 1284.26
     rank  9: 1279.65
     rank 10: 1279.37
     rank 11: 1288.51
     rank 12: 1215.63
     rank 13: 1214.48
     rank 14: 1210.90
     rank 15: 1215.75
     rank 16: 643.55
     rank 17: 640.34
     rank 18: 631.91
     rank 19: 637.74
  layernorm-grads-all-reduce:
     rank  0: 0.03
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.05
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.72
     rank  1: 0.69
     rank  2: 0.64
     rank  3: 0.64
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.29
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 11.00
     rank  1: 10.58
     rank  2: 10.23
     rank  3: 10.53
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.08
     rank  8: 0.05
     rank  9: 0.11
     rank 10: 0.06
     rank 11: 0.10
     rank 12: 0.10
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.09
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 11.84
     rank  1: 11.47
     rank  2: 11.12
     rank  3: 11.42
     rank  4: 0.95
     rank  5: 0.92
     rank  6: 0.92
     rank  7: 0.96
     rank  8: 0.94
     rank  9: 1.00
     rank 10: 0.94
     rank 11: 0.99
     rank 12: 0.99
     rank 13: 0.93
     rank 14: 0.94
     rank 15: 0.97
     rank 16: 0.94
     rank 17: 0.93
     rank 18: 0.92
     rank 19: 0.93
 [2024-12-05 19:07:49] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 9901.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.874218E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9850.21
     rank  1: 9850.23
     rank  2: 9850.19
     rank  3: 9850.24
     rank  4: 9849.45
     rank  5: 9849.46
     rank  6: 9849.43
     rank  7: 9849.49
     rank  8: 9849.49
     rank  9: 9849.46
     rank 10: 9849.55
     rank 11: 9849.68
     rank 12: 9849.56
     rank 13: 9849.71
     rank 14: 9849.56
     rank 15: 9849.60
     rank 16: 9849.50
     rank 17: 9849.50
     rank 18: 9849.40
     rank 19: 9849.40
  forward-compute:
     rank  0: 3545.56
     rank  1: 3551.69
     rank  2: 3548.39
     rank  3: 3541.57
     rank  4: 3809.05
     rank  5: 3813.03
     rank  6: 3811.60
     rank  7: 3811.63
     rank  8: 3089.02
     rank  9: 3096.21
     rank 10: 3095.92
     rank 11: 3088.34
     rank 12: 3066.10
     rank 13: 3067.36
     rank 14: 3074.30
     rank 15: 3065.98
     rank 16: 3586.08
     rank 17: 3590.24
     rank 18: 3600.94
     rank 19: 3594.92
  backward-compute:
     rank  0: 3604.22
     rank  1: 3604.49
     rank  2: 3603.89
     rank  3: 3608.36
     rank  4: 4078.92
     rank  5: 4085.19
     rank  6: 4092.48
     rank  7: 4079.61
     rank  8: 3219.08
     rank  9: 3222.75
     rank 10: 3236.23
     rank 11: 3223.21
     rank 12: 3183.88
     rank 13: 3182.83
     rank 14: 3184.41
     rank 15: 3185.60
     rank 16: 3800.50
     rank 17: 3797.83
     rank 18: 3803.01
     rank 19: 3800.06
  pure-backward-compute:
     rank  0: 3603.44
     rank  1: 3603.83
     rank  2: 3603.21
     rank  3: 3607.67
     rank  4: 4078.06
     rank  5: 4084.30
     rank  6: 4091.64
     rank  7: 4078.38
     rank  8: 3217.33
     rank  9: 3221.30
     rank 10: 3234.99
     rank 11: 3221.21
     rank 12: 3182.16
     rank 13: 3181.29
     rank 14: 3183.16
     rank 15: 3183.84
     rank 16: 3798.04
     rank 17: 3795.39
     rank 18: 3801.30
     rank 19: 3798.44
  batch-generator:
     rank  0: 75.99
     rank  1: 78.72
     rank  2: 75.99
     rank  3: 73.57
     rank  4: 51.79
     rank  5: 57.20
     rank  6: 54.65
     rank  7: 54.85
     rank  8: 68.45
     rank  9: 81.23
     rank 10: 86.29
     rank 11: 75.32
     rank 12: 80.31
     rank 13: 84.60
     rank 14: 96.41
     rank 15: 85.32
     rank 16: 68.79
     rank 17: 75.46
     rank 18: 88.27
     rank 19: 83.19
  forward-recv:
     rank  4: 273.88
     rank  5: 271.58
     rank  6: 273.18
     rank  7: 273.76
     rank  8: 607.85
     rank  9: 607.14
     rank 10: 609.95
     rank 11: 608.64
     rank 12: 759.80
     rank 13: 759.65
     rank 14: 759.91
     rank 15: 759.45
     rank 16: 898.81
     rank 17: 898.78
     rank 18: 896.88
     rank 19: 898.90
  forward-send:
     rank  0: 112.93
     rank  1: 107.24
     rank  2: 110.26
     rank  3: 113.07
     rank  4: 34.94
     rank  5: 33.09
     rank  6: 33.96
     rank  7: 34.88
     rank  8: 21.35
     rank  9: 20.90
     rank 10: 19.47
     rank 11: 21.14
     rank 12: 10.58
     rank 13: 10.58
     rank 14: 8.99
     rank 15: 10.57
  backward-recv:
     rank  0: 927.20
     rank  1: 927.86
     rank  2: 928.08
     rank  3: 926.04
     rank  4: 515.14
     rank  5: 515.03
     rank  6: 512.34
     rank  7: 514.22
     rank  8: 469.78
     rank  9: 470.32
     rank 10: 470.05
     rank 11: 469.00
     rank 12: 263.04
     rank 13: 263.16
     rank 14: 264.54
     rank 15: 263.10
  backward-send:
     rank  4: 41.62
     rank  5: 40.84
     rank  6: 41.46
     rank  7: 41.54
     rank  8: 31.74
     rank  9: 31.41
     rank 10: 29.26
     rank 11: 31.69
     rank 12: 21.44
     rank 13: 21.47
     rank 14: 19.85
     rank 15: 20.82
     rank 16: 10.71
     rank 17: 10.76
     rank 18: 10.65
     rank 19: 10.00
  forward-send-backward-recv:
     rank  0: 1640.74
     rank  1: 1642.81
     rank  2: 1643.61
     rank  3: 1641.89
     rank  4: 726.38
     rank  5: 721.86
     rank  6: 717.65
     rank  7: 723.19
     rank  8: 645.82
     rank  9: 644.46
     rank 10: 632.62
     rank 11: 641.57
     rank 12: 648.63
     rank 13: 653.27
     rank 14: 652.42
     rank 15: 652.92
  backward-send-forward-recv:
     rank  4: 134.11
     rank  5: 133.70
     rank  6: 134.37
     rank  7: 133.94
     rank  8: 1264.05
     rank  9: 1258.22
     rank 10: 1257.17
     rank 11: 1266.78
     rank 12: 1182.05
     rank 13: 1180.78
     rank 14: 1175.31
     rank 15: 1181.92
     rank 16: 632.59
     rank 17: 630.89
     rank 18: 620.27
     rank 19: 627.01
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.66
     rank  2: 0.62
     rank  3: 0.62
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.05
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.20
     rank  3: 0.39
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.09
     rank 12: 0.01
     rank 13: 0.08
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.22
     rank  1: 10.22
     rank  2: 10.29
     rank  3: 12.37
     rank  4: 0.03
     rank  5: 0.06
     rank  6: 0.06
     rank  7: 0.08
     rank  8: 0.08
     rank  9: 0.04
     rank 10: 0.10
     rank 11: 0.13
     rank 12: 0.06
     rank 13: 0.11
     rank 14: 0.09
     rank 15: 0.09
     rank 16: 0.04
     rank 17: 0.07
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 11.78
     rank  1: 11.79
     rank  2: 11.85
     rank  3: 13.89
     rank  4: 1.59
     rank  5: 1.62
     rank  6: 1.62
     rank  7: 1.64
     rank  8: 1.64
     rank  9: 1.60
     rank 10: 1.66
     rank 11: 1.69
     rank 12: 1.62
     rank 13: 1.67
     rank 14: 1.65
     rank 15: 1.65
     rank 16: 1.60
     rank 17: 1.62
     rank 18: 1.60
     rank 19: 1.60
 [2024-12-05 19:07:59] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 9879.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.473380E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9830.61
     rank  1: 9830.60
     rank  2: 9830.60
     rank  3: 9830.62
     rank  4: 9829.88
     rank  5: 9829.84
     rank  6: 9829.85
     rank  7: 9829.85
     rank  8: 9829.94
     rank  9: 9829.95
     rank 10: 9829.95
     rank 11: 9830.11
     rank 12: 9829.96
     rank 13: 9830.68
     rank 14: 9829.99
     rank 15: 9830.05
     rank 16: 9829.93
     rank 17: 9829.93
     rank 18: 9829.82
     rank 19: 9829.81
  forward-compute:
     rank  0: 3474.49
     rank  1: 3481.66
     rank  2: 3477.06
     rank  3: 3471.15
     rank  4: 3807.43
     rank  5: 3815.08
     rank  6: 3811.93
     rank  7: 3809.41
     rank  8: 3093.20
     rank  9: 3100.84
     rank 10: 3099.57
     rank 11: 3091.37
     rank 12: 3058.59
     rank 13: 3060.11
     rank 14: 3066.53
     rank 15: 3059.41
     rank 16: 3575.62
     rank 17: 3579.89
     rank 18: 3589.43
     rank 19: 3585.29
  backward-compute:
     rank  0: 3602.25
     rank  1: 3603.24
     rank  2: 3601.84
     rank  3: 3606.00
     rank  4: 4075.30
     rank  5: 4080.47
     rank  6: 4087.18
     rank  7: 4077.79
     rank  8: 3217.34
     rank  9: 3222.13
     rank 10: 3234.15
     rank 11: 3221.66
     rank 12: 3182.81
     rank 13: 3181.60
     rank 14: 3182.20
     rank 15: 3183.41
     rank 16: 3818.30
     rank 17: 3815.51
     rank 18: 3820.17
     rank 19: 3817.66
  pure-backward-compute:
     rank  0: 3601.59
     rank  1: 3602.47
     rank  2: 3601.17
     rank  3: 3605.30
     rank  4: 4074.39
     rank  5: 4079.67
     rank  6: 4086.45
     rank  7: 4076.53
     rank  8: 3215.55
     rank  9: 3220.23
     rank 10: 3232.58
     rank 11: 3220.07
     rank 12: 3180.96
     rank 13: 3180.37
     rank 14: 3180.93
     rank 15: 3182.12
     rank 16: 3815.88
     rank 17: 3813.19
     rank 18: 3818.56
     rank 19: 3816.06
  batch-generator:
     rank  0: 52.70
     rank  1: 62.13
     rank  2: 58.47
     rank  3: 59.61
     rank  4: 50.88
     rank  5: 60.45
     rank  6: 55.18
     rank  7: 55.64
     rank  8: 70.06
     rank  9: 82.90
     rank 10: 86.76
     rank 11: 77.37
     rank 12: 70.96
     rank 13: 77.80
     rank 14: 89.71
     rank 15: 79.29
     rank 16: 68.41
     rank 17: 75.25
     rank 18: 86.71
     rank 19: 83.61
  forward-recv:
     rank  4: 258.04
     rank  5: 254.32
     rank  6: 256.97
     rank  7: 257.29
     rank  8: 589.46
     rank  9: 589.19
     rank 10: 592.28
     rank 11: 590.02
     rank 12: 740.07
     rank 13: 739.05
     rank 14: 739.72
     rank 15: 740.87
     rank 16: 882.86
     rank 17: 882.83
     rank 18: 881.11
     rank 19: 881.73
  forward-send:
     rank  0: 137.63
     rank  1: 130.80
     rank  2: 135.45
     rank  3: 137.01
     rank  4: 34.26
     rank  5: 32.48
     rank  6: 33.78
     rank  7: 34.26
     rank  8: 21.21
     rank  9: 20.69
     rank 10: 19.24
     rank 11: 21.13
     rank 12: 10.65
     rank 13: 10.93
     rank 14: 9.66
     rank 15: 10.07
  backward-recv:
     rank  0: 928.61
     rank  1: 928.48
     rank  2: 928.95
     rank  3: 928.25
     rank  4: 513.29
     rank  5: 513.03
     rank  6: 509.70
     rank  7: 511.70
     rank  8: 467.47
     rank  9: 466.98
     rank 10: 467.83
     rank 11: 468.13
     rank 12: 260.55
     rank 13: 260.95
     rank 14: 261.74
     rank 15: 260.43
  backward-send:
     rank  4: 41.42
     rank  5: 40.76
     rank  6: 41.68
     rank  7: 41.51
     rank  8: 31.77
     rank  9: 31.62
     rank 10: 29.29
     rank 11: 31.49
     rank 12: 21.41
     rank 13: 20.98
     rank 14: 19.69
     rank 15: 21.38
     rank 16: 10.72
     rank 17: 10.69
     rank 18: 10.56
     rank 19: 9.96
  forward-send-backward-recv:
     rank  0: 1669.15
     rank  1: 1670.42
     rank  2: 1671.07
     rank  3: 1669.78
     rank  4: 730.27
     rank  5: 726.63
     rank  6: 722.56
     rank  7: 728.32
     rank  8: 651.07
     rank  9: 649.51
     rank 10: 638.19
     rank 11: 649.12
     rank 12: 655.04
     rank 13: 659.50
     rank 14: 659.89
     rank 15: 658.73
  backward-send-forward-recv:
     rank  4: 134.49
     rank  5: 132.81
     rank  6: 133.96
     rank  7: 133.59
     rank  8: 1257.40
     rank  9: 1250.38
     rank 10: 1250.08
     rank 11: 1258.57
     rank 12: 1187.23
     rank 13: 1186.65
     rank 14: 1180.89
     rank 15: 1187.09
     rank 16: 622.84
     rank 17: 621.09
     rank 18: 611.61
     rank 19: 617.57
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.63
     rank  2: 0.62
     rank  3: 0.63
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.20
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.20
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.04
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.03
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.29
     rank  1: 10.40
     rank  2: 10.59
     rank  3: 10.47
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.09
     rank 10: 0.09
     rank 11: 0.11
     rank 12: 0.05
     rank 13: 0.20
     rank 14: 0.06
     rank 15: 0.10
     rank 16: 0.07
     rank 17: 0.04
     rank 18: 0.07
     rank 19: 0.04
  optimizer:
     rank  0: 11.52
     rank  1: 11.64
     rank  2: 11.81
     rank  3: 11.71
     rank  4: 1.27
     rank  5: 1.26
     rank  6: 1.26
     rank  7: 1.26
     rank  8: 1.29
     rank  9: 1.32
     rank 10: 1.32
     rank 11: 1.34
     rank 12: 1.28
     rank 13: 1.38
     rank 14: 1.29
     rank 15: 1.33
     rank 16: 1.29
     rank 17: 1.26
     rank 18: 1.29
     rank 19: 1.26
 [2024-12-05 19:08:09] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 9901.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.996079E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9853.78
     rank  1: 9853.86
     rank  2: 9853.77
     rank  3: 9853.91
     rank  4: 9853.00
     rank  5: 9853.09
     rank  6: 9853.00
     rank  7: 9853.05
     rank  8: 9853.04
     rank  9: 9853.24
     rank 10: 9853.19
     rank 11: 9853.34
     rank 12: 9853.34
     rank 13: 9853.76
     rank 14: 9853.37
     rank 15: 9853.21
     rank 16: 9853.08
     rank 17: 9853.10
     rank 18: 9853.03
     rank 19: 9853.02
  forward-compute:
     rank  0: 3628.42
     rank  1: 3632.24
     rank  2: 3631.53
     rank  3: 3621.97
     rank  4: 3812.73
     rank  5: 3817.42
     rank  6: 3816.10
     rank  7: 3814.49
     rank  8: 3101.20
     rank  9: 3106.58
     rank 10: 3108.13
     rank 11: 3097.68
     rank 12: 3060.27
     rank 13: 3062.05
     rank 14: 3066.30
     rank 15: 3060.58
     rank 16: 3582.18
     rank 17: 3585.95
     rank 18: 3595.23
     rank 19: 3590.24
  backward-compute:
     rank  0: 3596.41
     rank  1: 3597.89
     rank  2: 3597.52
     rank  3: 3602.29
     rank  4: 4082.58
     rank  5: 4086.10
     rank  6: 4094.26
     rank  7: 4082.85
     rank  8: 3220.63
     rank  9: 3226.30
     rank 10: 3233.78
     rank 11: 3225.58
     rank 12: 3177.46
     rank 13: 3175.96
     rank 14: 3178.25
     rank 15: 3178.08
     rank 16: 3814.81
     rank 17: 3812.31
     rank 18: 3816.96
     rank 19: 3814.49
  pure-backward-compute:
     rank  0: 3595.70
     rank  1: 3597.08
     rank  2: 3596.83
     rank  3: 3601.20
     rank  4: 4081.57
     rank  5: 4085.31
     rank  6: 4093.55
     rank  7: 4081.70
     rank  8: 3218.99
     rank  9: 3224.68
     rank 10: 3232.57
     rank 11: 3223.92
     rank 12: 3175.40
     rank 13: 3174.63
     rank 14: 3177.01
     rank 15: 3176.30
     rank 16: 3812.40
     rank 17: 3809.95
     rank 18: 3815.16
     rank 19: 3812.90
  batch-generator:
     rank  0: 51.31
     rank  1: 58.71
     rank  2: 58.60
     rank  3: 67.66
     rank  4: 55.18
     rank  5: 61.08
     rank  6: 59.80
     rank  7: 58.68
     rank  8: 68.54
     rank  9: 78.72
     rank 10: 85.91
     rank 11: 77.33
     rank 12: 72.35
     rank 13: 78.14
     rank 14: 87.98
     rank 15: 79.66
     rank 16: 68.62
     rank 17: 74.98
     rank 18: 86.10
     rank 19: 82.00
  forward-recv:
     rank  4: 257.73
     rank  5: 255.69
     rank  6: 256.29
     rank  7: 257.36
     rank  8: 587.43
     rank  9: 587.34
     rank 10: 589.43
     rank 11: 587.97
     rank 12: 741.10
     rank 13: 740.98
     rank 14: 740.50
     rank 15: 741.69
     rank 16: 885.29
     rank 17: 885.36
     rank 18: 884.19
     rank 19: 884.94
  forward-send:
     rank  0: 138.88
     rank  1: 134.29
     rank  2: 135.46
     rank  3: 139.44
     rank  4: 33.46
     rank  5: 31.86
     rank  6: 32.33
     rank  7: 33.57
     rank  8: 21.29
     rank  9: 20.83
     rank 10: 19.50
     rank 11: 21.27
     rank 12: 10.78
     rank 13: 10.56
     rank 14: 9.93
     rank 15: 10.45
  backward-recv:
     rank  0: 933.61
     rank  1: 934.05
     rank  2: 933.38
     rank  3: 932.52
     rank  4: 519.30
     rank  5: 519.14
     rank  6: 516.33
     rank  7: 518.63
     rank  8: 470.74
     rank  9: 470.30
     rank 10: 471.95
     rank 11: 469.94
     rank 12: 264.20
     rank 13: 264.59
     rank 14: 263.88
     rank 15: 263.97
  backward-send:
     rank  4: 41.70
     rank  5: 41.43
     rank  6: 41.49
     rank  7: 41.36
     rank  8: 31.61
     rank  9: 31.66
     rank 10: 28.92
     rank 11: 31.56
     rank 12: 21.52
     rank 13: 21.05
     rank 14: 20.09
     rank 15: 21.03
     rank 16: 10.32
     rank 17: 10.41
     rank 18: 9.30
     rank 19: 9.99
  forward-send-backward-recv:
     rank  0: 1534.55
     rank  1: 1538.80
     rank  2: 1539.52
     rank  3: 1537.95
     rank  4: 734.42
     rank  5: 732.81
     rank  6: 729.36
     rank  7: 733.81
     rank  8: 640.31
     rank  9: 639.29
     rank 10: 631.89
     rank 11: 639.29
     rank 12: 659.91
     rank 13: 664.24
     rank 14: 664.49
     rank 15: 664.40
  backward-send-forward-recv:
     rank  4: 134.66
     rank  5: 133.74
     rank  6: 134.20
     rank  7: 134.33
     rank  8: 1276.47
     rank  9: 1272.13
     rank 10: 1269.62
     rank 11: 1279.23
     rank 12: 1201.71
     rank 13: 1201.26
     rank 14: 1197.69
     rank 15: 1201.27
     rank 16: 637.66
     rank 17: 636.18
     rank 18: 625.97
     rank 19: 632.96
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.68
     rank  2: 0.62
     rank  3: 0.67
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.19
     rank  2: 0.19
     rank  3: 0.22
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.08
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.20
     rank  1: 10.42
     rank  2: 10.42
     rank  3: 10.56
     rank  4: 0.06
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.11
     rank 10: 0.09
     rank 11: 0.11
     rank 12: 0.12
     rank 13: 0.55
     rank 14: 0.06
     rank 15: 0.09
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  optimizer:
     rank  0: 11.30
     rank  1: 11.51
     rank  2: 11.51
     rank  3: 11.64
     rank  4: 1.15
     rank  5: 1.12
     rank  6: 1.12
     rank  7: 1.16
     rank  8: 1.13
     rank  9: 1.20
     rank 10: 1.18
     rank 11: 1.20
     rank 12: 1.22
     rank 13: 1.64
     rank 14: 1.17
     rank 15: 1.18
     rank 16: 1.14
     rank 17: 1.14
     rank 18: 1.14
     rank 19: 1.16
 [2024-12-05 19:08:19] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10051.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.916180E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9999.90
     rank  1: 9999.92
     rank  2: 9999.92
     rank  3: 9999.95
     rank  4: 9999.13
     rank  5: 9999.17
     rank  6: 9999.17
     rank  7: 9999.17
     rank  8: 9999.16
     rank  9: 9999.49
     rank 10: 9999.22
     rank 11: 9999.33
     rank 12: 9999.27
     rank 13: 9999.39
     rank 14: 9999.27
     rank 15: 9999.27
     rank 16: 9999.17
     rank 17: 9999.16
     rank 18: 9999.08
     rank 19: 9999.08
  forward-compute:
     rank  0: 3786.57
     rank  1: 3791.42
     rank  2: 3788.96
     rank  3: 3782.56
     rank  4: 3811.55
     rank  5: 3816.53
     rank  6: 3813.42
     rank  7: 3813.25
     rank  8: 3099.90
     rank  9: 3109.18
     rank 10: 3108.31
     rank 11: 3096.77
     rank 12: 3060.68
     rank 13: 3062.14
     rank 14: 3064.34
     rank 15: 3062.45
     rank 16: 3591.10
     rank 17: 3598.85
     rank 18: 3595.45
     rank 19: 3602.31
  backward-compute:
     rank  0: 3599.71
     rank  1: 3601.22
     rank  2: 3600.74
     rank  3: 3605.34
     rank  4: 4076.13
     rank  5: 4082.50
     rank  6: 4089.65
     rank  7: 4077.13
     rank  8: 3224.69
     rank  9: 3228.86
     rank 10: 3234.41
     rank 11: 3230.54
     rank 12: 3179.12
     rank 13: 3180.36
     rank 14: 3173.08
     rank 15: 3181.94
     rank 16: 3819.92
     rank 17: 3817.29
     rank 18: 3822.05
     rank 19: 3819.34
  pure-backward-compute:
     rank  0: 3599.03
     rank  1: 3600.52
     rank  2: 3599.99
     rank  3: 3604.67
     rank  4: 4075.23
     rank  5: 4081.71
     rank  6: 4088.93
     rank  7: 4076.11
     rank  8: 3222.96
     rank  9: 3227.48
     rank 10: 3233.14
     rank 11: 3229.18
     rank 12: 3177.38
     rank 13: 3179.18
     rank 14: 3171.33
     rank 15: 3180.59
     rank 16: 3817.48
     rank 17: 3814.79
     rank 18: 3820.42
     rank 19: 3817.72
  batch-generator:
     rank  0: 56.88
     rank  1: 63.73
     rank  2: 61.36
     rank  3: 60.77
     rank  4: 50.23
     rank  5: 57.76
     rank  6: 54.85
     rank  7: 54.01
     rank  8: 68.73
     rank  9: 83.25
     rank 10: 87.76
     rank 11: 75.41
     rank 12: 75.32
     rank 13: 79.29
     rank 14: 92.84
     rank 15: 83.95
     rank 16: 68.84
     rank 17: 79.30
     rank 18: 77.94
     rank 19: 85.71
  forward-recv:
     rank  4: 271.71
     rank  5: 269.36
     rank  6: 270.77
     rank  7: 271.62
     rank  8: 602.35
     rank  9: 601.81
     rank 10: 603.49
     rank 11: 603.29
     rank 12: 753.99
     rank 13: 754.25
     rank 14: 753.48
     rank 15: 754.37
     rank 16: 900.66
     rank 17: 900.40
     rank 18: 900.11
     rank 19: 900.28
  forward-send:
     rank  0: 143.34
     rank  1: 138.12
     rank  2: 140.46
     rank  3: 143.47
     rank  4: 34.84
     rank  5: 32.98
     rank  6: 33.82
     rank  7: 34.75
     rank  8: 21.53
     rank  9: 20.91
     rank 10: 20.37
     rank 11: 21.12
     rank 12: 10.93
     rank 13: 10.65
     rank 14: 10.64
     rank 15: 10.42
  backward-recv:
     rank  0: 930.72
     rank  1: 930.58
     rank  2: 930.66
     rank  3: 929.65
     rank  4: 518.86
     rank  5: 518.93
     rank  6: 516.59
     rank  7: 519.03
     rank  8: 466.18
     rank  9: 465.46
     rank 10: 467.83
     rank 11: 466.24
     rank 12: 262.30
     rank 13: 262.78
     rank 14: 262.74
     rank 15: 262.43
  backward-send:
     rank  4: 41.69
     rank  5: 41.25
     rank  6: 41.57
     rank  7: 40.84
     rank  8: 31.96
     rank  9: 32.17
     rank 10: 29.64
     rank 11: 31.37
     rank 12: 21.62
     rank 13: 21.22
     rank 14: 21.22
     rank 15: 21.12
     rank 16: 10.69
     rank 17: 10.65
     rank 18: 10.60
     rank 19: 10.06
  forward-send-backward-recv:
     rank  0: 1520.51
     rank  1: 1519.91
     rank  2: 1520.67
     rank  3: 1520.55
     rank  4: 732.35
     rank  5: 727.83
     rank  6: 725.14
     rank  7: 729.53
     rank  8: 653.21
     rank  9: 652.50
     rank 10: 647.18
     rank 11: 651.17
     rank 12: 666.68
     rank 13: 669.21
     rank 14: 672.61
     rank 15: 667.45
  backward-send-forward-recv:
     rank  4: 272.61
     rank  5: 271.47
     rank  6: 272.15
     rank  7: 272.51
     rank  8: 1393.66
     rank  9: 1386.00
     rank 10: 1385.09
     rank 11: 1396.17
     rank 12: 1325.98
     rank 13: 1325.07
     rank 14: 1322.41
     rank 15: 1324.61
     rank 16: 751.74
     rank 17: 746.56
     rank 18: 748.69
     rank 19: 744.19
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.03
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.67
     rank  2: 0.63
     rank  3: 0.68
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.05
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.18
     rank  2: 0.19
     rank  3: 0.42
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.05
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.10
     rank  1: 10.54
     rank  2: 10.55
     rank  3: 12.20
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.08
     rank  9: 0.06
     rank 10: 0.05
     rank 11: 0.11
     rank 12: 0.10
     rank 13: 0.11
     rank 14: 0.08
     rank 15: 0.09
     rank 16: 0.04
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  optimizer:
     rank  0: 11.87
     rank  1: 12.30
     rank  2: 12.31
     rank  3: 13.92
     rank  4: 1.79
     rank  5: 1.79
     rank  6: 1.79
     rank  7: 1.83
     rank  8: 1.84
     rank  9: 1.82
     rank 10: 1.81
     rank 11: 1.86
     rank 12: 1.86
     rank 13: 1.87
     rank 14: 1.85
     rank 15: 1.85
     rank 16: 1.80
     rank 17: 1.81
     rank 18: 1.80
     rank 19: 1.81
 [2024-12-05 19:08:29] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 9888.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.781174E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 9847.52
     rank  1: 9847.49
     rank  2: 9847.50
     rank  3: 9848.09
     rank  4: 9846.72
     rank  5: 9846.69
     rank  6: 9846.69
     rank  7: 9847.04
     rank  8: 9846.89
     rank  9: 9846.80
     rank 10: 9846.81
     rank 11: 9847.05
     rank 12: 9846.84
     rank 13: 9846.83
     rank 14: 9846.99
     rank 15: 9847.12
     rank 16: 9846.76
     rank 17: 9846.77
     rank 18: 9846.69
     rank 19: 9846.69
  forward-compute:
     rank  0: 3586.43
     rank  1: 3591.02
     rank  2: 3588.28
     rank  3: 3584.07
     rank  4: 3810.85
     rank  5: 3816.55
     rank  6: 3812.66
     rank  7: 3813.77
     rank  8: 3104.20
     rank  9: 3110.61
     rank 10: 3108.49
     rank 11: 3099.18
     rank 12: 3059.93
     rank 13: 3060.43
     rank 14: 3063.36
     rank 15: 3060.07
     rank 16: 3587.36
     rank 17: 3592.49
     rank 18: 3599.42
     rank 19: 3596.85
  backward-compute:
     rank  0: 3562.00
     rank  1: 3564.32
     rank  2: 3561.94
     rank  3: 3563.49
     rank  4: 4078.16
     rank  5: 4082.02
     rank  6: 4088.58
     rank  7: 4078.70
     rank  8: 3230.41
     rank  9: 3235.60
     rank 10: 3240.21
     rank 11: 3235.92
     rank 12: 3182.73
     rank 13: 3182.05
     rank 14: 3180.56
     rank 15: 3183.45
     rank 16: 3823.35
     rank 17: 3820.49
     rank 18: 3825.40
     rank 19: 3823.01
  pure-backward-compute:
     rank  0: 3560.68
     rank  1: 3563.42
     rank  2: 3561.27
     rank  3: 3562.44
     rank  4: 4077.28
     rank  5: 4081.25
     rank  6: 4087.79
     rank  7: 4077.57
     rank  8: 3228.49
     rank  9: 3234.30
     rank 10: 3238.87
     rank 11: 3234.65
     rank 12: 3180.94
     rank 13: 3180.67
     rank 14: 3179.13
     rank 15: 3182.08
     rank 16: 3820.96
     rank 17: 3818.07
     rank 18: 3823.81
     rank 19: 3821.42
  batch-generator:
     rank  0: 62.28
     rank  1: 66.56
     rank  2: 65.46
     rank  3: 64.96
     rank  4: 52.41
     rank  5: 60.07
     rank  6: 56.51
     rank  7: 56.66
     rank  8: 69.02
     rank  9: 80.61
     rank 10: 84.50
     rank 11: 72.92
     rank 12: 70.76
     rank 13: 74.85
     rank 14: 85.81
     rank 15: 78.62
     rank 16: 67.68
     rank 17: 75.40
     rank 18: 84.28
     rank 19: 82.79
  forward-recv:
     rank  4: 257.88
     rank  5: 255.94
     rank  6: 257.67
     rank  7: 257.86
     rank  8: 585.85
     rank  9: 584.58
     rank 10: 588.12
     rank 11: 585.86
     rank 12: 742.06
     rank 13: 742.45
     rank 14: 741.38
     rank 15: 743.12
     rank 16: 884.60
     rank 17: 884.62
     rank 18: 882.80
     rank 19: 883.63
  forward-send:
     rank  0: 140.35
     rank  1: 135.14
     rank  2: 138.15
     rank  3: 140.69
     rank  4: 33.73
     rank  5: 31.73
     rank  6: 32.64
     rank  7: 33.69
     rank  8: 21.03
     rank  9: 21.07
     rank 10: 18.94
     rank 11: 21.21
     rank 12: 10.82
     rank 13: 10.72
     rank 14: 9.50
     rank 15: 10.14
  backward-recv:
     rank  0: 940.07
     rank  1: 941.24
     rank  2: 942.21
     rank  3: 941.46
     rank  4: 518.94
     rank  5: 518.66
     rank  6: 516.33
     rank  7: 518.03
     rank  8: 467.86
     rank  9: 467.72
     rank 10: 468.88
     rank 11: 468.38
     rank 12: 262.77
     rank 13: 263.24
     rank 14: 263.78
     rank 15: 262.71
  backward-send:
     rank  4: 41.44
     rank  5: 41.17
     rank  6: 41.61
     rank  7: 41.73
     rank  8: 31.88
     rank  9: 31.78
     rank 10: 29.51
     rank 11: 31.37
     rank 12: 21.47
     rank 13: 21.30
     rank 14: 20.50
     rank 15: 21.20
     rank 16: 10.71
     rank 17: 10.68
     rank 18: 10.62
     rank 19: 10.00
  forward-send-backward-recv:
     rank  0: 1590.80
     rank  1: 1598.33
     rank  2: 1600.05
     rank  3: 1597.73
     rank  4: 732.72
     rank  5: 731.88
     rank  6: 729.50
     rank  7: 731.79
     rank  8: 649.06
     rank  9: 647.96
     rank 10: 642.99
     rank 11: 646.90
     rank 12: 658.12
     rank 13: 662.74
     rank 14: 662.61
     rank 15: 662.08
  backward-send-forward-recv:
     rank  4: 135.61
     rank  5: 133.48
     rank  6: 134.55
     rank  7: 134.27
     rank  8: 1253.82
     rank  9: 1248.68
     rank 10: 1249.23
     rank 11: 1258.54
     rank 12: 1193.56
     rank 13: 1192.82
     rank 14: 1191.98
     rank 15: 1193.29
     rank 16: 619.21
     rank 17: 616.44
     rank 18: 609.58
     rank 19: 613.80
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.14
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.62
     rank  2: 0.67
     rank  3: 0.95
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.05
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.19
     rank  2: 0.20
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.22
     rank  1: 10.17
     rank  2: 10.61
     rank  3: 10.23
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.06
     rank  9: 0.13
     rank 10: 0.13
     rank 11: 0.15
     rank 12: 0.04
     rank 13: 0.11
     rank 14: 0.16
     rank 15: 0.10
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.04
     rank 19: 0.04
  optimizer:
     rank  0: 11.07
     rank  1: 11.03
     rank  2: 11.46
     rank  3: 11.07
     rank  4: 0.89
     rank  5: 0.88
     rank  6: 0.88
     rank  7: 0.92
     rank  8: 0.92
     rank  9: 0.98
     rank 10: 0.99
     rank 11: 1.01
     rank 12: 0.89
     rank 13: 0.98
     rank 14: 1.02
     rank 15: 0.96
     rank 16: 0.93
     rank 17: 0.92
     rank 18: 0.90
     rank 19: 0.89
