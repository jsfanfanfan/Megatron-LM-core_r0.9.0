examples/multimodal/pretrain-freeze-llm-hete-3090first.sh: line 4: activate: No such file or directory
4
[2024-12-05 16:09:01,622] torch.distributed.run: [WARNING] 
[2024-12-05 16:09:01,622] torch.distributed.run: [WARNING] *****************************************
[2024-12-05 16:09:01,622] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-05 16:09:01,622] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 251695104 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 251695104

 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 251695104
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 251695104
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 98.54
     rank  1: 97.63
     rank  2: 148.88
     rank  3: 100.47
     rank  4: 40.84
     rank  5: 56.44
     rank  6: 40.60
     rank  7: 54.63
     rank  8: 59.63
     rank  9: 56.44
     rank 10: 56.24
     rank 11: 56.35
     rank 12: 66.28
     rank 13: 60.72
     rank 14: 58.92
     rank 15: 58.48
     rank 16: 51.81
     rank 17: 53.06
     rank 18: 53.03
     rank 19: 51.82
  train/valid/test-data-iterators-setup:
     rank  0: 1035.21
     rank  1: 1035.23
     rank  2: 1035.45
     rank  3: 1035.23
     rank  4: 1084.86
     rank  5: 1084.81
     rank  6: 1084.84
     rank  7: 1085.08
     rank  8: 1427.14
     rank  9: 1426.69
     rank 10: 1085.28
     rank 11: 1427.00
     rank 12: 1426.98
     rank 13: 1427.36
     rank 14: 1427.06
     rank 15: 1427.79
     rank 16: 1426.89
     rank 17: 1426.86
     rank 18: 1427.11
     rank 19: 1426.91
 [2024-12-05 16:09:58] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21567.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.143766E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 16] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0[Rank 18] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0

[Rank 19] (after 1 iterations) memory (MB) | allocated: 1045.3935546875 | max allocated: 2129.0107421875 | reserved: 2496.0 | max reserved: 2496.0
times across ranks (ms):
  forward-backward:
     rank  0: 21504.75
     rank  1: 21504.68
     rank  2: 21504.72
     rank  3: 21504.75
     rank  4: 21546.08
     rank  5: 21544.95
     rank  6: 21550.22
     rank  7: 21544.03
     rank  8: 21545.54
     rank  9: 21544.20
     rank 10: 21543.90
     rank 11: 21543.79
     rank 12: 21539.10
     rank 13: 21545.26
     rank 14: 21543.52
     rank 15: 21542.92
     rank 16: 21503.80
     rank 17: 21503.97
     rank 18: 21504.06
     rank 19: 21503.89
  forward-compute:
     rank  0: 7041.12
     rank  1: 7100.33
     rank  2: 7055.41
     rank  3: 7039.81
     rank  4: 6439.10
     rank  5: 6446.54
     rank  6: 6448.06
     rank  7: 6454.10
     rank  8: 4987.47
     rank  9: 5000.20
     rank 10: 4984.41
     rank 11: 4982.46
     rank 12: 4328.71
     rank 13: 4343.75
     rank 14: 4334.91
     rank 15: 4336.58
     rank 16: 4732.56
     rank 17: 4743.42
     rank 18: 4737.91
     rank 19: 4748.51
  backward-compute:
     rank  0: 4119.98
     rank  1: 4067.43
     rank  2: 4112.54
     rank  3: 4137.75
     rank  4: 4452.17
     rank  5: 4456.79
     rank  6: 4460.18
     rank  7: 4452.91
     rank  8: 3619.39
     rank  9: 3625.36
     rank 10: 3619.78
     rank 11: 3626.54
     rank 12: 2989.90
     rank 13: 2989.51
     rank 14: 2992.63
     rank 15: 2992.02
     rank 16: 3758.39
     rank 17: 3757.27
     rank 18: 3759.29
     rank 19: 3755.82
  pure-backward-compute:
     rank  0: 4119.12
     rank  1: 4066.67
     rank  2: 4111.48
     rank  3: 4136.85
     rank  4: 4451.27
     rank  5: 4455.42
     rank  6: 4458.48
     rank  7: 4451.95
     rank  8: 3617.44
     rank  9: 3623.87
     rank 10: 3618.00
     rank 11: 3625.07
     rank 12: 2988.19
     rank 13: 2987.63
     rank 14: 2990.65
     rank 15: 2990.65
     rank 16: 3756.04
     rank 17: 3755.38
     rank 18: 3756.95
     rank 19: 3753.48
  batch-generator:
     rank  0: 1085.35
     rank  1: 1145.24
     rank  2: 1107.16
     rank  3: 1087.47
     rank  4: 1277.46
     rank  5: 1287.39
     rank  6: 1288.94
     rank  7: 1295.11
     rank  8: 1470.16
     rank  9: 1474.63
     rank 10: 1479.01
     rank 11: 1472.67
     rank 12: 1401.31
     rank 13: 1427.44
     rank 14: 1417.77
     rank 15: 1416.87
     rank 16: 1114.08
     rank 17: 1124.52
     rank 18: 1122.28
     rank 19: 1135.40
  forward-recv:
     rank  4: 3954.09
     rank  5: 3957.06
     rank  6: 3953.44
     rank  7: 3951.97
     rank  8: 6556.56
     rank  9: 6558.27
     rank 10: 6558.05
     rank 11: 6560.58
     rank 12: 8299.68
     rank 13: 8294.09
     rank 14: 8302.95
     rank 15: 8303.27
     rank 16: 9938.72
     rank 17: 9930.32
     rank 18: 9941.49
     rank 19: 9930.62
  forward-send:
     rank  0: 5529.72
     rank  1: 5522.43
     rank  2: 5525.70
     rank  3: 5527.60
     rank  4: 2907.01
     rank  5: 2895.80
     rank  6: 2903.11
     rank  7: 2906.92
     rank  8: 1448.09
     rank  9: 1435.17
     rank 10: 1443.90
     rank 11: 1444.70
     rank 12: 33.67
     rank 13: 25.30
     rank 14: 24.54
     rank 15: 24.19
  backward-recv:
     rank  0: 1084.20
     rank  1: 1084.09
     rank  2: 1082.63
     rank  3: 1082.29
     rank  4: 471.64
     rank  5: 470.19
     rank  6: 465.16
     rank  7: 470.00
     rank  8: 390.27
     rank  9: 392.69
     rank 10: 394.10
     rank 11: 391.55
     rank 12: 269.32
     rank 13: 268.99
     rank 14: 269.68
     rank 15: 269.99
  backward-send:
     rank  4: 41.44
     rank  5: 41.19
     rank  6: 42.07
     rank  7: 40.65
     rank  8: 33.20
     rank  9: 31.31
     rank 10: 28.73
     rank 11: 31.13
     rank 12: 21.50
     rank 13: 21.53
     rank 14: 20.81
     rank 15: 20.77
     rank 16: 10.80
     rank 17: 10.25
     rank 18: 10.76
     rank 19: 10.57
  forward-send-backward-recv:
     rank  0: 3665.81
     rank  1: 3666.62
     rank  2: 3661.74
     rank  3: 3652.87
     rank  4: 2044.33
     rank  5: 2042.90
     rank  6: 2042.93
     rank  7: 2044.84
     rank  8: 1844.97
     rank  9: 1842.60
     rank 10: 1845.59
     rank 11: 1840.91
     rank 12: 1801.20
     rank 13: 1803.31
     rank 14: 1800.37
     rank 15: 1801.75
  backward-send-forward-recv:
     rank  4: 933.65
     rank  5: 933.67
     rank  6: 928.86
     rank  7: 922.06
     rank  8: 2074.33
     rank  9: 2072.98
     rank 10: 2079.43
     rank 11: 2078.94
     rank 12: 2974.34
     rank 13: 2972.58
     rank 14: 2971.68
     rank 15: 2970.89
     rank 16: 2083.00
     rank 17: 2083.46
     rank 18: 2075.07
     rank 19: 2077.59
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.13
     rank  1: 0.12
     rank  2: 0.13
     rank  3: 0.12
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.08
     rank 19: 0.07
  all-grads-sync:
     rank  0: 46.90
     rank  1: 43.47
     rank  2: 46.64
     rank  3: 45.48
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.22
     rank  2: 0.23
     rank  3: 0.24
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.09
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 12.98
     rank  1: 12.95
     rank  2: 13.09
     rank  3: 13.25
     rank  4: 0.16
     rank  5: 0.15
     rank  6: 0.20
     rank  7: 0.08
     rank  8: 0.26
     rank  9: 0.20
     rank 10: 0.19
     rank 11: 0.24
     rank 12: 0.18
     rank 13: 0.32
     rank 14: 0.29
     rank 15: 0.38
     rank 16: 0.10
     rank 17: 0.09
     rank 18: 0.09
     rank 19: 0.09
  optimizer:
     rank  0: 14.58
     rank  1: 14.55
     rank  2: 14.69
     rank  3: 14.85
     rank  4: 1.78
     rank  5: 1.76
     rank  6: 1.86
     rank  7: 1.67
     rank  8: 1.81
     rank  9: 1.79
     rank 10: 1.73
     rank 11: 1.84
     rank 12: 1.77
     rank 13: 1.93
     rank 14: 1.91
     rank 15: 1.98
     rank 16: 1.72
     rank 17: 1.71
     rank 18: 1.70
     rank 19: 1.70
 [2024-12-05 16:10:08] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 10428.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.078311E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10377.08
     rank  1: 10377.14
     rank  2: 10377.13
     rank  3: 10377.16
     rank  4: 10376.31
     rank  5: 10376.45
     rank  6: 10376.41
     rank  7: 10376.39
     rank  8: 10376.40
     rank  9: 10376.49
     rank 10: 10376.49
     rank 11: 10376.43
     rank 12: 10376.47
     rank 13: 10376.59
     rank 14: 10376.47
     rank 15: 10376.49
     rank 16: 10376.34
     rank 17: 10376.28
     rank 18: 10376.43
     rank 19: 10376.34
  forward-compute:
     rank  0: 3212.20
     rank  1: 3217.50
     rank  2: 3215.33
     rank  3: 3213.09
     rank  4: 4117.61
     rank  5: 4125.23
     rank  6: 4119.58
     rank  7: 4119.35
     rank  8: 3466.38
     rank  9: 3473.52
     rank 10: 3461.26
     rank 11: 3461.62
     rank 12: 2903.34
     rank 13: 2905.16
     rank 14: 2907.91
     rank 15: 2907.15
     rank 16: 3537.15
     rank 17: 3537.91
     rank 18: 3541.36
     rank 19: 3547.36
  backward-compute:
     rank  0: 3210.63
     rank  1: 3210.59
     rank  2: 3208.98
     rank  3: 3213.95
     rank  4: 4441.29
     rank  5: 4448.11
     rank  6: 4446.44
     rank  7: 4442.74
     rank  8: 3611.36
     rank  9: 3617.31
     rank 10: 3616.37
     rank 11: 3618.73
     rank 12: 2981.03
     rank 13: 2980.07
     rank 14: 2979.41
     rank 15: 2983.89
     rank 16: 3767.70
     rank 17: 3767.45
     rank 18: 3769.48
     rank 19: 3764.78
  pure-backward-compute:
     rank  0: 3209.89
     rank  1: 3209.77
     rank  2: 3208.19
     rank  3: 3213.30
     rank  4: 4440.43
     rank  5: 4447.16
     rank  6: 4445.26
     rank  7: 4441.74
     rank  8: 3609.85
     rank  9: 3616.15
     rank 10: 3615.03
     rank 11: 3617.54
     rank 12: 2979.67
     rank 13: 2977.28
     rank 14: 2977.72
     rank 15: 2982.54
     rank 16: 3765.38
     rank 17: 3765.60
     rank 18: 3767.56
     rank 19: 3762.27
  batch-generator:
     rank  0: 67.79
     rank  1: 71.58
     rank  2: 75.60
     rank  3: 68.71
     rank  4: 50.11
     rank  5: 60.11
     rank  6: 53.75
     rank  7: 54.66
     rank  8: 71.62
     rank  9: 83.23
     rank 10: 80.68
     rank 11: 76.93
     rank 12: 69.74
     rank 13: 76.19
     rank 14: 84.27
     rank 15: 81.21
     rank 16: 71.76
     rank 17: 71.84
     rank 18: 78.99
     rank 19: 87.25
  forward-recv:
     rank  4: 254.36
     rank  5: 251.89
     rank  6: 253.70
     rank  7: 254.56
     rank  8: 592.06
     rank  9: 591.53
     rank 10: 594.28
     rank 11: 593.69
     rank 12: 803.73
     rank 13: 803.78
     rank 14: 802.51
     rank 15: 804.16
     rank 16: 911.77
     rank 17: 911.56
     rank 18: 911.80
     rank 19: 911.00
  forward-send:
     rank  0: 213.61
     rank  1: 208.35
     rank  2: 211.74
     rank  3: 213.07
     rank  4: 34.23
     rank  5: 32.48
     rank  6: 33.57
     rank  7: 33.96
     rank  8: 21.34
     rank  9: 21.10
     rank 10: 20.21
     rank 11: 20.99
     rank 12: 10.83
     rank 13: 10.57
     rank 14: 10.89
     rank 15: 10.03
  backward-recv:
     rank  0: 1095.99
     rank  1: 1095.46
     rank  2: 1095.56
     rank  3: 1094.41
     rank  4: 470.56
     rank  5: 471.00
     rank  6: 468.55
     rank  7: 469.70
     rank  8: 391.87
     rank  9: 392.32
     rank 10: 393.90
     rank 11: 391.84
     rank 12: 269.09
     rank 13: 268.74
     rank 14: 269.25
     rank 15: 269.68
  backward-send:
     rank  4: 41.56
     rank  5: 40.79
     rank  6: 41.93
     rank  7: 41.03
     rank  8: 31.70
     rank  9: 31.50
     rank 10: 30.61
     rank 11: 30.77
     rank 12: 21.55
     rank 13: 21.61
     rank 14: 21.16
     rank 15: 20.65
     rank 16: 10.83
     rank 17: 10.34
     rank 18: 10.73
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 2627.22
     rank  1: 2627.10
     rank  2: 2629.33
     rank  3: 2626.40
     rank  4: 671.26
     rank  5: 666.65
     rank  6: 668.52
     rank  7: 670.15
     rank  8: 503.49
     rank  9: 502.47
     rank 10: 501.32
     rank 11: 501.64
     rank 12: 588.44
     rank 13: 586.82
     rank 14: 590.67
     rank 15: 587.30
  backward-send-forward-recv:
     rank  4: 133.86
     rank  5: 130.66
     rank  6: 133.75
     rank  7: 133.21
     rank  8: 1252.33
     rank  9: 1246.45
     rank 10: 1256.09
     rank 11: 1255.88
     rank 12: 2059.55
     rank 13: 2056.22
     rank 14: 2053.53
     rank 15: 2055.78
     rank 16: 1211.97
     rank 17: 1214.04
     rank 18: 1208.21
     rank 19: 1205.18
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.08
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.63
     rank  2: 0.67
     rank  3: 0.68
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.07
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.19
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.04
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.24
     rank  1: 10.43
     rank  2: 10.57
     rank  3: 10.58
     rank  4: 0.03
     rank  5: 0.07
     rank  6: 0.08
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.07
     rank 10: 0.11
     rank 11: 0.12
     rank 12: 0.10
     rank 13: 0.11
     rank 14: 0.08
     rank 15: 0.14
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.09
     rank 19: 0.07
  optimizer:
     rank  0: 11.02
     rank  1: 11.22
     rank  2: 11.36
     rank  3: 11.37
     rank  4: 0.81
     rank  5: 0.85
     rank  6: 0.86
     rank  7: 0.81
     rank  8: 0.84
     rank  9: 0.87
     rank 10: 0.89
     rank 11: 0.91
     rank 12: 0.88
     rank 13: 0.89
     rank 14: 0.87
     rank 15: 0.93
     rank 16: 0.85
     rank 17: 0.86
     rank 18: 0.87
     rank 19: 0.85
 [2024-12-05 16:10:19] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 10399.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 6.932635E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10356.57
     rank  1: 10356.55
     rank  2: 10356.62
     rank  3: 10356.70
     rank  4: 10355.78
     rank  5: 10355.88
     rank  6: 10355.84
     rank  7: 10355.82
     rank  8: 10356.03
     rank  9: 10355.88
     rank 10: 10356.14
     rank 11: 10355.92
     rank 12: 10355.96
     rank 13: 10356.51
     rank 14: 10356.09
     rank 15: 10356.00
     rank 16: 10355.86
     rank 17: 10355.85
     rank 18: 10355.93
     rank 19: 10355.86
  forward-compute:
     rank  0: 3169.35
     rank  1: 3174.39
     rank  2: 3172.61
     rank  3: 3168.60
     rank  4: 4116.19
     rank  5: 4124.05
     rank  6: 4118.44
     rank  7: 4117.74
     rank  8: 3471.31
     rank  9: 3481.76
     rank 10: 3471.27
     rank 11: 3467.89
     rank 12: 2907.11
     rank 13: 2909.43
     rank 14: 2913.12
     rank 15: 2909.32
     rank 16: 3538.95
     rank 17: 3540.79
     rank 18: 3541.40
     rank 19: 3551.04
  backward-compute:
     rank  0: 3212.14
     rank  1: 3212.27
     rank  2: 3210.92
     rank  3: 3216.85
     rank  4: 4435.39
     rank  5: 4444.61
     rank  6: 4446.98
     rank  7: 4438.48
     rank  8: 3623.59
     rank  9: 3629.13
     rank 10: 3630.25
     rank 11: 3628.78
     rank 12: 2989.64
     rank 13: 2990.03
     rank 14: 2987.04
     rank 15: 2993.71
     rank 16: 3770.17
     rank 17: 3769.12
     rank 18: 3771.44
     rank 19: 3766.25
  pure-backward-compute:
     rank  0: 3211.26
     rank  1: 3211.59
     rank  2: 3210.23
     rank  3: 3215.87
     rank  4: 4434.57
     rank  5: 4443.61
     rank  6: 4446.09
     rank  7: 4437.60
     rank  8: 3622.12
     rank  9: 3627.86
     rank 10: 3629.10
     rank 11: 3627.53
     rank 12: 2988.13
     rank 13: 2987.44
     rank 14: 2985.33
     rank 15: 2992.58
     rank 16: 3767.64
     rank 17: 3767.26
     rank 18: 3769.62
     rank 19: 3763.94
  batch-generator:
     rank  0: 61.45
     rank  1: 67.54
     rank  2: 65.65
     rank  3: 63.66
     rank  4: 50.41
     rank  5: 60.35
     rank  6: 54.72
     rank  7: 54.97
     rank  8: 69.34
     rank  9: 83.47
     rank 10: 83.77
     rank 11: 76.29
     rank 12: 70.81
     rank 13: 81.25
     rank 14: 86.18
     rank 15: 80.86
     rank 16: 70.69
     rank 17: 71.90
     rank 18: 76.07
     rank 19: 88.06
  forward-recv:
     rank  4: 236.45
     rank  5: 234.10
     rank  6: 235.51
     rank  7: 236.28
     rank  8: 574.87
     rank  9: 574.49
     rank 10: 576.94
     rank 11: 576.93
     rank 12: 790.18
     rank 13: 789.47
     rank 14: 789.20
     rank 15: 791.28
     rank 16: 893.36
     rank 17: 893.59
     rank 18: 893.33
     rank 19: 892.09
  forward-send:
     rank  0: 254.29
     rank  1: 248.77
     rank  2: 251.45
     rank  3: 253.93
     rank  4: 33.29
     rank  5: 31.05
     rank  6: 32.05
     rank  7: 33.39
     rank  8: 21.48
     rank  9: 21.08
     rank 10: 20.20
     rank 11: 21.05
     rank 12: 10.57
     rank 13: 10.88
     rank 14: 10.67
     rank 15: 9.47
  backward-recv:
     rank  0: 1087.24
     rank  1: 1089.28
     rank  2: 1089.47
     rank  3: 1086.21
     rank  4: 472.65
     rank  5: 472.06
     rank  6: 469.53
     rank  7: 471.99
     rank  8: 391.28
     rank  9: 392.09
     rank 10: 393.53
     rank 11: 391.67
     rank 12: 269.42
     rank 13: 269.30
     rank 14: 270.01
     rank 15: 269.70
  backward-send:
     rank  4: 41.31
     rank  5: 41.44
     rank  6: 41.68
     rank  7: 40.98
     rank  8: 31.58
     rank  9: 31.40
     rank 10: 29.45
     rank 11: 31.16
     rank 12: 21.44
     rank 13: 21.79
     rank 14: 20.81
     rank 15: 20.97
     rank 16: 10.82
     rank 17: 10.30
     rank 18: 10.75
     rank 19: 10.56
  forward-send-backward-recv:
     rank  0: 2614.00
     rank  1: 2613.64
     rank  2: 2616.30
     rank  3: 2613.48
     rank  4: 674.60
     rank  5: 667.49
     rank  6: 668.23
     rank  7: 671.40
     rank  8: 501.85
     rank  9: 501.10
     rank 10: 498.04
     rank 11: 501.37
     rank 12: 587.44
     rank 13: 587.61
     rank 14: 590.29
     rank 15: 585.44
  backward-send-forward-recv:
     rank  4: 133.59
     rank  5: 130.40
     rank  6: 133.72
     rank  7: 133.06
     rank  8: 1235.80
     rank  9: 1226.38
     rank 10: 1234.50
     rank 11: 1237.46
     rank 12: 2040.96
     rank 13: 2037.13
     rank 14: 2033.88
     rank 15: 2038.88
     rank 16: 1205.44
     rank 17: 1206.39
     rank 18: 1204.15
     rank 19: 1198.12
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.05
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.10
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.65
     rank  2: 0.62
     rank  3: 0.69
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.08
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.37
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.21
     rank  1: 10.18
     rank  2: 10.23
     rank  3: 11.52
     rank  4: 0.03
     rank  5: 0.07
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.11
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.10
     rank 12: 0.09
     rank 13: 0.18
     rank 14: 0.06
     rank 15: 0.09
     rank 16: 0.05
     rank 17: 0.09
     rank 18: 0.08
     rank 19: 0.05
  optimizer:
     rank  0: 11.65
     rank  1: 11.62
     rank  2: 11.67
     rank  3: 13.04
     rank  4: 1.47
     rank  5: 1.51
     rank  6: 1.48
     rank  7: 1.48
     rank  8: 1.55
     rank  9: 1.51
     rank 10: 1.50
     rank 11: 1.54
     rank 12: 1.53
     rank 13: 1.63
     rank 14: 1.50
     rank 15: 1.52
     rank 16: 1.50
     rank 17: 1.53
     rank 18: 1.52
     rank 19: 1.50
 [2024-12-05 16:10:29] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 10419.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.560428E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10370.03
     rank  1: 10370.05
     rank  2: 10370.10
     rank  3: 10370.13
     rank  4: 10369.24
     rank  5: 10369.26
     rank  6: 10369.37
     rank  7: 10369.30
     rank  8: 10369.32
     rank  9: 10369.38
     rank 10: 10369.51
     rank 11: 10369.38
     rank 12: 10369.41
     rank 13: 10369.82
     rank 14: 10369.42
     rank 15: 10369.42
     rank 16: 10369.33
     rank 17: 10369.30
     rank 18: 10369.29
     rank 19: 10369.31
  forward-compute:
     rank  0: 3192.15
     rank  1: 3197.41
     rank  2: 3195.44
     rank  3: 3191.97
     rank  4: 4112.21
     rank  5: 4119.22
     rank  6: 4113.13
     rank  7: 4113.22
     rank  8: 3479.52
     rank  9: 3488.15
     rank 10: 3474.46
     rank 11: 3473.50
     rank 12: 2918.85
     rank 13: 2920.20
     rank 14: 2925.43
     rank 15: 2921.38
     rank 16: 3547.27
     rank 17: 3549.53
     rank 18: 3550.36
     rank 19: 3557.54
  backward-compute:
     rank  0: 3205.83
     rank  1: 3206.84
     rank  2: 3204.14
     rank  3: 3208.06
     rank  4: 4425.42
     rank  5: 4432.90
     rank  6: 4431.95
     rank  7: 4426.14
     rank  8: 3624.92
     rank  9: 3629.78
     rank 10: 3633.32
     rank 11: 3630.14
     rank 12: 3000.14
     rank 13: 2998.72
     rank 14: 2997.85
     rank 15: 3003.06
     rank 16: 3775.47
     rank 17: 3775.27
     rank 18: 3776.80
     rank 19: 3772.18
  pure-backward-compute:
     rank  0: 3205.08
     rank  1: 3206.17
     rank  2: 3203.40
     rank  3: 3207.12
     rank  4: 4424.47
     rank  5: 4432.03
     rank  6: 4430.82
     rank  7: 4425.02
     rank  8: 3623.40
     rank  9: 3628.56
     rank 10: 3632.24
     rank 11: 3628.93
     rank 12: 2998.63
     rank 13: 2997.20
     rank 14: 2996.13
     rank 15: 3001.96
     rank 16: 3773.10
     rank 17: 3773.37
     rank 18: 3775.06
     rank 19: 3769.87
  batch-generator:
     rank  0: 61.63
     rank  1: 68.04
     rank  2: 63.29
     rank  3: 69.18
     rank  4: 51.80
     rank  5: 60.02
     rank  6: 53.55
     rank  7: 55.77
     rank  8: 70.34
     rank  9: 82.69
     rank 10: 79.62
     rank 11: 74.48
     rank 12: 70.73
     rank 13: 79.60
     rank 14: 86.42
     rank 15: 80.65
     rank 16: 70.56
     rank 17: 72.26
     rank 18: 76.65
     rank 19: 85.99
  forward-recv:
     rank  4: 249.30
     rank  5: 246.72
     rank  6: 249.08
     rank  7: 249.48
     rank  8: 586.04
     rank  9: 585.48
     rank 10: 587.54
     rank 11: 587.38
     rank 12: 806.56
     rank 13: 806.64
     rank 14: 805.40
     rank 15: 806.67
     rank 16: 909.50
     rank 17: 909.26
     rank 18: 909.45
     rank 19: 908.93
  forward-send:
     rank  0: 231.51
     rank  1: 226.09
     rank  2: 229.80
     rank  3: 230.98
     rank  4: 33.76
     rank  5: 32.09
     rank  6: 33.01
     rank  7: 33.49
     rank  8: 21.35
     rank  9: 21.11
     rank 10: 20.37
     rank 11: 20.85
     rank 12: 10.85
     rank 13: 10.58
     rank 14: 10.93
     rank 15: 10.21
  backward-recv:
     rank  0: 1094.63
     rank  1: 1093.46
     rank  2: 1095.10
     rank  3: 1093.35
     rank  4: 479.85
     rank  5: 481.49
     rank  6: 477.15
     rank  7: 479.70
     rank  8: 393.41
     rank  9: 393.65
     rank 10: 395.10
     rank 11: 393.41
     rank 12: 268.81
     rank 13: 268.69
     rank 14: 269.23
     rank 15: 269.50
  backward-send:
     rank  4: 41.92
     rank  5: 40.14
     rank  6: 41.63
     rank  7: 41.39
     rank  8: 31.46
     rank  9: 31.38
     rank 10: 28.99
     rank 11: 31.16
     rank 12: 21.56
     rank 13: 21.49
     rank 14: 20.78
     rank 15: 20.80
     rank 16: 10.88
     rank 17: 10.43
     rank 18: 10.71
     rank 19: 10.60
  forward-send-backward-recv:
     rank  0: 2627.80
     rank  1: 2627.53
     rank  2: 2628.03
     rank  3: 2628.33
     rank  4: 678.60
     rank  5: 673.45
     rank  6: 677.03
     rank  7: 677.88
     rank  8: 503.47
     rank  9: 502.82
     rank 10: 498.30
     rank 11: 502.60
     rank 12: 582.45
     rank 13: 584.24
     rank 14: 585.45
     rank 15: 581.35
  backward-send-forward-recv:
     rank  4: 133.64
     rank  5: 131.04
     rank  6: 134.07
     rank  7: 133.46
     rank  8: 1225.91
     rank  9: 1218.52
     rank 10: 1229.95
     rank 11: 1231.22
     rank 12: 2021.82
     rank 13: 2020.34
     rank 14: 2013.84
     rank 15: 2019.99
     rank 16: 1189.43
     rank 17: 1190.00
     rank 18: 1187.20
     rank 19: 1182.72
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.64
     rank  2: 0.67
     rank  3: 0.93
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.19
     rank  3: 0.22
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.05
     rank 14: 0.01
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.06
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.28
     rank  1: 10.11
     rank  2: 10.37
     rank  3: 10.82
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.10
     rank 10: 0.10
     rank 11: 0.06
     rank 12: 0.09
     rank 13: 0.40
     rank 14: 0.08
     rank 15: 0.09
     rank 16: 0.04
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 11.36
     rank  1: 11.19
     rank  2: 11.45
     rank  3: 11.89
     rank  4: 1.11
     rank  5: 1.12
     rank  6: 1.15
     rank  7: 1.14
     rank  8: 1.12
     rank  9: 1.18
     rank 10: 1.18
     rank 11: 1.14
     rank 12: 1.16
     rank 13: 1.50
     rank 14: 1.16
     rank 15: 1.18
     rank 16: 1.12
     rank 17: 1.13
     rank 18: 1.13
     rank 19: 1.13
 [2024-12-05 16:10:40] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 10429.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.475786E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10377.53
     rank  1: 10377.45
     rank  2: 10377.47
     rank  3: 10377.49
     rank  4: 10376.73
     rank  5: 10376.69
     rank  6: 10376.68
     rank  7: 10376.72
     rank  8: 10376.77
     rank  9: 10376.91
     rank 10: 10376.76
     rank 11: 10376.76
     rank 12: 10376.85
     rank 13: 10377.08
     rank 14: 10376.86
     rank 15: 10376.84
     rank 16: 10376.74
     rank 17: 10376.71
     rank 18: 10376.71
     rank 19: 10376.74
  forward-compute:
     rank  0: 3189.69
     rank  1: 3198.99
     rank  2: 3195.15
     rank  3: 3194.11
     rank  4: 4118.26
     rank  5: 4128.09
     rank  6: 4120.88
     rank  7: 4119.95
     rank  8: 3487.22
     rank  9: 3495.22
     rank 10: 3482.29
     rank 11: 3481.23
     rank 12: 2928.08
     rank 13: 2929.96
     rank 14: 2934.58
     rank 15: 2931.71
     rank 16: 3548.77
     rank 17: 3551.32
     rank 18: 3550.87
     rank 19: 3559.37
  backward-compute:
     rank  0: 3207.11
     rank  1: 3203.80
     rank  2: 3201.91
     rank  3: 3203.52
     rank  4: 4431.46
     rank  5: 4439.04
     rank  6: 4439.02
     rank  7: 4432.41
     rank  8: 3629.76
     rank  9: 3635.20
     rank 10: 3639.38
     rank 11: 3638.07
     rank 12: 3007.53
     rank 13: 3006.42
     rank 14: 3005.07
     rank 15: 3010.77
     rank 16: 3781.82
     rank 17: 3781.33
     rank 18: 3783.44
     rank 19: 3778.86
  pure-backward-compute:
     rank  0: 3205.91
     rank  1: 3203.13
     rank  2: 3201.19
     rank  3: 3202.34
     rank  4: 4430.58
     rank  5: 4438.27
     rank  6: 4438.31
     rank  7: 4431.57
     rank  8: 3628.39
     rank  9: 3633.99
     rank 10: 3638.30
     rank 11: 3636.83
     rank 12: 3006.08
     rank 13: 3005.08
     rank 14: 3003.40
     rank 15: 3009.68
     rank 16: 3779.49
     rank 17: 3779.67
     rank 18: 3781.62
     rank 19: 3776.37
  batch-generator:
     rank  0: 80.26
     rank  1: 86.71
     rank  2: 82.37
     rank  3: 80.52
     rank  4: 53.48
     rank  5: 65.10
     rank  6: 57.10
     rank  7: 58.28
     rank  8: 73.30
     rank  9: 83.94
     rank 10: 81.81
     rank 11: 76.23
     rank 12: 68.12
     rank 13: 77.74
     rank 14: 83.67
     rank 15: 79.43
     rank 16: 70.68
     rank 17: 72.57
     rank 18: 76.07
     rank 19: 86.76
  forward-recv:
     rank  4: 248.77
     rank  5: 245.21
     rank  6: 248.17
     rank  7: 248.72
     rank  8: 590.39
     rank  9: 589.84
     rank 10: 592.29
     rank 11: 591.89
     rank 12: 804.86
     rank 13: 805.17
     rank 14: 803.92
     rank 15: 805.14
     rank 16: 909.96
     rank 17: 909.73
     rank 18: 910.09
     rank 19: 909.25
  forward-send:
     rank  0: 247.74
     rank  1: 241.20
     rank  2: 246.00
     rank  3: 247.10
     rank  4: 35.32
     rank  5: 33.34
     rank  6: 34.72
     rank  7: 35.07
     rank  8: 21.44
     rank  9: 21.16
     rank 10: 20.84
     rank 11: 20.77
     rank 12: 10.75
     rank 13: 10.35
     rank 14: 10.84
     rank 15: 9.95
  backward-recv:
     rank  0: 1092.74
     rank  1: 1095.18
     rank  2: 1095.91
     rank  3: 1093.49
     rank  4: 474.80
     rank  5: 475.34
     rank  6: 471.35
     rank  7: 474.47
     rank  8: 393.31
     rank  9: 393.78
     rank 10: 395.70
     rank 11: 393.34
     rank 12: 269.19
     rank 13: 268.89
     rank 14: 269.60
     rank 15: 269.89
  backward-send:
     rank  4: 41.32
     rank  5: 41.05
     rank  6: 41.88
     rank  7: 40.90
     rank  8: 31.46
     rank  9: 31.39
     rank 10: 28.93
     rank 11: 31.00
     rank 12: 21.60
     rank 13: 21.53
     rank 14: 21.26
     rank 15: 20.66
     rank 16: 10.78
     rank 17: 10.19
     rank 18: 10.76
     rank 19: 10.59
  forward-send-backward-recv:
     rank  0: 2620.93
     rank  1: 2620.23
     rank  2: 2622.79
     rank  3: 2621.08
     rank  4: 677.61
     rank  5: 672.75
     rank  6: 675.29
     rank  7: 677.01
     rank  8: 501.52
     rank  9: 503.28
     rank 10: 497.08
     rank 11: 501.06
     rank 12: 580.52
     rank 13: 581.94
     rank 14: 583.23
     rank 15: 578.81
  backward-send-forward-recv:
     rank  4: 134.51
     rank  5: 130.66
     rank  6: 134.38
     rank  7: 134.23
     rank  8: 1211.89
     rank  9: 1204.76
     rank 10: 1214.71
     rank 11: 1216.67
     rank 12: 2012.17
     rank 13: 2010.32
     rank 14: 2004.28
     rank 15: 2009.39
     rank 16: 1184.54
     rank 17: 1185.03
     rank 18: 1183.36
     rank 19: 1177.50
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.08
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.68
     rank  1: 0.63
     rank  2: 0.63
     rank  3: 0.67
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.08
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.38
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.04
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.03
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 12.35
     rank  1: 10.35
     rank  2: 10.44
     rank  3: 10.36
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.08
     rank  9: 0.11
     rank 10: 0.08
     rank 11: 0.08
     rank 12: 0.05
     rank 13: 0.08
     rank 14: 0.06
     rank 15: 0.08
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.04
     rank 19: 0.07
  optimizer:
     rank  0: 13.90
     rank  1: 11.94
     rank  2: 12.03
     rank  3: 11.96
     rank  4: 1.62
     rank  5: 1.61
     rank  6: 1.62
     rank  7: 1.62
     rank  8: 1.67
     rank  9: 1.70
     rank 10: 1.67
     rank 11: 1.67
     rank 12: 1.63
     rank 13: 1.66
     rank 14: 1.65
     rank 15: 1.67
     rank 16: 1.63
     rank 17: 1.67
     rank 18: 1.63
     rank 19: 1.67
 [2024-12-05 16:10:50] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 10416.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.278138E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10365.50
     rank  1: 10365.51
     rank  2: 10366.25
     rank  3: 10365.51
     rank  4: 10364.71
     rank  5: 10364.75
     rank  6: 10364.95
     rank  7: 10364.71
     rank  8: 10364.79
     rank  9: 10364.93
     rank 10: 10365.04
     rank 11: 10364.83
     rank 12: 10364.82
     rank 13: 10364.87
     rank 14: 10365.04
     rank 15: 10364.88
     rank 16: 10364.79
     rank 17: 10364.75
     rank 18: 10364.74
     rank 19: 10364.79
  forward-compute:
     rank  0: 3154.49
     rank  1: 3159.28
     rank  2: 3157.82
     rank  3: 3156.97
     rank  4: 4119.89
     rank  5: 4125.47
     rank  6: 4121.84
     rank  7: 4121.35
     rank  8: 3492.23
     rank  9: 3499.29
     rank 10: 3488.39
     rank 11: 3484.95
     rank 12: 2940.08
     rank 13: 2942.64
     rank 14: 2946.64
     rank 15: 2944.34
     rank 16: 3549.77
     rank 17: 3552.93
     rank 18: 3552.40
     rank 19: 3561.37
  backward-compute:
     rank  0: 3177.99
     rank  1: 3178.09
     rank  2: 3176.58
     rank  3: 3178.23
     rank  4: 4434.27
     rank  5: 4441.67
     rank  6: 4443.70
     rank  7: 4433.68
     rank  8: 3633.05
     rank  9: 3640.39
     rank 10: 3641.43
     rank 11: 3641.16
     rank 12: 3008.64
     rank 13: 3007.91
     rank 14: 3006.24
     rank 15: 3011.86
     rank 16: 3785.50
     rank 17: 3785.35
     rank 18: 3786.90
     rank 19: 3781.95
  pure-backward-compute:
     rank  0: 3177.23
     rank  1: 3177.40
     rank  2: 3175.86
     rank  3: 3177.23
     rank  4: 4433.42
     rank  5: 4440.83
     rank  6: 4442.78
     rank  7: 4432.92
     rank  8: 3631.57
     rank  9: 3639.24
     rank 10: 3640.34
     rank 11: 3639.98
     rank 12: 3007.14
     rank 13: 3006.63
     rank 14: 3004.60
     rank 15: 3010.79
     rank 16: 3783.20
     rank 17: 3783.49
     rank 18: 3785.05
     rank 19: 3779.64
  batch-generator:
     rank  0: 71.53
     rank  1: 74.42
     rank  2: 72.19
     rank  3: 70.34
     rank  4: 59.80
     rank  5: 64.86
     rank  6: 60.83
     rank  7: 62.34
     rank  8: 69.70
     rank  9: 80.32
     rank 10: 80.27
     rank 11: 72.54
     rank 12: 69.71
     rank 13: 79.40
     rank 14: 84.51
     rank 15: 81.13
     rank 16: 70.21
     rank 17: 72.87
     rank 18: 75.95
     rank 19: 87.18
  forward-recv:
     rank  4: 238.01
     rank  5: 236.04
     rank  6: 237.76
     rank  7: 237.62
     rank  8: 576.79
     rank  9: 576.66
     rank 10: 579.16
     rank 11: 578.33
     rank 12: 793.77
     rank 13: 793.05
     rank 14: 792.60
     rank 15: 794.42
     rank 16: 901.11
     rank 17: 901.04
     rank 18: 900.64
     rank 19: 899.72
  forward-send:
     rank  0: 267.26
     rank  1: 262.97
     rank  2: 265.76
     rank  3: 266.97
     rank  4: 34.13
     rank  5: 32.82
     rank  6: 33.31
     rank  7: 34.49
     rank  8: 21.36
     rank  9: 21.06
     rank 10: 20.24
     rank 11: 20.85
     rank 12: 10.67
     rank 13: 10.76
     rank 14: 10.54
     rank 15: 9.51
  backward-recv:
     rank  0: 1107.62
     rank  1: 1107.73
     rank  2: 1108.78
     rank  3: 1107.30
     rank  4: 473.83
     rank  5: 473.73
     rank  6: 470.29
     rank  7: 473.83
     rank  8: 397.10
     rank  9: 397.34
     rank 10: 399.19
     rank 11: 397.04
     rank 12: 270.78
     rank 13: 270.70
     rank 14: 271.04
     rank 15: 271.48
  backward-send:
     rank  4: 41.42
     rank  5: 40.84
     rank  6: 41.72
     rank  7: 40.79
     rank  8: 31.49
     rank  9: 31.48
     rank 10: 29.10
     rank 11: 31.30
     rank 12: 21.61
     rank 13: 21.59
     rank 14: 21.14
     rank 15: 20.91
     rank 16: 10.85
     rank 17: 10.48
     rank 18: 10.70
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 2638.97
     rank  1: 2638.37
     rank  2: 2640.67
     rank  3: 2638.62
     rank  4: 676.04
     rank  5: 670.96
     rank  6: 672.18
     rank  7: 676.61
     rank  8: 503.69
     rank  9: 504.16
     rank 10: 500.78
     rank 11: 503.55
     rank 12: 581.37
     rank 13: 582.22
     rank 14: 583.71
     rank 15: 579.65
  backward-send-forward-recv:
     rank  4: 134.50
     rank  5: 132.31
     rank  6: 134.47
     rank  7: 134.18
     rank  8: 1203.07
     rank  9: 1196.57
     rank 10: 1205.13
     rank 11: 1209.23
     rank 12: 1999.24
     rank 13: 1997.03
     rank 14: 1992.51
     rank 15: 1996.13
     rank 16: 1179.31
     rank 17: 1178.75
     rank 18: 1178.37
     rank 19: 1172.34
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.06
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.14
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.62
     rank  2: 0.97
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.35
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.04
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.03
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.45
     rank  1: 10.19
     rank  2: 12.25
     rank  3: 10.10
     rank  4: 0.03
     rank  5: 0.06
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.10
     rank 11: 0.06
     rank 12: 0.08
     rank 13: 0.07
     rank 14: 0.08
     rank 15: 0.05
     rank 16: 0.08
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.05
  optimizer:
     rank  0: 11.87
     rank  1: 11.61
     rank  2: 13.64
     rank  3: 11.53
     rank  4: 1.45
     rank  5: 1.49
     rank  6: 1.46
     rank  7: 1.45
     rank  8: 1.47
     rank  9: 1.48
     rank 10: 1.52
     rank 11: 1.48
     rank 12: 1.50
     rank 13: 1.49
     rank 14: 1.50
     rank 15: 1.47
     rank 16: 1.50
     rank 17: 1.47
     rank 18: 1.49
     rank 19: 1.47
 [2024-12-05 16:11:00] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 10421.5 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.594132E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10374.14
     rank  1: 10374.15
     rank  2: 10374.15
     rank  3: 10374.16
     rank  4: 10373.38
     rank  5: 10373.40
     rank  6: 10373.37
     rank  7: 10373.37
     rank  8: 10373.44
     rank  9: 10373.53
     rank 10: 10373.49
     rank 11: 10373.41
     rank 12: 10373.51
     rank 13: 10373.72
     rank 14: 10373.52
     rank 15: 10373.45
     rank 16: 10373.41
     rank 17: 10373.41
     rank 18: 10373.41
     rank 19: 10373.41
  forward-compute:
     rank  0: 3172.12
     rank  1: 3175.60
     rank  2: 3169.97
     rank  3: 3172.51
     rank  4: 4114.25
     rank  5: 4120.31
     rank  6: 4115.29
     rank  7: 4116.23
     rank  8: 3500.08
     rank  9: 3507.90
     rank 10: 3496.75
     rank 11: 3494.55
     rank 12: 2946.18
     rank 13: 2949.80
     rank 14: 2952.58
     rank 15: 2952.03
     rank 16: 3555.33
     rank 17: 3558.01
     rank 18: 3557.34
     rank 19: 3566.96
  backward-compute:
     rank  0: 3183.37
     rank  1: 3184.84
     rank  2: 3186.54
     rank  3: 3185.84
     rank  4: 4444.43
     rank  5: 4452.47
     rank  6: 4452.27
     rank  7: 4445.00
     rank  8: 3636.51
     rank  9: 3644.34
     rank 10: 3645.31
     rank 11: 3647.09
     rank 12: 3015.90
     rank 13: 3015.79
     rank 14: 3013.58
     rank 15: 3020.69
     rank 16: 3783.26
     rank 17: 3783.10
     rank 18: 3785.32
     rank 19: 3780.09
  pure-backward-compute:
     rank  0: 3182.67
     rank  1: 3184.17
     rank  2: 3185.74
     rank  3: 3184.90
     rank  4: 4443.66
     rank  5: 4451.75
     rank  6: 4451.50
     rank  7: 4444.24
     rank  8: 3635.17
     rank  9: 3643.12
     rank 10: 3644.24
     rank 11: 3645.94
     rank 12: 3014.44
     rank 13: 3014.01
     rank 14: 3011.90
     rank 15: 3019.61
     rank 16: 3780.96
     rank 17: 3781.27
     rank 18: 3783.44
     rank 19: 3777.80
  batch-generator:
     rank  0: 58.25
     rank  1: 63.50
     rank  2: 59.57
     rank  3: 58.91
     rank  4: 52.58
     rank  5: 60.38
     rank  6: 54.52
     rank  7: 57.02
     rank  8: 70.89
     rank  9: 82.36
     rank 10: 81.63
     rank 11: 75.45
     rank 12: 70.16
     rank 13: 81.47
     rank 14: 85.44
     rank 15: 83.32
     rank 16: 69.81
     rank 17: 71.86
     rank 18: 74.95
     rank 19: 86.84
  forward-recv:
     rank  4: 237.70
     rank  5: 234.74
     rank  6: 237.37
     rank  7: 237.36
     rank  8: 572.55
     rank  9: 572.84
     rank 10: 575.66
     rank 11: 574.59
     rank 12: 788.59
     rank 13: 788.65
     rank 14: 786.98
     rank 15: 788.78
     rank 16: 896.84
     rank 17: 896.81
     rank 18: 896.76
     rank 19: 896.50
  forward-send:
     rank  0: 202.98
     rank  1: 198.28
     rank  2: 202.80
     rank  3: 203.57
     rank  4: 33.17
     rank  5: 32.02
     rank  6: 33.37
     rank  7: 33.89
     rank  8: 21.39
     rank  9: 20.99
     rank 10: 20.07
     rank 11: 20.92
     rank 12: 10.84
     rank 13: 10.65
     rank 14: 10.85
     rank 15: 10.49
  backward-recv:
     rank  0: 1105.07
     rank  1: 1104.56
     rank  2: 1103.86
     rank  3: 1102.90
     rank  4: 479.48
     rank  5: 479.60
     rank  6: 476.18
     rank  7: 479.01
     rank  8: 394.60
     rank  9: 394.97
     rank 10: 397.15
     rank 11: 394.58
     rank 12: 270.83
     rank 13: 270.64
     rank 14: 271.04
     rank 15: 271.54
  backward-send:
     rank  4: 41.53
     rank  5: 41.00
     rank  6: 41.74
     rank  7: 41.16
     rank  8: 31.51
     rank  9: 31.36
     rank 10: 29.11
     rank 11: 31.23
     rank 12: 21.58
     rank 13: 21.54
     rank 14: 21.35
     rank 15: 20.68
     rank 16: 10.84
     rank 17: 10.35
     rank 18: 10.70
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 2693.49
     rank  1: 2693.30
     rank  2: 2695.43
     rank  3: 2692.85
     rank  4: 678.11
     rank  5: 673.72
     rank  6: 675.88
     rank  7: 677.34
     rank  8: 502.55
     rank  9: 504.36
     rank 10: 499.66
     rank 11: 501.79
     rank 12: 580.31
     rank 13: 581.03
     rank 14: 582.89
     rank 15: 577.34
  backward-send-forward-recv:
     rank  4: 133.82
     rank  5: 131.57
     rank  6: 133.71
     rank  7: 133.06
     rank  8: 1210.49
     rank  9: 1202.69
     rank 10: 1212.14
     rank 11: 1214.23
     rank 12: 2004.59
     rank 13: 2000.34
     rank 14: 1997.83
     rank 15: 1999.15
     rank 16: 1193.03
     rank 17: 1192.91
     rank 18: 1191.60
     rank 19: 1184.69
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.63
     rank  2: 0.63
     rank  3: 0.64
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.44
     rank  1: 10.03
     rank  2: 10.18
     rank  3: 10.48
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.09
     rank  9: 0.04
     rank 10: 0.07
     rank 11: 0.05
     rank 12: 0.09
     rank 13: 0.10
     rank 14: 0.05
     rank 15: 0.07
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.05
     rank 19: 0.04
  optimizer:
     rank  0: 11.20
     rank  1: 10.80
     rank  2: 10.94
     rank  3: 11.25
     rank  4: 0.80
     rank  5: 0.80
     rank  6: 0.79
     rank  7: 0.80
     rank  8: 0.85
     rank  9: 0.80
     rank 10: 0.83
     rank 11: 0.81
     rank 12: 0.85
     rank 13: 0.86
     rank 14: 0.81
     rank 15: 0.83
     rank 16: 0.81
     rank 17: 0.84
     rank 18: 0.81
     rank 19: 0.81
 [2024-12-05 16:11:11] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 10421.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.972523E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10373.83
     rank  1: 10373.85
     rank  2: 10373.83
     rank  3: 10373.85
     rank  4: 10373.07
     rank  5: 10373.06
     rank  6: 10373.09
     rank  7: 10373.04
     rank  8: 10373.12
     rank  9: 10373.09
     rank 10: 10373.10
     rank 11: 10373.08
     rank 12: 10373.17
     rank 13: 10373.24
     rank 14: 10373.14
     rank 15: 10373.16
     rank 16: 10373.08
     rank 17: 10373.06
     rank 18: 10373.07
     rank 19: 10373.09
  forward-compute:
     rank  0: 3204.96
     rank  1: 3208.99
     rank  2: 3205.12
     rank  3: 3205.94
     rank  4: 4125.13
     rank  5: 4134.80
     rank  6: 4128.05
     rank  7: 4127.37
     rank  8: 3495.69
     rank  9: 3505.11
     rank 10: 3491.73
     rank 11: 3490.89
     rank 12: 2957.32
     rank 13: 2959.09
     rank 14: 2963.44
     rank 15: 2961.38
     rank 16: 3560.22
     rank 17: 3563.75
     rank 18: 3561.84
     rank 19: 3570.86
  backward-compute:
     rank  0: 3182.81
     rank  1: 3187.31
     rank  2: 3185.45
     rank  3: 3184.65
     rank  4: 4433.97
     rank  5: 4441.59
     rank  6: 4441.45
     rank  7: 4435.66
     rank  8: 3637.41
     rank  9: 3643.90
     rank 10: 3643.24
     rank 11: 3645.49
     rank 12: 3026.19
     rank 13: 3025.56
     rank 14: 3023.66
     rank 15: 3029.29
     rank 16: 3794.91
     rank 17: 3794.20
     rank 18: 3796.46
     rank 19: 3791.64
  pure-backward-compute:
     rank  0: 3182.01
     rank  1: 3186.59
     rank  2: 3184.77
     rank  3: 3183.68
     rank  4: 4433.13
     rank  5: 4440.88
     rank  6: 4440.66
     rank  7: 4434.85
     rank  8: 3635.96
     rank  9: 3642.57
     rank 10: 3642.18
     rank 11: 3644.40
     rank 12: 3024.71
     rank 13: 3024.26
     rank 14: 3021.95
     rank 15: 3028.21
     rank 16: 3792.61
     rank 17: 3792.53
     rank 18: 3794.77
     rank 19: 3789.37
  batch-generator:
     rank  0: 56.97
     rank  1: 63.44
     rank  2: 64.40
     rank  3: 60.64
     rank  4: 55.19
     rank  5: 66.31
     rank  6: 58.75
     rank  7: 59.63
     rank  8: 74.13
     rank  9: 86.11
     rank 10: 83.31
     rank 11: 78.28
     rank 12: 69.21
     rank 13: 78.29
     rank 14: 84.50
     rank 15: 81.36
     rank 16: 70.78
     rank 17: 73.81
     rank 18: 75.63
     rank 19: 86.76
  forward-recv:
     rank  4: 234.72
     rank  5: 231.41
     rank  6: 234.12
     rank  7: 234.41
     rank  8: 574.87
     rank  9: 573.87
     rank 10: 576.57
     rank 11: 576.42
     rank 12: 790.27
     rank 13: 790.79
     rank 14: 788.75
     rank 15: 790.59
     rank 16: 894.91
     rank 17: 894.62
     rank 18: 894.87
     rank 19: 894.21
  forward-send:
     rank  0: 268.58
     rank  1: 261.96
     rank  2: 266.49
     rank  3: 268.42
     rank  4: 33.86
     rank  5: 31.56
     rank  6: 32.88
     rank  7: 33.91
     rank  8: 21.34
     rank  9: 21.06
     rank 10: 20.22
     rank 11: 20.88
     rank 12: 10.81
     rank 13: 10.53
     rank 14: 10.83
     rank 15: 10.09
  backward-recv:
     rank  0: 1112.30
     rank  1: 1111.47
     rank  2: 1112.27
     rank  3: 1111.09
     rank  4: 484.13
     rank  5: 484.99
     rank  6: 480.78
     rank  7: 483.79
     rank  8: 398.26
     rank  9: 398.60
     rank 10: 400.53
     rank 11: 398.52
     rank 12: 270.03
     rank 13: 269.79
     rank 14: 270.33
     rank 15: 270.71
  backward-send:
     rank  4: 41.62
     rank  5: 40.80
     rank  6: 41.83
     rank  7: 40.79
     rank  8: 31.37
     rank  9: 31.35
     rank 10: 28.97
     rank 11: 30.92
     rank 12: 21.57
     rank 13: 21.54
     rank 14: 21.16
     rank 15: 20.79
     rank 16: 10.84
     rank 17: 10.36
     rank 18: 10.76
     rank 19: 10.54
  forward-send-backward-recv:
     rank  0: 2587.06
     rank  1: 2586.34
     rank  2: 2588.76
     rank  3: 2586.49
     rank  4: 673.72
     rank  5: 669.46
     rank  6: 671.76
     rank  7: 673.15
     rank  8: 503.36
     rank  9: 503.52
     rank 10: 501.84
     rank 11: 501.93
     rank 12: 581.28
     rank 13: 582.20
     rank 14: 583.76
     rank 15: 579.58
  backward-send-forward-recv:
     rank  4: 133.77
     rank  5: 130.31
     rank  6: 133.77
     rank  7: 133.17
     rank  8: 1204.55
     rank  9: 1196.70
     rank 10: 1207.53
     rank 11: 1208.30
     rank 12: 1975.24
     rank 13: 1973.09
     rank 14: 1969.14
     rank 15: 1971.88
     rank 16: 1172.57
     rank 17: 1172.18
     rank 18: 1171.83
     rank 19: 1165.66
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.66
     rank  2: 0.62
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.19
     rank  2: 0.18
     rank  3: 0.19
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.26
     rank  1: 10.56
     rank  2: 10.20
     rank  3: 10.14
     rank  4: 0.07
     rank  5: 0.03
     rank  6: 0.07
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.08
     rank 11: 0.09
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.09
     rank 15: 0.09
     rank 16: 0.04
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.07
  optimizer:
     rank  0: 10.97
     rank  1: 11.28
     rank  2: 10.92
     rank  3: 10.86
     rank  4: 0.78
     rank  5: 0.75
     rank  6: 0.79
     rank  7: 0.75
     rank  8: 0.77
     rank  9: 0.78
     rank 10: 0.80
     rank 11: 0.81
     rank 12: 0.77
     rank 13: 0.78
     rank 14: 0.81
     rank 15: 0.81
     rank 16: 0.77
     rank 17: 0.77
     rank 18: 0.76
     rank 19: 0.80
 [2024-12-05 16:11:21] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 10445.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.886439E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10397.76
     rank  1: 10397.83
     rank  2: 10397.78
     rank  3: 10397.79
     rank  4: 10396.98
     rank  5: 10397.05
     rank  6: 10397.00
     rank  7: 10397.02
     rank  8: 10397.17
     rank  9: 10397.11
     rank 10: 10397.21
     rank 11: 10397.12
     rank 12: 10397.12
     rank 13: 10397.21
     rank 14: 10397.15
     rank 15: 10397.11
     rank 16: 10397.05
     rank 17: 10397.11
     rank 18: 10397.05
     rank 19: 10397.05
  forward-compute:
     rank  0: 3250.43
     rank  1: 3252.89
     rank  2: 3249.19
     rank  3: 3250.63
     rank  4: 4135.24
     rank  5: 4143.93
     rank  6: 4137.35
     rank  7: 4137.04
     rank  8: 3501.91
     rank  9: 3512.19
     rank 10: 3499.50
     rank 11: 3497.60
     rank 12: 2962.67
     rank 13: 2964.74
     rank 14: 2969.63
     rank 15: 2965.25
     rank 16: 3561.49
     rank 17: 3565.17
     rank 18: 3564.00
     rank 19: 3574.51
  backward-compute:
     rank  0: 3187.28
     rank  1: 3192.04
     rank  2: 3189.49
     rank  3: 3188.22
     rank  4: 4435.72
     rank  5: 4445.26
     rank  6: 4444.49
     rank  7: 4438.20
     rank  8: 3649.36
     rank  9: 3656.81
     rank 10: 3657.39
     rank 11: 3655.89
     rank 12: 3037.58
     rank 13: 3037.10
     rank 14: 3036.35
     rank 15: 3042.81
     rank 16: 3800.74
     rank 17: 3800.27
     rank 18: 3801.96
     rank 19: 3796.80
  pure-backward-compute:
     rank  0: 3186.38
     rank  1: 3191.35
     rank  2: 3188.72
     rank  3: 3187.51
     rank  4: 4434.79
     rank  5: 4444.51
     rank  6: 4443.75
     rank  7: 4437.32
     rank  8: 3647.73
     rank  9: 3655.63
     rank 10: 3656.04
     rank 11: 3654.73
     rank 12: 3036.06
     rank 13: 3035.81
     rank 14: 3034.40
     rank 15: 3041.78
     rank 16: 3798.33
     rank 17: 3798.59
     rank 18: 3800.15
     rank 19: 3794.49
  batch-generator:
     rank  0: 54.81
     rank  1: 61.32
     rank  2: 61.26
     rank  3: 58.85
     rank  4: 56.89
     rank  5: 65.74
     rank  6: 58.19
     rank  7: 59.40
     rank  8: 72.86
     rank  9: 85.75
     rank 10: 83.77
     rank 11: 77.58
     rank 12: 68.41
     rank 13: 77.54
     rank 14: 83.17
     rank 15: 78.45
     rank 16: 70.69
     rank 17: 73.96
     rank 18: 76.53
     rank 19: 89.33
  forward-recv:
     rank  4: 237.63
     rank  5: 234.62
     rank  6: 237.22
     rank  7: 237.31
     rank  8: 574.83
     rank  9: 574.60
     rank 10: 577.37
     rank 11: 576.52
     rank 12: 791.93
     rank 13: 792.08
     rank 14: 790.30
     rank 15: 791.94
     rank 16: 901.57
     rank 17: 901.06
     rank 18: 901.30
     rank 19: 900.80
  forward-send:
     rank  0: 261.91
     rank  1: 257.07
     rank  2: 261.73
     rank  3: 262.49
     rank  4: 33.34
     rank  5: 32.13
     rank  6: 33.45
     rank  7: 33.93
     rank  8: 21.27
     rank  9: 21.02
     rank 10: 20.13
     rank 11: 20.81
     rank 12: 10.88
     rank 13: 10.52
     rank 14: 10.84
     rank 15: 10.23
  backward-recv:
     rank  0: 1106.22
     rank  1: 1105.52
     rank  2: 1106.22
     rank  3: 1106.26
     rank  4: 481.20
     rank  5: 481.27
     rank  6: 477.75
     rank  7: 480.81
     rank  8: 395.32
     rank  9: 395.55
     rank 10: 397.26
     rank 11: 394.90
     rank 12: 269.28
     rank 13: 268.91
     rank 14: 269.54
     rank 15: 270.17
  backward-send:
     rank  4: 41.52
     rank  5: 40.83
     rank  6: 41.69
     rank  7: 41.06
     rank  8: 31.68
     rank  9: 31.49
     rank 10: 29.25
     rank 11: 31.25
     rank 12: 21.55
     rank 13: 21.48
     rank 14: 21.28
     rank 15: 20.48
     rank 16: 10.77
     rank 17: 10.17
     rank 18: 10.73
     rank 19: 10.58
  forward-send-backward-recv:
     rank  0: 2573.73
     rank  1: 2572.32
     rank  2: 2574.62
     rank  3: 2573.65
     rank  4: 684.99
     rank  5: 678.08
     rank  6: 681.01
     rank  7: 681.71
     rank  8: 507.94
     rank  9: 508.38
     rank 10: 505.81
     rank 11: 509.98
     rank 12: 588.13
     rank 13: 588.91
     rank 14: 589.21
     rank 15: 584.60
  backward-send-forward-recv:
     rank  4: 133.59
     rank  5: 129.84
     rank  6: 133.35
     rank  7: 133.10
     rank  8: 1209.88
     rank  9: 1200.32
     rank 10: 1210.16
     rank 11: 1212.94
     rank 12: 1975.84
     rank 13: 1974.01
     rank 14: 1968.89
     rank 15: 1974.35
     rank 16: 1183.54
     rank 17: 1183.33
     rank 18: 1182.70
     rank 19: 1174.83
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.04
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.10
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.65
     rank  2: 0.63
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.06
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.05
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.17
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.19
     rank  1: 10.36
     rank  2: 10.48
     rank  3: 10.47
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.10
     rank  9: 0.08
     rank 10: 0.07
     rank 11: 0.06
     rank 12: 0.07
     rank 13: 0.09
     rank 14: 0.05
     rank 15: 0.05
     rank 16: 0.08
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.05
  optimizer:
     rank  0: 10.95
     rank  1: 11.13
     rank  2: 11.26
     rank  3: 11.24
     rank  4: 0.81
     rank  5: 0.80
     rank  6: 0.81
     rank  7: 0.84
     rank  8: 0.88
     rank  9: 0.85
     rank 10: 0.84
     rank 11: 0.83
     rank 12: 0.84
     rank 13: 0.86
     rank 14: 0.82
     rank 15: 0.82
     rank 16: 0.85
     rank 17: 0.82
     rank 18: 0.82
     rank 19: 0.83
 [2024-12-05 16:11:32] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 10434.9 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.469390E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 10387.54
     rank  1: 10387.44
     rank  2: 10387.47
     rank  3: 10387.49
     rank  4: 10386.71
     rank  5: 10386.66
     rank  6: 10386.65
     rank  7: 10386.71
     rank  8: 10386.79
     rank  9: 10386.68
     rank 10: 10386.72
     rank 11: 10386.75
     rank 12: 10386.83
     rank 13: 10386.92
     rank 14: 10386.79
     rank 15: 10386.83
     rank 16: 10386.72
     rank 17: 10386.70
     rank 18: 10386.70
     rank 19: 10386.72
  forward-compute:
     rank  0: 3210.11
     rank  1: 3212.71
     rank  2: 3208.93
     rank  3: 3211.10
     rank  4: 4125.01
     rank  5: 4134.23
     rank  6: 4127.68
     rank  7: 4127.01
     rank  8: 3507.53
     rank  9: 3512.75
     rank 10: 3501.49
     rank 11: 3501.63
     rank 12: 2964.68
     rank 13: 2966.98
     rank 14: 2972.00
     rank 15: 2969.58
     rank 16: 3562.09
     rank 17: 3565.21
     rank 18: 3564.06
     rank 19: 3573.76
  backward-compute:
     rank  0: 3182.90
     rank  1: 3188.00
     rank  2: 3187.18
     rank  3: 3184.50
     rank  4: 4445.88
     rank  5: 4453.47
     rank  6: 4452.46
     rank  7: 4446.52
     rank  8: 3650.88
     rank  9: 3657.84
     rank 10: 3658.76
     rank 11: 3660.32
     rank 12: 3036.41
     rank 13: 3034.71
     rank 14: 3034.77
     rank 15: 3040.86
     rank 16: 3798.92
     rank 17: 3798.48
     rank 18: 3800.06
     rank 19: 3795.74
  pure-backward-compute:
     rank  0: 3181.96
     rank  1: 3187.22
     rank  2: 3186.38
     rank  3: 3183.65
     rank  4: 4445.10
     rank  5: 4452.76
     rank  6: 4451.69
     rank  7: 4445.58
     rank  8: 3649.42
     rank  9: 3656.70
     rank 10: 3657.70
     rank 11: 3659.19
     rank 12: 3034.92
     rank 13: 3033.40
     rank 14: 3032.98
     rank 15: 3039.80
     rank 16: 3796.63
     rank 17: 3796.81
     rank 18: 3798.33
     rank 19: 3793.36
  batch-generator:
     rank  0: 56.48
     rank  1: 69.80
     rank  2: 68.58
     rank  3: 66.47
     rank  4: 49.89
     rank  5: 60.27
     rank  6: 53.01
     rank  7: 54.57
     rank  8: 68.60
     rank  9: 77.21
     rank 10: 76.70
     rank 11: 72.52
     rank 12: 69.56
     rank 13: 79.49
     rank 14: 84.45
     rank 15: 82.02
     rank 16: 69.85
     rank 17: 72.30
     rank 18: 74.85
     rank 19: 86.90
  forward-recv:
     rank  4: 238.01
     rank  5: 234.56
     rank  6: 236.99
     rank  7: 237.55
     rank  8: 576.50
     rank  9: 576.34
     rank 10: 579.37
     rank 11: 578.50
     rank 12: 791.19
     rank 13: 791.42
     rank 14: 789.59
     rank 15: 791.55
     rank 16: 898.74
     rank 17: 898.42
     rank 18: 898.70
     rank 19: 898.06
  forward-send:
     rank  0: 210.61
     rank  1: 205.02
     rank  2: 209.47
     rank  3: 210.40
     rank  4: 32.94
     rank  5: 31.27
     rank  6: 32.67
     rank  7: 33.01
     rank  8: 21.41
     rank  9: 21.01
     rank 10: 20.15
     rank 11: 20.90
     rank 12: 10.83
     rank 13: 10.48
     rank 14: 10.87
     rank 15: 10.07
  backward-recv:
     rank  0: 1110.51
     rank  1: 1110.71
     rank  2: 1110.44
     rank  3: 1109.89
     rank  4: 478.67
     rank  5: 478.13
     rank  6: 475.36
     rank  7: 478.58
     rank  8: 396.51
     rank  9: 396.91
     rank 10: 398.57
     rank 11: 396.23
     rank 12: 268.84
     rank 13: 269.61
     rank 14: 269.18
     rank 15: 269.56
  backward-send:
     rank  4: 41.51
     rank  5: 41.26
     rank  6: 41.71
     rank  7: 40.65
     rank  8: 31.52
     rank  9: 31.35
     rank 10: 29.14
     rank 11: 31.14
     rank 12: 21.59
     rank 13: 21.65
     rank 14: 21.18
     rank 15: 20.60
     rank 16: 10.26
     rank 17: 11.26
     rank 18: 10.19
     rank 19: 10.03
  forward-send-backward-recv:
     rank  0: 2654.75
     rank  1: 2653.23
     rank  2: 2655.82
     rank  3: 2655.06
     rank  4: 679.55
     rank  5: 674.75
     rank  6: 678.12
     rank  7: 679.28
     rank  8: 503.14
     rank  9: 504.89
     rank 10: 501.54
     rank 11: 503.00
     rank 12: 576.19
     rank 13: 577.09
     rank 14: 578.26
     rank 15: 573.64
  backward-send-forward-recv:
     rank  4: 134.25
     rank  5: 130.71
     rank  6: 134.08
     rank  7: 133.78
     rank  8: 1194.19
     rank  9: 1189.77
     rank 10: 1198.31
     rank 11: 1198.67
     rank 12: 1979.54
     rank 13: 1977.38
     rank 14: 1972.21
     rank 15: 1975.34
     rank 16: 1177.87
     rank 17: 1177.83
     rank 18: 1177.31
     rank 19: 1170.04
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.67
     rank  1: 0.63
     rank  2: 0.66
     rank  3: 0.66
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.05
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.19
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.41
     rank  1: 10.15
     rank  2: 10.22
     rank  3: 10.22
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.04
     rank 10: 0.08
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.08
     rank 14: 0.09
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.04
  optimizer:
     rank  0: 11.26
     rank  1: 11.00
     rank  2: 11.07
     rank  3: 11.08
     rank  4: 0.88
     rank  5: 0.88
     rank  6: 0.88
     rank  7: 0.89
     rank  8: 0.90
     rank  9: 0.88
     rank 10: 0.93
     rank 11: 0.89
     rank 12: 0.91
     rank 13: 0.94
     rank 14: 0.94
     rank 15: 0.88
     rank 16: 0.89
     rank 17: 0.93
     rank 18: 0.92
     rank 19: 0.89
