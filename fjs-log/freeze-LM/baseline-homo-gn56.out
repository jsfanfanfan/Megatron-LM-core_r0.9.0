examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-04 23:20:01,479] torch.distributed.run: [WARNING] 
[2024-12-04 23:20:01,479] torch.distributed.run: [WARNING] *****************************************
[2024-12-04 23:20:01,479] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-04 23:20:01,479] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]

---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4][rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 91.20
     rank  1: 80.09
     rank  2: 81.59
     rank  3: 78.03
     rank  4: 31.73
     rank  5: 29.26
     rank  6: 29.29
     rank  7: 27.55
     rank  8: 80.33
     rank  9: 70.83
     rank 10: 80.87
     rank 11: 83.89
     rank 12: 28.17
     rank 13: 29.08
     rank 14: 30.88
     rank 15: 31.57
     rank 16: 110.39
     rank 17: 110.92
     rank 18: 118.79
     rank 19: 108.27
  train/valid/test-data-iterators-setup:
     rank  0: 964.09
     rank  1: 964.16
     rank  2: 963.83
     rank  3: 963.79
     rank  4: 963.83
     rank  5: 963.79
     rank  6: 964.05
     rank  7: 963.80
     rank  8: 963.86
     rank  9: 963.81
     rank 10: 963.82
     rank 11: 963.91
     rank 12: 963.85
     rank 13: 964.08
     rank 14: 963.95
     rank 15: 963.92
     rank 16: 1304.78
     rank 17: 1304.92
     rank 18: 1304.85
     rank 19: 1305.01
 [2024-12-04 23:21:18] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 32215.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.108734E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4520.0 | max reserved: 4520.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4540.0 | max reserved: 4540.0[Rank 17] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4540.0 | max reserved: 4540.0

[Rank 19] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4540.0 | max reserved: 4540.0
times across ranks (ms):
  forward-backward:
     rank  0: 32139.50
     rank  1: 32139.15
     rank  2: 32139.37
     rank  3: 32139.53
     rank  4: 32177.92
     rank  5: 32176.74
     rank  6: 32172.07
     rank  7: 32179.23
     rank  8: 32176.78
     rank  9: 32178.33
     rank 10: 32174.43
     rank 11: 32182.06
     rank 12: 32185.47
     rank 13: 32185.48
     rank 14: 32177.95
     rank 15: 32183.86
     rank 16: 32138.89
     rank 17: 32137.91
     rank 18: 32137.88
     rank 19: 32137.85
  forward-compute:
     rank  0: 13554.55
     rank  1: 13584.81
     rank  2: 13568.72
     rank  3: 13564.42
     rank  4: 5598.24
     rank  5: 5632.27
     rank  6: 5611.07
     rank  7: 5622.31
     rank  8: 5240.90
     rank  9: 5264.89
     rank 10: 5245.62
     rank 11: 5253.35
     rank 12: 5203.43
     rank 13: 5224.70
     rank 14: 5208.62
     rank 15: 5213.42
     rank 16: 4999.42
     rank 17: 5001.48
     rank 18: 5001.08
     rank 19: 5009.13
  backward-compute:
     rank  0: 3672.20
     rank  1: 3675.95
     rank  2: 3672.99
     rank  3: 3687.18
     rank  4: 2992.36
     rank  5: 2996.24
     rank  6: 2994.94
     rank  7: 2990.01
     rank  8: 3039.83
     rank  9: 3050.14
     rank 10: 3044.98
     rank 11: 3042.37
     rank 12: 2998.33
     rank 13: 2997.12
     rank 14: 3000.49
     rank 15: 2999.80
     rank 16: 3183.43
     rank 17: 3184.96
     rank 18: 3188.86
     rank 19: 3186.92
  pure-backward-compute:
     rank  0: 3671.25
     rank  1: 3674.92
     rank  2: 3672.14
     rank  3: 3685.55
     rank  4: 2990.59
     rank  5: 2995.14
     rank  6: 2993.22
     rank  7: 2988.92
     rank  8: 3038.40
     rank  9: 3049.40
     rank 10: 3044.02
     rank 11: 3041.04
     rank 12: 2997.00
     rank 13: 2995.99
     rank 14: 2999.30
     rank 15: 2998.61
     rank 16: 3180.83
     rank 17: 3182.60
     rank 18: 3187.09
     rank 19: 3185.26
  batch-generator:
     rank  0: 1108.85
     rank  1: 1141.21
     rank  2: 1123.48
     rank  3: 1134.82
     rank  4: 1351.43
     rank  5: 1387.25
     rank  6: 1368.35
     rank  7: 1373.29
     rank  8: 1071.23
     rank  9: 1098.07
     rank 10: 1079.48
     rank 11: 1088.82
     rank 12: 1128.51
     rank 13: 1153.73
     rank 14: 1137.65
     rank 15: 1142.34
     rank 16: 994.09
     rank 17: 997.26
     rank 18: 996.69
     rank 19: 1004.34
  forward-recv:
     rank  4: 12474.37
     rank  5: 12464.14
     rank  6: 12463.66
     rank  7: 12473.54
     rank  8: 15402.12
     rank  9: 15395.06
     rank 10: 15400.37
     rank 11: 15397.19
     rank 12: 18037.29
     rank 13: 18028.63
     rank 14: 18036.14
     rank 15: 18035.35
     rank 16: 20666.86
     rank 17: 20663.99
     rank 18: 20666.96
     rank 19: 20659.66
  forward-send:
     rank  0: 8159.47
     rank  1: 8129.70
     rank  2: 8145.35
     rank  3: 8145.01
     rank  4: 4952.20
     rank  5: 4933.19
     rank  6: 4949.26
     rank  7: 4939.83
     rank  8: 2493.24
     rank  9: 2481.79
     rank 10: 2492.34
     rank 11: 2486.20
     rank 12: 35.82
     rank 13: 32.94
     rank 14: 35.91
     rank 15: 28.43
  backward-recv:
     rank  0: 1286.12
     rank  1: 1285.62
     rank  2: 1286.43
     rank  3: 1282.19
     rank  4: 597.13
     rank  5: 597.73
     rank  6: 597.08
     rank  7: 599.73
     rank  8: 378.76
     rank  9: 380.17
     rank 10: 379.25
     rank 11: 377.92
     rank 12: 194.06
     rank 13: 194.14
     rank 14: 194.33
     rank 15: 194.82
  backward-send:
     rank  4: 22.39
     rank  5: 21.68
     rank  6: 22.45
     rank  7: 21.10
     rank  8: 31.35
     rank  9: 29.76
     rank 10: 30.84
     rank 11: 31.33
     rank 12: 20.99
     rank 13: 20.62
     rank 14: 20.80
     rank 15: 20.43
     rank 16: 10.70
     rank 17: 10.41
     rank 18: 10.29
     rank 19: 10.35
  forward-send-backward-recv:
     rank  0: 5404.16
     rank  1: 5399.00
     rank  2: 5403.40
     rank  3: 5392.34
     rank  4: 2936.00
     rank  5: 2934.91
     rank  6: 2935.67
     rank  7: 2938.58
     rank  8: 2709.15
     rank  9: 2702.66
     rank 10: 2708.61
     rank 11: 2705.57
     rank 12: 2563.47
     rank 13: 2563.82
     rank 14: 2561.35
     rank 15: 2561.18
  backward-send-forward-recv:
     rank  4: 2440.46
     rank  5: 2438.07
     rank  6: 2439.86
     rank  7: 2432.94
     rank  8: 2526.90
     rank  9: 2521.85
     rank 10: 2524.12
     rank 11: 2524.94
     rank 12: 2570.53
     rank 13: 2560.92
     rank 14: 2566.46
     rank 15: 2570.00
     rank 16: 2564.31
     rank 17: 2564.03
     rank 18: 2560.78
     rank 19: 2561.57
  layernorm-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.04
     rank  2: 0.06
     rank  3: 0.06
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.14
     rank  1: 0.11
     rank  2: 0.13
     rank  3: 0.16
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.10
     rank 17: 0.06
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 44.84
     rank  1: 43.06
     rank  2: 44.99
     rank  3: 47.91
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.24
     rank  2: 0.24
     rank  3: 0.38
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.02
     rank 10: 0.03
     rank 11: 0.13
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.05
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 25.93
     rank  1: 26.00
     rank  2: 25.76
     rank  3: 26.28
     rank  4: 0.12
     rank  5: 0.05
     rank  6: 0.08
     rank  7: 0.05
     rank  8: 0.12
     rank  9: 0.06
     rank 10: 0.12
     rank 11: 0.33
     rank 12: 0.17
     rank 13: 0.17
     rank 14: 0.14
     rank 15: 0.17
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.07
  optimizer:
     rank  0: 27.80
     rank  1: 27.87
     rank  2: 27.63
     rank  3: 28.15
     rank  4: 1.98
     rank  5: 1.92
     rank  6: 1.93
     rank  7: 1.92
     rank  8: 2.00
     rank  9: 1.92
     rank 10: 1.98
     rank 11: 2.17
     rank 12: 2.03
     rank 13: 2.05
     rank 14: 1.86
     rank 15: 2.05
     rank 16: 1.93
     rank 17: 1.94
     rank 18: 1.94
     rank 19: 1.94
 [2024-12-04 23:21:25] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7615.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.285778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7572.70
     rank  1: 7572.49
     rank  2: 7572.86
     rank  3: 7573.71
     rank  4: 7571.94
     rank  5: 7571.64
     rank  6: 7571.81
     rank  7: 7572.10
     rank  8: 7571.86
     rank  9: 7571.58
     rank 10: 7571.75
     rank 11: 7572.16
     rank 12: 7571.84
     rank 13: 7571.61
     rank 14: 7571.84
     rank 15: 7572.18
     rank 16: 7572.40
     rank 17: 7571.65
     rank 18: 7571.53
     rank 19: 7571.54
  forward-compute:
     rank  0: 1086.36
     rank  1: 1089.88
     rank  2: 1089.74
     rank  3: 1084.33
     rank  4: 2858.52
     rank  5: 2864.48
     rank  6: 2859.85
     rank  7: 2866.19
     rank  8: 2800.87
     rank  9: 2809.86
     rank 10: 2807.05
     rank 11: 2801.98
     rank 12: 2763.65
     rank 13: 2765.41
     rank 14: 2765.55
     rank 15: 2764.69
     rank 16: 2914.73
     rank 17: 2917.82
     rank 18: 2919.18
     rank 19: 2919.98
  backward-compute:
     rank  0: 1076.21
     rank  1: 1079.62
     rank  2: 1076.88
     rank  3: 1085.73
     rank  4: 2986.88
     rank  5: 2991.40
     rank  6: 2991.08
     rank  7: 2983.72
     rank  8: 3044.66
     rank  9: 3043.44
     rank 10: 3044.88
     rank 11: 3046.75
     rank 12: 2984.19
     rank 13: 2985.38
     rank 14: 2986.15
     rank 15: 2987.83
     rank 16: 3183.54
     rank 17: 3181.32
     rank 18: 3185.58
     rank 19: 3184.45
  pure-backward-compute:
     rank  0: 1075.47
     rank  1: 1078.90
     rank  2: 1076.17
     rank  3: 1084.79
     rank  4: 2985.44
     rank  5: 2990.68
     rank  6: 2990.07
     rank  7: 2983.04
     rank  8: 3042.99
     rank  9: 3042.73
     rank 10: 3043.96
     rank 11: 3044.66
     rank 12: 2983.11
     rank 13: 2984.36
     rank 14: 2985.14
     rank 15: 2986.74
     rank 16: 3180.65
     rank 17: 3179.31
     rank 18: 3183.81
     rank 19: 3182.58
  batch-generator:
     rank  0: 55.00
     rank  1: 60.11
     rank  2: 61.14
     rank  3: 57.52
     rank  4: 53.85
     rank  5: 62.71
     rank  6: 61.01
     rank  7: 63.61
     rank  8: 52.10
     rank  9: 64.47
     rank 10: 61.67
     rank 11: 57.66
     rank 12: 51.41
     rank 13: 56.74
     rank 14: 57.14
     rank 15: 55.37
     rank 16: 66.55
     rank 17: 68.70
     rank 18: 70.74
     rank 19: 71.37
  forward-recv:
     rank  4: 83.72
     rank  5: 83.53
     rank  6: 83.25
     rank  7: 83.11
     rank  8: 276.45
     rank  9: 276.05
     rank 10: 275.93
     rank 11: 275.95
     rank 12: 455.71
     rank 13: 455.37
     rank 14: 455.65
     rank 15: 455.83
     rank 16: 624.71
     rank 17: 624.58
     rank 18: 624.45
     rank 19: 624.76
  forward-send:
     rank  0: 390.75
     rank  1: 387.26
     rank  2: 386.99
     rank  3: 389.03
     rank  4: 36.19
     rank  5: 34.31
     rank  6: 34.14
     rank  7: 36.22
     rank  8: 21.13
     rank  9: 20.23
     rank 10: 20.07
     rank 11: 20.92
     rank 12: 10.53
     rank 13: 10.36
     rank 14: 10.13
     rank 15: 10.46
  backward-recv:
     rank  0: 1302.44
     rank  1: 1302.56
     rank  2: 1302.73
     rank  3: 1299.65
     rank  4: 598.63
     rank  5: 597.92
     rank  6: 597.81
     rank  7: 600.35
     rank  8: 379.04
     rank  9: 380.28
     rank 10: 379.22
     rank 11: 378.17
     rank 12: 196.16
     rank 13: 196.37
     rank 14: 196.71
     rank 15: 195.99
  backward-send:
     rank  4: 22.72
     rank  5: 22.13
     rank  6: 22.57
     rank  7: 21.17
     rank  8: 31.30
     rank  9: 29.71
     rank 10: 30.74
     rank 11: 31.37
     rank 12: 21.03
     rank 13: 20.84
     rank 14: 20.60
     rank 15: 20.42
     rank 16: 10.69
     rank 17: 10.40
     rank 18: 10.40
     rank 19: 10.01
  forward-send-backward-recv:
     rank  0: 3700.32
     rank  1: 3698.02
     rank  2: 3700.01
     rank  3: 3694.66
     rank  4: 829.47
     rank  5: 827.63
     rank  6: 826.67
     rank  7: 832.09
     rank  8: 601.01
     rank  9: 603.56
     rank 10: 602.51
     rank 11: 599.31
     rank 12: 508.56
     rank 13: 507.30
     rank 14: 507.79
     rank 15: 505.21
  backward-send-forward-recv:
     rank  4: 73.87
     rank  5: 72.32
     rank  6: 74.59
     rank  7: 70.84
     rank  8: 142.68
     rank  9: 137.36
     rank 10: 139.74
     rank 11: 141.90
     rank 12: 158.23
     rank 13: 156.76
     rank 14: 156.72
     rank 15: 157.73
     rank 16: 167.32
     rank 17: 167.66
     rank 18: 165.45
     rank 19: 165.05
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.05
     rank  3: 0.09
     rank  4: 0.03
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.11
     rank  3: 0.22
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.17
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.69
     rank  1: 0.69
     rank  2: 0.67
     rank  3: 1.66
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.18
     rank  2: 0.33
     rank  3: 0.66
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.08
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.41
     rank  1: 10.42
     rank  2: 11.54
     rank  3: 13.33
     rank  4: 0.07
     rank  5: 0.03
     rank  6: 0.06
     rank  7: 0.03
     rank  8: 0.07
     rank  9: 0.03
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.07
     rank 16: 0.20
     rank 17: 0.03
     rank 18: 0.06
     rank 19: 0.07
  optimizer:
     rank  0: 12.88
     rank  1: 12.88
     rank  2: 14.00
     rank  3: 15.76
     rank  4: 2.53
     rank  5: 2.49
     rank  6: 2.53
     rank  7: 2.50
     rank  8: 2.53
     rank  9: 2.49
     rank 10: 2.53
     rank 11: 2.54
     rank 12: 2.50
     rank 13: 2.49
     rank 14: 2.50
     rank 15: 2.53
     rank 16: 2.68
     rank 17: 2.50
     rank 18: 2.52
     rank 19: 2.53
 [2024-12-04 23:21:33] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7603.7 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.115121E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7562.95
     rank  1: 7562.96
     rank  2: 7562.90
     rank  3: 7562.94
     rank  4: 7562.15
     rank  5: 7562.08
     rank  6: 7562.09
     rank  7: 7562.06
     rank  8: 7562.12
     rank  9: 7562.64
     rank 10: 7562.03
     rank 11: 7562.12
     rank 12: 7562.09
     rank 13: 7562.20
     rank 14: 7562.10
     rank 15: 7562.14
     rank 16: 7562.18
     rank 17: 7562.14
     rank 18: 7561.98
     rank 19: 7561.97
  forward-compute:
     rank  0: 1283.00
     rank  1: 1292.41
     rank  2: 1285.51
     rank  3: 1281.36
     rank  4: 2856.59
     rank  5: 2867.66
     rank  6: 2857.21
     rank  7: 2862.69
     rank  8: 2796.18
     rank  9: 2804.30
     rank 10: 2800.89
     rank 11: 2796.70
     rank 12: 2769.73
     rank 13: 2771.59
     rank 14: 2773.04
     rank 15: 2769.86
     rank 16: 2903.37
     rank 17: 2905.72
     rank 18: 2907.06
     rank 19: 2912.97
  backward-compute:
     rank  0: 1063.55
     rank  1: 1066.00
     rank  2: 1065.61
     rank  3: 1071.10
     rank  4: 2993.73
     rank  5: 2996.77
     rank  6: 2996.50
     rank  7: 2990.49
     rank  8: 3035.12
     rank  9: 3035.36
     rank 10: 3037.48
     rank 11: 3037.32
     rank 12: 2993.47
     rank 13: 2993.04
     rank 14: 2993.16
     rank 15: 2999.40
     rank 16: 3181.84
     rank 17: 3181.74
     rank 18: 3185.73
     rank 19: 3184.74
  pure-backward-compute:
     rank  0: 1062.81
     rank  1: 1065.27
     rank  2: 1064.78
     rank  3: 1070.34
     rank  4: 2992.51
     rank  5: 2995.96
     rank  6: 2995.62
     rank  7: 2989.69
     rank  8: 3033.90
     rank  9: 3034.67
     rank 10: 3036.66
     rank 11: 3035.62
     rank 12: 2992.23
     rank 13: 2992.02
     rank 14: 2992.14
     rank 15: 2998.32
     rank 16: 3178.29
     rank 17: 3179.60
     rank 18: 3184.39
     rank 19: 3183.39
  batch-generator:
     rank  0: 61.79
     rank  1: 72.05
     rank  2: 70.69
     rank  3: 65.93
     rank  4: 57.33
     rank  5: 73.46
     rank  6: 63.86
     rank  7: 65.68
     rank  8: 52.17
     rank  9: 63.64
     rank 10: 60.52
     rank 11: 56.58
     rank 12: 51.77
     rank 13: 57.47
     rank 14: 58.70
     rank 15: 54.62
     rank 16: 66.76
     rank 17: 68.21
     rank 18: 68.73
     rank 19: 76.03
  forward-recv:
     rank  4: 84.19
     rank  5: 79.80
     rank  6: 84.14
     rank  7: 84.09
     rank  8: 279.95
     rank  9: 278.28
     rank 10: 279.75
     rank 11: 279.45
     rank 12: 457.56
     rank 13: 456.94
     rank 14: 457.62
     rank 15: 457.89
     rank 16: 629.91
     rank 17: 629.83
     rank 18: 629.52
     rank 19: 629.92
  forward-send:
     rank  0: 396.64
     rank  1: 387.27
     rank  2: 393.71
     rank  3: 396.08
     rank  4: 34.70
     rank  5: 31.03
     rank  6: 33.11
     rank  7: 34.87
     rank  8: 20.96
     rank  9: 19.79
     rank 10: 19.97
     rank 11: 20.87
     rank 12: 10.55
     rank 13: 10.45
     rank 14: 10.02
     rank 15: 10.42
  backward-recv:
     rank  0: 1295.72
     rank  1: 1296.07
     rank  2: 1294.61
     rank  3: 1293.91
     rank  4: 587.99
     rank  5: 588.79
     rank  6: 589.22
     rank  7: 590.07
     rank  8: 379.37
     rank  9: 380.24
     rank 10: 379.44
     rank 11: 378.63
     rank 12: 193.28
     rank 13: 193.56
     rank 14: 194.06
     rank 15: 192.57
  backward-send:
     rank  4: 22.53
     rank  5: 22.03
     rank  6: 21.41
     rank  7: 21.94
     rank  8: 31.22
     rank  9: 29.96
     rank 10: 30.82
     rank 11: 31.37
     rank 12: 20.93
     rank 13: 20.46
     rank 14: 20.36
     rank 15: 20.88
     rank 16: 10.66
     rank 17: 10.53
     rank 18: 10.29
     rank 19: 9.91
  forward-send-backward-recv:
     rank  0: 3509.24
     rank  1: 3507.71
     rank  2: 3509.46
     rank  3: 3504.33
     rank  4: 829.46
     rank  5: 828.31
     rank  6: 827.10
     rank  7: 832.14
     rank  8: 605.48
     rank  9: 608.55
     rank 10: 607.40
     rank 11: 603.88
     rank 12: 489.70
     rank 13: 489.75
     rank 14: 490.86
     rank 15: 485.34
  backward-send-forward-recv:
     rank  4: 74.62
     rank  5: 73.18
     rank  6: 75.25
     rank  7: 71.32
     rank  8: 143.64
     rank  9: 138.99
     rank 10: 139.96
     rank 11: 142.43
     rank 12: 157.67
     rank 13: 156.66
     rank 14: 155.11
     rank 15: 157.55
     rank 16: 168.98
     rank 17: 169.03
     rank 18: 167.72
     rank 19: 162.27
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.08
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.06
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.08
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.74
     rank  1: 0.67
     rank  2: 0.62
     rank  3: 0.66
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.17
     rank  2: 0.20
     rank  3: 0.29
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.04
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.04
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.12
     rank  1: 10.51
     rank  2: 10.34
     rank  3: 10.46
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.09
     rank 10: 0.08
     rank 11: 0.23
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.06
     rank 16: 0.09
     rank 17: 0.03
     rank 18: 0.04
     rank 19: 0.03
  optimizer:
     rank  0: 11.34
     rank  1: 11.73
     rank  2: 11.56
     rank  3: 11.67
     rank  4: 1.25
     rank  5: 1.25
     rank  6: 1.26
     rank  7: 1.28
     rank  8: 1.25
     rank  9: 1.34
     rank 10: 1.29
     rank 11: 1.45
     rank 12: 1.28
     rank 13: 1.25
     rank 14: 1.29
     rank 15: 1.28
     rank 16: 1.31
     rank 17: 1.25
     rank 18: 1.26
     rank 19: 1.25
 [2024-12-04 23:21:41] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7593.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.811421E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7551.88
     rank  1: 7552.38
     rank  2: 7551.89
     rank  3: 7551.99
     rank  4: 7551.06
     rank  5: 7551.55
     rank  6: 7551.07
     rank  7: 7551.07
     rank  8: 7551.05
     rank  9: 7551.66
     rank 10: 7551.06
     rank 11: 7551.09
     rank 12: 7551.04
     rank 13: 7551.56
     rank 14: 7551.11
     rank 15: 7551.09
     rank 16: 7551.03
     rank 17: 7552.00
     rank 18: 7550.94
     rank 19: 7550.96
  forward-compute:
     rank  0: 1023.31
     rank  1: 1027.37
     rank  2: 1033.46
     rank  3: 1020.92
     rank  4: 2854.95
     rank  5: 2860.56
     rank  6: 2865.37
     rank  7: 2860.03
     rank  8: 2796.98
     rank  9: 2795.95
     rank 10: 2806.66
     rank 11: 2796.53
     rank 12: 2774.73
     rank 13: 2774.77
     rank 14: 2778.10
     rank 15: 2777.91
     rank 16: 2902.25
     rank 17: 2903.65
     rank 18: 2906.66
     rank 19: 2907.42
  backward-compute:
     rank  0: 1079.18
     rank  1: 1081.95
     rank  2: 1081.48
     rank  3: 1086.24
     rank  4: 2983.69
     rank  5: 2981.92
     rank  6: 2986.89
     rank  7: 2980.49
     rank  8: 3037.76
     rank  9: 3038.52
     rank 10: 3040.37
     rank 11: 3041.89
     rank 12: 2980.98
     rank 13: 2983.46
     rank 14: 2982.46
     rank 15: 2983.63
     rank 16: 3177.36
     rank 17: 3177.24
     rank 18: 3181.26
     rank 19: 3180.27
  pure-backward-compute:
     rank  0: 1078.50
     rank  1: 1081.22
     rank  2: 1080.68
     rank  3: 1085.32
     rank  4: 2982.56
     rank  5: 2981.27
     rank  6: 2986.04
     rank  7: 2979.77
     rank  8: 3036.56
     rank  9: 3036.96
     rank 10: 3039.49
     rank 11: 3040.25
     rank 12: 2979.74
     rank 13: 2982.08
     rank 14: 2981.40
     rank 15: 2982.76
     rank 16: 3174.75
     rank 17: 3175.10
     rank 18: 3179.80
     rank 19: 3178.69
  batch-generator:
     rank  0: 52.71
     rank  1: 59.23
     rank  2: 66.14
     rank  3: 57.66
     rank  4: 57.85
     rank  5: 64.26
     rank  6: 74.30
     rank  7: 64.76
     rank  8: 50.62
     rank  9: 55.62
     rank 10: 64.26
     rank 11: 54.61
     rank 12: 55.68
     rank 13: 58.31
     rank 14: 62.60
     rank 15: 60.71
     rank 16: 55.42
     rank 17: 58.92
     rank 18: 61.27
     rank 19: 62.54
  forward-recv:
     rank  4: 80.11
     rank  5: 78.33
     rank  6: 75.19
     rank  7: 80.02
     rank  8: 274.63
     rank  9: 274.16
     rank 10: 270.82
     rank 11: 274.56
     rank 12: 451.07
     rank 13: 451.07
     rank 14: 450.21
     rank 15: 451.25
     rank 16: 625.14
     rank 17: 625.04
     rank 18: 624.63
     rank 19: 625.19
  forward-send:
     rank  0: 387.55
     rank  1: 383.41
     rank  2: 375.91
     rank  3: 386.87
     rank  4: 34.48
     rank  5: 33.36
     rank  6: 28.14
     rank  7: 34.64
     rank  8: 20.88
     rank  9: 20.90
     rank 10: 19.03
     rank 11: 20.76
     rank 12: 10.55
     rank 13: 10.42
     rank 14: 9.84
     rank 15: 10.44
  backward-recv:
     rank  0: 1288.58
     rank  1: 1288.34
     rank  2: 1287.98
     rank  3: 1286.21
     rank  4: 597.35
     rank  5: 598.99
     rank  6: 598.04
     rank  7: 599.35
     rank  8: 376.31
     rank  9: 376.94
     rank 10: 376.15
     rank 11: 375.64
     rank 12: 192.60
     rank 13: 192.34
     rank 14: 193.88
     rank 15: 192.39
  backward-send:
     rank  4: 22.73
     rank  5: 21.86
     rank  6: 22.29
     rank  7: 22.09
     rank  8: 31.45
     rank  9: 30.96
     rank 10: 30.48
     rank 11: 31.43
     rank 12: 20.72
     rank 13: 20.72
     rank 14: 19.71
     rank 15: 20.38
     rank 16: 10.60
     rank 17: 10.56
     rank 18: 10.38
     rank 19: 10.03
  forward-send-backward-recv:
     rank  0: 3759.43
     rank  1: 3757.07
     rank  2: 3759.43
     rank  3: 3754.07
     rank  4: 824.33
     rank  5: 826.64
     rank  6: 821.46
     rank  7: 827.33
     rank  8: 599.18
     rank  9: 600.76
     rank 10: 600.77
     rank 11: 596.08
     rank 12: 492.79
     rank 13: 493.58
     rank 14: 492.88
     rank 15: 491.47
  backward-send-forward-recv:
     rank  4: 75.10
     rank  5: 74.01
     rank  6: 75.71
     rank  7: 72.34
     rank  8: 144.18
     rank  9: 143.54
     rank 10: 139.94
     rank 11: 144.00
     rank 12: 157.86
     rank 13: 154.85
     rank 14: 155.86
     rank 15: 155.59
     rank 16: 169.56
     rank 17: 169.42
     rank 18: 166.41
     rank 19: 165.59
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.07
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.18
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.68
     rank  2: 0.68
     rank  3: 0.74
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.07
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.18
     rank  2: 0.21
     rank  3: 0.30
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.05
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.04
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.18
     rank  1: 10.43
     rank  2: 10.21
     rank  3: 10.74
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.06
     rank  9: 0.09
     rank 10: 0.07
     rank 11: 0.04
     rank 12: 0.04
     rank 13: 0.07
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.07
     rank 17: 0.10
     rank 18: 0.03
     rank 19: 0.07
  optimizer:
     rank  0: 11.38
     rank  1: 11.63
     rank  2: 11.42
     rank  3: 11.94
     rank  4: 1.24
     rank  5: 1.24
     rank  6: 1.24
     rank  7: 1.23
     rank  8: 1.27
     rank  9: 1.29
     rank 10: 1.28
     rank 11: 1.24
     rank 12: 1.24
     rank 13: 1.27
     rank 14: 1.24
     rank 15: 1.24
     rank 16: 1.28
     rank 17: 1.32
     rank 18: 1.25
     rank 19: 1.28
 [2024-12-04 23:21:48] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7613.0 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.601167E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7573.25
     rank  1: 7573.23
     rank  2: 7573.25
     rank  3: 7573.22
     rank  4: 7572.43
     rank  5: 7572.40
     rank  6: 7572.43
     rank  7: 7572.37
     rank  8: 7572.45
     rank  9: 7572.40
     rank 10: 7572.37
     rank 11: 7572.42
     rank 12: 7572.37
     rank 13: 7572.36
     rank 14: 7572.39
     rank 15: 7572.35
     rank 16: 7572.57
     rank 17: 7572.43
     rank 18: 7572.27
     rank 19: 7572.27
  forward-compute:
     rank  0: 1016.98
     rank  1: 1020.92
     rank  2: 1026.40
     rank  3: 1016.46
     rank  4: 2844.54
     rank  5: 2848.79
     rank  6: 2853.19
     rank  7: 2851.01
     rank  8: 2802.49
     rank  9: 2809.21
     rank 10: 2811.42
     rank 11: 2803.70
     rank 12: 2774.65
     rank 13: 2774.74
     rank 14: 2779.09
     rank 15: 2777.79
     rank 16: 2903.49
     rank 17: 2902.85
     rank 18: 2907.22
     rank 19: 2911.91
  backward-compute:
     rank  0: 1067.70
     rank  1: 1069.74
     rank  2: 1070.37
     rank  3: 1075.20
     rank  4: 2968.69
     rank  5: 2971.73
     rank  6: 2971.73
     rank  7: 2965.27
     rank  8: 3037.30
     rank  9: 3037.00
     rank 10: 3039.15
     rank 11: 3040.55
     rank 12: 2986.60
     rank 13: 2984.51
     rank 14: 2988.98
     rank 15: 2992.17
     rank 16: 3196.47
     rank 17: 3197.02
     rank 18: 3199.87
     rank 19: 3199.20
  pure-backward-compute:
     rank  0: 1067.03
     rank  1: 1068.98
     rank  2: 1069.62
     rank  3: 1074.29
     rank  4: 2967.42
     rank  5: 2970.94
     rank  6: 2970.86
     rank  7: 2964.60
     rank  8: 3035.84
     rank  9: 3035.98
     rank 10: 3038.42
     rank 11: 3038.85
     rank 12: 2985.47
     rank 13: 2983.51
     rank 14: 2987.92
     rank 15: 2991.22
     rank 16: 3194.01
     rank 17: 3193.95
     rank 18: 3198.47
     rank 19: 3197.81
  batch-generator:
     rank  0: 50.20
     rank  1: 56.39
     rank  2: 61.98
     rank  3: 59.80
     rank  4: 49.87
     rank  5: 58.68
     rank  6: 65.66
     rank  7: 60.08
     rank  8: 58.57
     rank  9: 67.50
     rank 10: 69.91
     rank 11: 61.88
     rank 12: 52.71
     rank 13: 56.69
     rank 14: 59.83
     rank 15: 57.02
     rank 16: 53.36
     rank 17: 56.16
     rank 18: 59.48
     rank 19: 64.75
  forward-recv:
     rank  4: 82.41
     rank  5: 81.37
     rank  6: 77.84
     rank  7: 82.29
     rank  8: 276.04
     rank  9: 275.79
     rank 10: 273.19
     rank 11: 275.71
     rank 12: 452.36
     rank 13: 452.24
     rank 14: 451.59
     rank 15: 452.55
     rank 16: 625.81
     rank 17: 625.79
     rank 18: 625.56
     rank 19: 625.74
  forward-send:
     rank  0: 393.15
     rank  1: 388.85
     rank  2: 382.83
     rank  3: 391.76
     rank  4: 34.07
     rank  5: 32.80
     rank  6: 27.34
     rank  7: 33.86
     rank  8: 20.96
     rank  9: 20.49
     rank 10: 19.38
     rank 11: 20.87
     rank 12: 10.56
     rank 13: 10.38
     rank 14: 10.14
     rank 15: 10.40
  backward-recv:
     rank  0: 1290.72
     rank  1: 1290.17
     rank  2: 1289.87
     rank  3: 1287.58
     rank  4: 598.18
     rank  5: 598.20
     rank  6: 597.86
     rank  7: 599.21
     rank  8: 379.81
     rank  9: 381.09
     rank 10: 380.22
     rank 11: 379.32
     rank 12: 196.22
     rank 13: 196.27
     rank 14: 196.71
     rank 15: 196.09
  backward-send:
     rank  4: 22.61
     rank  5: 22.02
     rank  6: 22.36
     rank  7: 22.26
     rank  8: 31.35
     rank  9: 29.97
     rank 10: 30.73
     rank 11: 31.36
     rank 12: 20.91
     rank 13: 20.68
     rank 14: 20.30
     rank 15: 20.70
     rank 16: 10.62
     rank 17: 10.63
     rank 18: 10.23
     rank 19: 10.00
  forward-send-backward-recv:
     rank  0: 3791.10
     rank  1: 3788.57
     rank  2: 3790.36
     rank  3: 3785.08
     rank  4: 867.97
     rank  5: 866.66
     rank  6: 865.27
     rank  7: 870.22
     rank  8: 607.54
     rank  9: 610.65
     rank 10: 610.61
     rank 11: 605.01
     rank 12: 502.92
     rank 13: 504.97
     rank 14: 502.35
     rank 15: 498.36
  backward-send-forward-recv:
     rank  4: 74.34
     rank  5: 74.16
     rank  6: 75.40
     rank  7: 71.96
     rank  8: 144.78
     rank  9: 139.31
     rank 10: 140.21
     rank 11: 143.30
     rank 12: 157.75
     rank 13: 157.61
     rank 14: 153.79
     rank 15: 155.65
     rank 16: 168.42
     rank 17: 167.53
     rank 18: 165.51
     rank 19: 161.43
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.09
     rank 17: 0.06
     rank 18: 0.04
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.67
     rank  2: 0.63
     rank  3: 0.66
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.22
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.28
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.11
     rank  1: 10.44
     rank  2: 10.14
     rank  3: 10.27
     rank  4: 0.03
     rank  5: 0.07
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.07
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.03
     rank 16: 0.09
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 10.87
     rank  1: 11.20
     rank  2: 10.90
     rank  3: 11.02
     rank  4: 0.79
     rank  5: 0.82
     rank  6: 0.79
     rank  7: 0.78
     rank  8: 0.81
     rank  9: 0.82
     rank 10: 0.79
     rank 11: 0.83
     rank 12: 0.79
     rank 13: 0.80
     rank 14: 0.82
     rank 15: 0.79
     rank 16: 0.85
     rank 17: 0.82
     rank 18: 0.83
     rank 19: 0.79
 [2024-12-04 23:21:56] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7610.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.495011E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7569.97
     rank  1: 7569.60
     rank  2: 7569.50
     rank  3: 7569.58
     rank  4: 7569.18
     rank  5: 7568.76
     rank  6: 7568.71
     rank  7: 7568.71
     rank  8: 7569.09
     rank  9: 7568.73
     rank 10: 7568.64
     rank 11: 7568.71
     rank 12: 7569.11
     rank 13: 7568.72
     rank 14: 7568.68
     rank 15: 7568.75
     rank 16: 7569.65
     rank 17: 7568.67
     rank 18: 7568.69
     rank 19: 7568.58
  forward-compute:
     rank  0: 1110.64
     rank  1: 1113.94
     rank  2: 1115.34
     rank  3: 1109.07
     rank  4: 2849.54
     rank  5: 2854.33
     rank  6: 2852.70
     rank  7: 2855.42
     rank  8: 2801.37
     rank  9: 2804.98
     rank 10: 2805.75
     rank 11: 2801.89
     rank 12: 2773.64
     rank 13: 2774.57
     rank 14: 2778.19
     rank 15: 2775.67
     rank 16: 2898.96
     rank 17: 2898.71
     rank 18: 2903.70
     rank 19: 2908.77
  backward-compute:
     rank  0: 1074.81
     rank  1: 1077.66
     rank  2: 1076.33
     rank  3: 1082.51
     rank  4: 2976.46
     rank  5: 2977.61
     rank  6: 2979.72
     rank  7: 2974.47
     rank  8: 3033.68
     rank  9: 3033.91
     rank 10: 3037.17
     rank 11: 3036.23
     rank 12: 2995.73
     rank 13: 2993.64
     rank 14: 2997.19
     rank 15: 3003.16
     rank 16: 3196.79
     rank 17: 3197.72
     rank 18: 3199.90
     rank 19: 3199.59
  pure-backward-compute:
     rank  0: 1074.14
     rank  1: 1076.93
     rank  2: 1075.63
     rank  3: 1081.59
     rank  4: 2975.33
     rank  5: 2976.91
     rank  6: 2978.87
     rank  7: 2973.75
     rank  8: 3032.55
     rank  9: 3033.06
     rank 10: 3036.41
     rank 11: 3034.66
     rank 12: 2994.59
     rank 13: 2992.67
     rank 14: 2996.24
     rank 15: 3002.14
     rank 16: 3194.38
     rank 17: 3194.84
     rank 18: 3198.37
     rank 19: 3198.17
  batch-generator:
     rank  0: 49.89
     rank  1: 55.33
     rank  2: 57.58
     rank  3: 60.79
     rank  4: 54.18
     rank  5: 64.20
     rank  6: 63.44
     rank  7: 65.77
     rank  8: 51.68
     rank  9: 59.12
     rank 10: 60.38
     rank 11: 56.44
     rank 12: 52.54
     rank 13: 59.18
     rank 14: 60.33
     rank 15: 55.94
     rank 16: 54.62
     rank 17: 56.69
     rank 18: 62.08
     rank 19: 66.34
  forward-recv:
     rank  4: 82.53
     rank  5: 82.74
     rank  6: 81.50
     rank  7: 82.31
     rank  8: 274.78
     rank  9: 274.62
     rank 10: 274.06
     rank 11: 274.46
     rank 12: 450.50
     rank 13: 450.00
     rank 14: 450.14
     rank 15: 450.75
     rank 16: 624.72
     rank 17: 624.65
     rank 18: 624.27
     rank 19: 624.54
  forward-send:
     rank  0: 396.03
     rank  1: 392.80
     rank  2: 390.73
     rank  3: 394.43
     rank  4: 35.45
     rank  5: 33.27
     rank  6: 32.68
     rank  7: 35.26
     rank  8: 21.08
     rank  9: 20.05
     rank 10: 19.82
     rank 11: 20.91
     rank 12: 10.55
     rank 13: 10.44
     rank 14: 10.17
     rank 15: 10.38
  backward-recv:
     rank  0: 1290.15
     rank  1: 1290.12
     rank  2: 1289.81
     rank  3: 1287.47
     rank  4: 598.00
     rank  5: 596.99
     rank  6: 597.24
     rank  7: 598.21
     rank  8: 379.46
     rank  9: 380.49
     rank 10: 379.38
     rank 11: 378.91
     rank 12: 192.63
     rank 13: 192.88
     rank 14: 193.18
     rank 15: 192.16
  backward-send:
     rank  4: 22.72
     rank  5: 21.81
     rank  6: 22.23
     rank  7: 22.22
     rank  8: 31.33
     rank  9: 29.94
     rank 10: 30.73
     rank 11: 31.43
     rank 12: 20.99
     rank 13: 20.72
     rank 14: 20.37
     rank 15: 20.84
     rank 16: 10.54
     rank 17: 10.54
     rank 18: 10.43
     rank 19: 9.84
  forward-send-backward-recv:
     rank  0: 3682.81
     rank  1: 3681.22
     rank  2: 3682.28
     rank  3: 3678.09
     rank  4: 849.08
     rank  5: 851.21
     rank  6: 847.28
     rank  7: 852.14
     rank  8: 611.18
     rank  9: 613.46
     rank 10: 612.30
     rank 11: 609.68
     rank 12: 494.81
     rank 13: 496.95
     rank 14: 494.73
     rank 15: 489.10
  backward-send-forward-recv:
     rank  4: 74.52
     rank  5: 73.43
     rank  6: 75.26
     rank  7: 72.14
     rank  8: 143.27
     rank  9: 141.04
     rank 10: 140.21
     rank 11: 142.18
     rank 12: 158.15
     rank 13: 157.36
     rank 14: 154.62
     rank 15: 156.55
     rank 16: 169.34
     rank 17: 168.39
     rank 18: 165.52
     rank 19: 161.05
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.13
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.06
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.30
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.67
     rank  2: 0.62
     rank  3: 0.72
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.15
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.16
     rank  2: 0.21
     rank  3: 0.31
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.26
     rank  1: 10.31
     rank  2: 10.40
     rank  3: 10.43
     rank  4: 0.07
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.07
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.07
     rank 16: 0.21
     rank 17: 0.07
     rank 18: 0.03
     rank 19: 0.03
  optimizer:
     rank  0: 11.51
     rank  1: 11.57
     rank  2: 11.66
     rank  3: 11.68
     rank  4: 1.32
     rank  5: 1.28
     rank  6: 1.28
     rank  7: 1.28
     rank  8: 1.29
     rank  9: 1.32
     rank 10: 1.28
     rank 11: 1.29
     rank 12: 1.29
     rank 13: 1.29
     rank 14: 1.30
     rank 15: 1.32
     rank 16: 1.46
     rank 17: 1.32
     rank 18: 1.27
     rank 19: 1.28
 [2024-12-04 23:22:03] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7624.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.829230E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7590.07
     rank  1: 7590.03
     rank  2: 7589.93
     rank  3: 7590.05
     rank  4: 7589.25
     rank  5: 7589.17
     rank  6: 7589.10
     rank  7: 7589.18
     rank  8: 7589.20
     rank  9: 7589.13
     rank 10: 7589.06
     rank 11: 7589.15
     rank 12: 7589.21
     rank 13: 7589.13
     rank 14: 7589.07
     rank 15: 7589.11
     rank 16: 7589.30
     rank 17: 7589.12
     rank 18: 7588.99
     rank 19: 7588.99
  forward-compute:
     rank  0: 1013.98
     rank  1: 1023.52
     rank  2: 1014.40
     rank  3: 1012.10
     rank  4: 2845.41
     rank  5: 2857.50
     rank  6: 2845.83
     rank  7: 2849.69
     rank  8: 2806.27
     rank  9: 2814.24
     rank 10: 2808.97
     rank 11: 2806.58
     rank 12: 2775.59
     rank 13: 2776.64
     rank 14: 2780.58
     rank 15: 2778.07
     rank 16: 2916.55
     rank 17: 2918.34
     rank 18: 2921.00
     rank 19: 2924.44
  backward-compute:
     rank  0: 1072.81
     rank  1: 1076.81
     rank  2: 1075.74
     rank  3: 1079.20
     rank  4: 2975.88
     rank  5: 2977.66
     rank  6: 2976.16
     rank  7: 2972.00
     rank  8: 3035.00
     rank  9: 3035.04
     rank 10: 3038.97
     rank 11: 3037.65
     rank 12: 2997.77
     rank 13: 2997.29
     rank 14: 2999.69
     rank 15: 3003.46
     rank 16: 3196.55
     rank 17: 3196.48
     rank 18: 3200.98
     rank 19: 3200.11
  pure-backward-compute:
     rank  0: 1072.13
     rank  1: 1076.14
     rank  2: 1074.93
     rank  3: 1078.43
     rank  4: 2974.39
     rank  5: 2977.01
     rank  6: 2975.30
     rank  7: 2971.28
     rank  8: 3033.83
     rank  9: 3034.29
     rank 10: 3038.23
     rank 11: 3036.24
     rank 12: 2996.65
     rank 13: 2996.26
     rank 14: 2998.81
     rank 15: 3002.64
     rank 16: 3194.01
     rank 17: 3194.21
     rank 18: 3199.69
     rank 19: 3198.78
  batch-generator:
     rank  0: 54.02
     rank  1: 64.63
     rank  2: 61.25
     rank  3: 58.67
     rank  4: 49.81
     rank  5: 65.71
     rank  6: 58.83
     rank  7: 58.32
     rank  8: 51.12
     rank  9: 62.99
     rank 10: 57.70
     rank 11: 55.71
     rank 12: 50.94
     rank 13: 56.38
     rank 14: 59.37
     rank 15: 54.85
     rank 16: 62.46
     rank 17: 65.84
     rank 18: 67.38
     rank 19: 71.22
  forward-recv:
     rank  4: 82.66
     rank  5: 78.07
     rank  6: 82.74
     rank  7: 82.53
     rank  8: 277.86
     rank  9: 276.32
     rank 10: 277.44
     rank 11: 277.68
     rank 12: 456.22
     rank 13: 455.44
     rank 14: 455.55
     rank 15: 456.06
     rank 16: 629.30
     rank 17: 629.09
     rank 18: 629.27
     rank 19: 629.28
  forward-send:
     rank  0: 400.94
     rank  1: 391.38
     rank  2: 398.61
     rank  3: 400.59
     rank  4: 37.92
     rank  5: 34.24
     rank  6: 36.37
     rank  7: 38.06
     rank  8: 21.08
     rank  9: 19.85
     rank 10: 20.17
     rank 11: 21.00
     rank 12: 10.39
     rank 13: 10.33
     rank 14: 10.53
     rank 15: 10.56
  backward-recv:
     rank  0: 1298.22
     rank  1: 1298.10
     rank  2: 1297.02
     rank  3: 1296.06
     rank  4: 598.84
     rank  5: 599.59
     rank  6: 600.13
     rank  7: 601.52
     rank  8: 381.50
     rank  9: 382.30
     rank 10: 381.50
     rank 11: 380.61
     rank 12: 194.43
     rank 13: 194.41
     rank 14: 194.23
     rank 15: 193.94
  backward-send:
     rank  4: 22.89
     rank  5: 22.04
     rank  6: 22.43
     rank  7: 21.65
     rank  8: 31.09
     rank  9: 29.93
     rank 10: 30.83
     rank 11: 31.33
     rank 12: 20.95
     rank 13: 20.80
     rank 14: 20.90
     rank 15: 20.75
     rank 16: 10.66
     rank 17: 10.62
     rank 18: 10.23
     rank 19: 10.08
  forward-send-backward-recv:
     rank  0: 3789.75
     rank  1: 3787.48
     rank  2: 3789.12
     rank  3: 3785.79
     rank  4: 873.43
     rank  5: 872.53
     rank  6: 872.38
     rank  7: 876.28
     rank  8: 621.30
     rank  9: 624.40
     rank 10: 621.70
     rank 11: 619.35
     rank 12: 506.00
     rank 13: 506.84
     rank 14: 506.01
     rank 15: 502.41
  backward-send-forward-recv:
     rank  4: 74.43
     rank  5: 72.92
     rank  6: 75.91
     rank  7: 73.07
     rank  8: 145.10
     rank  9: 139.84
     rank 10: 142.87
     rank 11: 144.39
     rank 12: 158.13
     rank 13: 157.38
     rank 14: 153.99
     rank 15: 156.28
     rank 16: 169.70
     rank 17: 168.91
     rank 18: 165.77
     rank 19: 163.06
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.05
     rank  2: 0.05
     rank  3: 0.07
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.12
     rank 17: 0.06
     rank 18: 0.04
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.68
     rank  2: 0.67
     rank  3: 0.84
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.21
     rank  2: 0.19
     rank  3: 0.29
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.17
     rank  1: 10.29
     rank  2: 10.25
     rank  3: 10.73
     rank  4: 0.04
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 10.93
     rank  1: 11.04
     rank  2: 11.00
     rank  3: 11.46
     rank  4: 0.79
     rank  5: 0.78
     rank  6: 0.78
     rank  7: 0.78
     rank  8: 0.78
     rank  9: 0.78
     rank 10: 0.78
     rank 11: 0.78
     rank 12: 0.78
     rank 13: 0.78
     rank 14: 0.78
     rank 15: 0.78
     rank 16: 0.83
     rank 17: 0.82
     rank 18: 0.78
     rank 19: 0.78
 [2024-12-04 23:22:11] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7604.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.286381E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7570.31
     rank  1: 7570.08
     rank  2: 7570.04
     rank  3: 7570.07
     rank  4: 7569.49
     rank  5: 7569.24
     rank  6: 7569.23
     rank  7: 7569.24
     rank  8: 7569.51
     rank  9: 7569.18
     rank 10: 7569.17
     rank 11: 7569.19
     rank 12: 7569.44
     rank 13: 7569.20
     rank 14: 7569.21
     rank 15: 7569.21
     rank 16: 7569.95
     rank 17: 7569.26
     rank 18: 7569.10
     rank 19: 7569.11
  forward-compute:
     rank  0: 1043.22
     rank  1: 1050.38
     rank  2: 1046.92
     rank  3: 1042.57
     rank  4: 2868.38
     rank  5: 2876.81
     rank  6: 2870.94
     rank  7: 2873.74
     rank  8: 2802.39
     rank  9: 2809.07
     rank 10: 2806.80
     rank 11: 2802.87
     rank 12: 2761.07
     rank 13: 2760.92
     rank 14: 2763.97
     rank 15: 2765.42
     rank 16: 2902.63
     rank 17: 2903.15
     rank 18: 2906.33
     rank 19: 2908.99
  backward-compute:
     rank  0: 1066.24
     rank  1: 1069.96
     rank  2: 1068.51
     rank  3: 1072.98
     rank  4: 2979.94
     rank  5: 2982.95
     rank  6: 2981.62
     rank  7: 2976.38
     rank  8: 3034.10
     rank  9: 3033.54
     rank 10: 3036.38
     rank 11: 3038.01
     rank 12: 2983.48
     rank 13: 2982.31
     rank 14: 2984.68
     rank 15: 2986.55
     rank 16: 3190.93
     rank 17: 3189.77
     rank 18: 3194.40
     rank 19: 3193.77
  pure-backward-compute:
     rank  0: 1065.50
     rank  1: 1069.29
     rank  2: 1067.87
     rank  3: 1072.10
     rank  4: 2978.55
     rank  5: 2982.16
     rank  6: 2980.75
     rank  7: 2975.52
     rank  8: 3032.97
     rank  9: 3032.75
     rank 10: 3035.53
     rank 11: 3036.30
     rank 12: 2982.35
     rank 13: 2981.28
     rank 14: 2983.76
     rank 15: 2985.71
     rank 16: 3188.66
     rank 17: 3187.56
     rank 18: 3193.10
     rank 19: 3192.37
  batch-generator:
     rank  0: 56.06
     rank  1: 64.08
     rank  2: 61.94
     rank  3: 60.29
     rank  4: 53.72
     rank  5: 66.03
     rank  6: 62.30
     rank  7: 67.21
     rank  8: 51.25
     rank  9: 61.97
     rank 10: 59.60
     rank 11: 55.99
     rank 12: 51.29
     rank 13: 55.09
     rank 14: 56.95
     rank 15: 56.86
     rank 16: 53.66
     rank 17: 56.47
     rank 18: 58.81
     rank 19: 61.98
  forward-recv:
     rank  4: 83.31
     rank  5: 80.95
     rank  6: 82.34
     rank  7: 82.48
     rank  8: 276.82
     rank  9: 276.27
     rank 10: 276.25
     rank 11: 276.58
     rank 12: 455.27
     rank 13: 454.78
     rank 14: 455.16
     rank 15: 455.45
     rank 16: 626.15
     rank 17: 626.04
     rank 18: 625.85
     rank 19: 626.27
  forward-send:
     rank  0: 393.83
     rank  1: 388.50
     rank  2: 389.74
     rank  3: 392.43
     rank  4: 35.91
     rank  5: 33.73
     rank  6: 34.01
     rank  7: 35.94
     rank  8: 21.09
     rank  9: 20.00
     rank 10: 19.98
     rank 11: 20.95
     rank 12: 10.56
     rank 13: 10.38
     rank 14: 10.07
     rank 15: 10.40
  backward-recv:
     rank  0: 1300.42
     rank  1: 1300.40
     rank  2: 1300.12
     rank  3: 1297.24
     rank  4: 600.81
     rank  5: 600.90
     rank  6: 601.60
     rank  7: 602.86
     rank  8: 386.56
     rank  9: 387.40
     rank 10: 386.16
     rank 11: 386.08
     rank 12: 194.80
     rank 13: 195.07
     rank 14: 195.45
     rank 15: 194.91
  backward-send:
     rank  4: 22.86
     rank  5: 22.24
     rank  6: 22.59
     rank  7: 21.94
     rank  8: 31.34
     rank  9: 29.86
     rank 10: 31.04
     rank 11: 31.30
     rank 12: 21.00
     rank 13: 20.36
     rank 14: 20.35
     rank 15: 20.84
     rank 16: 10.50
     rank 17: 10.42
     rank 18: 10.45
     rank 19: 10.32
  forward-send-backward-recv:
     rank  0: 3752.13
     rank  1: 3747.28
     rank  2: 3751.18
     rank  3: 3747.82
     rank  4: 823.65
     rank  5: 822.33
     rank  6: 821.33
     rank  7: 825.50
     rank  8: 600.75
     rank  9: 604.72
     rank 10: 602.87
     rank 11: 597.91
     rank 12: 513.84
     rank 13: 515.43
     rank 14: 514.10
     rank 15: 511.52
  backward-send-forward-recv:
     rank  4: 74.15
     rank  5: 72.31
     rank  6: 75.24
     rank  7: 72.91
     rank  8: 145.01
     rank  9: 139.85
     rank 10: 141.70
     rank 11: 143.69
     rank 12: 158.73
     rank 13: 158.68
     rank 14: 156.54
     rank 15: 155.95
     rank 16: 169.59
     rank 17: 169.87
     rank 18: 166.52
     rank 19: 164.06
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.17
     rank 17: 0.09
     rank 18: 0.04
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.66
     rank  1: 0.67
     rank  2: 0.64
     rank  3: 0.67
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.04
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.21
     rank  1: 0.16
     rank  2: 0.20
     rank  3: 0.28
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.19
     rank  1: 10.40
     rank  2: 10.12
     rank  3: 10.26
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.06
     rank  7: 0.06
     rank  8: 0.06
     rank  9: 0.06
     rank 10: 0.07
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.08
     rank 17: 0.07
     rank 18: 0.03
     rank 19: 0.06
  optimizer:
     rank  0: 11.13
     rank  1: 11.35
     rank  2: 11.08
     rank  3: 11.20
     rank  4: 0.97
     rank  5: 0.97
     rank  6: 1.00
     rank  7: 1.01
     rank  8: 1.00
     rank  9: 1.00
     rank 10: 1.01
     rank 11: 0.97
     rank 12: 0.98
     rank 13: 0.97
     rank 14: 0.97
     rank 15: 0.97
     rank 16: 1.05
     rank 17: 1.02
     rank 18: 0.98
     rank 19: 1.00
 [2024-12-04 23:22:19] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7629.6 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.953053E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7589.76
     rank  1: 7589.68
     rank  2: 7589.63
     rank  3: 7589.74
     rank  4: 7588.94
     rank  5: 7588.84
     rank  6: 7588.84
     rank  7: 7588.88
     rank  8: 7588.90
     rank  9: 7588.82
     rank 10: 7588.79
     rank 11: 7588.87
     rank 12: 7588.90
     rank 13: 7588.80
     rank 14: 7588.81
     rank 15: 7588.83
     rank 16: 7588.85
     rank 17: 7588.78
     rank 18: 7588.67
     rank 19: 7588.66
  forward-compute:
     rank  0: 1028.51
     rank  1: 1037.62
     rank  2: 1035.35
     rank  3: 1027.76
     rank  4: 2842.57
     rank  5: 2853.29
     rank  6: 2848.29
     rank  7: 2849.88
     rank  8: 2798.52
     rank  9: 2806.99
     rank 10: 2803.42
     rank 11: 2798.95
     rank 12: 2762.62
     rank 13: 2762.53
     rank 14: 2765.84
     rank 15: 2766.79
     rank 16: 2910.35
     rank 17: 2911.78
     rank 18: 2915.42
     rank 19: 2915.01
  backward-compute:
     rank  0: 1062.27
     rank  1: 1064.90
     rank  2: 1062.98
     rank  3: 1069.85
     rank  4: 2977.63
     rank  5: 2980.00
     rank  6: 2978.57
     rank  7: 2973.44
     rank  8: 3033.01
     rank  9: 3033.13
     rank 10: 3035.32
     rank 11: 3036.72
     rank 12: 2981.34
     rank 13: 2980.72
     rank 14: 2983.69
     rank 15: 2984.04
     rank 16: 3200.70
     rank 17: 3200.77
     rank 18: 3204.29
     rank 19: 3205.33
  pure-backward-compute:
     rank  0: 1061.31
     rank  1: 1064.21
     rank  2: 1062.30
     rank  3: 1069.01
     rank  4: 2976.32
     rank  5: 2979.30
     rank  6: 2977.73
     rank  7: 2972.80
     rank  8: 3031.88
     rank  9: 3032.45
     rank 10: 3034.62
     rank 11: 3035.25
     rank 12: 2980.23
     rank 13: 2979.61
     rank 14: 2982.66
     rank 15: 2983.17
     rank 16: 3198.17
     rank 17: 3198.67
     rank 18: 3202.88
     rank 19: 3204.02
  batch-generator:
     rank  0: 55.11
     rank  1: 65.27
     rank  2: 64.67
     rank  3: 58.67
     rank  4: 53.72
     rank  5: 66.55
     rank  6: 63.85
     rank  7: 62.75
     rank  8: 50.96
     rank  9: 63.17
     rank 10: 59.72
     rank 11: 55.58
     rank 12: 51.19
     rank 13: 55.06
     rank 14: 57.23
     rank 15: 56.70
     rank 16: 61.14
     rank 17: 63.67
     rank 18: 66.24
     rank 19: 66.37
  forward-recv:
     rank  4: 82.29
     rank  5: 78.18
     rank  6: 79.45
     rank  7: 82.18
     rank  8: 276.64
     rank  9: 274.81
     rank 10: 275.00
     rank 11: 276.43
     rank 12: 453.87
     rank 13: 453.61
     rank 14: 452.90
     rank 15: 453.83
     rank 16: 626.66
     rank 17: 626.11
     rank 18: 626.52
     rank 19: 626.60
  forward-send:
     rank  0: 385.09
     rank  1: 376.43
     rank  2: 378.49
     rank  3: 384.39
     rank  4: 35.84
     rank  5: 32.45
     rank  6: 32.85
     rank  7: 35.87
     rank  8: 21.02
     rank  9: 20.03
     rank 10: 19.83
     rank 11: 20.92
     rank 12: 10.49
     rank 13: 10.06
     rank 14: 10.58
     rank 15: 10.51
  backward-recv:
     rank  0: 1303.94
     rank  1: 1303.66
     rank  2: 1303.82
     rank  3: 1300.51
     rank  4: 603.45
     rank  5: 604.28
     rank  6: 604.36
     rank  7: 605.61
     rank  8: 383.85
     rank  9: 384.37
     rank 10: 384.48
     rank 11: 383.19
     rank 12: 195.22
     rank 13: 195.82
     rank 14: 194.93
     rank 15: 195.28
  backward-send:
     rank  4: 22.71
     rank  5: 22.04
     rank  6: 22.50
     rank  7: 21.67
     rank  8: 31.38
     rank  9: 29.93
     rank 10: 30.76
     rank 11: 31.42
     rank 12: 20.97
     rank 13: 20.17
     rank 14: 21.12
     rank 15: 20.70
     rank 16: 10.51
     rank 17: 10.56
     rank 18: 10.19
     rank 19: 10.39
  forward-send-backward-recv:
     rank  0: 3796.41
     rank  1: 3793.35
     rank  2: 3795.67
     rank  3: 3789.73
     rank  4: 868.26
     rank  5: 867.19
     rank  6: 866.94
     rank  7: 870.81
     rank  8: 626.17
     rank  9: 630.28
     rank 10: 628.69
     rank 11: 623.27
     rank 12: 531.57
     rank 13: 533.11
     rank 14: 531.08
     rank 15: 529.65
  backward-send-forward-recv:
     rank  4: 74.46
     rank  5: 73.35
     rank  6: 74.92
     rank  7: 71.14
     rank  8: 145.44
     rank  9: 139.42
     rank 10: 142.04
     rank 11: 144.22
     rank 12: 158.20
     rank 13: 158.06
     rank 14: 156.07
     rank 15: 155.56
     rank 16: 169.57
     rank 17: 169.51
     rank 18: 165.54
     rank 19: 165.17
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.06
     rank  2: 0.04
     rank  3: 0.11
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.67
     rank  2: 0.63
     rank  3: 0.73
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.20
     rank  3: 0.30
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.29
     rank  1: 10.34
     rank  2: 10.20
     rank  3: 10.57
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.07
     rank 15: 0.03
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.04
  optimizer:
     rank  0: 11.05
     rank  1: 11.10
     rank  2: 10.95
     rank  3: 11.29
     rank  4: 0.79
     rank  5: 0.79
     rank  6: 0.80
     rank  7: 0.79
     rank  8: 0.80
     rank  9: 0.80
     rank 10: 0.80
     rank 11: 0.82
     rank 12: 0.79
     rank 13: 0.78
     rank 14: 0.82
     rank 15: 0.79
     rank 16: 0.82
     rank 17: 0.80
     rank 18: 0.82
     rank 19: 0.80
 [2024-12-04 23:22:26] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7611.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.852459E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7576.90
     rank  1: 7576.55
     rank  2: 7576.52
     rank  3: 7576.53
     rank  4: 7576.08
     rank  5: 7575.76
     rank  6: 7575.70
     rank  7: 7575.68
     rank  8: 7576.05
     rank  9: 7575.84
     rank 10: 7575.63
     rank 11: 7575.65
     rank 12: 7576.03
     rank 13: 7575.81
     rank 14: 7575.67
     rank 15: 7575.65
     rank 16: 7576.59
     rank 17: 7575.68
     rank 18: 7575.59
     rank 19: 7575.59
  forward-compute:
     rank  0: 1054.79
     rank  1: 1062.90
     rank  2: 1059.84
     rank  3: 1053.15
     rank  4: 2848.86
     rank  5: 2859.48
     rank  6: 2852.57
     rank  7: 2854.98
     rank  8: 2802.08
     rank  9: 2809.61
     rank 10: 2806.28
     rank 11: 2802.44
     rank 12: 2767.71
     rank 13: 2768.08
     rank 14: 2772.00
     rank 15: 2772.26
     rank 16: 2904.36
     rank 17: 2904.78
     rank 18: 2909.28
     rank 19: 2910.40
  backward-compute:
     rank  0: 1061.97
     rank  1: 1065.06
     rank  2: 1063.12
     rank  3: 1070.65
     rank  4: 2975.77
     rank  5: 2978.06
     rank  6: 2976.32
     rank  7: 2971.90
     rank  8: 3031.47
     rank  9: 3031.62
     rank 10: 3034.78
     rank 11: 3035.64
     rank 12: 2982.64
     rank 13: 2981.39
     rank 14: 2984.30
     rank 15: 2985.60
     rank 16: 3193.74
     rank 17: 3193.34
     rank 18: 3197.07
     rank 19: 3196.99
  pure-backward-compute:
     rank  0: 1061.30
     rank  1: 1064.37
     rank  2: 1062.45
     rank  3: 1069.87
     rank  4: 2974.36
     rank  5: 2977.36
     rank  6: 2975.49
     rank  7: 2971.20
     rank  8: 3030.41
     rank  9: 3030.94
     rank 10: 3034.08
     rank 11: 3034.33
     rank 12: 2981.55
     rank 13: 2980.40
     rank 14: 2983.36
     rank 15: 2984.76
     rank 16: 3191.48
     rank 17: 3191.15
     rank 18: 3195.81
     rank 19: 3195.72
  batch-generator:
     rank  0: 54.18
     rank  1: 64.17
     rank  2: 61.09
     rank  3: 56.25
     rank  4: 56.88
     rank  5: 69.50
     rank  6: 65.55
     rank  7: 64.67
     rank  8: 50.85
     rank  9: 62.11
     rank 10: 58.92
     rank 11: 55.29
     rank 12: 53.56
     rank 13: 57.87
     rank 14: 60.58
     rank 15: 59.09
     rank 16: 53.10
     rank 17: 55.85
     rank 18: 59.50
     rank 19: 61.12
  forward-recv:
     rank  4: 82.13
     rank  5: 78.50
     rank  6: 80.28
     rank  7: 82.04
     rank  8: 276.72
     rank  9: 275.25
     rank 10: 275.81
     rank 11: 276.44
     rank 12: 455.16
     rank 13: 454.70
     rank 14: 455.02
     rank 15: 455.42
     rank 16: 627.79
     rank 17: 627.50
     rank 18: 627.26
     rank 19: 627.57
  forward-send:
     rank  0: 363.07
     rank  1: 355.07
     rank  2: 357.97
     rank  3: 362.49
     rank  4: 37.23
     rank  5: 34.18
     rank  6: 34.97
     rank  7: 37.32
     rank  8: 21.06
     rank  9: 20.06
     rank 10: 20.00
     rank 11: 21.02
     rank 12: 10.57
     rank 13: 10.43
     rank 14: 10.13
     rank 15: 10.40
  backward-recv:
     rank  0: 1299.69
     rank  1: 1299.85
     rank  2: 1299.52
     rank  3: 1295.86
     rank  4: 599.02
     rank  5: 597.93
     rank  6: 599.13
     rank  7: 600.38
     rank  8: 382.25
     rank  9: 383.03
     rank 10: 382.16
     rank 11: 381.65
     rank 12: 195.83
     rank 13: 196.00
     rank 14: 196.14
     rank 15: 196.29
  backward-send:
     rank  4: 22.61
     rank  5: 22.66
     rank  6: 22.40
     rank  7: 21.63
     rank  8: 31.32
     rank  9: 30.18
     rank 10: 31.04
     rank 11: 31.42
     rank 12: 21.16
     rank 13: 20.72
     rank 14: 20.81
     rank 15: 20.87
     rank 16: 10.56
     rank 17: 10.61
     rank 18: 10.18
     rank 19: 10.39
  forward-send-backward-recv:
     rank  0: 3783.10
     rank  1: 3779.80
     rank  2: 3782.28
     rank  3: 3776.98
     rank  4: 856.11
     rank  5: 855.05
     rank  6: 855.01
     rank  7: 858.77
     rank  8: 612.54
     rank  9: 615.89
     rank 10: 614.17
     rank 11: 609.29
     rank 12: 512.14
     rank 13: 513.67
     rank 14: 512.07
     rank 15: 509.77
  backward-send-forward-recv:
     rank  4: 73.55
     rank  5: 72.50
     rank  6: 74.92
     rank  7: 71.51
     rank  8: 145.09
     rank  9: 139.98
     rank 10: 141.96
     rank 11: 144.10
     rank 12: 158.30
     rank 13: 157.91
     rank 14: 154.82
     rank 15: 155.09
     rank 16: 170.21
     rank 17: 170.61
     rank 18: 166.33
     rank 19: 165.11
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.06
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.26
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.06
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.19
     rank 17: 0.05
     rank 18: 0.04
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.63
     rank  2: 0.67
     rank  3: 0.66
     rank  4: 0.01
     rank  5: 0.03
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.06
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.20
     rank  1: 0.17
     rank  2: 0.20
     rank  3: 0.29
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.06
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.09
     rank  1: 10.13
     rank  2: 10.30
     rank  3: 10.27
     rank  4: 0.03
     rank  5: 0.08
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.06
     rank 13: 0.03
     rank 14: 0.04
     rank 15: 0.03
     rank 16: 0.19
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.03
  optimizer:
     rank  0: 11.27
     rank  1: 11.32
     rank  2: 11.50
     rank  3: 11.47
     rank  4: 1.22
     rank  5: 1.26
     rank  6: 1.22
     rank  7: 1.22
     rank  8: 1.22
     rank  9: 1.22
     rank 10: 1.22
     rank 11: 1.23
     rank 12: 1.25
     rank 13: 1.22
     rank 14: 1.23
     rank 15: 1.22
     rank 16: 1.43
     rank 17: 1.22
     rank 18: 1.23
     rank 19: 1.22
