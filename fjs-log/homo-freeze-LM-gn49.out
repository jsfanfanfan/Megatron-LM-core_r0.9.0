examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-02 16:44:15,697] torch.distributed.run: [WARNING] 
[2024-12-02 16:44:15,697] torch.distributed.run: [WARNING] *****************************************
[2024-12-02 16:44:15,697] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-02 16:44:15,697] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]

---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]


---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0][rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False


name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Falsename:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Falsename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False

name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False


name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Falsename:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False


name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:Falsename:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:Falsename:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False

name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:False
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:False
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:False
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:False
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 115.20
     rank  1: 77.58
     rank  2: 79.66
     rank  3: 83.41
     rank  4: 29.33
     rank  5: 29.78
     rank  6: 37.15
     rank  7: 45.95
     rank  8: 39.77
     rank  9: 28.66
     rank 10: 42.46
     rank 11: 42.25
     rank 12: 47.70
     rank 13: 48.94
     rank 14: 46.99
     rank 15: 50.21
     rank 16: 42.96
     rank 17: 31.48
     rank 18: 43.27
     rank 19: 42.97
  train/valid/test-data-iterators-setup:
     rank  0: 864.33
     rank  1: 864.34
     rank  2: 864.26
     rank  3: 864.28
     rank  4: 960.13
     rank  5: 960.39
     rank  6: 960.26
     rank  7: 960.16
     rank  8: 1063.80
     rank  9: 1063.84
     rank 10: 1063.96
     rank 11: 1064.57
     rank 12: 1095.06
     rank 13: 1095.36
     rank 14: 1095.14
     rank 15: 1095.18
     rank 16: 1095.21
     rank 17: 1095.28
     rank 18: 1096.28
     rank 19: 1095.35
 [2024-12-02 16:44:54] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 21490.2 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.108734E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 17] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4488.0 | max reserved: 4488.0
[Rank 18] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4508.0 | max reserved: 4508.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4488.0 | max reserved: 4488.0
[Rank 19] (after 1 iterations) memory (MB) | allocated: 1877.5185546875 | max allocated: 3913.1982421875 | reserved: 4540.0 | max reserved: 4540.0
times across ranks (ms):
  forward-backward:
     rank  0: 21407.65
     rank  1: 21407.54
     rank  2: 21407.34
     rank  3: 21407.42
     rank  4: 21449.58
     rank  5: 21444.10
     rank  6: 21447.65
     rank  7: 21444.12
     rank  8: 21451.72
     rank  9: 21451.26
     rank 10: 21444.47
     rank 11: 21448.93
     rank 12: 21456.97
     rank 13: 21456.76
     rank 14: 21447.06
     rank 15: 21454.76
     rank 16: 21406.02
     rank 17: 21405.96
     rank 18: 21405.97
     rank 19: 21405.96
  forward-compute:
     rank  0: 5086.99
     rank  1: 5086.12
     rank  2: 5149.86
     rank  3: 5077.60
     rank  4: 5142.04
     rank  5: 5140.89
     rank  6: 5168.34
     rank  7: 5137.66
     rank  8: 4970.87
     rank  9: 4968.30
     rank 10: 4973.18
     rank 11: 4969.45
     rank 12: 4934.63
     rank 13: 4929.92
     rank 14: 4934.97
     rank 15: 4933.48
     rank 16: 5412.93
     rank 17: 5420.02
     rank 18: 5414.43
     rank 19: 5422.12
  backward-compute:
     rank  0: 1971.23
     rank  1: 1966.74
     rank  2: 1929.39
     rank  3: 1972.77
     rank  4: 3009.21
     rank  5: 3015.85
     rank  6: 3011.54
     rank  7: 3011.56
     rank  8: 2997.09
     rank  9: 2998.97
     rank 10: 2998.37
     rank 11: 2996.73
     rank 12: 3009.67
     rank 13: 3015.89
     rank 14: 3013.34
     rank 15: 3021.04
     rank 16: 3228.12
     rank 17: 3228.42
     rank 18: 3233.84
     rank 19: 3230.86
  pure-backward-compute:
     rank  0: 1970.26
     rank  1: 1965.80
     rank  2: 1928.41
     rank  3: 1971.69
     rank  4: 3008.30
     rank  5: 3014.69
     rank  6: 3010.12
     rank  7: 3009.67
     rank  8: 2995.64
     rank  9: 2997.62
     rank 10: 2996.83
     rank 11: 2995.05
     rank 12: 3008.23
     rank 13: 3014.35
     rank 14: 3012.05
     rank 15: 3019.80
     rank 16: 3225.66
     rank 17: 3226.20
     rank 18: 3231.70
     rank 19: 3228.58
  batch-generator:
     rank  0: 1213.38
     rank  1: 1211.89
     rank  2: 1283.59
     rank  3: 1215.58
     rank  4: 1200.83
     rank  5: 1199.71
     rank  6: 1229.93
     rank  7: 1206.28
     rank  8: 1136.31
     rank  9: 1134.20
     rank 10: 1140.51
     rank 11: 1137.86
     rank 12: 1086.74
     rank 13: 1087.63
     rank 14: 1092.64
     rank 15: 1087.02
     rank 16: 1145.52
     rank 17: 1153.91
     rank 18: 1154.63
     rank 19: 1159.04
  forward-recv:
     rank  4: 4038.61
     rank  5: 4034.02
     rank  6: 4020.19
     rank  7: 4034.37
     rank  8: 6485.45
     rank  9: 6486.55
     rank 10: 6484.83
     rank 11: 6485.98
     rank 12: 8854.49
     rank 13: 8858.25
     rank 14: 8855.71
     rank 15: 8857.41
     rank 16: 11190.73
     rank 17: 11191.23
     rank 18: 11188.74
     rank 19: 11189.95
  forward-send:
     rank  0: 7107.39
     rank  1: 7109.96
     rank  2: 7088.77
     rank  3: 7107.79
     rank  4: 4385.31
     rank  5: 4391.76
     rank  6: 4383.70
     rank  7: 4389.12
     rank  8: 2196.29
     rank  9: 2201.47
     rank 10: 2194.98
     rank 11: 2198.80
     rank 12: 34.41
     rank 13: 35.17
     rank 14: 32.27
     rank 15: 33.32
  backward-recv:
     rank  0: 1319.98
     rank  1: 1323.16
     rank  2: 1323.54
     rank  3: 1323.73
     rank  4: 614.65
     rank  5: 611.69
     rank  6: 611.13
     rank  7: 610.25
     rank  8: 402.46
     rank  9: 401.42
     rank 10: 402.20
     rank 11: 402.95
     rank 12: 199.64
     rank 13: 200.13
     rank 14: 199.67
     rank 15: 199.67
  backward-send:
     rank  4: 18.83
     rank  5: 21.68
     rank  6: 22.18
     rank  7: 22.76
     rank  8: 31.30
     rank  9: 31.49
     rank 10: 31.26
     rank 11: 30.75
     rank 12: 20.88
     rank 13: 20.20
     rank 14: 20.50
     rank 15: 20.90
     rank 16: 10.59
     rank 17: 10.10
     rank 18: 10.53
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 5857.78
     rank  1: 5857.50
     rank  2: 5852.55
     rank  3: 5861.71
     rank  4: 3316.53
     rank  5: 3313.24
     rank  6: 3315.61
     rank  7: 3316.85
     rank  8: 3149.36
     rank  9: 3151.23
     rank 10: 3150.13
     rank 11: 3148.53
     rank 12: 2965.57
     rank 13: 2961.83
     rank 14: 2960.77
     rank 15: 2955.64
  backward-send-forward-recv:
     rank  4: 753.17
     rank  5: 751.93
     rank  6: 745.66
     rank  7: 755.88
     rank  8: 851.55
     rank  9: 847.63
     rank 10: 850.71
     rank 11: 850.25
     rank 12: 870.03
     rank 13: 868.56
     rank 14: 870.45
     rank 15: 869.66
     rank 16: 847.14
     rank 17: 840.93
     rank 18: 842.97
     rank 19: 836.23
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.14
     rank  1: 0.12
     rank  2: 0.12
     rank  3: 0.14
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 49.05
     rank  1: 48.86
     rank  2: 43.04
     rank  3: 46.17
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.23
     rank  1: 0.23
     rank  2: 0.24
     rank  3: 0.25
     rank  4: 0.05
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.05
     rank  9: 0.05
     rank 10: 0.03
     rank 11: 0.03
     rank 12: 0.03
     rank 13: 0.05
     rank 14: 0.02
     rank 15: 0.05
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.04
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.01
     rank 15: 0.04
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.03
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.01
     rank 15: 0.03
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 27.54
     rank  1: 27.22
     rank  2: 29.80
     rank  3: 27.58
     rank  4: 0.19
     rank  5: 0.11
     rank  6: 0.14
     rank  7: 0.11
     rank  8: 0.17
     rank  9: 0.20
     rank 10: 0.11
     rank 11: 0.21
     rank 12: 0.15
     rank 13: 0.19
     rank 14: 0.05
     rank 15: 0.19
     rank 16: 0.09
     rank 17: 0.08
     rank 18: 0.07
     rank 19: 0.03
  optimizer:
     rank  0: 29.04
     rank  1: 28.71
     rank  2: 31.29
     rank  3: 29.07
     rank  4: 1.70
     rank  5: 1.58
     rank  6: 1.64
     rank  7: 1.58
     rank  8: 1.68
     rank  9: 1.71
     rank 10: 1.60
     rank 11: 1.68
     rank 12: 1.64
     rank 13: 1.70
     rank 14: 1.52
     rank 15: 1.70
     rank 16: 1.58
     rank 17: 1.57
     rank 18: 1.57
     rank 19: 1.53
 [2024-12-02 16:45:02] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 7692.4 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.285778E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7649.69
     rank  1: 7649.62
     rank  2: 7649.65
     rank  3: 7649.60
     rank  4: 7648.79
     rank  5: 7648.77
     rank  6: 7648.72
     rank  7: 7648.78
     rank  8: 7648.95
     rank  9: 7648.81
     rank 10: 7648.79
     rank 11: 7648.81
     rank 12: 7648.84
     rank 13: 7648.84
     rank 14: 7648.78
     rank 15: 7648.89
     rank 16: 7648.69
     rank 17: 7648.72
     rank 18: 7648.76
     rank 19: 7648.85
  forward-compute:
     rank  0: 1125.28
     rank  1: 1133.41
     rank  2: 1128.27
     rank  3: 1126.14
     rank  4: 2876.89
     rank  5: 2883.63
     rank  6: 2874.98
     rank  7: 2878.14
     rank  8: 2775.76
     rank  9: 2784.39
     rank 10: 2779.37
     rank 11: 2778.74
     rank 12: 2801.79
     rank 13: 2806.47
     rank 14: 2802.20
     rank 15: 2806.44
     rank 16: 2947.91
     rank 17: 2955.29
     rank 18: 2952.26
     rank 19: 2947.59
  backward-compute:
     rank  0: 1069.22
     rank  1: 1067.24
     rank  2: 1063.53
     rank  3: 1068.70
     rank  4: 2990.46
     rank  5: 2996.02
     rank  6: 2994.89
     rank  7: 2990.62
     rank  8: 2969.95
     rank  9: 2973.96
     rank 10: 2972.26
     rank 11: 2972.28
     rank 12: 2993.19
     rank 13: 2994.34
     rank 14: 2992.88
     rank 15: 2990.62
     rank 16: 3234.46
     rank 17: 3234.15
     rank 18: 3233.86
     rank 19: 3235.70
  pure-backward-compute:
     rank  0: 1068.35
     rank  1: 1066.42
     rank  2: 1062.81
     rank  3: 1068.01
     rank  4: 2989.69
     rank  5: 2995.08
     rank  6: 2993.44
     rank  7: 2989.63
     rank  8: 2968.56
     rank  9: 2973.07
     rank 10: 2971.26
     rank 11: 2970.84
     rank 12: 2991.77
     rank 13: 2993.34
     rank 14: 2991.55
     rank 15: 2989.30
     rank 16: 3232.29
     rank 17: 3232.23
     rank 18: 3231.95
     rank 19: 3233.61
  batch-generator:
     rank  0: 75.15
     rank  1: 82.31
     rank  2: 75.98
     rank  3: 74.22
     rank  4: 59.33
     rank  5: 69.73
     rank  6: 76.30
     rank  7: 74.26
     rank  8: 56.67
     rank  9: 66.57
     rank 10: 59.99
     rank 11: 61.65
     rank 12: 59.38
     rank 13: 66.50
     rank 14: 62.34
     rank 15: 63.93
     rank 16: 52.77
     rank 17: 62.14
     rank 18: 58.59
     rank 19: 56.28
  forward-recv:
     rank  4: 84.56
     rank  5: 81.34
     rank  6: 85.21
     rank  7: 86.95
     rank  8: 280.85
     rank  9: 278.94
     rank 10: 280.25
     rank 11: 279.91
     rank 12: 457.41
     rank 13: 457.02
     rank 14: 457.12
     rank 15: 457.33
     rank 16: 628.97
     rank 17: 628.27
     rank 18: 628.98
     rank 19: 628.99
  forward-send:
     rank  0: 392.87
     rank  1: 384.39
     rank  2: 391.56
     rank  3: 393.69
     rank  4: 33.89
     rank  5: 29.14
     rank  6: 31.84
     rank  7: 32.37
     rank  8: 21.16
     rank  9: 19.47
     rank 10: 20.63
     rank 11: 20.89
     rank 12: 10.41
     rank 13: 9.62
     rank 14: 10.45
     rank 15: 10.39
  backward-recv:
     rank  0: 1323.83
     rank  1: 1325.36
     rank  2: 1326.27
     rank  3: 1325.48
     rank  4: 614.88
     rank  5: 613.38
     rank  6: 613.25
     rank  7: 614.59
     rank  8: 404.92
     rank  9: 405.64
     rank 10: 404.85
     rank 11: 405.01
     rank 12: 201.57
     rank 13: 201.90
     rank 14: 201.63
     rank 15: 201.69
  backward-send:
     rank  4: 20.46
     rank  5: 22.23
     rank  6: 22.76
     rank  7: 21.74
     rank  8: 31.43
     rank  9: 30.31
     rank 10: 31.24
     rank 11: 31.29
     rank 12: 20.93
     rank 13: 20.50
     rank 14: 20.57
     rank 15: 20.98
     rank 16: 10.60
     rank 17: 9.99
     rank 18: 10.09
     rank 19: 10.49
  forward-send-backward-recv:
     rank  0: 3723.52
     rank  1: 3725.03
     rank  2: 3728.41
     rank  3: 3723.96
     rank  4: 883.14
     rank  5: 882.00
     rank  6: 881.57
     rank  7: 883.91
     rank  8: 754.06
     rank  9: 752.50
     rank 10: 754.22
     rank 11: 750.37
     rank 12: 548.87
     rank 13: 548.00
     rank 14: 547.55
     rank 15: 550.01
  backward-send-forward-recv:
     rank  4: 72.43
     rank  5: 72.55
     rank  6: 73.42
     rank  7: 70.91
     rank  8: 146.10
     rank  9: 141.97
     rank 10: 145.35
     rank 11: 146.04
     rank 12: 154.23
     rank 13: 152.41
     rank 14: 155.91
     rank 15: 153.69
     rank 16: 168.51
     rank 17: 163.82
     rank 18: 166.57
     rank 19: 167.74
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.07
     rank  1: 0.06
     rank  2: 0.06
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.08
  all-grads-sync:
     rank  0: 0.67
     rank  1: 0.66
     rank  2: 0.64
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.18
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.03
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.04
  optimizer-inner-step:
     rank  0: 10.40
     rank  1: 10.23
     rank  2: 10.32
     rank  3: 10.36
     rank  4: 0.04
     rank  5: 0.07
     rank  6: 0.04
     rank  7: 0.08
     rank  8: 0.17
     rank  9: 0.07
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.06
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.05
     rank 16: 0.03
     rank 17: 0.08
     rank 18: 0.17
     rank 19: 0.10
  optimizer:
     rank  0: 11.59
     rank  1: 11.41
     rank  2: 11.51
     rank  3: 11.54
     rank  4: 1.23
     rank  5: 1.25
     rank  6: 1.23
     rank  7: 1.26
     rank  8: 1.35
     rank  9: 1.25
     rank 10: 1.25
     rank 11: 1.27
     rank 12: 1.25
     rank 13: 1.25
     rank 14: 1.25
     rank 15: 1.23
     rank 16: 1.21
     rank 17: 1.26
     rank 18: 1.35
     rank 19: 1.30
 [2024-12-02 16:45:09] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 7702.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 7.115121E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7663.19
     rank  1: 7663.76
     rank  2: 7663.20
     rank  3: 7663.55
     rank  4: 7662.29
     rank  5: 7662.26
     rank  6: 7662.30
     rank  7: 7662.64
     rank  8: 7662.38
     rank  9: 7662.28
     rank 10: 7662.30
     rank 11: 7662.74
     rank 12: 7662.31
     rank 13: 7662.32
     rank 14: 7662.28
     rank 15: 7662.71
     rank 16: 7662.30
     rank 17: 7662.33
     rank 18: 7662.32
     rank 19: 7663.13
  forward-compute:
     rank  0: 1248.70
     rank  1: 1259.09
     rank  2: 1250.08
     rank  3: 1248.74
     rank  4: 2867.84
     rank  5: 2880.74
     rank  6: 2867.25
     rank  7: 2872.91
     rank  8: 2771.19
     rank  9: 2781.44
     rank 10: 2774.39
     rank 11: 2772.12
     rank 12: 2802.74
     rank 13: 2804.79
     rank 14: 2800.09
     rank 15: 2801.27
     rank 16: 2954.95
     rank 17: 2963.40
     rank 18: 2958.99
     rank 19: 2954.17
  backward-compute:
     rank  0: 1044.00
     rank  1: 1046.47
     rank  2: 1041.71
     rank  3: 1049.71
     rank  4: 2992.67
     rank  5: 2995.91
     rank  6: 2997.02
     rank  7: 2991.34
     rank  8: 2970.36
     rank  9: 2974.50
     rank 10: 2971.74
     rank 11: 2969.74
     rank 12: 3003.21
     rank 13: 3007.85
     rank 14: 3005.65
     rank 15: 3002.86
     rank 16: 3242.71
     rank 17: 3242.91
     rank 18: 3243.88
     rank 19: 3244.45
  pure-backward-compute:
     rank  0: 1043.19
     rank  1: 1045.69
     rank  2: 1041.05
     rank  3: 1049.02
     rank  4: 2991.79
     rank  5: 2995.16
     rank  6: 2995.54
     rank  7: 2990.58
     rank  8: 2968.90
     rank  9: 2973.70
     rank 10: 2970.80
     rank 11: 2968.34
     rank 12: 3002.14
     rank 13: 3006.85
     rank 14: 3004.54
     rank 15: 3001.83
     rank 16: 3240.48
     rank 17: 3241.25
     rank 18: 3242.20
     rank 19: 3241.84
  batch-generator:
     rank  0: 60.37
     rank  1: 71.72
     rank  2: 63.45
     rank  3: 62.55
     rank  4: 54.19
     rank  5: 68.83
     rank  6: 67.98
     rank  7: 70.25
     rank  8: 54.96
     rank  9: 66.77
     rank 10: 58.50
     rank 11: 59.76
     rank 12: 52.37
     rank 13: 61.21
     rank 14: 56.48
     rank 15: 56.39
     rank 16: 52.31
     rank 17: 62.24
     rank 18: 58.72
     rank 19: 57.50
  forward-recv:
     rank  4: 83.41
     rank  5: 78.27
     rank  6: 83.44
     rank  7: 83.69
     rank  8: 278.51
     rank  9: 274.69
     rank 10: 278.58
     rank 11: 278.63
     rank 12: 453.81
     rank 13: 452.30
     rank 14: 453.77
     rank 15: 453.74
     rank 16: 629.29
     rank 17: 628.93
     rank 18: 629.29
     rank 19: 629.29
  forward-send:
     rank  0: 398.23
     rank  1: 386.71
     rank  2: 397.52
     rank  3: 398.85
     rank  4: 33.66
     rank  5: 26.73
     rank  6: 32.92
     rank  7: 34.03
     rank  8: 22.57
     rank  9: 20.24
     rank 10: 22.26
     rank 11: 22.54
     rank 12: 10.46
     rank 13: 10.07
     rank 14: 10.45
     rank 15: 10.59
  backward-recv:
     rank  0: 1330.20
     rank  1: 1330.07
     rank  2: 1331.23
     rank  3: 1329.36
     rank  4: 619.43
     rank  5: 619.64
     rank  6: 616.81
     rank  7: 620.60
     rank  8: 402.99
     rank  9: 403.16
     rank 10: 402.79
     rank 11: 403.54
     rank 12: 199.93
     rank 13: 200.27
     rank 14: 199.75
     rank 15: 199.69
  backward-send:
     rank  4: 21.73
     rank  5: 21.56
     rank  6: 23.10
     rank  7: 20.85
     rank  8: 31.32
     rank  9: 30.71
     rank 10: 31.35
     rank 11: 30.79
     rank 12: 20.79
     rank 13: 20.18
     rank 14: 20.57
     rank 15: 20.90
     rank 16: 10.55
     rank 17: 9.99
     rank 18: 10.19
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 3626.78
     rank  1: 3626.64
     rank  2: 3630.69
     rank  3: 3624.48
     rank  4: 898.82
     rank  5: 898.01
     rank  6: 896.71
     rank  7: 900.02
     rank  8: 777.26
     rank  9: 776.49
     rank 10: 778.07
     rank 11: 776.29
     rank 12: 557.94
     rank 13: 558.62
     rank 14: 557.80
     rank 15: 561.14
  backward-send-forward-recv:
     rank  4: 74.11
     rank  5: 73.04
     rank  6: 73.37
     rank  7: 70.10
     rank  8: 145.27
     rank  9: 141.53
     rank 10: 143.74
     rank 11: 145.67
     rank 12: 155.86
     rank 13: 152.44
     rank 14: 155.98
     rank 15: 155.76
     rank 16: 167.72
     rank 17: 162.05
     rank 18: 165.30
     rank 19: 166.77
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.08
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.18
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.62
     rank  2: 0.63
     rank  3: 0.62
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.03
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.19
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.22
     rank  1: 10.31
     rank  2: 10.32
     rank  3: 10.27
     rank  4: 0.03
     rank  5: 0.05
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.13
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.09
     rank 12: 0.04
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.07
     rank 16: 0.03
     rank 17: 0.04
     rank 18: 0.03
     rank 19: 0.08
  optimizer:
     rank  0: 10.83
     rank  1: 10.92
     rank  2: 10.94
     rank  3: 10.89
     rank  4: 0.64
     rank  5: 0.66
     rank  6: 0.64
     rank  7: 0.65
     rank  8: 0.78
     rank  9: 0.65
     rank 10: 0.65
     rank 11: 0.71
     rank 12: 0.65
     rank 13: 0.65
     rank 14: 0.64
     rank 15: 0.68
     rank 16: 0.64
     rank 17: 0.65
     rank 18: 0.65
     rank 19: 0.69
 [2024-12-02 16:45:17] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 7703.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 5.811421E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7664.77
     rank  1: 7664.76
     rank  2: 7664.78
     rank  3: 7664.80
     rank  4: 7663.88
     rank  5: 7663.86
     rank  6: 7663.89
     rank  7: 7663.91
     rank  8: 7663.93
     rank  9: 7663.88
     rank 10: 7663.93
     rank 11: 7663.96
     rank 12: 7663.91
     rank 13: 7663.91
     rank 14: 7663.93
     rank 15: 7663.95
     rank 16: 7663.92
     rank 17: 7663.87
     rank 18: 7663.89
     rank 19: 7663.93
  forward-compute:
     rank  0: 1008.74
     rank  1: 1021.77
     rank  2: 1010.72
     rank  3: 1010.13
     rank  4: 2870.51
     rank  5: 2886.75
     rank  6: 2871.07
     rank  7: 2877.88
     rank  8: 2792.07
     rank  9: 2804.26
     rank 10: 2794.75
     rank 11: 2794.15
     rank 12: 2803.15
     rank 13: 2810.19
     rank 14: 2803.51
     rank 15: 2804.01
     rank 16: 2949.93
     rank 17: 2957.50
     rank 18: 2954.25
     rank 19: 2949.06
  backward-compute:
     rank  0: 1016.37
     rank  1: 1018.52
     rank  2: 1014.26
     rank  3: 1021.51
     rank  4: 2985.51
     rank  5: 2990.12
     rank  6: 2989.91
     rank  7: 2984.41
     rank  8: 2973.08
     rank  9: 2974.71
     rank 10: 2973.04
     rank 11: 2971.10
     rank 12: 3011.64
     rank 13: 3014.67
     rank 14: 3013.78
     rank 15: 3012.24
     rank 16: 3245.67
     rank 17: 3246.17
     rank 18: 3246.71
     rank 19: 3247.40
  pure-backward-compute:
     rank  0: 1015.59
     rank  1: 1017.87
     rank  2: 1013.61
     rank  3: 1020.82
     rank  4: 2984.51
     rank  5: 2989.29
     rank  6: 2988.05
     rank  7: 2983.52
     rank  8: 2971.67
     rank  9: 2973.86
     rank 10: 2972.12
     rank 11: 2969.66
     rank 12: 3010.64
     rank 13: 3013.79
     rank 14: 3012.62
     rank 15: 3011.20
     rank 16: 3243.42
     rank 17: 3244.34
     rank 18: 3244.94
     rank 19: 3244.40
  batch-generator:
     rank  0: 56.37
     rank  1: 71.62
     rank  2: 60.62
     rank  3: 60.49
     rank  4: 64.19
     rank  5: 78.41
     rank  6: 74.19
     rank  7: 77.13
     rank  8: 52.56
     rank  9: 67.38
     rank 10: 56.97
     rank 11: 57.75
     rank 12: 51.99
     rank 13: 62.89
     rank 14: 56.43
     rank 15: 55.22
     rank 16: 51.77
     rank 17: 60.82
     rank 18: 58.22
     rank 19: 55.11
  forward-recv:
     rank  4: 82.81
     rank  5: 77.68
     rank  6: 82.88
     rank  7: 82.84
     rank  8: 275.55
     rank  9: 272.10
     rank 10: 275.42
     rank 11: 275.41
     rank 12: 451.39
     rank 13: 449.28
     rank 14: 451.25
     rank 15: 451.39
     rank 16: 626.88
     rank 17: 626.15
     rank 18: 626.76
     rank 19: 626.83
  forward-send:
     rank  0: 393.59
     rank  1: 379.85
     rank  2: 392.07
     rank  3: 392.69
     rank  4: 32.12
     rank  5: 24.45
     rank  6: 30.85
     rank  7: 31.80
     rank  8: 21.42
     rank  9: 18.09
     rank 10: 20.89
     rank 11: 21.17
     rank 12: 10.45
     rank 13: 9.69
     rank 14: 10.38
     rank 15: 10.38
  backward-recv:
     rank  0: 1334.16
     rank  1: 1335.63
     rank  2: 1335.27
     rank  3: 1334.40
     rank  4: 617.17
     rank  5: 615.16
     rank  6: 615.43
     rank  7: 616.67
     rank  8: 402.85
     rank  9: 403.82
     rank 10: 404.21
     rank 11: 404.29
     rank 12: 199.39
     rank 13: 199.80
     rank 14: 199.19
     rank 15: 199.39
  backward-send:
     rank  4: 21.03
     rank  5: 23.00
     rank  6: 22.42
     rank  7: 21.45
     rank  8: 31.62
     rank  9: 30.43
     rank 10: 31.11
     rank 11: 30.97
     rank 12: 20.74
     rank 13: 20.07
     rank 14: 20.74
     rank 15: 20.98
     rank 16: 10.71
     rank 17: 10.37
     rank 18: 10.32
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 3897.87
     rank  1: 3895.19
     rank  2: 3900.64
     rank  3: 3894.19
     rank  4: 904.97
     rank  5: 902.43
     rank  6: 901.51
     rank  7: 905.27
     rank  8: 754.55
     rank  9: 755.01
     rank 10: 756.49
     rank 11: 753.99
     rank 12: 547.01
     rank 13: 546.17
     rank 14: 544.66
     rank 15: 547.83
  backward-send-forward-recv:
     rank  4: 73.79
     rank  5: 71.92
     rank  6: 73.01
     rank  7: 69.60
     rank  8: 144.34
     rank  9: 139.95
     rank 10: 143.56
     rank 11: 144.88
     rank 12: 156.83
     rank 13: 151.98
     rank 14: 156.63
     rank 15: 156.21
     rank 16: 167.75
     rank 17: 162.97
     rank 18: 164.93
     rank 19: 166.33
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.63
     rank  1: 0.62
     rank  2: 0.63
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.31
     rank  1: 10.33
     rank  2: 10.34
     rank  3: 10.28
     rank  4: 0.03
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.08
     rank  9: 0.06
     rank 10: 0.07
     rank 11: 0.07
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.04
     rank 17: 0.08
     rank 18: 0.08
     rank 19: 0.08
  optimizer:
     rank  0: 10.93
     rank  1: 10.95
     rank  2: 10.96
     rank  3: 10.89
     rank  4: 0.65
     rank  5: 0.65
     rank  6: 0.68
     rank  7: 0.65
     rank  8: 0.70
     rank  9: 0.68
     rank 10: 0.69
     rank 11: 0.69
     rank 12: 0.65
     rank 13: 0.69
     rank 14: 0.65
     rank 15: 0.65
     rank 16: 0.65
     rank 17: 0.69
     rank 18: 0.69
     rank 19: 0.69
 [2024-12-02 16:45:25] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 7674.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 4.601167E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7640.67
     rank  1: 7640.58
     rank  2: 7640.67
     rank  3: 7640.73
     rank  4: 7639.77
     rank  5: 7639.70
     rank  6: 7639.75
     rank  7: 7639.83
     rank  8: 7639.85
     rank  9: 7639.80
     rank 10: 7639.75
     rank 11: 7639.92
     rank 12: 7639.77
     rank 13: 7639.74
     rank 14: 7639.77
     rank 15: 7639.91
     rank 16: 7639.87
     rank 17: 7639.75
     rank 18: 7639.75
     rank 19: 7639.88
  forward-compute:
     rank  0: 1096.91
     rank  1: 1102.98
     rank  2: 1101.97
     rank  3: 1096.88
     rank  4: 2875.23
     rank  5: 2886.19
     rank  6: 2879.25
     rank  7: 2881.23
     rank  8: 2788.82
     rank  9: 2794.23
     rank 10: 2792.90
     rank 11: 2790.42
     rank 12: 2803.92
     rank 13: 2807.45
     rank 14: 2804.50
     rank 15: 2803.76
     rank 16: 2947.70
     rank 17: 2955.42
     rank 18: 2952.86
     rank 19: 2948.66
  backward-compute:
     rank  0: 1041.47
     rank  1: 1046.46
     rank  2: 1039.70
     rank  3: 1046.34
     rank  4: 2977.66
     rank  5: 2978.86
     rank  6: 2980.96
     rank  7: 2975.57
     rank  8: 2970.87
     rank  9: 2975.19
     rank 10: 2971.97
     rank 11: 2969.70
     rank 12: 3006.68
     rank 13: 3011.16
     rank 14: 3010.52
     rank 15: 3009.72
     rank 16: 3223.90
     rank 17: 3224.73
     rank 18: 3225.41
     rank 19: 3224.58
  pure-backward-compute:
     rank  0: 1040.72
     rank  1: 1045.77
     rank  2: 1039.05
     rank  3: 1045.61
     rank  4: 2976.55
     rank  5: 2978.06
     rank  6: 2979.21
     rank  7: 2974.81
     rank  8: 2969.67
     rank  9: 2974.22
     rank 10: 2971.25
     rank 11: 2968.29
     rank 12: 3005.66
     rank 13: 3010.37
     rank 14: 3009.34
     rank 15: 3008.50
     rank 16: 3221.40
     rank 17: 3223.12
     rank 18: 3223.82
     rank 19: 3220.60
  batch-generator:
     rank  0: 57.78
     rank  1: 64.59
     rank  2: 63.26
     rank  3: 58.47
     rank  4: 57.38
     rank  5: 71.81
     rank  6: 82.70
     rank  7: 74.86
     rank  8: 54.22
     rank  9: 63.25
     rank 10: 59.51
     rank 11: 57.75
     rank 12: 52.46
     rank 13: 59.91
     rank 14: 57.27
     rank 15: 53.96
     rank 16: 57.50
     rank 17: 65.43
     rank 18: 63.51
     rank 19: 61.25
  forward-recv:
     rank  4: 82.75
     rank  5: 78.91
     rank  6: 80.25
     rank  7: 82.88
     rank  8: 277.32
     rank  9: 275.15
     rank 10: 276.73
     rank 11: 277.37
     rank 12: 452.98
     rank 13: 453.64
     rank 14: 452.20
     rank 15: 453.30
     rank 16: 627.45
     rank 17: 626.74
     rank 18: 627.39
     rank 19: 627.62
  forward-send:
     rank  0: 395.27
     rank  1: 388.40
     rank  2: 390.70
     rank  3: 395.66
     rank  4: 34.58
     rank  5: 31.98
     rank  6: 32.28
     rank  7: 35.07
     rank  8: 20.72
     rank  9: 19.91
     rank 10: 19.67
     rank 11: 20.83
     rank 12: 10.40
     rank 13: 9.67
     rank 14: 10.40
     rank 15: 10.37
  backward-recv:
     rank  0: 1323.35
     rank  1: 1323.24
     rank  2: 1324.14
     rank  3: 1323.29
     rank  4: 612.66
     rank  5: 613.50
     rank  6: 611.95
     rank  7: 613.91
     rank  8: 401.99
     rank  9: 401.51
     rank 10: 402.44
     rank 11: 402.39
     rank 12: 198.16
     rank 13: 198.78
     rank 14: 198.18
     rank 15: 198.28
  backward-send:
     rank  4: 21.83
     rank  5: 21.85
     rank  6: 22.78
     rank  7: 21.74
     rank  8: 31.41
     rank  9: 31.05
     rank 10: 30.85
     rank 11: 31.29
     rank 12: 20.59
     rank 13: 19.86
     rank 14: 20.93
     rank 15: 20.86
     rank 16: 10.59
     rank 17: 10.17
     rank 18: 10.28
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 3769.15
     rank  1: 3765.53
     rank  2: 3772.45
     rank  3: 3766.44
     rank  4: 886.55
     rank  5: 885.51
     rank  6: 884.67
     rank  7: 887.01
     rank  8: 737.87
     rank  9: 736.73
     rank 10: 739.33
     rank 11: 736.88
     rank 12: 529.10
     rank 13: 525.97
     rank 14: 525.03
     rank 15: 527.98
  backward-send-forward-recv:
     rank  4: 73.53
     rank  5: 71.15
     rank  6: 72.93
     rank  7: 70.51
     rank  8: 145.08
     rank  9: 142.39
     rank 10: 143.47
     rank 11: 145.17
     rank 12: 156.14
     rank 13: 152.05
     rank 14: 156.30
     rank 15: 155.91
     rank 16: 168.58
     rank 17: 163.73
     rank 18: 165.16
     rank 19: 167.03
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.10
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.62
     rank  2: 0.67
     rank  3: 0.67
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.05
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.17
     rank  2: 0.18
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.31
     rank  1: 10.18
     rank  2: 10.40
     rank  3: 10.35
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.03
     rank 11: 0.08
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.06
     rank 15: 0.07
     rank 16: 0.09
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.07
  optimizer:
     rank  0: 10.97
     rank  1: 10.84
     rank  2: 11.06
     rank  3: 10.99
     rank  4: 0.69
     rank  5: 0.69
     rank  6: 0.69
     rank  7: 0.72
     rank  8: 0.70
     rank  9: 0.71
     rank 10: 0.69
     rank 11: 0.74
     rank 12: 0.69
     rank 13: 0.69
     rank 14: 0.72
     rank 15: 0.72
     rank 16: 0.75
     rank 17: 0.68
     rank 18: 0.68
     rank 19: 0.72
 [2024-12-02 16:45:33] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 7683.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 3.495010E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7644.35
     rank  1: 7644.35
     rank  2: 7644.40
     rank  3: 7644.38
     rank  4: 7643.48
     rank  5: 7643.47
     rank  6: 7643.47
     rank  7: 7643.48
     rank  8: 7643.48
     rank  9: 7643.48
     rank 10: 7643.49
     rank 11: 7643.51
     rank 12: 7643.49
     rank 13: 7643.52
     rank 14: 7643.50
     rank 15: 7643.53
     rank 16: 7643.50
     rank 17: 7643.45
     rank 18: 7643.46
     rank 19: 7643.52
  forward-compute:
     rank  0: 1008.08
     rank  1: 1021.18
     rank  2: 1009.72
     rank  3: 1008.73
     rank  4: 2872.97
     rank  5: 2891.40
     rank  6: 2874.91
     rank  7: 2880.22
     rank  8: 2795.68
     rank  9: 2806.22
     rank 10: 2798.14
     rank 11: 2796.96
     rank 12: 2801.30
     rank 13: 2807.27
     rank 14: 2801.22
     rank 15: 2801.80
     rank 16: 2944.29
     rank 17: 2951.94
     rank 18: 2948.29
     rank 19: 2944.21
  backward-compute:
     rank  0: 1043.28
     rank  1: 1047.48
     rank  2: 1043.03
     rank  3: 1049.90
     rank  4: 2990.67
     rank  5: 2994.59
     rank  6: 2993.50
     rank  7: 2988.35
     rank  8: 2971.61
     rank  9: 2974.57
     rank 10: 2973.23
     rank 11: 2971.10
     rank 12: 3010.49
     rank 13: 3014.30
     rank 14: 3012.68
     rank 15: 3011.71
     rank 16: 3229.60
     rank 17: 3229.37
     rank 18: 3229.92
     rank 19: 3229.39
  pure-backward-compute:
     rank  0: 1042.63
     rank  1: 1046.84
     rank  2: 1042.40
     rank  3: 1049.24
     rank  4: 2989.44
     rank  5: 2993.78
     rank  6: 2992.03
     rank  7: 2987.59
     rank  8: 2970.47
     rank  9: 2973.81
     rank 10: 2972.40
     rank 11: 2969.73
     rank 12: 3009.54
     rank 13: 3013.38
     rank 14: 3011.56
     rank 15: 3010.78
     rank 16: 3227.26
     rank 17: 3227.65
     rank 18: 3228.24
     rank 19: 3227.23
  batch-generator:
     rank  0: 52.24
     rank  1: 67.32
     rank  2: 55.63
     rank  3: 55.46
     rank  4: 62.71
     rank  5: 79.36
     rank  6: 68.77
     rank  7: 72.90
     rank  8: 51.80
     rank  9: 65.32
     rank 10: 55.48
     rank 11: 56.77
     rank 12: 52.73
     rank 13: 62.79
     rank 14: 56.85
     rank 15: 54.85
     rank 16: 52.35
     rank 17: 61.17
     rank 18: 58.23
     rank 19: 55.89
  forward-recv:
     rank  4: 82.93
     rank  5: 76.59
     rank  6: 82.69
     rank  7: 82.77
     rank  8: 276.05
     rank  9: 271.72
     rank 10: 275.99
     rank 11: 275.97
     rank 12: 451.70
     rank 13: 450.09
     rank 14: 451.49
     rank 15: 451.57
     rank 16: 627.15
     rank 17: 626.76
     rank 18: 627.12
     rank 19: 627.31
  forward-send:
     rank  0: 395.62
     rank  1: 381.31
     rank  2: 394.28
     rank  3: 395.30
     rank  4: 33.29
     rank  5: 25.83
     rank  6: 32.35
     rank  7: 33.43
     rank  8: 21.55
     rank  9: 19.15
     rank 10: 21.16
     rank 11: 21.29
     rank 12: 10.47
     rank 13: 10.04
     rank 14: 10.46
     rank 15: 10.54
  backward-recv:
     rank  0: 1328.46
     rank  1: 1329.12
     rank  2: 1328.82
     rank  3: 1327.84
     rank  4: 607.76
     rank  5: 605.51
     rank  6: 607.55
     rank  7: 608.01
     rank  8: 398.17
     rank  9: 398.88
     rank 10: 397.86
     rank 11: 398.46
     rank 12: 195.29
     rank 13: 195.36
     rank 14: 195.48
     rank 15: 195.31
  backward-send:
     rank  4: 21.77
     rank  5: 22.54
     rank  6: 22.17
     rank  7: 21.22
     rank  8: 31.27
     rank  9: 30.35
     rank 10: 31.40
     rank 11: 31.10
     rank 12: 20.88
     rank 13: 20.46
     rank 14: 20.65
     rank 15: 20.97
     rank 16: 10.50
     rank 17: 10.19
     rank 18: 10.47
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 3855.02
     rank  1: 3851.38
     rank  2: 3856.86
     rank  3: 3850.64
     rank  4: 884.32
     rank  5: 881.37
     rank  6: 881.99
     rank  7: 885.73
     rank  8: 733.51
     rank  9: 734.25
     rank 10: 735.26
     rank 11: 732.92
     rank 12: 531.02
     rank 13: 530.07
     rank 14: 528.47
     rank 15: 531.84
  backward-send-forward-recv:
     rank  4: 73.08
     rank  5: 71.73
     rank  6: 72.92
     rank  7: 69.75
     rank  8: 145.25
     rank  9: 141.24
     rank 10: 143.75
     rank 11: 145.38
     rank 12: 156.85
     rank 13: 152.06
     rank 14: 156.95
     rank 15: 156.48
     rank 16: 167.10
     rank 17: 162.70
     rank 18: 165.56
     rank 19: 167.17
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.04
     rank  3: 0.05
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.62
     rank  2: 0.63
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.04
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.18
     rank  3: 0.17
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.20
     rank  1: 10.30
     rank  2: 10.21
     rank  3: 10.29
     rank  4: 0.06
     rank  5: 0.07
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.08
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.05
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.05
  optimizer:
     rank  0: 10.79
     rank  1: 10.90
     rank  2: 10.79
     rank  3: 10.89
     rank  4: 0.66
     rank  5: 0.66
     rank  6: 0.62
     rank  7: 0.62
     rank  8: 0.62
     rank  9: 0.62
     rank 10: 0.62
     rank 11: 0.67
     rank 12: 0.62
     rank 13: 0.62
     rank 14: 0.63
     rank 15: 0.62
     rank 16: 0.62
     rank 17: 0.62
     rank 18: 0.62
     rank 19: 0.64
 [2024-12-02 16:45:40] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 7684.3 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.829232E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7650.92
     rank  1: 7650.92
     rank  2: 7650.99
     rank  3: 7650.93
     rank  4: 7650.00
     rank  5: 7650.07
     rank  6: 7650.08
     rank  7: 7650.08
     rank  8: 7650.00
     rank  9: 7650.04
     rank 10: 7650.13
     rank 11: 7650.05
     rank 12: 7650.02
     rank 13: 7650.04
     rank 14: 7650.11
     rank 15: 7650.06
     rank 16: 7649.96
     rank 17: 7650.02
     rank 18: 7650.08
     rank 19: 7650.03
  forward-compute:
     rank  0: 1012.60
     rank  1: 1016.74
     rank  2: 1014.21
     rank  3: 1016.27
     rank  4: 2867.17
     rank  5: 2872.57
     rank  6: 2869.09
     rank  7: 2879.36
     rank  8: 2796.83
     rank  9: 2801.78
     rank 10: 2798.94
     rank 11: 2798.79
     rank 12: 2801.78
     rank 13: 2807.23
     rank 14: 2802.78
     rank 15: 2802.40
     rank 16: 2946.62
     rank 17: 2951.95
     rank 18: 2949.35
     rank 19: 2945.13
  backward-compute:
     rank  0: 1047.89
     rank  1: 1049.52
     rank  2: 1046.98
     rank  3: 1057.46
     rank  4: 3002.10
     rank  5: 3008.96
     rank  6: 3005.69
     rank  7: 3000.44
     rank  8: 2974.54
     rank  9: 2977.41
     rank 10: 2975.97
     rank 11: 2973.84
     rank 12: 3009.58
     rank 13: 3011.92
     rank 14: 3010.68
     rank 15: 3010.34
     rank 16: 3230.46
     rank 17: 3232.53
     rank 18: 3231.14
     rank 19: 3232.23
  pure-backward-compute:
     rank  0: 1047.22
     rank  1: 1048.81
     rank  2: 1046.35
     rank  3: 1056.82
     rank  4: 3001.15
     rank  5: 3007.96
     rank  6: 3004.46
     rank  7: 2999.69
     rank  8: 2973.37
     rank  9: 2976.69
     rank 10: 2975.24
     rank 11: 2972.53
     rank 12: 3008.61
     rank 13: 3011.04
     rank 14: 3009.67
     rank 15: 3009.38
     rank 16: 3228.30
     rank 17: 3230.60
     rank 18: 3229.44
     rank 19: 3229.80
  batch-generator:
     rank  0: 51.92
     rank  1: 57.52
     rank  2: 55.81
     rank  3: 58.04
     rank  4: 58.89
     rank  5: 69.23
     rank  6: 64.84
     rank  7: 75.56
     rank  8: 53.83
     rank  9: 61.28
     rank 10: 58.22
     rank 11: 58.21
     rank 12: 52.73
     rank 13: 62.13
     rank 14: 57.85
     rank 15: 54.93
     rank 16: 53.13
     rank 17: 60.52
     rank 18: 58.57
     rank 19: 55.93
  forward-recv:
     rank  4: 84.04
     rank  5: 81.82
     rank  6: 84.05
     rank  7: 82.33
     rank  8: 280.64
     rank  9: 282.82
     rank 10: 280.71
     rank 11: 280.07
     rank 12: 455.12
     rank 13: 452.95
     rank 14: 454.94
     rank 15: 455.10
     rank 16: 630.99
     rank 17: 630.31
     rank 18: 630.87
     rank 19: 631.02
  forward-send:
     rank  0: 402.47
     rank  1: 397.64
     rank  2: 401.22
     rank  3: 399.14
     rank  4: 33.20
     rank  5: 31.06
     rank  6: 32.02
     rank  7: 32.34
     rank  8: 21.31
     rank  9: 17.96
     rank 10: 20.85
     rank 11: 21.22
     rank 12: 10.45
     rank 13: 9.71
     rank 14: 10.33
     rank 15: 10.39
  backward-recv:
     rank  0: 1325.62
     rank  1: 1325.62
     rank  2: 1326.38
     rank  3: 1324.50
     rank  4: 607.36
     rank  5: 606.53
     rank  6: 606.52
     rank  7: 608.58
     rank  8: 398.00
     rank  9: 398.33
     rank 10: 398.38
     rank 11: 397.99
     rank 12: 199.78
     rank 13: 200.33
     rank 14: 199.78
     rank 15: 199.81
  backward-send:
     rank  4: 22.10
     rank  5: 22.37
     rank  6: 22.60
     rank  7: 20.74
     rank  8: 31.36
     rank  9: 30.62
     rank 10: 31.10
     rank 11: 31.11
     rank 12: 20.90
     rank 13: 20.20
     rank 14: 20.99
     rank 15: 20.84
     rank 16: 10.52
     rank 17: 10.37
     rank 18: 10.56
     rank 19: 10.46
  forward-send-backward-recv:
     rank  0: 3848.12
     rank  1: 3846.72
     rank  2: 3850.05
     rank  3: 3841.67
     rank  4: 882.71
     rank  5: 879.47
     rank  6: 879.22
     rank  7: 883.57
     rank  8: 729.62
     rank  9: 731.22
     rank 10: 731.94
     rank 11: 729.61
     rank 12: 528.17
     rank 13: 528.90
     rank 14: 526.80
     rank 15: 529.53
  backward-send-forward-recv:
     rank  4: 74.14
     rank  5: 72.76
     rank  6: 73.84
     rank  7: 67.47
     rank  8: 145.01
     rank  9: 141.25
     rank 10: 143.83
     rank 11: 145.29
     rank 12: 156.61
     rank 13: 153.01
     rank 14: 155.98
     rank 15: 156.16
     rank 16: 167.61
     rank 17: 162.71
     rank 18: 166.01
     rank 19: 167.14
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.04
     rank 19: 0.03
  embedding-grads-all-reduce:
     rank  0: 0.06
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.09
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.66
     rank  1: 0.62
     rank  2: 0.64
     rank  3: 0.63
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.05
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.19
     rank  1: 10.35
     rank  2: 10.16
     rank  3: 10.26
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.06
     rank  9: 0.03
     rank 10: 0.04
     rank 11: 0.08
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.03
     rank 17: 0.07
     rank 18: 0.07
     rank 19: 0.07
  optimizer:
     rank  0: 10.82
     rank  1: 10.99
     rank  2: 10.78
     rank  3: 10.89
     rank  4: 0.69
     rank  5: 0.66
     rank  6: 0.70
     rank  7: 0.66
     rank  8: 0.69
     rank  9: 0.66
     rank 10: 0.66
     rank 11: 0.71
     rank 12: 0.69
     rank 13: 0.66
     rank 14: 0.66
     rank 15: 0.66
     rank 16: 0.66
     rank 17: 0.69
     rank 18: 0.69
     rank 19: 0.69
 [2024-12-02 16:45:48] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 7683.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 2.286391E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7650.37
     rank  1: 7650.40
     rank  2: 7650.37
     rank  3: 7650.34
     rank  4: 7649.45
     rank  5: 7649.48
     rank  6: 7649.46
     rank  7: 7649.42
     rank  8: 7649.49
     rank  9: 7649.49
     rank 10: 7649.49
     rank 11: 7649.54
     rank 12: 7649.46
     rank 13: 7649.54
     rank 14: 7649.47
     rank 15: 7649.53
     rank 16: 7649.42
     rank 17: 7649.47
     rank 18: 7649.47
     rank 19: 7649.42
  forward-compute:
     rank  0: 1018.98
     rank  1: 1032.06
     rank  2: 1021.05
     rank  3: 1020.51
     rank  4: 2866.91
     rank  5: 2884.72
     rank  6: 2868.84
     rank  7: 2873.62
     rank  8: 2792.15
     rank  9: 2802.02
     rank 10: 2793.74
     rank 11: 2793.39
     rank 12: 2801.85
     rank 13: 2806.08
     rank 14: 2801.79
     rank 15: 2802.35
     rank 16: 2944.42
     rank 17: 2950.56
     rank 18: 2948.05
     rank 19: 2944.12
  backward-compute:
     rank  0: 1047.77
     rank  1: 1050.74
     rank  2: 1046.52
     rank  3: 1053.64
     rank  4: 3002.00
     rank  5: 3005.57
     rank  6: 3005.11
     rank  7: 3000.04
     rank  8: 2975.12
     rank  9: 2978.57
     rank 10: 2975.96
     rank 11: 2974.48
     rank 12: 3010.37
     rank 13: 3013.42
     rank 14: 3012.08
     rank 15: 3011.53
     rank 16: 3230.35
     rank 17: 3230.08
     rank 18: 3230.61
     rank 19: 3231.42
  pure-backward-compute:
     rank  0: 1047.06
     rank  1: 1050.09
     rank  2: 1045.86
     rank  3: 1052.99
     rank  4: 3000.99
     rank  5: 3004.81
     rank  6: 3003.51
     rank  7: 2999.23
     rank  8: 2974.05
     rank  9: 2977.78
     rank 10: 2975.14
     rank 11: 2973.19
     rank 12: 3009.41
     rank 13: 3012.55
     rank 14: 3011.00
     rank 15: 3010.63
     rank 16: 3228.15
     rank 17: 3228.33
     rank 18: 3228.92
     rank 19: 3229.24
  batch-generator:
     rank  0: 57.74
     rank  1: 71.92
     rank  2: 61.21
     rank  3: 61.22
     rank  4: 55.79
     rank  5: 73.18
     rank  6: 63.77
     rank  7: 65.77
     rank  8: 52.37
     rank  9: 64.43
     rank 10: 55.19
     rank 11: 56.01
     rank 12: 52.39
     rank 13: 60.49
     rank 14: 56.63
     rank 15: 54.48
     rank 16: 51.94
     rank 17: 59.90
     rank 18: 57.97
     rank 19: 55.47
  forward-recv:
     rank  4: 84.62
     rank  5: 78.62
     rank  6: 83.98
     rank  7: 84.42
     rank  8: 280.02
     rank  9: 275.50
     rank 10: 279.88
     rank 11: 279.61
     rank 12: 455.51
     rank 13: 454.62
     rank 14: 455.44
     rank 15: 455.76
     rank 16: 630.42
     rank 17: 629.79
     rank 18: 630.36
     rank 19: 630.41
  forward-send:
     rank  0: 392.74
     rank  1: 378.76
     rank  2: 391.18
     rank  3: 391.82
     rank  4: 34.40
     rank  5: 26.75
     rank  6: 33.41
     rank  7: 34.16
     rank  8: 20.94
     rank  9: 18.83
     rank 10: 20.75
     rank 11: 20.87
     rank 12: 10.44
     rank 13: 9.77
     rank 14: 10.44
     rank 15: 10.41
  backward-recv:
     rank  0: 1326.04
     rank  1: 1326.55
     rank  2: 1326.70
     rank  3: 1325.06
     rank  4: 611.33
     rank  5: 609.37
     rank  6: 609.38
     rank  7: 612.17
     rank  8: 401.82
     rank  9: 401.99
     rank 10: 401.94
     rank 11: 402.30
     rank 12: 199.08
     rank 13: 199.38
     rank 14: 199.12
     rank 15: 199.24
  backward-send:
     rank  4: 21.86
     rank  5: 22.40
     rank  6: 22.76
     rank  7: 20.70
     rank  8: 31.38
     rank  9: 30.50
     rank 10: 31.24
     rank 11: 31.01
     rank 12: 20.85
     rank 13: 20.11
     rank 14: 20.61
     rank 15: 20.90
     rank 16: 10.42
     rank 17: 9.87
     rank 18: 10.15
     rank 19: 10.44
  forward-send-backward-recv:
     rank  0: 3850.76
     rank  1: 3848.21
     rank  2: 3852.98
     rank  3: 3847.38
     rank  4: 878.06
     rank  5: 875.97
     rank  6: 876.42
     rank  7: 879.82
     rank  8: 732.79
     rank  9: 733.48
     rank 10: 735.36
     rank 11: 732.37
     rank 12: 529.15
     rank 13: 528.93
     rank 14: 527.33
     rank 15: 529.71
  backward-send-forward-recv:
     rank  4: 74.86
     rank  5: 72.87
     rank  6: 73.70
     rank  7: 70.94
     rank  8: 146.01
     rank  9: 142.29
     rank 10: 144.78
     rank 11: 146.19
     rank 12: 156.78
     rank 13: 153.30
     rank 14: 156.78
     rank 15: 156.28
     rank 16: 169.08
     rank 17: 165.70
     rank 18: 167.13
     rank 19: 168.53
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.05
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.64
     rank  1: 0.67
     rank  2: 0.63
     rank  3: 0.64
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.31
     rank  1: 10.43
     rank  2: 10.21
     rank  3: 10.14
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.04
     rank  7: 0.03
     rank  8: 0.03
     rank  9: 0.06
     rank 10: 0.07
     rank 11: 0.08
     rank 12: 0.03
     rank 13: 0.07
     rank 14: 0.03
     rank 15: 0.04
     rank 16: 0.03
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  optimizer:
     rank  0: 10.95
     rank  1: 11.07
     rank  2: 10.85
     rank  3: 10.76
     rank  4: 0.66
     rank  5: 0.67
     rank  6: 0.67
     rank  7: 0.66
     rank  8: 0.67
     rank  9: 0.69
     rank 10: 0.70
     rank 11: 0.70
     rank 12: 0.66
     rank 13: 0.70
     rank 14: 0.66
     rank 15: 0.66
     rank 16: 0.66
     rank 17: 0.67
     rank 18: 0.67
     rank 19: 0.68
 [2024-12-02 16:45:56] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 7685.1 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.953106E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7651.09
     rank  1: 7650.42
     rank  2: 7650.48
     rank  3: 7650.42
     rank  4: 7650.18
     rank  5: 7649.56
     rank  6: 7649.58
     rank  7: 7649.60
     rank  8: 7650.22
     rank  9: 7649.57
     rank 10: 7649.59
     rank 11: 7650.17
     rank 12: 7650.20
     rank 13: 7649.57
     rank 14: 7649.59
     rank 15: 7649.72
     rank 16: 7651.03
     rank 17: 7649.59
     rank 18: 7649.60
     rank 19: 7649.54
  forward-compute:
     rank  0: 1018.20
     rank  1: 1020.95
     rank  2: 1019.98
     rank  3: 1021.73
     rank  4: 2876.49
     rank  5: 2883.58
     rank  6: 2877.93
     rank  7: 2886.06
     rank  8: 2781.33
     rank  9: 2786.37
     rank 10: 2784.21
     rank 11: 2783.37
     rank 12: 2801.30
     rank 13: 2806.31
     rank 14: 2801.25
     rank 15: 2801.97
     rank 16: 2946.74
     rank 17: 2950.09
     rank 18: 2950.72
     rank 19: 2946.54
  backward-compute:
     rank  0: 1055.10
     rank  1: 1058.48
     rank  2: 1053.82
     rank  3: 1060.49
     rank  4: 2993.47
     rank  5: 2998.47
     rank  6: 2997.54
     rank  7: 2992.03
     rank  8: 2977.91
     rank  9: 2981.63
     rank 10: 2979.11
     rank 11: 2977.32
     rank 12: 3012.64
     rank 13: 3013.75
     rank 14: 3014.30
     rank 15: 3013.96
     rank 16: 3225.19
     rank 17: 3224.99
     rank 18: 3226.35
     rank 19: 3226.73
  pure-backward-compute:
     rank  0: 1054.36
     rank  1: 1057.63
     rank  2: 1053.18
     rank  3: 1059.85
     rank  4: 2992.63
     rank  5: 2997.68
     rank  6: 2996.06
     rank  7: 2991.21
     rank  8: 2976.85
     rank  9: 2980.93
     rank 10: 2978.38
     rank 11: 2976.12
     rank 12: 3011.66
     rank 13: 3012.95
     rank 14: 3013.26
     rank 15: 3013.07
     rank 16: 3223.04
     rank 17: 3222.86
     rank 18: 3224.67
     rank 19: 3224.67
  batch-generator:
     rank  0: 54.79
     rank  1: 61.35
     rank  2: 58.77
     rank  3: 60.69
     rank  4: 60.89
     rank  5: 65.90
     rank  6: 69.89
     rank  7: 74.65
     rank  8: 51.70
     rank  9: 58.27
     rank 10: 54.90
     rank 11: 56.09
     rank 12: 51.99
     rank 13: 60.71
     rank 14: 56.11
     rank 15: 54.19
     rank 16: 52.57
     rank 17: 60.41
     rank 18: 59.37
     rank 19: 56.26
  forward-recv:
     rank  4: 84.12
     rank  5: 81.56
     rank  6: 83.68
     rank  7: 82.48
     rank  8: 279.54
     rank  9: 279.52
     rank 10: 279.83
     rank 11: 278.98
     rank 12: 452.63
     rank 13: 452.80
     rank 14: 452.45
     rank 15: 452.81
     rank 16: 625.52
     rank 17: 624.81
     rank 18: 625.54
     rank 19: 625.50
  forward-send:
     rank  0: 399.77
     rank  1: 395.12
     rank  2: 398.50
     rank  3: 396.95
     rank  4: 32.11
     rank  5: 30.06
     rank  6: 31.34
     rank  7: 31.56
     rank  8: 21.08
     rank  9: 19.93
     rank 10: 20.41
     rank 11: 21.06
     rank 12: 10.43
     rank 13: 9.65
     rank 14: 10.45
     rank 15: 10.33
  backward-recv:
     rank  0: 1327.11
     rank  1: 1326.92
     rank  2: 1327.64
     rank  3: 1326.98
     rank  4: 616.42
     rank  5: 614.99
     rank  6: 614.44
     rank  7: 616.89
     rank  8: 402.05
     rank  9: 402.94
     rank 10: 402.59
     rank 11: 402.30
     rank 12: 199.37
     rank 13: 199.55
     rank 14: 199.39
     rank 15: 199.13
  backward-send:
     rank  4: 22.03
     rank  5: 22.56
     rank  6: 22.63
     rank  7: 21.79
     rank  8: 31.42
     rank  9: 30.29
     rank 10: 31.00
     rank 11: 31.56
     rank 12: 20.84
     rank 13: 20.51
     rank 14: 20.69
     rank 15: 21.04
     rank 16: 10.49
     rank 17: 10.45
     rank 18: 10.44
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 3834.24
     rank  1: 3832.85
     rank  2: 3836.22
     rank  3: 3830.36
     rank  4: 869.64
     rank  5: 868.01
     rank  6: 867.61
     rank  7: 870.53
     rank  8: 735.39
     rank  9: 734.87
     rank 10: 736.55
     rank 11: 734.93
     rank 12: 523.34
     rank 13: 524.67
     rank 14: 521.27
     rank 15: 523.10
  backward-send-forward-recv:
     rank  4: 74.73
     rank  5: 72.70
     rank  6: 73.32
     rank  7: 69.40
     rank  8: 145.37
     rank  9: 141.71
     rank 10: 144.03
     rank 11: 145.06
     rank 12: 156.44
     rank 13: 151.49
     rank 14: 156.57
     rank 15: 156.00
     rank 16: 168.82
     rank 17: 167.71
     rank 18: 166.09
     rank 19: 168.29
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.06
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.26
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.07
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.48
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.04
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.63
     rank  2: 0.63
     rank  3: 0.62
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.25
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.19
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.01
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.16
     rank  2: 0.19
     rank  3: 0.18
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.06
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.03
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.05
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.09
     rank  1: 10.28
     rank  2: 10.15
     rank  3: 10.27
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.07
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.04
     rank 11: 0.08
     rank 12: 0.03
     rank 13: 0.03
     rank 14: 0.03
     rank 15: 0.06
     rank 16: 0.58
     rank 17: 0.04
     rank 18: 0.04
     rank 19: 0.03
  optimizer:
     rank  0: 11.33
     rank  1: 11.52
     rank  2: 11.56
     rank  3: 11.50
     rank  4: 1.26
     rank  5: 1.26
     rank  6: 1.26
     rank  7: 1.29
     rank  8: 1.27
     rank  9: 1.27
     rank 10: 1.26
     rank 11: 1.33
     rank 12: 1.26
     rank 13: 1.26
     rank 14: 1.26
     rank 15: 1.29
     rank 16: 1.80
     rank 17: 1.27
     rank 18: 1.27
     rank 19: 1.26
 [2024-12-02 16:46:03] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 7690.8 | learning rate: 1.000000E-05 | global batch size:    32 | lm loss: 1.852520E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7650.32
     rank  1: 7650.28
     rank  2: 7650.36
     rank  3: 7650.28
     rank  4: 7649.42
     rank  5: 7649.39
     rank  6: 7649.42
     rank  7: 7649.40
     rank  8: 7649.46
     rank  9: 7649.39
     rank 10: 7649.43
     rank 11: 7649.50
     rank 12: 7649.42
     rank 13: 7649.41
     rank 14: 7649.46
     rank 15: 7649.42
     rank 16: 7649.41
     rank 17: 7649.42
     rank 18: 7649.43
     rank 19: 7649.40
  forward-compute:
     rank  0: 1005.06
     rank  1: 1015.06
     rank  2: 1007.67
     rank  3: 1005.57
     rank  4: 2872.77
     rank  5: 2887.77
     rank  6: 2874.80
     rank  7: 2877.92
     rank  8: 2784.11
     rank  9: 2792.99
     rank 10: 2788.67
     rank 11: 2786.75
     rank 12: 2800.19
     rank 13: 2805.75
     rank 14: 2800.31
     rank 15: 2800.56
     rank 16: 2946.35
     rank 17: 2954.22
     rank 18: 2950.48
     rank 19: 2946.69
  backward-compute:
     rank  0: 1051.77
     rank  1: 1055.58
     rank  2: 1050.86
     rank  3: 1057.18
     rank  4: 2990.69
     rank  5: 2996.48
     rank  6: 2994.97
     rank  7: 2989.60
     rank  8: 2973.68
     rank  9: 2978.81
     rank 10: 2975.21
     rank 11: 2973.22
     rank 12: 3011.22
     rank 13: 3015.37
     rank 14: 3013.86
     rank 15: 3012.91
     rank 16: 3231.12
     rank 17: 3230.95
     rank 18: 3231.39
     rank 19: 3231.53
  pure-backward-compute:
     rank  0: 1051.11
     rank  1: 1054.87
     rank  2: 1050.21
     rank  3: 1056.52
     rank  4: 2989.69
     rank  5: 2995.69
     rank  6: 2993.78
     rank  7: 2988.69
     rank  8: 2972.53
     rank  9: 2978.09
     rank 10: 2974.50
     rank 11: 2971.50
     rank 12: 3010.25
     rank 13: 3014.58
     rank 14: 3012.80
     rank 15: 3012.07
     rank 16: 3228.43
     rank 17: 3229.30
     rank 18: 3229.66
     rank 19: 3229.46
  batch-generator:
     rank  0: 52.26
     rank  1: 64.26
     rank  2: 56.58
     rank  3: 55.18
     rank  4: 63.98
     rank  5: 76.25
     rank  6: 71.21
     rank  7: 75.31
     rank  8: 50.89
     rank  9: 61.59
     rank 10: 55.72
     rank 11: 55.74
     rank 12: 51.42
     rank 13: 61.02
     rank 14: 55.83
     rank 15: 53.49
     rank 16: 53.93
     rank 17: 63.43
     rank 18: 60.42
     rank 19: 57.94
  forward-recv:
     rank  4: 82.00
     rank  5: 75.89
     rank  6: 81.13
     rank  7: 81.52
     rank  8: 276.50
     rank  9: 273.84
     rank 10: 276.39
     rank 11: 275.98
     rank 12: 454.29
     rank 13: 454.09
     rank 14: 453.99
     rank 15: 454.59
     rank 16: 627.32
     rank 17: 626.82
     rank 18: 627.60
     rank 19: 627.64
  forward-send:
     rank  0: 398.49
     rank  1: 387.08
     rank  2: 396.27
     rank  3: 398.58
     rank  4: 37.15
     rank  5: 32.12
     rank  6: 36.04
     rank  7: 37.84
     rank  8: 20.70
     rank  9: 19.30
     rank 10: 20.22
     rank 11: 20.70
     rank 12: 10.38
     rank 13: 9.63
     rank 14: 10.42
     rank 15: 10.35
  backward-recv:
     rank  0: 1322.73
     rank  1: 1323.07
     rank  2: 1323.56
     rank  3: 1322.04
     rank  4: 616.91
     rank  5: 615.52
     rank  6: 615.54
     rank  7: 617.87
     rank  8: 402.63
     rank  9: 403.34
     rank 10: 402.83
     rank 11: 402.93
     rank 12: 198.88
     rank 13: 199.17
     rank 14: 198.71
     rank 15: 198.96
  backward-send:
     rank  4: 21.94
     rank  5: 22.43
     rank  6: 22.60
     rank  7: 21.01
     rank  8: 31.48
     rank  9: 30.02
     rank 10: 30.99
     rank 11: 31.46
     rank 12: 20.73
     rank 13: 20.35
     rank 14: 20.45
     rank 15: 20.96
     rank 16: 10.61
     rank 17: 9.85
     rank 18: 10.22
     rank 19: 10.50
  forward-send-backward-recv:
     rank  0: 3858.26
     rank  1: 3855.90
     rank  2: 3860.19
     rank  3: 3855.22
     rank  4: 876.81
     rank  5: 872.81
     rank  6: 872.62
     rank  7: 877.43
     rank  8: 745.84
     rank  9: 743.67
     rank 10: 746.35
     rank 11: 742.40
     rank 12: 531.78
     rank 13: 530.63
     rank 14: 528.74
     rank 15: 532.33
  backward-send-forward-recv:
     rank  4: 74.73
     rank  5: 72.65
     rank  6: 74.78
     rank  7: 71.55
     rank  8: 145.54
     rank  9: 141.05
     rank 10: 143.03
     rank 11: 144.90
     rank 12: 155.97
     rank 13: 150.93
     rank 14: 156.06
     rank 15: 155.59
     rank 16: 167.55
     rank 17: 163.71
     rank 18: 166.14
     rank 19: 167.74
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.04
     rank  1: 0.04
     rank  2: 0.05
     rank  3: 0.04
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.62
     rank  1: 0.62
     rank  2: 0.66
     rank  3: 0.62
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.02
  optimizer-copy-to-main-grad:
     rank  0: 0.16
     rank  1: 0.17
     rank  2: 0.19
     rank  3: 0.17
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 10.28
     rank  1: 10.21
     rank  2: 10.19
     rank  3: 10.35
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.03
     rank 10: 0.03
     rank 11: 0.09
     rank 12: 0.03
     rank 13: 0.04
     rank 14: 0.07
     rank 15: 0.05
     rank 16: 0.07
     rank 17: 0.03
     rank 18: 0.03
     rank 19: 0.04
  optimizer:
     rank  0: 10.96
     rank  1: 10.87
     rank  2: 10.87
     rank  3: 11.01
     rank  4: 0.69
     rank  5: 0.70
     rank  6: 0.69
     rank  7: 0.72
     rank  8: 0.70
     rank  9: 0.69
     rank 10: 0.69
     rank 11: 0.75
     rank 12: 0.69
     rank 13: 0.69
     rank 14: 0.73
     rank 15: 0.70
     rank 16: 0.73
     rank 17: 0.69
     rank 18: 0.69
     rank 19: 0.70
