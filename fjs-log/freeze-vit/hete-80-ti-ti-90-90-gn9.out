examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
0
[2024-12-31 15:30:51,647] torch.distributed.run: [WARNING] 
[2024-12-31 15:30:51,647] torch.distributed.run: [WARNING] *****************************************
[2024-12-31 15:30:51,647] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-31 15:30:51,647] torch.distributed.run: [WARNING] *****************************************
using world size: 20, data-parallel size: 1, context-parallel size: 1, tensor-model-parallel size: 4, pipeline-model-parallel size: 5, 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:HuggingFaceTokenizer
WARNING: Setting args.overlap_p2p_comm and args.align_param_gather to False since non-interleaved schedule does not support overlapping p2p communication and aligned param AG
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  allow_missing_vision_projection_checkpoint ...... True
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. True
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 0.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/pretrain_dataset.yaml']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. None
  dataloader_type ................................. external
  dataset_config .................................. None
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. 1024
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  disable_vision_class_token ...................... True
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 60
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 576
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... True
  eval_interval ................................... 1000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 230
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_wgrad ....................................... True
  freeze_LM ....................................... False
  freeze_ViT ...................................... True
  global_batch_size ............................... 32
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 336
  img_w ........................................... 336
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  language_model_type ............................. mistral_7b
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 20000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_num_tiles ................................... 1
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 14
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 5
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  prompt_path ..................................... /gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/examples/multimodal/manual_prompts.json
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 576
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  split_spec ...................................... 26,5,6,11,10
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /dat/fjs/llama_mistral/hete_mistral_clip_freeze_vit-tp4pp5/output/llava-mistral-7b-clip336-pretraining/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 2
  timing_log_option ............................... all
  titles_data_path ................................ None
  tokenizer_model ................................. /dat/fjs/llama_mistral/hf-mistral/
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 5
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_te .......................................... True
  use_thumbnail ................................... False
  use_tiling ...................................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  valid_path ...................................... None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_model_type ............................... clip
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 20
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 32
> building HuggingFaceTokenizer tokenizer ...
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
 > padded vocab (size: 32768) with 0 dummy tokens (new size: 32768)
> initializing torch distributed ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 5
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
make: Nothing to be done for 'default'.
make: Leaving directory '/gf3/home/fjs/project/MLLM/Megatron-LM-core_r0.9.0/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.087 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
>>> done with compiling and loading fused kernels. Compilation time: 31.980 seconds---Rank 1---Tensor Parallel Group GPUs: [1, 1, 1, 1]

---Rank 3---Tensor Parallel Group GPUs: [3, 3, 3, 3]
---Rank 2---Tensor Parallel Group GPUs: [2, 2, 2, 2]---Rank 1---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]

---Rank 3---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]
---Rank 2---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0]
[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 0---Tensor Parallel Group GPUs: [0, 0, 0, 0]
---Rank 0---Pipeline Parallel Group GPUs: [0, 0, 0, 0, 0][rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 43.117
[after megatron is initialized] datetime: 2024-12-31 15:31:53 
building a multimodal model ...WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.

WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 95234048
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 95234048
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 95234048
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 95234048
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (18350080 elements):
	vision_projection.encoder.linear_fc2.weight
	vision_projection.encoder.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, clip_grad=0.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f2b517999c0>, config_logger_dir='')
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-12-31 15:32:13 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      320
    validation: 320
    test:       320
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2494>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=250000
rank=0, worker=1: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<2494>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=250000
> building HuggingFaceTokenizer tokenizer ...
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=29064
rank=0, worker=1: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 10000), pretrain-9.tar[0, 10000)] sum(count)=29064
[after dataloaders are built] datetime: 2024-12-31 15:32:14 
done with setup ...
training ...
[before the start of training step] datetime: 2024-12-31 15:32:21 
Layer: TransformerLayer, Forward time: 935.682981 msLayer: TransformerLayer, Forward time: 935.711067 ms

Layer: TransformerLayer, Forward time: 934.641538 ms
Layer: TransformerLayer, Forward time: 942.837226 ms
Layer: TransformerLayer, Forward time: 4.322837 msLayer: TransformerLayer, Forward time: 4.329959 ms

Layer: TransformerLayer, Forward time: 4.360859 ms
Layer: TransformerLayer, Forward time: 6.126349 ms
Layer: TransformerLayer, Forward time: 6.036237 ms
Layer: TransformerLayer, Forward time: 6.069610 ms
Layer: TransformerLayer, Forward time: 6.105324 ms
Layer: TransformerLayer, Forward time: 4.139761 ms
Layer: TransformerLayer, Forward time: 3.281816 ms
Layer: TransformerLayer, Forward time: 3.279731 ms
Layer: TransformerLayer, Forward time: 3.290990 ms
Layer: TransformerLayer, Forward time: 3.292598 ms
Layer: TransformerLayer, Forward time: 3.283490 ms
Layer: TransformerLayer, Forward time: 3.285033 ms
Layer: TransformerLayer, Forward time: 3.263551 ms
Layer: TransformerLayer, Forward time: 3.269719 ms
Layer: TransformerLayer, Forward time: 3.250610 ms
Layer: TransformerLayer, Forward time: 3.256309 ms
Layer: TransformerLayer, Forward time: 3.248735 ms
Layer: TransformerLayer, Forward time: 3.246020 ms
Layer: TransformerLayer, Forward time: 3.255460 msLayer: TransformerLayer, Forward time: 3.237217 ms

Layer: TransformerLayer, Forward time: 3.235771 ms
Layer: TransformerLayer, Forward time: 3.243272 ms
Layer: TransformerLayer, Forward time: 3.249214 msLayer: TransformerLayer, Forward time: 3.248684 ms

Layer: TransformerLayer, Forward time: 3.254706 ms
Layer: TransformerLayer, Forward time: 3.258777 ms
Layer: TransformerLayer, Forward time: 3.250348 ms
Layer: TransformerLayer, Forward time: 3.238479 msLayer: TransformerLayer, Forward time: 3.272221 ms

Layer: TransformerLayer, Forward time: 3.239214 ms
Layer: TransformerLayer, Forward time: 3.190387 ms
Layer: TransformerLayer, Forward time: 3.224700 ms
Layer: TransformerLayer, Forward time: 3.241770 ms
Layer: TransformerLayer, Forward time: 3.240296 ms
Layer: TransformerLayer, Forward time: 3.245970 ms
Layer: TransformerLayer, Forward time: 3.244560 ms
Layer: TransformerLayer, Forward time: 3.221835 ms
Layer: TransformerLayer, Forward time: 3.233568 ms
Layer: TransformerLayer, Forward time: 3.202084 ms
Layer: TransformerLayer, Forward time: 3.205433 ms
Layer: TransformerLayer, Forward time: 3.204427 ms
Layer: TransformerLayer, Forward time: 3.204748 ms
Layer: TransformerLayer, Forward time: 3.222235 ms
Layer: TransformerLayer, Forward time: 3.248824 ms
Layer: TransformerLayer, Forward time: 3.219727 ms
Layer: TransformerLayer, Forward time: 3.226818 ms
Layer: TransformerLayer, Forward time: 3.206034 msLayer: TransformerLayer, Forward time: 3.196688 ms

Layer: TransformerLayer, Forward time: 3.259163 ms
Layer: TransformerLayer, Forward time: 3.271467 ms
Layer: TransformerLayer, Forward time: 3.216724 ms
Layer: TransformerLayer, Forward time: 3.232669 ms
Layer: TransformerLayer, Forward time: 3.253871 ms
Layer: TransformerLayer, Forward time: 3.253564 ms
Layer: TransformerLayer, Forward time: 3.236158 ms
Layer: TransformerLayer, Forward time: 3.234806 ms
Layer: TransformerLayer, Forward time: 3.228806 ms
Layer: TransformerLayer, Forward time: 3.233225 ms
Layer: TransformerLayer, Forward time: 3.211253 ms
Layer: TransformerLayer, Forward time: 3.197958 ms
Layer: TransformerLayer, Forward time: 3.208169 ms
Layer: TransformerLayer, Forward time: 3.207793 ms
Layer: TransformerLayer, Forward time: 3.229325 ms
Layer: TransformerLayer, Forward time: 3.254527 msLayer: TransformerLayer, Forward time: 3.215697 ms

Layer: TransformerLayer, Forward time: 3.215031 ms
Layer: TransformerLayer, Forward time: 3.134288 ms
Layer: TransformerLayer, Forward time: 3.160347 ms
Layer: TransformerLayer, Forward time: 3.204614 ms
Layer: TransformerLayer, Forward time: 3.224503 ms
Layer: TransformerLayer, Forward time: 3.205607 ms
Layer: TransformerLayer, Forward time: 3.169947 ms
Layer: TransformerLayer, Forward time: 3.232208 ms
Layer: TransformerLayer, Forward time: 3.170815 ms
Layer: TransformerLayer, Forward time: 3.156925 ms
Layer: TransformerLayer, Forward time: 3.190435 ms
Layer: TransformerLayer, Forward time: 3.209305 ms
Layer: TransformerLayer, Forward time: 3.219088 ms
Layer: TransformerLayer, Forward time: 3.228244 ms
Layer: TransformerLayer, Forward time: 3.256262 msLayer: TransformerLayer, Forward time: 3.208985 ms

Layer: TransformerLayer, Forward time: 3.212548 ms
Layer: TransformerLayer, Forward time: 3.148655 ms
Layer: TransformerLayer, Forward time: 3.173847 ms
Layer: TransformerLayer, Forward time: 3.228097 ms
Layer: TransformerLayer, Forward time: 3.233261 ms
Layer: TransformerLayer, Forward time: 3.214708 ms
Layer: TransformerLayer, Forward time: 3.211013 ms
Layer: TransformerLayer, Forward time: 3.220227 ms
Layer: TransformerLayer, Forward time: 3.218598 ms
Layer: TransformerLayer, Forward time: 4.581580 msLayer: TransformerLayer, Forward time: 4.835023 msLayer: TransformerLayer, Forward time: 4.507163 ms


Layer: TransformerLayer, Forward time: 4.107063 ms
Layer: TransformerLayer, Forward time: 3.380043 msLayer: TransformerLayer, Forward time: 3.377662 ms

Layer: TransformerLayer, Forward time: 3.372476 msLayer: TransformerLayer, Forward time: 3.402272 ms

Layer: TransformerLayer, Forward time: 3.243830 ms
Layer: TransformerLayer, Forward time: 3.252068 ms
Layer: TransformerLayer, Forward time: 3.245885 ms
Layer: TransformerLayer, Forward time: 3.257454 ms
Layer: TransformerLayer, Forward time: 3.222612 ms
Layer: TransformerLayer, Forward time: 3.215196 ms
Layer: TransformerLayer, Forward time: 3.236330 ms
Layer: TransformerLayer, Forward time: 3.238255 ms
Layer: TransformerLayer, Forward time: 3.371647 ms
Layer: TransformerLayer, Forward time: 3.357898 msLayer: TransformerLayer, Forward time: 3.375283 ms

Layer: TransformerLayer, Forward time: 3.358192 ms
Layer: TransformerLayer, Forward time: 3.243553 ms
Layer: TransformerLayer, Forward time: 3.249397 ms
Layer: TransformerLayer, Forward time: 3.285857 ms
Layer: TransformerLayer, Forward time: 3.303316 ms
Layer: TransformerLayer, Forward time: 3.399745 ms
Layer: TransformerLayer, Forward time: 3.423414 ms
Layer: TransformerLayer, Forward time: 3.431034 ms
Layer: TransformerLayer, Forward time: 3.426496 ms
Layer: TransformerLayer, Forward time: 3.257332 msLayer: TransformerLayer, Forward time: 3.277034 ms

Layer: TransformerLayer, Forward time: 3.334962 ms
Layer: TransformerLayer, Forward time: 3.369041 ms
Layer: TransformerLayer, Forward time: 3.219316 msLayer: TransformerLayer, Forward time: 3.240667 ms

Layer: TransformerLayer, Forward time: 3.218562 ms
Layer: TransformerLayer, Forward time: 3.267340 ms
Layer: TransformerLayer, Forward time: 3.196977 ms
Layer: TransformerLayer, Forward time: 3.218637 ms
Layer: TransformerLayer, Forward time: 3.267486 ms
Layer: TransformerLayer, Forward time: 3.263085 ms
Layer: TransformerLayer, Forward time: 3.228789 msLayer: TransformerLayer, Forward time: 3.216117 ms

Layer: TransformerLayer, Forward time: 3.286536 ms
Layer: TransformerLayer, Forward time: 3.317718 ms
Layer: TransformerLayer, Forward time: 3.371462 ms
Layer: TransformerLayer, Forward time: 3.387262 msLayer: TransformerLayer, Forward time: 3.379175 ms

Layer: TransformerLayer, Forward time: 3.379240 ms
Layer: TransformerLayer, Forward time: 3.231527 ms
Layer: TransformerLayer, Forward time: 3.257188 ms
Layer: TransformerLayer, Forward time: 3.305024 ms
Layer: TransformerLayer, Forward time: 3.316333 ms
Layer: TransformerLayer, Forward time: 3.357044 msLayer: TransformerLayer, Forward time: 3.377730 ms

Layer: TransformerLayer, Forward time: 3.352574 ms
Layer: TransformerLayer, Forward time: 3.351324 ms
Layer: TransformerLayer, Forward time: 3.324921 ms
Layer: TransformerLayer, Forward time: 3.359026 msLayer: TransformerLayer, Forward time: 3.332522 ms

Layer: TransformerLayer, Forward time: 3.337728 ms
Layer: TransformerLayer, Forward time: 3.312795 ms
Layer: TransformerLayer, Forward time: 3.344753 msLayer: TransformerLayer, Forward time: 3.322418 ms

Layer: TransformerLayer, Forward time: 3.352535 ms
Layer: TransformerLayer, Forward time: 3.235305 msLayer: TransformerLayer, Forward time: 3.223979 ms

Layer: TransformerLayer, Forward time: 3.282515 ms
Layer: TransformerLayer, Forward time: 3.283750 ms
Layer: TransformerLayer, Forward time: 3.284997 msLayer: TransformerLayer, Forward time: 3.290037 ms

Layer: TransformerLayer, Forward time: 3.287359 ms
Layer: TransformerLayer, Forward time: 3.290697 ms
Layer: TransformerLayer, Forward time: 3.337561 ms
Layer: TransformerLayer, Forward time: 3.359936 ms
Layer: TransformerLayer, Forward time: 3.361224 ms
Layer: TransformerLayer, Forward time: 3.352597 ms
Layer: TransformerLayer, Forward time: 3.271336 ms
Layer: TransformerLayer, Forward time: 3.309973 ms
Layer: TransformerLayer, Forward time: 3.295161 ms
Layer: TransformerLayer, Forward time: 3.308882 ms
Layer: TransformerLayer, Forward time: 3.249019 ms
Layer: TransformerLayer, Forward time: 3.242174 ms
Layer: TransformerLayer, Forward time: 3.246618 ms
Layer: TransformerLayer, Forward time: 3.252133 ms
Layer: TransformerLayer, Forward time: 3.276873 ms
Layer: TransformerLayer, Forward time: 3.262101 ms
Layer: TransformerLayer, Forward time: 3.263506 ms
Layer: TransformerLayer, Forward time: 3.256870 ms
Layer: TransformerLayer, Forward time: 3.252831 ms
Layer: TransformerLayer, Forward time: 3.258406 ms
Layer: TransformerLayer, Forward time: 3.260591 ms
Layer: TransformerLayer, Forward time: 3.260835 ms
Layer: TransformerLayer, Forward time: 3.263110 ms
Layer: TransformerLayer, Forward time: 3.252251 ms
Layer: TransformerLayer, Forward time: 3.270200 ms
Layer: TransformerLayer, Forward time: 3.266264 ms
Layer: TransformerLayer, Forward time: 6.195990 msLayer: TransformerLayer, Forward time: 5.329678 msLayer: TransformerLayer, Forward time: 7.271462 ms


Layer: TransformerLayer, Forward time: 5.113053 ms
Layer: TransformerLayer, Forward time: 3.391969 ms
Layer: TransformerLayer, Forward time: 3.401892 msLayer: TransformerLayer, Forward time: 3.393977 ms

Layer: TransformerLayer, Forward time: 3.421474 ms
Layer: TransformerLayer, Forward time: 3.396829 ms
Layer: TransformerLayer, Forward time: 3.421957 ms
Layer: TransformerLayer, Forward time: 3.450874 ms
Layer: TransformerLayer, Forward time: 3.461269 ms
Layer: TransformerLayer, Forward time: 3.330159 ms
Layer: TransformerLayer, Forward time: 3.360066 msLayer: TransformerLayer, Forward time: 3.302946 ms

Layer: TransformerLayer, Forward time: 3.310489 ms
Layer: TransformerLayer, Forward time: 3.270429 msLayer: TransformerLayer, Forward time: 3.294216 ms

Layer: TransformerLayer, Forward time: 3.274497 msLayer: TransformerLayer, Forward time: 3.298829 ms

Layer: TransformerLayer, Forward time: 3.288967 ms
Layer: TransformerLayer, Forward time: 3.305729 msLayer: TransformerLayer, Forward time: 3.298469 ms

Layer: TransformerLayer, Forward time: 3.315263 ms
Layer: TransformerLayer, Forward time: 3.270339 msLayer: TransformerLayer, Forward time: 3.256949 ms

Layer: TransformerLayer, Forward time: 3.273168 ms
Layer: TransformerLayer, Forward time: 3.281900 ms
Layer: TransformerLayer, Forward time: 3.352929 ms
Layer: TransformerLayer, Forward time: 3.367606 ms
Layer: TransformerLayer, Forward time: 3.387741 ms
Layer: TransformerLayer, Forward time: 3.390363 ms
Layer: TransformerLayer, Forward time: 3.276297 ms
Layer: TransformerLayer, Forward time: 3.241333 ms
Layer: TransformerLayer, Forward time: 3.312118 ms
Layer: TransformerLayer, Forward time: 3.249943 ms
Layer: TransformerLayer, Forward time: 3.305176 ms
Layer: TransformerLayer, Forward time: 3.331469 ms
Layer: TransformerLayer, Forward time: 3.355631 ms
Layer: TransformerLayer, Forward time: 3.358272 ms
Layer: TransformerLayer, Forward time: 3.385917 msLayer: TransformerLayer, Forward time: 3.385151 ms

Layer: TransformerLayer, Forward time: 3.393520 ms
Layer: TransformerLayer, Forward time: 3.398696 ms
Layer: TransformerLayer, Forward time: 3.280672 ms
Layer: TransformerLayer, Forward time: 3.254730 ms
Layer: TransformerLayer, Forward time: 3.306139 ms
Layer: TransformerLayer, Forward time: 3.261726 ms
Layer: TransformerLayer, Forward time: 3.244280 ms
Layer: TransformerLayer, Forward time: 3.263857 ms
Layer: TransformerLayer, Forward time: 3.296136 ms
Layer: TransformerLayer, Forward time: 3.309303 ms
Layer: TransformerLayer, Forward time: 3.314614 ms
Layer: TransformerLayer, Forward time: 3.310567 ms
Layer: TransformerLayer, Forward time: 3.351020 ms
Layer: TransformerLayer, Forward time: 3.389556 ms
Layer: TransformerLayer, Forward time: 3.311025 ms
Layer: TransformerLayer, Forward time: 3.330888 ms
Layer: TransformerLayer, Forward time: 3.344399 ms
Layer: TransformerLayer, Forward time: 3.342864 ms
Layer: TransformerLayer, Forward time: 3.306871 ms
Layer: TransformerLayer, Forward time: 3.336744 msLayer: TransformerLayer, Forward time: 3.286922 ms

Layer: TransformerLayer, Forward time: 3.292032 ms
Layer: TransformerLayer, Forward time: 3.290958 ms
Layer: TransformerLayer, Forward time: 3.320755 ms
Layer: TransformerLayer, Forward time: 3.352246 ms
Layer: TransformerLayer, Forward time: 3.357298 ms
Layer: TransformerLayer, Forward time: 3.301306 ms
Layer: TransformerLayer, Forward time: 3.299520 ms
Layer: TransformerLayer, Forward time: 3.276665 ms
Layer: TransformerLayer, Forward time: 3.283593 ms
Layer: TransformerLayer, Forward time: 3.304195 ms
Layer: TransformerLayer, Forward time: 3.328523 msLayer: TransformerLayer, Forward time: 3.284761 ms

Layer: TransformerLayer, Forward time: 3.286110 ms
Layer: TransformerLayer, Forward time: 3.308574 msLayer: TransformerLayer, Forward time: 3.329152 ms

Layer: TransformerLayer, Forward time: 3.336498 ms
Layer: TransformerLayer, Forward time: 3.348179 ms
Layer: TransformerLayer, Forward time: 3.346083 ms
Layer: TransformerLayer, Forward time: 3.352576 ms
Layer: TransformerLayer, Forward time: 3.365561 ms
Layer: TransformerLayer, Forward time: 3.363148 ms
Layer: TransformerLayer, Forward time: 3.327650 msLayer: TransformerLayer, Forward time: 3.315555 ms

Layer: TransformerLayer, Forward time: 3.306301 ms
Layer: TransformerLayer, Forward time: 3.315533 ms
Layer: TransformerLayer, Forward time: 3.252586 ms
Layer: TransformerLayer, Forward time: 3.240063 msLayer: TransformerLayer, Forward time: 3.278032 ms

Layer: TransformerLayer, Forward time: 3.248988 ms
Layer: TransformerLayer, Forward time: 3.197092 ms
Layer: TransformerLayer, Forward time: 3.186527 msLayer: TransformerLayer, Forward time: 3.195075 ms

Layer: TransformerLayer, Forward time: 3.198231 ms
Layer: TransformerLayer, Forward time: 8.679236 msLayer: TransformerLayer, Forward time: 6.569903 ms
Layer: TransformerLayer, Forward time: 5.864519 ms
Layer: TransformerLayer, Forward time: 7.554877 ms

Layer: TransformerLayer, Forward time: 3.660782 ms
Layer: TransformerLayer, Forward time: 3.687680 ms
Layer: TransformerLayer, Forward time: 3.694753 msLayer: TransformerLayer, Forward time: 3.680569 ms

Layer: TransformerLayer, Forward time: 3.583398 ms
Layer: TransformerLayer, Forward time: 3.632990 ms
Layer: TransformerLayer, Forward time: 3.589474 ms
Layer: TransformerLayer, Forward time: 3.617502 ms
Layer: TransformerLayer, Forward time: 3.509162 ms
Layer: TransformerLayer, Forward time: 3.533128 ms
Layer: TransformerLayer, Forward time: 3.501007 ms
Layer: TransformerLayer, Forward time: 3.506991 ms
Layer: TransformerLayer, Forward time: 3.502105 ms
Layer: TransformerLayer, Forward time: 3.496706 ms
Layer: TransformerLayer, Forward time: 3.501402 ms
Layer: TransformerLayer, Forward time: 3.517392 ms
Layer: TransformerLayer, Forward time: 3.519944 ms
Layer: TransformerLayer, Forward time: 3.506403 ms
Layer: TransformerLayer, Forward time: 3.511744 ms
Layer: TransformerLayer, Forward time: 3.526938 ms
Layer: TransformerLayer, Forward time: 3.448594 ms
Layer: TransformerLayer, Forward time: 3.461537 msLayer: TransformerLayer, Forward time: 3.430863 ms
Layer: TransformerLayer, Forward time: 3.443974 ms

Layer: TransformerLayer, Forward time: 3.463196 msLayer: TransformerLayer, Forward time: 3.419977 ms

Layer: TransformerLayer, Forward time: 3.446117 ms
Layer: TransformerLayer, Forward time: 3.472926 ms
Layer: TransformerLayer, Forward time: 3.454482 ms
Layer: TransformerLayer, Forward time: 3.464505 ms
Layer: TransformerLayer, Forward time: 3.446272 ms
Layer: TransformerLayer, Forward time: 3.449365 ms
Layer: TransformerLayer, Forward time: 3.409328 ms
Layer: TransformerLayer, Forward time: 3.391938 msLayer: TransformerLayer, Forward time: 3.436476 ms

Layer: TransformerLayer, Forward time: 3.413994 ms
Layer: TransformerLayer, Forward time: 3.507936 ms
Layer: TransformerLayer, Forward time: 3.499567 msLayer: TransformerLayer, Forward time: 3.497971 ms

Layer: TransformerLayer, Forward time: 3.510447 ms
Layer: TransformerLayer, Forward time: 3.384728 ms
Layer: TransformerLayer, Forward time: 3.378270 ms
Layer: TransformerLayer, Forward time: 3.386829 ms
Layer: TransformerLayer, Forward time: 3.403521 ms
Layer: TransformerLayer, Forward time: 3.397850 ms
Layer: TransformerLayer, Forward time: 3.387731 ms
Layer: TransformerLayer, Forward time: 3.376017 ms
Layer: TransformerLayer, Forward time: 3.384003 ms
Layer: TransformerLayer, Forward time: 3.450462 ms
Layer: TransformerLayer, Forward time: 3.423513 ms
Layer: TransformerLayer, Forward time: 3.466907 msLayer: TransformerLayer, Forward time: 3.438821 ms

Layer: TransformerLayer, Forward time: 3.414318 ms
Layer: TransformerLayer, Forward time: 3.404493 ms
Layer: TransformerLayer, Forward time: 3.432931 ms
Layer: TransformerLayer, Forward time: 3.447926 ms
Layer: TransformerLayer, Forward time: 3.401802 ms
Layer: TransformerLayer, Forward time: 3.405773 ms
Layer: TransformerLayer, Forward time: 3.389708 ms
Layer: TransformerLayer, Forward time: 3.398661 ms
Layer: TransformerLayer, Forward time: 3.399422 ms
Layer: TransformerLayer, Forward time: 3.388727 ms
Layer: TransformerLayer, Forward time: 3.392111 ms
Layer: TransformerLayer, Forward time: 3.399703 ms
Layer: TransformerLayer, Forward time: 3.475048 ms
Layer: TransformerLayer, Forward time: 3.475012 ms
Layer: TransformerLayer, Forward time: 3.456951 ms
Layer: TransformerLayer, Forward time: 3.475122 ms
Layer: TransformerLayer, Forward time: 3.432919 ms
Layer: TransformerLayer, Forward time: 3.433857 ms
Layer: TransformerLayer, Forward time: 3.442715 msLayer: TransformerLayer, Forward time: 3.436929 ms

Layer: TransformerLayer, Forward time: 3.432164 msLayer: TransformerLayer, Forward time: 3.403669 ms

Layer: TransformerLayer, Forward time: 3.410493 ms
Layer: TransformerLayer, Forward time: 3.435999 ms
Layer: TransformerLayer, Forward time: 3.418841 ms
Layer: TransformerLayer, Forward time: 3.438238 ms
Layer: TransformerLayer, Forward time: 3.403731 ms
Layer: TransformerLayer, Forward time: 3.418585 ms
Layer: TransformerLayer, Forward time: 3.373033 ms
Layer: TransformerLayer, Forward time: 3.357569 ms
Layer: TransformerLayer, Forward time: 3.365555 ms
Layer: TransformerLayer, Forward time: 3.383683 ms
Layer: TransformerLayer, Forward time: 3.430074 ms
Layer: TransformerLayer, Forward time: 3.413514 msLayer: TransformerLayer, Forward time: 3.444572 ms

Layer: TransformerLayer, Forward time: 3.416346 ms
Layer: TransformerLayer, Forward time: 3.315424 ms
Layer: TransformerLayer, Forward time: 3.299173 msLayer: TransformerLayer, Forward time: 3.309497 ms

Layer: TransformerLayer, Forward time: 3.318906 ms
Layer: TransformerLayer, Forward time: 5.156755 ms
Layer: TransformerLayer, Forward time: 4.823694 msLayer: TransformerLayer, Forward time: 5.066955 ms

Layer: TransformerLayer, Forward time: 5.248876 ms
Layer: TransformerLayer, Forward time: 3.425998 msLayer: TransformerLayer, Forward time: 3.447425 ms

Layer: TransformerLayer, Forward time: 3.449729 ms
Layer: TransformerLayer, Forward time: 3.458032 ms
Layer: TransformerLayer, Forward time: 3.323815 ms
Layer: TransformerLayer, Forward time: 3.335773 ms
Layer: TransformerLayer, Forward time: 3.317309 ms
Layer: TransformerLayer, Forward time: 3.319466 ms
Layer: TransformerLayer, Forward time: 3.261046 msLayer: TransformerLayer, Forward time: 3.245313 ms

Layer: TransformerLayer, Forward time: 3.256426 ms
Layer: TransformerLayer, Forward time: 3.256891 ms
Layer: TransformerLayer, Forward time: 3.302728 ms
Layer: TransformerLayer, Forward time: 3.301459 ms
Layer: TransformerLayer, Forward time: 3.327520 ms
Layer: TransformerLayer, Forward time: 3.307585 ms
Layer: TransformerLayer, Forward time: 3.311656 ms
Layer: TransformerLayer, Forward time: 3.311842 ms
Layer: TransformerLayer, Forward time: 3.335534 ms
Layer: TransformerLayer, Forward time: 3.340591 ms
Layer: TransformerLayer, Forward time: 3.335119 ms
Layer: TransformerLayer, Forward time: 3.305880 msLayer: TransformerLayer, Forward time: 3.340755 ms
Layer: TransformerLayer, Forward time: 3.305810 ms

Layer: TransformerLayer, Forward time: 3.224810 ms
Layer: TransformerLayer, Forward time: 3.258697 ms
Layer: TransformerLayer, Forward time: 3.264562 ms
Layer: TransformerLayer, Forward time: 3.275689 ms
Layer: TransformerLayer, Forward time: 3.199020 ms
Layer: TransformerLayer, Forward time: 3.196803 msLayer: TransformerLayer, Forward time: 3.246904 ms

Layer: TransformerLayer, Forward time: 3.198067 ms
Layer: TransformerLayer, Forward time: 3.276626 ms
Layer: TransformerLayer, Forward time: 3.283673 ms
Layer: TransformerLayer, Forward time: 3.328049 ms
Layer: TransformerLayer, Forward time: 3.331138 ms
Layer: TransformerLayer, Forward time: 3.274599 ms
Layer: TransformerLayer, Forward time: 3.263757 ms
Layer: TransformerLayer, Forward time: 3.278721 ms
Layer: TransformerLayer, Forward time: 3.257581 ms
Layer: TransformerLayer, Forward time: 3.247159 msLayer: TransformerLayer, Forward time: 3.263973 ms

Layer: TransformerLayer, Forward time: 3.290477 ms
Layer: TransformerLayer, Forward time: 3.299946 ms
Layer: TransformerLayer, Forward time: 3.276050 ms
Layer: TransformerLayer, Forward time: 3.278756 ms
Layer: TransformerLayer, Forward time: 3.329152 ms
Layer: TransformerLayer, Forward time: 3.355089 ms
Layer: TransformerLayer, Forward time: 3.307451 msLayer: TransformerLayer, Forward time: 3.301787 ms

Layer: TransformerLayer, Forward time: 3.305855 ms
Layer: TransformerLayer, Forward time: 3.301705 ms
Layer: TransformerLayer, Forward time: 3.377229 ms
Layer: TransformerLayer, Forward time: 3.380989 ms
Layer: TransformerLayer, Forward time: 3.387060 ms
Layer: TransformerLayer, Forward time: 3.386537 ms
Layer: TransformerLayer, Forward time: 3.300650 ms
Layer: TransformerLayer, Forward time: 3.286670 ms
Layer: TransformerLayer, Forward time: 3.336904 ms
Layer: TransformerLayer, Forward time: 3.290017 ms
Layer: TransformerLayer, Forward time: 3.281714 msLayer: TransformerLayer, Forward time: 3.268895 ms

Layer: TransformerLayer, Forward time: 3.268357 msLayer: TransformerLayer, Forward time: 3.283502 ms

Layer: TransformerLayer, Forward time: 3.301992 msLayer: TransformerLayer, Forward time: 3.303514 ms

Layer: TransformerLayer, Forward time: 3.306244 msLayer: TransformerLayer, Forward time: 3.309972 ms

Layer: TransformerLayer, Forward time: 3.288456 ms
Layer: TransformerLayer, Forward time: 3.295409 ms
Layer: TransformerLayer, Forward time: 3.320688 ms
Layer: TransformerLayer, Forward time: 3.328222 ms
Layer: TransformerLayer, Forward time: 3.307835 ms
Layer: TransformerLayer, Forward time: 3.340493 ms
Layer: TransformerLayer, Forward time: 3.349235 ms
Layer: TransformerLayer, Forward time: 3.347105 ms
Layer: TransformerLayer, Forward time: 3.282615 ms
Layer: TransformerLayer, Forward time: 3.323605 msLayer: TransformerLayer, Forward time: 3.261906 ms

Layer: TransformerLayer, Forward time: 3.262700 ms
Layer: TransformerLayer, Forward time: 3.309378 ms
Layer: TransformerLayer, Forward time: 3.306800 ms
Layer: TransformerLayer, Forward time: 3.317872 ms
Layer: TransformerLayer, Forward time: 3.322826 ms
Layer: TransformerLayer, Forward time: 3.238424 ms
Layer: TransformerLayer, Forward time: 3.246821 msLayer: TransformerLayer, Forward time: 3.233904 ms

Layer: TransformerLayer, Forward time: 3.230633 ms
Layer: TransformerLayer, Forward time: 3.281870 msLayer: TransformerLayer, Forward time: 3.262631 ms

Layer: TransformerLayer, Forward time: 3.281719 msLayer: TransformerLayer, Forward time: 3.269574 ms

Layer: TransformerLayer, Forward time: 6.256809 msLayer: TransformerLayer, Forward time: 6.324281 ms
Layer: TransformerLayer, Forward time: 6.462803 ms

Layer: TransformerLayer, Forward time: 5.505399 ms
Layer: TransformerLayer, Forward time: 3.897675 msLayer: TransformerLayer, Forward time: 3.888405 msLayer: TransformerLayer, Forward time: 3.884793 ms


Layer: TransformerLayer, Forward time: 3.892064 ms
Layer: TransformerLayer, Forward time: 3.594745 ms
Layer: TransformerLayer, Forward time: 3.609340 ms
Layer: TransformerLayer, Forward time: 3.645267 ms
Layer: TransformerLayer, Forward time: 3.672252 ms
Layer: TransformerLayer, Forward time: 3.644089 ms
Layer: TransformerLayer, Forward time: 3.642689 ms
Layer: TransformerLayer, Forward time: 3.611800 ms
Layer: TransformerLayer, Forward time: 3.615788 ms
Layer: TransformerLayer, Forward time: 3.622103 ms
Layer: TransformerLayer, Forward time: 3.624176 msLayer: TransformerLayer, Forward time: 3.611263 ms

Layer: TransformerLayer, Forward time: 3.636842 ms
Layer: TransformerLayer, Forward time: 3.709294 ms
Layer: TransformerLayer, Forward time: 3.684908 ms
Layer: TransformerLayer, Forward time: 3.680191 msLayer: TransformerLayer, Forward time: 3.692767 ms

Layer: TransformerLayer, Forward time: 3.899274 ms
Layer: TransformerLayer, Forward time: 3.928536 msLayer: TransformerLayer, Forward time: 3.897554 ms

Layer: TransformerLayer, Forward time: 3.922276 ms
Layer: TransformerLayer, Forward time: 3.762691 ms
Layer: TransformerLayer, Forward time: 3.751966 ms
Layer: TransformerLayer, Forward time: 3.771058 ms
Layer: TransformerLayer, Forward time: 3.775550 ms
Layer: TransformerLayer, Forward time: 3.425325 ms
Layer: TransformerLayer, Forward time: 3.426182 ms
Layer: TransformerLayer, Forward time: 3.461869 ms
Layer: TransformerLayer, Forward time: 3.494575 ms
Layer: TransformerLayer, Forward time: 3.547323 ms
Layer: TransformerLayer, Forward time: 3.526398 ms
Layer: TransformerLayer, Forward time: 3.519544 ms
Layer: TransformerLayer, Forward time: 3.554068 ms
Layer: TransformerLayer, Forward time: 3.329794 ms
Layer: TransformerLayer, Forward time: 3.315112 msLayer: TransformerLayer, Forward time: 3.335710 ms

Layer: TransformerLayer, Forward time: 3.344970 ms
Layer: TransformerLayer, Forward time: 3.370177 msLayer: TransformerLayer, Forward time: 3.384524 ms

Layer: TransformerLayer, Forward time: 3.420315 ms
Layer: TransformerLayer, Forward time: 3.421290 ms
Layer: TransformerLayer, Forward time: 3.321483 ms
Layer: TransformerLayer, Forward time: 3.329262 ms
Layer: TransformerLayer, Forward time: 3.357112 ms
Layer: TransformerLayer, Forward time: 3.365173 ms
Layer: TransformerLayer, Forward time: 3.278651 msLayer: TransformerLayer, Forward time: 3.264119 ms

Layer: TransformerLayer, Forward time: 3.320136 ms
Layer: TransformerLayer, Forward time: 3.362923 ms
Layer: TransformerLayer, Forward time: 3.255818 msLayer: TransformerLayer, Forward time: 3.258700 ms

Layer: TransformerLayer, Forward time: 3.289048 ms
Layer: TransformerLayer, Forward time: 3.294901 ms
Layer: TransformerLayer, Forward time: 3.301874 ms
Layer: TransformerLayer, Forward time: 3.317752 msLayer: TransformerLayer, Forward time: 3.276676 ms

Layer: TransformerLayer, Forward time: 3.275047 ms
Layer: TransformerLayer, Forward time: 3.203516 ms
Layer: TransformerLayer, Forward time: 3.233783 ms
Layer: TransformerLayer, Forward time: 3.248475 ms
Layer: TransformerLayer, Forward time: 3.257213 ms
Layer: TransformerLayer, Forward time: 3.294385 ms
Layer: TransformerLayer, Forward time: 3.327909 ms
Layer: TransformerLayer, Forward time: 3.283538 ms
Layer: TransformerLayer, Forward time: 3.280818 ms
Layer: TransformerLayer, Forward time: 3.311802 msLayer: TransformerLayer, Forward time: 3.290951 ms

Layer: TransformerLayer, Forward time: 3.336393 ms
Layer: TransformerLayer, Forward time: 3.333237 ms
Layer: TransformerLayer, Forward time: 3.314225 ms
Layer: TransformerLayer, Forward time: 3.330119 msLayer: TransformerLayer, Forward time: 3.273733 ms

Layer: TransformerLayer, Forward time: 3.288099 ms
Layer: TransformerLayer, Forward time: 3.210679 ms
Layer: TransformerLayer, Forward time: 3.240387 ms
Layer: TransformerLayer, Forward time: 3.254182 ms
Layer: TransformerLayer, Forward time: 3.258044 ms
Layer: TransformerLayer, Forward time: 3.382399 ms
Layer: TransformerLayer, Forward time: 3.412304 ms
Layer: TransformerLayer, Forward time: 3.481370 ms
Layer: TransformerLayer, Forward time: 3.509432 ms
Layer: TransformerLayer, Forward time: 3.246226 ms
Layer: TransformerLayer, Forward time: 3.243741 msLayer: TransformerLayer, Forward time: 3.283131 ms

Layer: TransformerLayer, Forward time: 3.317972 ms
Layer: TransformerLayer, Forward time: 3.224319 msLayer: TransformerLayer, Forward time: 3.246773 ms

Layer: TransformerLayer, Forward time: 3.272498 ms
Layer: TransformerLayer, Forward time: 3.270948 ms
Layer: TransformerLayer, Forward time: 10.966854 msLayer: TransformerLayer, Forward time: 10.754469 ms

Layer: TransformerLayer, Forward time: 11.179506 ms
Layer: TransformerLayer, Forward time: 8.779175 ms
Layer: TransformerLayer, Forward time: 3.372202 msLayer: TransformerLayer, Forward time: 3.393368 ms

Layer: TransformerLayer, Forward time: 3.279985 ms
Layer: TransformerLayer, Forward time: 3.397017 ms
Layer: TransformerLayer, Forward time: 3.231419 msLayer: TransformerLayer, Forward time: 3.260884 ms

Layer: TransformerLayer, Forward time: 3.301077 ms
Layer: TransformerLayer, Forward time: 3.324501 ms
Layer: TransformerLayer, Forward time: 3.159353 ms
Layer: TransformerLayer, Forward time: 3.168814 ms
Layer: TransformerLayer, Forward time: 3.178179 ms
Layer: TransformerLayer, Forward time: 3.183507 ms
Layer: TransformerLayer, Forward time: 3.128866 ms
Layer: TransformerLayer, Forward time: 3.179015 msLayer: TransformerLayer, Forward time: 3.134997 ms

Layer: TransformerLayer, Forward time: 3.203083 ms
Layer: TransformerLayer, Forward time: 3.120910 ms
Layer: TransformerLayer, Forward time: 3.112627 ms
Layer: TransformerLayer, Forward time: 3.124510 ms
Layer: TransformerLayer, Forward time: 3.126922 ms
Layer: TransformerLayer, Forward time: 3.129240 ms
Layer: TransformerLayer, Forward time: 3.144047 ms
Layer: TransformerLayer, Forward time: 3.170789 ms
Layer: TransformerLayer, Forward time: 3.170324 ms
Layer: TransformerLayer, Forward time: 3.295777 ms
Layer: TransformerLayer, Forward time: 3.304737 ms
Layer: TransformerLayer, Forward time: 3.284417 ms
Layer: TransformerLayer, Forward time: 3.282485 ms
Layer: TransformerLayer, Forward time: 3.236371 ms
Layer: TransformerLayer, Forward time: 3.237172 ms
Layer: TransformerLayer, Forward time: 3.284308 ms
Layer: TransformerLayer, Forward time: 3.234964 ms
Layer: TransformerLayer, Forward time: 3.237635 ms
Layer: TransformerLayer, Forward time: 3.238344 msLayer: TransformerLayer, Forward time: 3.270935 ms

Layer: TransformerLayer, Forward time: 3.278940 ms
Layer: TransformerLayer, Forward time: 3.271694 ms
Layer: TransformerLayer, Forward time: 3.253309 ms
Layer: TransformerLayer, Forward time: 3.265482 ms
Layer: TransformerLayer, Forward time: 3.256485 ms
Layer: TransformerLayer, Forward time: 3.172234 msLayer: TransformerLayer, Forward time: 3.192155 ms

Layer: TransformerLayer, Forward time: 3.179075 msLayer: TransformerLayer, Forward time: 3.182766 ms

Layer: TransformerLayer, Forward time: 3.268299 msLayer: TransformerLayer, Forward time: 3.246043 ms

Layer: TransformerLayer, Forward time: 3.299914 ms
Layer: TransformerLayer, Forward time: 3.309329 ms
Layer: TransformerLayer, Forward time: 3.168589 msLayer: TransformerLayer, Forward time: 3.185315 ms

Layer: TransformerLayer, Forward time: 3.229172 ms
Layer: TransformerLayer, Forward time: 3.245378 ms
Layer: TransformerLayer, Forward time: 3.155984 ms
Layer: TransformerLayer, Forward time: 3.161243 ms
Layer: TransformerLayer, Forward time: 3.186221 ms
Layer: TransformerLayer, Forward time: 3.177962 ms
Layer: TransformerLayer, Forward time: 3.173533 ms
Layer: TransformerLayer, Forward time: 3.211421 msLayer: TransformerLayer, Forward time: 3.164249 ms

Layer: TransformerLayer, Forward time: 3.178145 ms
Layer: TransformerLayer, Forward time: 3.211533 ms
Layer: TransformerLayer, Forward time: 3.247320 ms
Layer: TransformerLayer, Forward time: 3.207593 ms
Layer: TransformerLayer, Forward time: 3.251685 ms
Layer: TransformerLayer, Forward time: 3.168941 ms
Layer: TransformerLayer, Forward time: 3.167024 ms
Layer: TransformerLayer, Forward time: 3.216690 ms
Layer: TransformerLayer, Forward time: 3.211159 ms
Layer: TransformerLayer, Forward time: 3.332441 ms
Layer: TransformerLayer, Forward time: 3.335395 msLayer: TransformerLayer, Forward time: 3.350359 ms

Layer: TransformerLayer, Forward time: 3.327806 ms
Layer: TransformerLayer, Forward time: 3.226988 ms
Layer: TransformerLayer, Forward time: 3.223144 msLayer: TransformerLayer, Forward time: 3.230188 ms

Layer: TransformerLayer, Forward time: 3.227153 ms
Layer: TransformerLayer, Forward time: 3.241676 msLayer: TransformerLayer, Forward time: 3.224180 ms

Layer: TransformerLayer, Forward time: 3.285375 ms
Layer: TransformerLayer, Forward time: 3.291309 ms
Layer: TransformerLayer, Forward time: 3.319698 ms
Layer: TransformerLayer, Forward time: 3.341514 ms
Layer: TransformerLayer, Forward time: 3.416116 ms
Layer: TransformerLayer, Forward time: 3.430625 ms
Layer: TransformerLayer, Forward time: 3.203239 msLayer: TransformerLayer, Forward time: 3.232588 ms

Layer: TransformerLayer, Forward time: 3.207555 ms
Layer: TransformerLayer, Forward time: 3.193591 ms
Layer: TransformerLayer, Forward time: 3.192089 ms
Layer: TransformerLayer, Forward time: 3.184304 msLayer: TransformerLayer, Forward time: 3.213369 ms

Layer: TransformerLayer, Forward time: 3.182335 ms
Layer: TransformerLayer, Forward time: 7.819594 ms
Layer: TransformerLayer, Forward time: 6.089052 msLayer: TransformerLayer, Forward time: 7.635203 ms

Layer: TransformerLayer, Forward time: 5.232912 ms
Layer: TransformerLayer, Forward time: 3.878464 msLayer: TransformerLayer, Forward time: 3.845212 ms

Layer: TransformerLayer, Forward time: 3.956493 ms
Layer: TransformerLayer, Forward time: 3.916616 ms
Layer: TransformerLayer, Forward time: 3.767322 ms
Layer: TransformerLayer, Forward time: 3.791717 ms
Layer: TransformerLayer, Forward time: 3.760968 ms
Layer: TransformerLayer, Forward time: 3.754218 ms
Layer: TransformerLayer, Forward time: 3.703259 ms
Layer: TransformerLayer, Forward time: 3.699636 ms
Layer: TransformerLayer, Forward time: 3.760003 ms
Layer: TransformerLayer, Forward time: 3.795159 ms
Layer: TransformerLayer, Forward time: 3.759246 ms
Layer: TransformerLayer, Forward time: 3.740359 ms
Layer: TransformerLayer, Forward time: 3.768360 msLayer: TransformerLayer, Forward time: 3.735638 ms

Layer: TransformerLayer, Forward time: 3.630625 ms
Layer: TransformerLayer, Forward time: 3.598542 msLayer: TransformerLayer, Forward time: 3.622619 ms

Layer: TransformerLayer, Forward time: 3.640062 ms
Layer: TransformerLayer, Forward time: 3.200250 msLayer: TransformerLayer, Forward time: 3.215553 ms

Layer: TransformerLayer, Forward time: 3.237923 ms
Layer: TransformerLayer, Forward time: 3.240305 ms
Layer: TransformerLayer, Forward time: 3.197763 ms
Layer: TransformerLayer, Forward time: 3.210484 ms
Layer: TransformerLayer, Forward time: 3.218326 ms
Layer: TransformerLayer, Forward time: 3.209521 ms
Layer: TransformerLayer, Forward time: 3.200043 ms
Layer: TransformerLayer, Forward time: 3.196688 ms
Layer: TransformerLayer, Forward time: 3.197290 ms
Layer: TransformerLayer, Forward time: 3.195661 ms
Layer: TransformerLayer, Forward time: 3.169736 msLayer: TransformerLayer, Forward time: 3.192964 ms

Layer: TransformerLayer, Forward time: 3.179073 ms
Layer: TransformerLayer, Forward time: 3.178734 ms
Layer: TransformerLayer, Forward time: 3.253193 msLayer: TransformerLayer, Forward time: 3.232609 ms

Layer: TransformerLayer, Forward time: 3.276677 msLayer: TransformerLayer, Forward time: 3.231972 ms

Layer: TransformerLayer, Forward time: 3.145425 ms
Layer: TransformerLayer, Forward time: 3.185556 ms
Layer: TransformerLayer, Forward time: 3.219674 ms
Layer: TransformerLayer, Forward time: 3.210117 ms
Layer: TransformerLayer, Forward time: 3.250095 ms
Layer: TransformerLayer, Forward time: 3.246483 ms
Layer: TransformerLayer, Forward time: 3.246361 ms
Layer: TransformerLayer, Forward time: 3.246142 ms
Layer: TransformerLayer, Forward time: 3.274635 ms
Layer: TransformerLayer, Forward time: 3.311769 ms
Layer: TransformerLayer, Forward time: 3.269247 ms
Layer: TransformerLayer, Forward time: 3.269226 ms
Layer: TransformerLayer, Forward time: 3.231115 ms
Layer: TransformerLayer, Forward time: 3.233578 ms
Layer: TransformerLayer, Forward time: 3.236210 ms
Layer: TransformerLayer, Forward time: 3.232889 ms
Layer: TransformerLayer, Forward time: 3.202583 ms
Layer: TransformerLayer, Forward time: 3.220623 ms
Layer: TransformerLayer, Forward time: 3.254251 ms
Layer: TransformerLayer, Forward time: 3.248947 ms
Layer: TransformerLayer, Forward time: 3.221293 msLayer: TransformerLayer, Forward time: 3.184576 ms

Layer: TransformerLayer, Forward time: 3.253604 ms
Layer: TransformerLayer, Forward time: 3.182197 ms
Layer: TransformerLayer, Forward time: 3.388313 ms
Layer: TransformerLayer, Forward time: 3.402044 ms
Layer: TransformerLayer, Forward time: 3.399487 ms
Layer: TransformerLayer, Forward time: 3.402357 ms
Layer: TransformerLayer, Forward time: 3.203283 ms
Layer: TransformerLayer, Forward time: 3.241352 ms
Layer: TransformerLayer, Forward time: 3.266964 ms
Layer: TransformerLayer, Forward time: 3.263781 ms
Layer: TransformerLayer, Forward time: 3.260350 msLayer: TransformerLayer, Forward time: 3.240328 ms

Layer: TransformerLayer, Forward time: 3.266905 ms
Layer: TransformerLayer, Forward time: 3.263046 ms
Layer: TransformerLayer, Forward time: 3.258726 ms
Layer: TransformerLayer, Forward time: 3.277043 ms
Layer: TransformerLayer, Forward time: 3.249839 ms
Layer: TransformerLayer, Forward time: 3.253576 ms
Layer: TransformerLayer, Forward time: 3.202536 ms
Layer: TransformerLayer, Forward time: 3.222932 ms
Layer: TransformerLayer, Forward time: 3.240899 ms
Layer: TransformerLayer, Forward time: 3.227817 ms
Layer: TransformerLayer, Forward time: 3.187894 ms
Layer: TransformerLayer, Forward time: 3.174390 msLayer: TransformerLayer, Forward time: 3.226670 ms

Layer: TransformerLayer, Forward time: 3.170879 ms
Layer: TransformerLayer, Forward time: 3.151559 ms
Layer: TransformerLayer, Forward time: 3.177755 ms
Layer: TransformerLayer, Forward time: 3.210912 ms
Layer: TransformerLayer, Forward time: 3.210472 ms
Layer: TransformerLayer, Forward time: 4.520125 ms
Layer: TransformerLayer, Forward time: 4.394314 ms
Layer: TransformerLayer, Forward time: 4.436013 ms
Layer: TransformerLayer, Forward time: 4.214824 ms
Layer: TransformerLayer, Forward time: 3.290152 ms
Layer: TransformerLayer, Forward time: 3.332822 msLayer: TransformerLayer, Forward time: 3.306680 ms

Layer: TransformerLayer, Forward time: 3.354855 ms
Layer: TransformerLayer, Forward time: 3.215644 ms
Layer: TransformerLayer, Forward time: 3.240266 ms
Layer: TransformerLayer, Forward time: 3.217773 ms
Layer: TransformerLayer, Forward time: 3.236074 ms
Layer: TransformerLayer, Forward time: 3.150114 ms
Layer: TransformerLayer, Forward time: 3.176911 ms
Layer: TransformerLayer, Forward time: 3.158349 ms
Layer: TransformerLayer, Forward time: 3.178824 ms
Layer: TransformerLayer, Forward time: 3.219838 ms
Layer: TransformerLayer, Forward time: 3.207071 ms
Layer: TransformerLayer, Forward time: 3.259911 ms
Layer: TransformerLayer, Forward time: 3.273785 ms
Layer: TransformerLayer, Forward time: 3.182084 msLayer: TransformerLayer, Forward time: 3.132927 ms

Layer: TransformerLayer, Forward time: 3.185122 msLayer: TransformerLayer, Forward time: 3.135968 ms

Layer: TransformerLayer, Forward time: 3.185976 ms
Layer: TransformerLayer, Forward time: 3.216052 ms
Layer: TransformerLayer, Forward time: 3.238070 ms
Layer: TransformerLayer, Forward time: 3.252948 ms
Layer: TransformerLayer, Forward time: 3.222935 ms
Layer: TransformerLayer, Forward time: 3.182516 ms
Layer: TransformerLayer, Forward time: 3.249359 ms
Layer: TransformerLayer, Forward time: 3.195602 ms
Layer: TransformerLayer, Forward time: 3.262678 msLayer: TransformerLayer, Forward time: 3.242224 ms

Layer: TransformerLayer, Forward time: 3.276198 ms
Layer: TransformerLayer, Forward time: 3.281032 ms
Layer: TransformerLayer, Forward time: 3.274808 msLayer: TransformerLayer, Forward time: 3.309376 ms
Layer: TransformerLayer, Forward time: 3.277234 ms

Layer: TransformerLayer, Forward time: 3.324528 ms
Layer: TransformerLayer, Forward time: 3.210858 msLayer: TransformerLayer, Forward time: 3.225285 ms

Layer: TransformerLayer, Forward time: 3.254340 ms
Layer: TransformerLayer, Forward time: 3.278593 ms
Layer: TransformerLayer, Forward time: 3.236410 ms
Layer: TransformerLayer, Forward time: 3.246073 ms
Layer: TransformerLayer, Forward time: 3.236580 ms
Layer: TransformerLayer, Forward time: 3.243693 ms
Layer: TransformerLayer, Forward time: 3.234685 ms
Layer: TransformerLayer, Forward time: 3.227973 ms
Layer: TransformerLayer, Forward time: 3.219167 ms
Layer: TransformerLayer, Forward time: 3.239754 ms
Layer: TransformerLayer, Forward time: 3.205518 msLayer: TransformerLayer, Forward time: 3.241476 ms

Layer: TransformerLayer, Forward time: 3.203435 ms
Layer: TransformerLayer, Forward time: 3.243070 ms
Layer: TransformerLayer, Forward time: 3.199003 msLayer: TransformerLayer, Forward time: 3.198155 ms

Layer: TransformerLayer, Forward time: 3.214579 msLayer: TransformerLayer, Forward time: 3.197312 ms

Layer: TransformerLayer, Forward time: 3.226854 ms
Layer: TransformerLayer, Forward time: 3.226383 ms
Layer: TransformerLayer, Forward time: 3.253934 ms
Layer: TransformerLayer, Forward time: 3.265856 ms
Layer: TransformerLayer, Forward time: 4.115211 ms
Layer: TransformerLayer, Forward time: 4.105799 ms
Layer: TransformerLayer, Forward time: 4.106702 ms
Layer: TransformerLayer, Forward time: 4.103668 ms
Layer: TransformerLayer, Forward time: 3.273295 ms
Layer: TransformerLayer, Forward time: 3.271844 ms
Layer: TransformerLayer, Forward time: 3.304588 msLayer: TransformerLayer, Forward time: 3.279078 ms

Layer: TransformerLayer, Forward time: 3.209819 msLayer: TransformerLayer, Forward time: 3.198955 ms

Layer: TransformerLayer, Forward time: 3.211248 msLayer: TransformerLayer, Forward time: 3.209814 ms

Layer: TransformerLayer, Forward time: 3.195617 ms
Layer: TransformerLayer, Forward time: 3.182542 ms
Layer: TransformerLayer, Forward time: 3.219633 ms
Layer: TransformerLayer, Forward time: 3.216915 ms
Layer: TransformerLayer, Forward time: 3.296588 ms
Layer: TransformerLayer, Forward time: 3.295599 msLayer: TransformerLayer, Forward time: 3.281116 ms

Layer: TransformerLayer, Forward time: 3.292626 ms
Layer: TransformerLayer, Forward time: 3.284608 ms
Layer: TransformerLayer, Forward time: 3.272115 ms
Layer: TransformerLayer, Forward time: 3.281497 ms
Layer: TransformerLayer, Forward time: 3.285301 ms
Layer: TransformerLayer, Forward time: 3.190129 ms
Layer: TransformerLayer, Forward time: 3.181527 ms
Layer: TransformerLayer, Forward time: 3.185761 ms
Layer: TransformerLayer, Forward time: 3.202185 ms
Layer: TransformerLayer, Forward time: 3.214901 ms
Layer: TransformerLayer, Forward time: 3.184772 msLayer: TransformerLayer, Forward time: 3.216019 ms

Layer: TransformerLayer, Forward time: 3.187616 ms
