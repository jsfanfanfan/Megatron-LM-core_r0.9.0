examples/multimodal/pretrain-freeze-llm-hete-2080first.sh: line 4: activate: No such file or directory
2
[2024-12-24 22:18:34,267] torch.distributed.run: [WARNING] 
[2024-12-24 22:18:34,267] torch.distributed.run: [WARNING] *****************************************
[2024-12-24 22:18:34,267] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-24 22:18:34,267] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:8------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:8------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:8------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:4------
------world_size:16------
------total_model_size:8------
------num_pipeline_model_parallel_groups:4------
---Rank 8---Tensor Parallel Group GPUs: [0, 0]---Rank 9---Tensor Parallel Group GPUs: [1, 1]

---Rank 8---Pipeline Parallel Group GPUs: [2, 2, 2, 2]
---Rank 9---Pipeline Parallel Group GPUs: [2, 2, 2, 2]
---Rank 11---Tensor Parallel Group GPUs: [1, 1]
---Rank 10---Tensor Parallel Group GPUs: [0, 0][rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
---Rank 11---Pipeline Parallel Group GPUs: [2, 2, 2, 2][rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())


---Rank 10---Pipeline Parallel Group GPUs: [2, 2, 2, 2]
[rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 545300480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 545300480
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (545300480 elements):
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
rank=1, worker=0: shard_range=[pretrain-31.tar[0, 100), pretrain-31.tar[100, 200), pretrain-31.tar[200, 300), ...<1244>, pretrain-42.tar[4700, 4800), pretrain-42.tar[4800, 4900), pretrain-42.tar[4900, 5000)] sum(count)=125000
rank=1, worker=1: shard_range=[pretrain-42.tar[5000, 5100), pretrain-42.tar[5100, 5200), pretrain-42.tar[5200, 5300), ...<1244>, pretrain-53.tar[9700, 9800), pretrain-53.tar[9800, 9900), pretrain-53.tar[9900, 10000)] sum(count)=125000
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<1244>, pretrain-2.tar[4700, 4800), pretrain-2.tar[4800, 4900), pretrain-2.tar[4900, 5000)] sum(count)=125000
rank=0, worker=1: shard_range=[pretrain-2.tar[5000, 5100), pretrain-2.tar[5100, 5200), pretrain-2.tar[5200, 5300), ...<1244>, pretrain-30.tar[9700, 9800), pretrain-30.tar[9800, 9900), pretrain-30.tar[9900, 10000)] sum(count)=125000
rank=0, worker=0: shard_range=[pretrain-54.tar[0, 10000), pretrain-55.tar[0, 4532)] sum(count)=14532
rank=0, worker=1: shard_range=[pretrain-55.tar[4532, 8128), pretrain-6.tar[0, 10000), pretrain-7.tar[0, 936)] sum(count)=14532
rank=1, worker=0: shard_range=[pretrain-7.tar[936, 10000), pretrain-8.tar[0, 5468)] sum(count)=14532
rank=1, worker=1: shard_range=[pretrain-8.tar[5468, 10000), pretrain-9.tar[0, 10000)] sum(count)=14532
