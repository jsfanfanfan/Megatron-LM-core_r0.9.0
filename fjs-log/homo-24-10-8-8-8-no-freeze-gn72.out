examples/multimodal/pretrain-freeze-llm-homo.sh: line 4: activate: No such file or directory
4
[2024-12-03 23:10:37,014] torch.distributed.run: [WARNING] 
[2024-12-03 23:10:37,014] torch.distributed.run: [WARNING] *****************************************
[2024-12-03 23:10:37,014] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-03 23:10:37,014] torch.distributed.run: [WARNING] *****************************************
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
------pipeline_parallel_model_size:5------
------world_size:20------
------total_model_size:20------
------num_pipeline_model_parallel_groups:4------
---Rank 18---Tensor Parallel Group GPUs: [2, 2, 2, 2]
---Rank 17---Tensor Parallel Group GPUs: [1, 1, 1, 1]
---Rank 16---Tensor Parallel Group GPUs: [0, 0, 0, 0]---Rank 18---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 19---Tensor Parallel Group GPUs: [3, 3, 3, 3]---Rank 17---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]

---Rank 16---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
---Rank 19---Pipeline Parallel Group GPUs: [4, 4, 4, 4, 4]
[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
WARNING:megatron.core.models.multimodal.llava_model:LLaVA model is under active development. It may be missing features and its methods may change.
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 469831680
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 469831680 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 469831680

INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (469831680 elements):
	language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_qkv.weight
	language_model.decoder.layers.5.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_proj.weight
	language_model.decoder.layers.3.mlp.linear_fc1.weight
	language_model.decoder.layers.7.mlp.linear_fc1.weight
	language_model.decoder.layers.7.mlp.linear_fc2.weight
	language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.3.mlp.linear_fc2.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.mlp.linear_fc2.weight
	language_model.decoder.final_layernorm.weight
	language_model.decoder.layers.4.mlp.linear_fc2.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.mlp.linear_fc1.weight
	language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.self_attention.linear_qkv.weight
	language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.1.mlp.linear_fc1.weight
	language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.weight
	language_model.decoder.layers.0.self_attention.linear_proj.weight
	language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.6.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_proj.weight
	language_model.decoder.layers.3.self_attention.linear_qkv.weight
	language_model.decoder.layers.2.mlp.linear_fc2.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.6.mlp.linear_fc2.weight
	language_model.decoder.layers.6.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc1.weight
	language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.4.mlp.linear_fc1.weight
	language_model.decoder.layers.1.self_attention.linear_proj.weight
	language_model.decoder.layers.0.mlp.linear_fc1.weight
	language_model.output_layer.weight
	language_model.decoder.layers.7.self_attention.linear_qkv.weight
	language_model.decoder.layers.7.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	language_model.decoder.layers.2.self_attention.linear_proj.weight
	language_model.decoder.layers.5.self_attention.linear_qkv.weight
	language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	language_model.decoder.layers.3.self_attention.linear_proj.weight
	language_model.decoder.layers.2.self_attention.linear_qkv.weight
	language_model.decoder.layers.1.mlp.linear_fc2.weight
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:Truename:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:Truename:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True

name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True

name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True

name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:Truename:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True

name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:Truename:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True

name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 469831680
name:module.language_model.decoder.layers.0.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.0.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.1.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.2.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.3.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.4.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.5.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.6.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_proj.weight param:torch.Size([4096, 1024]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.self_attention.linear_qkv.weight param:torch.Size([1536, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.layer_norm_weight param:torch.Size([4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc1.weight param:torch.Size([7168, 4096]) require_grad:True
name:module.language_model.decoder.layers.7.mlp.linear_fc2.weight param:torch.Size([4096, 3584]) require_grad:True
name:module.language_model.decoder.final_layernorm.weight param:torch.Size([4096]) require_grad:True
name:module.language_model.output_layer.weight param:torch.Size([8192, 4096]) require_grad:True
rank=0, worker=0: shard_range=[pretrain-0.tar[0, 100), pretrain-0.tar[100, 200), pretrain-0.tar[200, 300), ...<2194>, pretrain-28.tar[9700, 9800), pretrain-28.tar[9800, 9900), pretrain-28.tar[9900, 10000)] sum(count)=220000
rank=0, worker=1: shard_range=[pretrain-29.tar[0, 100), pretrain-29.tar[100, 200), pretrain-29.tar[200, 300), ...<2194>, pretrain-48.tar[9700, 9800), pretrain-48.tar[9800, 9900), pretrain-48.tar[9900, 10000)] sum(count)=220000
rank=0, worker=0: shard_range=[pretrain-49.tar[0, 10000), pretrain-5.tar[0, 10000), pretrain-50.tar[0, 10000)] sum(count)=30000
rank=0, worker=1: shard_range=[pretrain-51.tar[0, 10000), pretrain-52.tar[0, 10000), pretrain-53.tar[0, 10000)] sum(count)=30000
times across ranks (ms):
  model-and-optimizer-setup:
     rank  0: 200.81
     rank  1: 200.44
     rank  2: 200.14
     rank  3: 201.16
     rank  4: 227.55
     rank  5: 219.14
     rank  6: 219.25
     rank  7: 219.32
     rank  8: 160.58
     rank  9: 158.63
     rank 10: 168.11
     rank 11: 168.68
     rank 12: 211.95
     rank 13: 212.41
     rank 14: 215.32
     rank 15: 211.71
     rank 16: 39.13
     rank 17: 41.69
     rank 18: 63.61
     rank 19: 41.87
  train/valid/test-data-iterators-setup:
     rank  0: 1189.75
     rank  1: 1190.03
     rank  2: 1189.74
     rank  3: 1189.76
     rank  4: 1410.69
     rank  5: 1410.52
     rank  6: 1410.69
     rank  7: 1410.73
     rank  8: 1410.67
     rank  9: 1410.61
     rank 10: 1411.19
     rank 11: 1414.33
     rank 12: 1411.09
     rank 13: 1410.59
     rank 14: 1410.69
     rank 15: 1410.63
     rank 16: 1410.68
     rank 17: 1410.75
     rank 18: 1413.72
     rank 19: 1410.74
 [2024-12-03 23:12:18] iteration        1/      10 | consumed samples:           32 | elapsed time per iteration (ms): 38564.1 | learning rate: 7.500000E-07 | global batch size:    32 | lm loss: 6.941158E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 18] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10114.0 | max reserved: 10114.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0[Rank 17] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0

[Rank 19] (after 1 iterations) memory (MB) | allocated: 7254.3154296875 | max allocated: 9045.791015625 | reserved: 10142.0 | max reserved: 10142.0
times across ranks (ms):
  forward-backward:
     rank  0: 38410.63
     rank  1: 38410.64
     rank  2: 38410.61
     rank  3: 38410.61
     rank  4: 38413.26
     rank  5: 38413.14
     rank  6: 38413.16
     rank  7: 38413.06
     rank  8: 38412.82
     rank  9: 38412.60
     rank 10: 38412.56
     rank 11: 38412.80
     rank 12: 38412.93
     rank 13: 38412.81
     rank 14: 38412.59
     rank 15: 38412.89
     rank 16: 38412.95
     rank 17: 38412.88
     rank 18: 38412.89
     rank 19: 38412.96
  forward-compute:
     rank  0: 12951.78
     rank  1: 12955.97
     rank  2: 12956.77
     rank  3: 12956.25
     rank  4: 9597.26
     rank  5: 9603.91
     rank  6: 9605.34
     rank  7: 9602.66
     rank  8: 5241.41
     rank  9: 5239.11
     rank 10: 5234.47
     rank 11: 5238.78
     rank 12: 7961.68
     rank 13: 7962.84
     rank 14: 7961.74
     rank 15: 7954.30
     rank 16: 5031.26
     rank 17: 5031.37
     rank 18: 5030.60
     rank 19: 5017.55
  backward-compute:
     rank  0: 3193.48
     rank  1: 3193.36
     rank  2: 3192.09
     rank  3: 3195.21
     rank  4: 3492.46
     rank  5: 3496.84
     rank  6: 3491.67
     rank  7: 3498.69
     rank  8: 3313.25
     rank  9: 3318.01
     rank 10: 3315.05
     rank 11: 3313.53
     rank 12: 3341.68
     rank 13: 3345.83
     rank 14: 3346.23
     rank 15: 3341.36
     rank 16: 3504.01
     rank 17: 3506.69
     rank 18: 3505.20
     rank 19: 3514.50
  pure-backward-compute:
     rank  0: 3192.61
     rank  1: 3192.42
     rank  2: 3191.13
     rank  3: 3194.42
     rank  4: 3491.56
     rank  5: 3495.81
     rank  6: 3490.23
     rank  7: 3497.56
     rank  8: 3311.68
     rank  9: 3316.87
     rank 10: 3313.69
     rank 11: 3312.37
     rank 12: 3340.29
     rank 13: 3344.96
     rank 14: 3345.20
     rank 15: 3340.01
     rank 16: 3501.50
     rank 17: 3504.33
     rank 18: 3502.52
     rank 19: 3511.85
  batch-generator:
     rank  0: 1275.38
     rank  1: 1281.45
     rank  2: 1283.74
     rank  3: 1283.07
     rank  4: 1763.09
     rank  5: 1780.55
     rank  6: 1785.12
     rank  7: 1783.93
     rank  8: 1166.28
     rank  9: 1167.79
     rank 10: 1164.23
     rank 11: 1174.03
     rank 12: 1389.45
     rank 13: 1388.26
     rank 14: 1388.96
     rank 15: 1386.23
     rank 16: 1067.53
     rank 17: 1069.86
     rank 18: 1072.07
     rank 19: 1064.29
  forward-recv:
     rank  4: 11981.66
     rank  5: 11973.30
     rank  6: 11966.53
     rank  7: 11968.24
     rank  8: 18803.46
     rank  9: 18800.01
     rank 10: 18807.13
     rank 11: 18804.15
     rank 12: 21428.85
     rank 13: 21434.88
     rank 14: 21433.13
     rank 15: 21430.45
     rank 16: 26780.84
     rank 17: 26784.50
     rank 18: 26782.27
     rank 19: 26786.83
  forward-send:
     rank  0: 14749.28
     rank  1: 14745.69
     rank  2: 14742.37
     rank  3: 14744.89
     rank  4: 7650.83
     rank  5: 7655.26
     rank  6: 7657.70
     rank  7: 7659.87
     rank  8: 5208.23
     rank  9: 5216.94
     rank 10: 5212.69
     rank 11: 5216.59
     rank 12: 27.07
     rank 13: 31.40
     rank 14: 29.34
     rank 15: 35.44
  backward-recv:
     rank  0: 1407.04
     rank  1: 1407.84
     rank  2: 1408.02
     rank  3: 1407.81
     rank  4: 611.89
     rank  5: 610.88
     rank  6: 610.67
     rank  7: 610.91
     rank  8: 400.64
     rank  9: 399.87
     rank 10: 400.26
     rank 11: 399.86
     rank 12: 197.30
     rank 13: 198.34
     rank 14: 197.70
     rank 15: 197.66
  backward-send:
     rank  4: 3.36
     rank  5: 4.18
     rank  6: 4.59
     rank  7: 4.08
     rank  8: 30.79
     rank  9: 31.29
     rank 10: 31.07
     rank 11: 31.16
     rank 12: 20.82
     rank 13: 20.15
     rank 14: 20.61
     rank 15: 20.74
     rank 16: 10.34
     rank 17: 10.47
     rank 18: 10.32
     rank 19: 10.58
  forward-send-backward-recv:
     rank  0: 6014.47
     rank  1: 6013.90
     rank  2: 6013.98
     rank  3: 6013.35
     rank  4: 3002.50
     rank  5: 2999.24
     rank  6: 3000.34
     rank  7: 2999.62
     rank  8: 2847.33
     rank  9: 2845.21
     rank 10: 2843.61
     rank 11: 2845.70
     rank 12: 2609.37
     rank 13: 2605.70
     rank 14: 2607.27
     rank 15: 2610.18
  backward-send-forward-recv:
     rank  4: 1912.77
     rank  5: 1911.73
     rank  6: 1912.76
     rank  7: 1910.62
     rank  8: 2196.13
     rank  9: 2192.94
     rank 10: 2197.13
     rank 11: 2191.49
     rank 12: 2237.11
     rank 13: 2231.56
     rank 14: 2234.44
     rank 15: 2236.86
     rank 16: 2287.06
     rank 17: 2282.50
     rank 18: 2284.34
     rank 19: 2285.96
  layernorm-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.04
     rank  3: 0.04
     rank  4: 0.03
     rank  5: 0.03
     rank  6: 0.03
     rank  7: 0.03
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.07
     rank 13: 0.06
     rank 14: 0.04
     rank 15: 0.06
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.05
     rank  1: 0.06
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.09
     rank  5: 0.10
     rank  6: 0.10
     rank  7: 0.08
     rank  8: 0.04
     rank  9: 0.04
     rank 10: 0.03
     rank 11: 0.04
     rank 12: 0.05
     rank 13: 0.05
     rank 14: 0.03
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.08
     rank 19: 0.09
  all-grads-sync:
     rank  0: 76.54
     rank  1: 76.26
     rank  2: 69.10
     rank  3: 69.13
     rank  4: 65.02
     rank  5: 72.87
     rank  6: 72.64
     rank  7: 67.30
     rank  8: 72.93
     rank  9: 70.33
     rank 10: 68.78
     rank 11: 71.29
     rank 12: 74.85
     rank 13: 75.28
     rank 14: 67.58
     rank 15: 78.16
     rank 16: 2.70
     rank 17: 2.72
     rank 18: 2.79
     rank 19: 3.01
  optimizer-copy-to-main-grad:
     rank  0: 0.27
     rank  1: 0.28
     rank  2: 0.28
     rank  3: 0.27
     rank  4: 0.07
     rank  5: 0.08
     rank  6: 0.09
     rank  7: 0.07
     rank  8: 0.08
     rank  9: 0.08
     rank 10: 0.06
     rank 11: 0.08
     rank 12: 0.11
     rank 13: 0.07
     rank 14: 0.07
     rank 15: 0.10
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.08
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.04
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.05
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 132.62
     rank  1: 139.65
     rank  2: 139.82
     rank  3: 134.75
     rank  4: 144.30
     rank  5: 144.31
     rank  6: 144.35
     rank  7: 144.30
     rank  8: 141.69
     rank  9: 141.65
     rank 10: 141.67
     rank 11: 141.63
     rank 12: 142.28
     rank 13: 142.29
     rank 14: 142.29
     rank 15: 142.31
     rank 16: 57.93
     rank 17: 57.13
     rank 18: 57.19
     rank 19: 58.23
  optimizer:
     rank  0: 135.22
     rank  1: 142.26
     rank  2: 142.46
     rank  3: 137.39
     rank  4: 146.95
     rank  5: 146.96
     rank  6: 146.99
     rank  7: 146.95
     rank  8: 144.34
     rank  9: 144.29
     rank 10: 144.32
     rank 11: 144.27
     rank 12: 144.93
     rank 13: 144.94
     rank 14: 144.93
     rank 15: 144.92
     rank 16: 60.58
     rank 17: 59.77
     rank 18: 59.84
     rank 19: 60.87
 [2024-12-03 23:12:26] iteration        2/      10 | consumed samples:           64 | elapsed time per iteration (ms): 8076.0 | learning rate: 1.500000E-06 | global batch size:    32 | lm loss: 7.144479E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7988.06
     rank  1: 7988.10
     rank  2: 7989.06
     rank  3: 7988.17
     rank  4: 7991.98
     rank  5: 7991.94
     rank  6: 7992.14
     rank  7: 7991.93
     rank  8: 7991.44
     rank  9: 7991.53
     rank 10: 7991.66
     rank 11: 7991.54
     rank 12: 7991.45
     rank 13: 7991.41
     rank 14: 7991.66
     rank 15: 7991.48
     rank 16: 7991.83
     rank 17: 7991.80
     rank 18: 7992.40
     rank 19: 7991.80
  forward-compute:
     rank  0: 1108.99
     rank  1: 1111.82
     rank  2: 1110.04
     rank  3: 1114.56
     rank  4: 2967.03
     rank  5: 2970.59
     rank  6: 2967.45
     rank  7: 2970.92
     rank  8: 2789.79
     rank  9: 2791.76
     rank 10: 2789.21
     rank 11: 2795.31
     rank 12: 2797.16
     rank 13: 2804.93
     rank 14: 2801.55
     rank 15: 2796.64
     rank 16: 2932.71
     rank 17: 2937.65
     rank 18: 2937.34
     rank 19: 2932.21
  backward-compute:
     rank  0: 1071.04
     rank  1: 1070.40
     rank  2: 1068.95
     rank  3: 1069.59
     rank  4: 3370.96
     rank  5: 3375.01
     rank  6: 3370.95
     rank  7: 3372.69
     rank  8: 3271.72
     rank  9: 3275.13
     rank 10: 3274.67
     rank 11: 3271.51
     rank 12: 3273.01
     rank 13: 3276.23
     rank 14: 3275.79
     rank 15: 3272.15
     rank 16: 3509.96
     rank 17: 3511.89
     rank 18: 3509.15
     rank 19: 3514.20
  pure-backward-compute:
     rank  0: 1070.34
     rank  1: 1069.72
     rank  2: 1068.23
     rank  3: 1068.86
     rank  4: 3370.06
     rank  5: 3374.23
     rank  6: 3370.07
     rank  7: 3371.97
     rank  8: 3270.38
     rank  9: 3274.22
     rank 10: 3273.33
     rank 11: 3270.60
     rank 12: 3271.40
     rank 13: 3275.51
     rank 14: 3274.91
     rank 15: 3270.95
     rank 16: 3507.15
     rank 17: 3509.71
     rank 18: 3506.70
     rank 19: 3511.86
  batch-generator:
     rank  0: 56.19
     rank  1: 61.11
     rank  2: 59.85
     rank  3: 63.88
     rank  4: 59.69
     rank  5: 67.22
     rank  6: 64.14
     rank  7: 70.90
     rank  8: 56.08
     rank  9: 61.33
     rank 10: 59.27
     rank 11: 64.04
     rank 12: 58.69
     rank 13: 64.91
     rank 14: 63.19
     rank 15: 63.29
     rank 16: 60.74
     rank 17: 67.32
     rank 18: 67.85
     rank 19: 63.30
  forward-recv:
     rank  4: 79.48
     rank  5: 78.66
     rank  6: 79.44
     rank  7: 77.91
     rank  8: 298.98
     rank  9: 299.91
     rank 10: 298.78
     rank 11: 296.80
     rank 12: 467.42
     rank 13: 466.62
     rank 14: 467.56
     rank 15: 467.72
     rank 16: 642.23
     rank 17: 642.18
     rank 18: 642.22
     rank 19: 641.88
  forward-send:
     rank  0: 365.59
     rank  1: 363.83
     rank  2: 365.80
     rank  3: 361.32
     rank  4: 31.23
     rank  5: 30.68
     rank  6: 31.28
     rank  7: 28.87
     rank  8: 20.92
     rank  9: 19.98
     rank 10: 21.03
     rank 11: 20.77
     rank 12: 10.52
     rank 13: 10.42
     rank 14: 10.36
     rank 15: 10.25
  backward-recv:
     rank  0: 1388.16
     rank  1: 1388.71
     rank  2: 1388.93
     rank  3: 1390.45
     rank  4: 607.70
     rank  5: 607.68
     rank  6: 607.86
     rank  7: 608.09
     rank  8: 396.83
     rank  9: 397.39
     rank 10: 396.44
     rank 11: 397.44
     rank 12: 199.16
     rank 13: 199.02
     rank 14: 199.88
     rank 15: 199.39
  backward-send:
     rank  4: 4.03
     rank  5: 4.12
     rank  6: 4.25
     rank  7: 3.73
     rank  8: 30.65
     rank  9: 30.75
     rank 10: 31.27
     rank 11: 30.86
     rank 12: 20.80
     rank 13: 20.49
     rank 14: 20.36
     rank 15: 20.79
     rank 16: 10.40
     rank 17: 9.82
     rank 18: 10.35
     rank 19: 10.43
  forward-send-backward-recv:
     rank  0: 4037.57
     rank  1: 4038.49
     rank  2: 4037.73
     rank  3: 4037.73
     rank  4: 823.64
     rank  5: 821.40
     rank  6: 824.06
     rank  7: 826.68
     rank  8: 740.01
     rank  9: 739.50
     rank 10: 738.20
     rank 11: 739.67
     rank 12: 554.11
     rank 13: 551.72
     rank 14: 552.57
     rank 15: 554.02
  backward-send-forward-recv:
     rank  4: 20.51
     rank  5: 20.12
     rank  6: 20.61
     rank  7: 19.44
     rank  8: 145.01
     rank  9: 142.17
     rank 10: 144.93
     rank 11: 144.15
     rank 12: 156.20
     rank 13: 153.06
     rank 14: 153.93
     rank 15: 157.09
     rank 16: 168.20
     rank 17: 164.58
     rank 18: 165.66
     rank 19: 168.52
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.03
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.03
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.03
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.07
     rank 18: 0.14
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.63
     rank  2: 0.64
     rank  3: 0.57
     rank  4: 2.48
     rank  5: 2.39
     rank  6: 2.40
     rank  7: 2.45
     rank  8: 2.19
     rank  9: 2.28
     rank 10: 2.20
     rank 11: 2.22
     rank 12: 2.38
     rank 13: 2.17
     rank 14: 2.22
     rank 15: 2.43
     rank 16: 2.42
     rank 17: 2.38
     rank 18: 2.64
     rank 19: 2.42
  optimizer-copy-to-main-grad:
     rank  0: 0.19
     rank  1: 0.20
     rank  2: 0.20
     rank  3: 0.20
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.08
     rank 13: 0.04
     rank 14: 0.05
     rank 15: 0.06
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.11
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.51
     rank  1: 8.76
     rank  2: 8.77
     rank  3: 8.66
     rank  4: 48.24
     rank  5: 48.11
     rank  6: 48.18
     rank  7: 48.13
     rank  8: 43.07
     rank  9: 43.11
     rank 10: 43.02
     rank 11: 43.06
     rank 12: 43.17
     rank 13: 42.99
     rank 14: 43.09
     rank 15: 43.16
     rank 16: 46.50
     rank 17: 46.41
     rank 18: 46.87
     rank 19: 46.44
  optimizer:
     rank  0: 9.72
     rank  1: 9.97
     rank  2: 9.99
     rank  3: 9.87
     rank  4: 49.46
     rank  5: 49.33
     rank  6: 49.40
     rank  7: 49.35
     rank  8: 44.29
     rank  9: 44.33
     rank 10: 44.24
     rank 11: 44.28
     rank 12: 44.39
     rank 13: 44.21
     rank 14: 44.31
     rank 15: 44.37
     rank 16: 47.73
     rank 17: 47.70
     rank 18: 48.06
     rank 19: 47.69
 [2024-12-03 23:12:34] iteration        3/      10 | consumed samples:           96 | elapsed time per iteration (ms): 8042.3 | learning rate: 2.250000E-06 | global batch size:    32 | lm loss: 2.732837E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7959.43
     rank  1: 7959.44
     rank  2: 7959.46
     rank  3: 7959.47
     rank  4: 7963.30
     rank  5: 7963.35
     rank  6: 7963.31
     rank  7: 7963.29
     rank  8: 7962.80
     rank  9: 7962.86
     rank 10: 7962.81
     rank 11: 7962.90
     rank 12: 7962.82
     rank 13: 7962.80
     rank 14: 7962.79
     rank 15: 7962.82
     rank 16: 7963.19
     rank 17: 7963.16
     rank 18: 7963.20
     rank 19: 7963.15
  forward-compute:
     rank  0: 1211.76
     rank  1: 1214.61
     rank  2: 1213.17
     rank  3: 1217.29
     rank  4: 2973.35
     rank  5: 2976.69
     rank  6: 2975.34
     rank  7: 2979.46
     rank  8: 2785.37
     rank  9: 2788.82
     rank 10: 2785.58
     rank 11: 2795.26
     rank 12: 2798.57
     rank 13: 2805.86
     rank 14: 2802.90
     rank 15: 2797.01
     rank 16: 2930.80
     rank 17: 2935.99
     rank 18: 2931.41
     rank 19: 2933.10
  backward-compute:
     rank  0: 1046.60
     rank  1: 1044.99
     rank  2: 1045.39
     rank  3: 1047.05
     rank  4: 3383.29
     rank  5: 3388.77
     rank  6: 3384.10
     rank  7: 3389.05
     rank  8: 3280.35
     rank  9: 3279.70
     rank 10: 3279.86
     rank 11: 3276.80
     rank 12: 3267.68
     rank 13: 3272.12
     rank 14: 3268.30
     rank 15: 3268.60
     rank 16: 3505.47
     rank 17: 3506.59
     rank 18: 3503.97
     rank 19: 3508.52
  pure-backward-compute:
     rank  0: 1045.95
     rank  1: 1044.29
     rank  2: 1044.69
     rank  3: 1046.32
     rank  4: 3382.29
     rank  5: 3387.78
     rank  6: 3383.21
     rank  7: 3388.23
     rank  8: 3278.86
     rank  9: 3278.85
     rank 10: 3278.58
     rank 11: 3275.79
     rank 12: 3266.63
     rank 13: 3271.35
     rank 14: 3267.53
     rank 15: 3267.35
     rank 16: 3502.86
     rank 17: 3504.70
     rank 18: 3500.97
     rank 19: 3506.64
  batch-generator:
     rank  0: 55.77
     rank  1: 60.95
     rank  2: 61.33
     rank  3: 65.20
     rank  4: 67.84
     rank  5: 71.81
     rank  6: 70.45
     rank  7: 77.49
     rank  8: 55.69
     rank  9: 62.51
     rank 10: 59.82
     rank 11: 67.32
     rank 12: 57.57
     rank 13: 63.02
     rank 14: 62.18
     rank 15: 61.77
     rank 16: 58.81
     rank 17: 65.40
     rank 18: 62.56
     rank 19: 63.67
  forward-recv:
     rank  4: 69.72
     rank  5: 68.96
     rank  6: 69.78
     rank  7: 67.96
     rank  8: 286.17
     rank  9: 287.50
     rank 10: 286.07
     rank 11: 284.07
     rank 12: 460.30
     rank 13: 459.45
     rank 14: 460.89
     rank 15: 460.46
     rank 16: 631.25
     rank 17: 631.20
     rank 18: 631.07
     rank 19: 631.13
  forward-send:
     rank  0: 397.09
     rank  1: 395.27
     rank  2: 397.08
     rank  3: 392.69
     rank  4: 31.35
     rank  5: 30.83
     rank  6: 31.26
     rank  7: 28.85
     rank  8: 20.87
     rank  9: 19.71
     rank 10: 20.79
     rank 11: 20.64
     rank 12: 10.56
     rank 13: 10.35
     rank 14: 10.05
     rank 15: 10.43
  backward-recv:
     rank  0: 1385.48
     rank  1: 1386.36
     rank  2: 1385.95
     rank  3: 1386.15
     rank  4: 600.20
     rank  5: 598.94
     rank  6: 600.54
     rank  7: 600.58
     rank  8: 394.94
     rank  9: 395.71
     rank 10: 394.84
     rank 11: 395.10
     rank 12: 199.19
     rank 13: 199.33
     rank 14: 199.74
     rank 15: 199.35
  backward-send:
     rank  4: 3.69
     rank  5: 4.34
     rank  6: 3.80
     rank  7: 4.00
     rank  8: 30.82
     rank  9: 30.43
     rank 10: 30.92
     rank 11: 31.25
     rank 12: 20.84
     rank 13: 20.35
     rank 14: 20.58
     rank 15: 20.78
     rank 16: 10.46
     rank 17: 9.79
     rank 18: 10.42
     rank 19: 10.43
  forward-send-backward-recv:
     rank  0: 3902.53
     rank  1: 3903.78
     rank  2: 3901.48
     rank  3: 3901.64
     rank  4: 803.73
     rank  5: 799.74
     rank  6: 802.83
     rank  7: 801.70
     rank  8: 731.23
     rank  9: 731.04
     rank 10: 729.84
     rank 11: 731.76
     rank 12: 545.83
     rank 13: 542.73
     rank 14: 547.79
     rank 15: 546.11
  backward-send-forward-recv:
     rank  4: 20.44
     rank  5: 20.28
     rank  6: 19.56
     rank  7: 18.13
     rank  8: 146.54
     rank  9: 145.14
     rank 10: 148.24
     rank 11: 144.03
     rank 12: 157.34
     rank 13: 154.39
     rank 14: 154.50
     rank 15: 158.25
     rank 16: 167.99
     rank 17: 164.53
     rank 18: 167.93
     rank 19: 166.05
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.07
     rank  6: 0.05
     rank  7: 0.06
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.58
     rank  1: 0.58
     rank  2: 0.57
     rank  3: 0.64
     rank  4: 2.40
     rank  5: 2.45
     rank  6: 2.39
     rank  7: 2.41
     rank  8: 2.17
     rank  9: 2.26
     rank 10: 2.22
     rank 11: 2.31
     rank 12: 2.34
     rank 13: 2.16
     rank 14: 2.17
     rank 15: 2.34
     rank 16: 2.47
     rank 17: 2.34
     rank 18: 2.52
     rank 19: 2.37
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.18
     rank  3: 0.20
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.07
     rank 12: 0.08
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.06
     rank 16: 0.07
     rank 17: 0.05
     rank 18: 0.07
     rank 19: 0.10
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.37
     rank  1: 8.61
     rank  2: 8.42
     rank  3: 8.59
     rank  4: 48.12
     rank  5: 48.29
     rank  6: 48.20
     rank  7: 48.12
     rank  8: 43.05
     rank  9: 43.14
     rank 10: 43.09
     rank 11: 43.16
     rank 12: 43.04
     rank 13: 43.03
     rank 14: 43.02
     rank 15: 43.08
     rank 16: 46.45
     rank 17: 46.36
     rank 18: 46.66
     rank 19: 46.39
  optimizer:
     rank  0: 9.13
     rank  1: 9.36
     rank  2: 9.17
     rank  3: 9.34
     rank  4: 48.88
     rank  5: 49.04
     rank  6: 48.95
     rank  7: 48.88
     rank  8: 43.81
     rank  9: 43.89
     rank 10: 43.85
     rank 11: 43.92
     rank 12: 43.79
     rank 13: 43.78
     rank 14: 43.79
     rank 15: 43.83
     rank 16: 47.21
     rank 17: 47.11
     rank 18: 47.42
     rank 19: 47.14
 [2024-12-03 23:12:42] iteration        4/      10 | consumed samples:          128 | elapsed time per iteration (ms): 8026.5 | learning rate: 3.000000E-06 | global batch size:    32 | lm loss: 4.380832E-01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7941.55
     rank  1: 7941.53
     rank  2: 7941.58
     rank  3: 7941.53
     rank  4: 7945.43
     rank  5: 7945.40
     rank  6: 7945.43
     rank  7: 7945.38
     rank  8: 7944.91
     rank  9: 7944.92
     rank 10: 7944.94
     rank 11: 7944.94
     rank 12: 7944.93
     rank 13: 7944.86
     rank 14: 7944.90
     rank 15: 7944.94
     rank 16: 7945.32
     rank 17: 7945.27
     rank 18: 7945.32
     rank 19: 7945.26
  forward-compute:
     rank  0: 1039.73
     rank  1: 1042.97
     rank  2: 1041.87
     rank  3: 1046.00
     rank  4: 2970.24
     rank  5: 2973.53
     rank  6: 2970.64
     rank  7: 2975.01
     rank  8: 2793.09
     rank  9: 2794.64
     rank 10: 2792.14
     rank 11: 2803.57
     rank 12: 2800.43
     rank 13: 2807.73
     rank 14: 2804.59
     rank 15: 2798.93
     rank 16: 2926.47
     rank 17: 2931.41
     rank 18: 2927.60
     rank 19: 2930.68
  backward-compute:
     rank  0: 1041.93
     rank  1: 1041.27
     rank  2: 1040.41
     rank  3: 1042.76
     rank  4: 3380.86
     rank  5: 3385.06
     rank  6: 3381.05
     rank  7: 3388.06
     rank  8: 3280.87
     rank  9: 3283.72
     rank 10: 3283.23
     rank 11: 3279.88
     rank 12: 3282.19
     rank 13: 3285.37
     rank 14: 3282.36
     rank 15: 3284.71
     rank 16: 3486.03
     rank 17: 3488.22
     rank 18: 3485.11
     rank 19: 3487.39
  pure-backward-compute:
     rank  0: 1041.27
     rank  1: 1040.53
     rank  2: 1039.55
     rank  3: 1042.07
     rank  4: 3380.05
     rank  5: 3384.13
     rank  6: 3380.15
     rank  7: 3387.35
     rank  8: 3279.44
     rank  9: 3282.79
     rank 10: 3282.11
     rank 11: 3278.90
     rank 12: 3280.77
     rank 13: 3284.65
     rank 14: 3281.57
     rank 15: 3283.22
     rank 16: 3483.33
     rank 17: 3486.03
     rank 18: 3482.61
     rank 19: 3485.49
  batch-generator:
     rank  0: 55.84
     rank  1: 61.71
     rank  2: 62.65
     rank  3: 65.74
     rank  4: 54.18
     rank  5: 64.35
     rank  6: 60.77
     rank  7: 68.23
     rank  8: 57.15
     rank  9: 62.31
     rank 10: 60.21
     rank 11: 69.62
     rank 12: 57.09
     rank 13: 62.35
     rank 14: 61.30
     rank 15: 61.10
     rank 16: 57.66
     rank 17: 64.63
     rank 18: 62.52
     rank 19: 64.95
  forward-recv:
     rank  4: 69.05
     rank  5: 68.36
     rank  6: 69.23
     rank  7: 67.48
     rank  8: 286.32
     rank  9: 287.14
     rank 10: 286.28
     rank 11: 283.51
     rank 12: 458.21
     rank 13: 457.47
     rank 14: 458.59
     rank 15: 458.37
     rank 16: 632.87
     rank 17: 632.84
     rank 18: 632.56
     rank 19: 632.71
  forward-send:
     rank  0: 395.41
     rank  1: 393.39
     rank  2: 395.02
     rank  3: 390.43
     rank  4: 31.20
     rank  5: 30.63
     rank  6: 31.10
     rank  7: 28.40
     rank  8: 20.83
     rank  9: 20.09
     rank 10: 20.87
     rank 11: 20.78
     rank 12: 10.56
     rank 13: 10.43
     rank 14: 10.06
     rank 15: 10.45
  backward-recv:
     rank  0: 1383.94
     rank  1: 1384.53
     rank  2: 1383.18
     rank  3: 1383.52
     rank  4: 597.48
     rank  5: 595.84
     rank  6: 597.01
     rank  7: 598.00
     rank  8: 393.07
     rank  9: 393.43
     rank 10: 392.91
     rank 11: 393.85
     rank 12: 196.28
     rank 13: 196.73
     rank 14: 196.67
     rank 15: 196.13
  backward-send:
     rank  4: 3.92
     rank  5: 4.65
     rank  6: 4.27
     rank  7: 3.30
     rank  8: 30.89
     rank  9: 30.88
     rank 10: 30.75
     rank 11: 30.80
     rank 12: 20.79
     rank 13: 20.35
     rank 14: 20.56
     rank 15: 20.77
     rank 16: 10.49
     rank 17: 10.48
     rank 18: 10.49
     rank 19: 10.56
  forward-send-backward-recv:
     rank  0: 4064.59
     rank  1: 4065.23
     rank  2: 4064.57
     rank  3: 4064.51
     rank  4: 795.85
     rank  5: 792.92
     rank  6: 795.23
     rank  7: 792.62
     rank  8: 699.68
     rank  9: 699.96
     rank 10: 697.93
     rank 11: 700.38
     rank 12: 514.69
     rank 13: 511.72
     rank 14: 516.25
     rank 15: 512.87
  backward-send-forward-recv:
     rank  4: 20.26
     rank  5: 19.88
     rank  6: 20.39
     rank  7: 18.98
     rank  8: 150.44
     rank  9: 147.97
     rank 10: 150.97
     rank 11: 145.34
     rank 12: 156.90
     rank 13: 154.12
     rank 14: 154.80
     rank 15: 157.61
     rank 16: 168.50
     rank 17: 164.59
     rank 18: 168.14
     rank 19: 165.61
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.04
     rank 13: 0.03
     rank 14: 0.02
     rank 15: 0.04
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.03
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.06
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.57
     rank  2: 0.62
     rank  3: 0.56
     rank  4: 2.45
     rank  5: 2.41
     rank  6: 2.47
     rank  7: 2.40
     rank  8: 2.21
     rank  9: 2.22
     rank 10: 2.18
     rank 11: 2.25
     rank 12: 2.39
     rank 13: 2.16
     rank 14: 2.16
     rank 15: 2.36
     rank 16: 2.44
     rank 17: 2.40
     rank 18: 2.48
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.20
     rank  2: 0.19
     rank  3: 0.20
     rank  4: 0.10
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.08
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.06
     rank 16: 0.12
     rank 17: 0.06
     rank 18: 0.12
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.02
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.02
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.03
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.64
     rank  1: 8.56
     rank  2: 8.72
     rank  3: 8.53
     rank  4: 48.15
     rank  5: 48.15
     rank  6: 48.29
     rank  7: 48.06
     rank  8: 43.21
     rank  9: 43.10
     rank 10: 43.01
     rank 11: 43.10
     rank 12: 43.17
     rank 13: 43.03
     rank 14: 43.04
     rank 15: 43.18
     rank 16: 46.44
     rank 17: 46.44
     rank 18: 47.18
     rank 19: 46.44
  optimizer:
     rank  0: 9.95
     rank  1: 9.86
     rank  2: 10.03
     rank  3: 9.84
     rank  4: 49.46
     rank  5: 49.46
     rank  6: 49.60
     rank  7: 49.37
     rank  8: 44.52
     rank  9: 44.41
     rank 10: 44.32
     rank 11: 44.41
     rank 12: 44.48
     rank 13: 44.33
     rank 14: 44.35
     rank 15: 44.49
     rank 16: 47.75
     rank 17: 47.75
     rank 18: 48.46
     rank 19: 47.75
 [2024-12-03 23:12:50] iteration        5/      10 | consumed samples:          160 | elapsed time per iteration (ms): 8026.2 | learning rate: 3.750000E-06 | global batch size:    32 | lm loss: 2.958251E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7945.26
     rank  1: 7945.26
     rank  2: 7945.59
     rank  3: 7945.28
     rank  4: 7949.15
     rank  5: 7949.14
     rank  6: 7949.46
     rank  7: 7949.15
     rank  8: 7948.64
     rank  9: 7948.70
     rank 10: 7948.93
     rank 11: 7948.73
     rank 12: 7948.71
     rank 13: 7948.59
     rank 14: 7948.91
     rank 15: 7948.65
     rank 16: 7949.03
     rank 17: 7949.03
     rank 18: 7949.78
     rank 19: 7949.03
  forward-compute:
     rank  0: 1061.83
     rank  1: 1064.46
     rank  2: 1063.15
     rank  3: 1067.56
     rank  4: 2977.86
     rank  5: 2982.02
     rank  6: 2977.27
     rank  7: 2982.80
     rank  8: 2800.87
     rank  9: 2800.25
     rank 10: 2799.74
     rank 11: 2811.65
     rank 12: 2798.34
     rank 13: 2804.39
     rank 14: 2802.78
     rank 15: 2796.80
     rank 16: 2920.44
     rank 17: 2925.99
     rank 18: 2919.49
     rank 19: 2923.53
  backward-compute:
     rank  0: 1043.76
     rank  1: 1043.62
     rank  2: 1040.63
     rank  3: 1044.89
     rank  4: 3393.98
     rank  5: 3395.47
     rank  6: 3393.99
     rank  7: 3402.25
     rank  8: 3292.91
     rank  9: 3294.63
     rank 10: 3295.43
     rank 11: 3291.27
     rank 12: 3288.33
     rank 13: 3293.47
     rank 14: 3288.78
     rank 15: 3291.10
     rank 16: 3494.71
     rank 17: 3496.79
     rank 18: 3495.50
     rank 19: 3497.50
  pure-backward-compute:
     rank  0: 1043.10
     rank  1: 1042.83
     rank  2: 1039.90
     rank  3: 1044.12
     rank  4: 3393.09
     rank  5: 3394.66
     rank  6: 3392.82
     rank  7: 3401.39
     rank  8: 3291.54
     rank  9: 3293.63
     rank 10: 3294.20
     rank 11: 3290.37
     rank 12: 3287.26
     rank 13: 3292.77
     rank 14: 3287.82
     rank 15: 3289.76
     rank 16: 3492.28
     rank 17: 3494.97
     rank 18: 3492.74
     rank 19: 3495.76
  batch-generator:
     rank  0: 59.60
     rank  1: 64.70
     rank  2: 62.33
     rank  3: 67.29
     rank  4: 58.59
     rank  5: 64.09
     rank  6: 65.47
     rank  7: 73.79
     rank  8: 55.97
     rank  9: 58.31
     rank 10: 58.79
     rank 11: 69.28
     rank 12: 54.94
     rank 13: 59.43
     rank 14: 59.94
     rank 15: 58.99
     rank 16: 55.97
     rank 17: 63.16
     rank 18: 58.30
     rank 19: 61.74
  forward-recv:
     rank  4: 69.03
     rank  5: 68.34
     rank  6: 68.92
     rank  7: 67.43
     rank  8: 284.18
     rank  9: 285.84
     rank 10: 285.01
     rank 11: 282.21
     rank 12: 458.97
     rank 13: 457.89
     rank 14: 458.76
     rank 15: 459.06
     rank 16: 632.19
     rank 17: 632.26
     rank 18: 632.16
     rank 19: 632.03
  forward-send:
     rank  0: 396.08
     rank  1: 394.38
     rank  2: 396.18
     rank  3: 391.44
     rank  4: 31.17
     rank  5: 30.65
     rank  6: 31.22
     rank  7: 28.49
     rank  8: 21.04
     rank  9: 19.76
     rank 10: 20.68
     rank 11: 20.68
     rank 12: 10.48
     rank 13: 10.38
     rank 14: 10.40
     rank 15: 10.32
  backward-recv:
     rank  0: 1382.91
     rank  1: 1382.91
     rank  2: 1383.91
     rank  3: 1382.94
     rank  4: 592.50
     rank  5: 591.65
     rank  6: 591.52
     rank  7: 591.30
     rank  8: 388.67
     rank  9: 388.63
     rank 10: 388.40
     rank 11: 388.98
     rank 12: 193.80
     rank 13: 194.00
     rank 14: 194.50
     rank 15: 193.79
  backward-send:
     rank  4: 3.83
     rank  5: 4.11
     rank  6: 4.54
     rank  7: 3.90
     rank  8: 31.14
     rank  9: 31.23
     rank 10: 31.18
     rank 11: 30.48
     rank 12: 20.77
     rank 13: 20.52
     rank 14: 20.32
     rank 15: 20.70
     rank 16: 10.52
     rank 17: 10.52
     rank 18: 10.44
     rank 19: 10.52
  forward-send-backward-recv:
     rank  0: 4043.48
     rank  1: 4044.02
     rank  2: 4044.31
     rank  3: 4042.83
     rank  4: 783.52
     rank  5: 783.65
     rank  6: 783.72
     rank  7: 780.33
     rank  8: 690.00
     rank  9: 690.96
     rank 10: 688.15
     rank 11: 690.47
     rank 12: 513.86
     rank 13: 509.80
     rank 14: 516.01
     rank 15: 512.73
  backward-send-forward-recv:
     rank  4: 19.95
     rank  5: 18.76
     rank  6: 20.56
     rank  7: 18.34
     rank  8: 150.37
     rank  9: 149.99
     rank 10: 150.66
     rank 11: 144.44
     rank 12: 157.03
     rank 13: 154.78
     rank 14: 154.09
     rank 15: 157.33
     rank 16: 168.24
     rank 17: 163.73
     rank 18: 168.76
     rank 19: 165.89
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.03
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.03
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.07
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.07
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.16
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.56
     rank  2: 0.63
     rank  3: 0.57
     rank  4: 2.41
     rank  5: 2.40
     rank  6: 2.47
     rank  7: 2.39
     rank  8: 2.18
     rank  9: 2.30
     rank 10: 2.17
     rank 11: 2.68
     rank 12: 2.29
     rank 13: 2.16
     rank 14: 2.22
     rank 15: 2.31
     rank 16: 2.45
     rank 17: 2.37
     rank 18: 3.03
     rank 19: 2.38
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.20
     rank  2: 0.19
     rank  3: 0.19
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.07
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.11
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.06
     rank 15: 0.06
     rank 16: 0.07
     rank 17: 0.06
     rank 18: 0.13
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.02
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.02
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.05
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.32
     rank  1: 8.60
     rank  2: 8.73
     rank  3: 8.47
     rank  4: 48.18
     rank  5: 48.14
     rank  6: 48.22
     rank  7: 48.13
     rank  8: 43.05
     rank  9: 43.08
     rank 10: 43.04
     rank 11: 43.61
     rank 12: 43.04
     rank 13: 43.05
     rank 14: 43.13
     rank 15: 43.08
     rank 16: 46.39
     rank 17: 46.59
     rank 18: 47.16
     rank 19: 46.37
  optimizer:
     rank  0: 9.75
     rank  1: 10.03
     rank  2: 10.17
     rank  3: 9.90
     rank  4: 49.61
     rank  5: 49.57
     rank  6: 49.65
     rank  7: 49.57
     rank  8: 44.49
     rank  9: 44.51
     rank 10: 44.47
     rank 11: 45.04
     rank 12: 44.47
     rank 13: 44.48
     rank 14: 44.57
     rank 15: 44.50
     rank 16: 47.83
     rank 17: 48.02
     rank 18: 48.57
     rank 19: 47.81
 [2024-12-03 23:12:58] iteration        6/      10 | consumed samples:          192 | elapsed time per iteration (ms): 8031.7 | learning rate: 4.500000E-06 | global batch size:    32 | lm loss: 1.529926E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7948.05
     rank  1: 7948.05
     rank  2: 7948.11
     rank  3: 7948.06
     rank  4: 7951.96
     rank  5: 7951.93
     rank  6: 7951.95
     rank  7: 7951.93
     rank  8: 7951.43
     rank  9: 7951.45
     rank 10: 7951.46
     rank 11: 7951.50
     rank 12: 7951.43
     rank 13: 7951.38
     rank 14: 7951.44
     rank 15: 7951.43
     rank 16: 7951.82
     rank 17: 7951.83
     rank 18: 7951.83
     rank 19: 7951.82
  forward-compute:
     rank  0: 1050.91
     rank  1: 1053.87
     rank  2: 1052.47
     rank  3: 1056.74
     rank  4: 2968.04
     rank  5: 2973.90
     rank  6: 2968.88
     rank  7: 2972.50
     rank  8: 2793.38
     rank  9: 2794.36
     rank 10: 2791.41
     rank 11: 2802.08
     rank 12: 2794.04
     rank 13: 2799.99
     rank 14: 2797.66
     rank 15: 2792.48
     rank 16: 2917.40
     rank 17: 2923.37
     rank 18: 2917.78
     rank 19: 2918.47
  backward-compute:
     rank  0: 1030.18
     rank  1: 1030.73
     rank  2: 1028.08
     rank  3: 1030.43
     rank  4: 3390.25
     rank  5: 3393.25
     rank  6: 3389.41
     rank  7: 3395.22
     rank  8: 3297.16
     rank  9: 3298.40
     rank 10: 3300.95
     rank 11: 3296.72
     rank 12: 3287.22
     rank 13: 3292.14
     rank 14: 3287.51
     rank 15: 3287.56
     rank 16: 3500.86
     rank 17: 3501.80
     rank 18: 3499.87
     rank 19: 3503.28
  pure-backward-compute:
     rank  0: 1029.54
     rank  1: 1029.97
     rank  2: 1027.37
     rank  3: 1029.68
     rank  4: 3389.41
     rank  5: 3392.43
     rank  6: 3388.43
     rank  7: 3394.37
     rank  8: 3295.82
     rank  9: 3297.54
     rank 10: 3299.83
     rank 11: 3295.93
     rank 12: 3286.16
     rank 13: 3291.23
     rank 14: 3286.76
     rank 15: 3286.36
     rank 16: 3498.39
     rank 17: 3500.02
     rank 18: 3497.00
     rank 19: 3501.61
  batch-generator:
     rank  0: 55.17
     rank  1: 61.66
     rank  2: 59.47
     rank  3: 63.64
     rank  4: 56.84
     rank  5: 65.43
     rank  6: 66.11
     rank  7: 71.15
     rank  8: 55.78
     rank  9: 59.88
     rank 10: 57.82
     rank 11: 66.28
     rank 12: 54.72
     rank 13: 59.00
     rank 14: 58.62
     rank 15: 58.60
     rank 16: 55.18
     rank 17: 62.33
     rank 18: 58.72
     rank 19: 58.56
  forward-recv:
     rank  4: 68.36
     rank  5: 67.31
     rank  6: 68.44
     rank  7: 67.04
     rank  8: 284.97
     rank  9: 286.68
     rank 10: 285.12
     rank 11: 282.48
     rank 12: 457.61
     rank 13: 456.06
     rank 14: 457.85
     rank 15: 457.56
     rank 16: 632.20
     rank 17: 632.26
     rank 18: 631.97
     rank 19: 632.21
  forward-send:
     rank  0: 396.98
     rank  1: 395.02
     rank  2: 396.98
     rank  3: 392.21
     rank  4: 31.24
     rank  5: 30.44
     rank  6: 31.20
     rank  7: 28.30
     rank  8: 20.81
     rank  9: 19.15
     rank 10: 20.80
     rank 11: 20.54
     rank 12: 10.48
     rank 13: 10.36
     rank 14: 10.17
     rank 15: 10.54
  backward-recv:
     rank  0: 1392.20
     rank  1: 1392.40
     rank  2: 1392.66
     rank  3: 1392.20
     rank  4: 600.57
     rank  5: 599.73
     rank  6: 600.15
     rank  7: 600.39
     rank  8: 387.92
     rank  9: 387.69
     rank 10: 388.07
     rank 11: 387.71
     rank 12: 195.49
     rank 13: 195.97
     rank 14: 196.00
     rank 15: 195.63
  backward-send:
     rank  4: 3.85
     rank  5: 4.11
     rank  6: 4.35
     rank  7: 3.74
     rank  8: 31.07
     rank  9: 30.97
     rank 10: 30.84
     rank 11: 31.26
     rank 12: 20.82
     rank 13: 20.35
     rank 14: 20.45
     rank 15: 20.98
     rank 16: 10.51
     rank 17: 10.41
     rank 18: 10.44
     rank 19: 10.53
  forward-send-backward-recv:
     rank  0: 4061.90
     rank  1: 4061.74
     rank  2: 4061.66
     rank  3: 4061.96
     rank  4: 791.38
     rank  5: 789.31
     rank  6: 791.85
     rank  7: 790.30
     rank  8: 699.79
     rank  9: 701.70
     rank 10: 698.45
     rank 11: 700.75
     rank 12: 520.38
     rank 13: 517.02
     rank 14: 522.76
     rank 15: 521.12
  backward-send-forward-recv:
     rank  4: 20.14
     rank  5: 18.40
     rank  6: 20.15
     rank  7: 19.22
     rank  8: 146.69
     rank  9: 144.93
     rank 10: 146.17
     rank 11: 142.76
     rank 12: 157.24
     rank 13: 155.28
     rank 14: 154.93
     rank 15: 158.07
     rank 16: 167.37
     rank 17: 162.77
     rank 18: 167.18
     rank 19: 166.29
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.06
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.06
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.56
     rank  2: 0.62
     rank  3: 0.57
     rank  4: 2.46
     rank  5: 2.39
     rank  6: 2.42
     rank  7: 2.39
     rank  8: 2.17
     rank  9: 2.28
     rank 10: 2.20
     rank 11: 2.38
     rank 12: 2.19
     rank 13: 2.19
     rank 14: 2.19
     rank 15: 2.18
     rank 16: 2.35
     rank 17: 2.42
     rank 18: 2.47
     rank 19: 2.41
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.19
     rank  3: 0.20
     rank  4: 0.07
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.05
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.05
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.07
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.02
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.02
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.31
     rank  1: 8.50
     rank  2: 8.51
     rank  3: 8.49
     rank  4: 48.27
     rank  5: 48.17
     rank  6: 48.17
     rank  7: 48.08
     rank  8: 43.04
     rank  9: 43.04
     rank 10: 43.07
     rank 11: 43.10
     rank 12: 43.09
     rank 13: 43.01
     rank 14: 43.04
     rank 15: 43.08
     rank 16: 46.35
     rank 17: 46.35
     rank 18: 46.43
     rank 19: 46.40
  optimizer:
     rank  0: 9.06
     rank  1: 9.17
     rank  2: 9.19
     rank  3: 9.16
     rank  4: 48.93
     rank  5: 48.84
     rank  6: 48.83
     rank  7: 48.75
     rank  8: 43.71
     rank  9: 43.71
     rank 10: 43.74
     rank 11: 43.77
     rank 12: 43.77
     rank 13: 43.68
     rank 14: 43.71
     rank 15: 43.76
     rank 16: 47.02
     rank 17: 47.02
     rank 18: 47.10
     rank 19: 47.07
 [2024-12-03 23:13:06] iteration        7/      10 | consumed samples:          224 | elapsed time per iteration (ms): 8046.3 | learning rate: 5.250000E-06 | global batch size:    32 | lm loss: 1.196540E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7965.85
     rank  1: 7965.64
     rank  2: 7965.65
     rank  3: 7965.65
     rank  4: 7969.72
     rank  5: 7969.47
     rank  6: 7969.54
     rank  7: 7969.48
     rank  8: 7969.21
     rank  9: 7969.01
     rank 10: 7968.96
     rank 11: 7969.04
     rank 12: 7969.23
     rank 13: 7969.03
     rank 14: 7968.95
     rank 15: 7969.20
     rank 16: 7969.97
     rank 17: 7969.41
     rank 18: 7969.38
     rank 19: 7969.39
  forward-compute:
     rank  0: 1052.84
     rank  1: 1057.86
     rank  2: 1052.77
     rank  3: 1056.97
     rank  4: 2959.90
     rank  5: 2966.54
     rank  6: 2960.38
     rank  7: 2962.53
     rank  8: 2788.66
     rank  9: 2792.96
     rank 10: 2788.55
     rank 11: 2793.29
     rank 12: 2791.51
     rank 13: 2796.63
     rank 14: 2797.00
     rank 15: 2790.28
     rank 16: 2923.00
     rank 17: 2928.95
     rank 18: 2922.87
     rank 19: 2924.07
  backward-compute:
     rank  0: 1036.23
     rank  1: 1036.19
     rank  2: 1034.20
     rank  3: 1036.30
     rank  4: 3376.11
     rank  5: 3378.81
     rank  6: 3376.31
     rank  7: 3378.38
     rank  8: 3298.33
     rank  9: 3298.87
     rank 10: 3300.57
     rank 11: 3298.33
     rank 12: 3283.67
     rank 13: 3288.39
     rank 14: 3284.38
     rank 15: 3284.15
     rank 16: 3506.28
     rank 17: 3506.74
     rank 18: 3506.70
     rank 19: 3508.57
  pure-backward-compute:
     rank  0: 1035.57
     rank  1: 1035.52
     rank  2: 1033.51
     rank  3: 1035.51
     rank  4: 3375.32
     rank  5: 3378.04
     rank  6: 3375.47
     rank  7: 3377.61
     rank  8: 3297.02
     rank  9: 3297.86
     rank 10: 3299.44
     rank 11: 3297.56
     rank 12: 3282.60
     rank 13: 3287.58
     rank 14: 3283.66
     rank 15: 3282.99
     rank 16: 3503.76
     rank 17: 3504.77
     rank 18: 3503.78
     rank 19: 3506.88
  batch-generator:
     rank  0: 54.83
     rank  1: 61.23
     rank  2: 57.39
     rank  3: 64.23
     rank  4: 54.10
     rank  5: 61.91
     rank  6: 60.46
     rank  7: 63.92
     rank  8: 55.15
     rank  9: 62.26
     rank 10: 58.79
     rank 11: 61.61
     rank 12: 54.44
     rank 13: 58.00
     rank 14: 60.05
     rank 15: 58.60
     rank 16: 56.29
     rank 17: 63.63
     rank 18: 59.55
     rank 19: 59.89
  forward-recv:
     rank  4: 68.83
     rank  5: 67.89
     rank  6: 68.84
     rank  7: 67.21
     rank  8: 282.02
     rank  9: 280.68
     rank 10: 283.34
     rank 11: 280.86
     rank 12: 459.70
     rank 13: 458.63
     rank 14: 458.71
     rank 15: 459.36
     rank 16: 634.09
     rank 17: 634.18
     rank 18: 633.94
     rank 19: 634.10
  forward-send:
     rank  0: 393.83
     rank  1: 389.60
     rank  2: 393.72
     rank  3: 390.52
     rank  4: 32.31
     rank  5: 29.18
     rank  6: 32.21
     rank  7: 30.96
     rank  8: 21.95
     rank  9: 21.20
     rank 10: 20.82
     rank 11: 21.74
     rank 12: 10.46
     rank 13: 10.38
     rank 14: 10.18
     rank 15: 10.49
  backward-recv:
     rank  0: 1392.36
     rank  1: 1392.80
     rank  2: 1392.96
     rank  3: 1392.28
     rank  4: 603.98
     rank  5: 603.46
     rank  6: 603.92
     rank  7: 604.33
     rank  8: 395.37
     rank  9: 395.47
     rank 10: 394.72
     rank 11: 395.27
     rank 12: 197.34
     rank 13: 197.39
     rank 14: 198.15
     rank 15: 197.39
  backward-send:
     rank  4: 3.89
     rank  5: 4.29
     rank  6: 4.21
     rank  7: 3.78
     rank  8: 31.05
     rank  9: 31.07
     rank 10: 31.17
     rank 11: 30.91
     rank 12: 20.84
     rank 13: 20.78
     rank 14: 20.10
     rank 15: 20.56
     rank 16: 10.42
     rank 17: 10.46
     rank 18: 10.42
     rank 19: 10.56
  forward-send-backward-recv:
     rank  0: 4071.80
     rank  1: 4072.02
     rank  2: 4071.88
     rank  3: 4072.12
     rank  4: 816.87
     rank  5: 815.25
     rank  6: 816.38
     rank  7: 817.89
     rank  8: 708.79
     rank  9: 710.39
     rank 10: 707.18
     rank 11: 709.74
     rank 12: 535.17
     rank 13: 531.45
     rank 14: 536.81
     rank 15: 535.63
  backward-send-forward-recv:
     rank  4: 20.48
     rank  5: 19.71
     rank  6: 20.69
     rank  7: 19.92
     rank  8: 145.83
     rank  9: 143.15
     rank 10: 145.73
     rank 11: 144.06
     rank 12: 157.36
     rank 13: 155.95
     rank 14: 154.51
     rank 15: 158.25
     rank 16: 167.90
     rank 17: 163.43
     rank 18: 167.15
     rank 19: 166.83
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.03
     rank  3: 0.03
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.03
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.04
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.05
     rank  7: 0.07
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.16
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.10
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.64
     rank  2: 0.62
     rank  3: 0.65
     rank  4: 2.51
     rank  5: 2.40
     rank  6: 2.51
     rank  7: 2.42
     rank  8: 2.20
     rank  9: 2.35
     rank 10: 2.19
     rank 11: 2.32
     rank 12: 2.21
     rank 13: 2.18
     rank 14: 2.18
     rank 15: 2.28
     rank 16: 2.78
     rank 17: 2.44
     rank 18: 2.43
     rank 19: 2.52
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.20
     rank  2: 0.19
     rank  3: 0.29
     rank  4: 0.06
     rank  5: 0.04
     rank  6: 0.06
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.07
     rank 10: 0.05
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.05
     rank 14: 0.04
     rank 15: 0.08
     rank 16: 0.07
     rank 17: 0.07
     rank 18: 0.06
     rank 19: 0.07
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.02
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.56
     rank  1: 8.69
     rank  2: 8.57
     rank  3: 8.75
     rank  4: 48.15
     rank  5: 48.13
     rank  6: 48.16
     rank  7: 48.18
     rank  8: 43.04
     rank  9: 43.20
     rank 10: 43.02
     rank 11: 43.12
     rank 12: 43.03
     rank 13: 43.06
     rank 14: 43.03
     rank 15: 43.09
     rank 16: 46.39
     rank 17: 46.48
     rank 18: 46.44
     rank 19: 46.47
  optimizer:
     rank  0: 9.38
     rank  1: 9.50
     rank  2: 9.42
     rank  3: 9.57
     rank  4: 48.96
     rank  5: 48.94
     rank  6: 48.97
     rank  7: 49.00
     rank  8: 43.85
     rank  9: 44.01
     rank 10: 43.83
     rank 11: 43.93
     rank 12: 43.84
     rank 13: 43.87
     rank 14: 43.84
     rank 15: 43.90
     rank 16: 47.19
     rank 17: 47.37
     rank 18: 47.25
     rank 19: 47.29
 [2024-12-03 23:13:14] iteration        8/      10 | consumed samples:          256 | elapsed time per iteration (ms): 8059.8 | learning rate: 6.000000E-06 | global batch size:    32 | lm loss: 1.432660E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7975.81
     rank  1: 7975.77
     rank  2: 7975.82
     rank  3: 7975.79
     rank  4: 7979.65
     rank  5: 7979.65
     rank  6: 7979.65
     rank  7: 7979.64
     rank  8: 7979.17
     rank  9: 7979.16
     rank 10: 7979.16
     rank 11: 7979.16
     rank 12: 7979.16
     rank 13: 7979.12
     rank 14: 7979.14
     rank 15: 7979.18
     rank 16: 7979.57
     rank 17: 7979.54
     rank 18: 7979.57
     rank 19: 7979.55
  forward-compute:
     rank  0: 1046.12
     rank  1: 1048.98
     rank  2: 1047.27
     rank  3: 1051.60
     rank  4: 2972.51
     rank  5: 2974.59
     rank  6: 2971.71
     rank  7: 2975.64
     rank  8: 2785.14
     rank  9: 2786.36
     rank 10: 2784.35
     rank 11: 2794.74
     rank 12: 2793.42
     rank 13: 2800.61
     rank 14: 2798.14
     rank 15: 2790.10
     rank 16: 2924.63
     rank 17: 2933.55
     rank 18: 2926.31
     rank 19: 2925.00
  backward-compute:
     rank  0: 1040.47
     rank  1: 1039.14
     rank  2: 1038.43
     rank  3: 1040.50
     rank  4: 3384.75
     rank  5: 3388.10
     rank  6: 3385.00
     rank  7: 3391.83
     rank  8: 3292.82
     rank  9: 3294.97
     rank 10: 3295.35
     rank 11: 3292.01
     rank 12: 3278.65
     rank 13: 3284.90
     rank 14: 3280.03
     rank 15: 3278.67
     rank 16: 3513.72
     rank 17: 3513.62
     rank 18: 3511.72
     rank 19: 3516.59
  pure-backward-compute:
     rank  0: 1039.80
     rank  1: 1038.49
     rank  2: 1037.73
     rank  3: 1039.80
     rank  4: 3383.88
     rank  5: 3387.13
     rank  6: 3383.94
     rank  7: 3390.99
     rank  8: 3291.41
     rank  9: 3294.12
     rank 10: 3294.35
     rank 11: 3291.20
     rank 12: 3277.46
     rank 13: 3284.18
     rank 14: 3279.29
     rank 15: 3277.23
     rank 16: 3511.14
     rank 17: 3511.96
     rank 18: 3509.13
     rank 19: 3514.92
  batch-generator:
     rank  0: 57.98
     rank  1: 62.68
     rank  2: 63.10
     rank  3: 67.08
     rank  4: 58.76
     rank  5: 67.83
     rank  6: 68.23
     rank  7: 73.74
     rank  8: 55.22
     rank  9: 59.55
     rank 10: 58.19
     rank 11: 66.48
     rank 12: 55.63
     rank 13: 60.91
     rank 14: 60.50
     rank 15: 58.40
     rank 16: 55.31
     rank 17: 65.61
     rank 18: 60.26
     rank 19: 58.19
  forward-recv:
     rank  4: 73.92
     rank  5: 73.12
     rank  6: 73.85
     rank  7: 72.45
     rank  8: 288.98
     rank  9: 290.04
     rank 10: 289.78
     rank 11: 286.97
     rank 12: 462.71
     rank 13: 461.87
     rank 14: 462.46
     rank 15: 462.69
     rank 16: 637.67
     rank 17: 637.66
     rank 18: 637.77
     rank 19: 637.58
  forward-send:
     rank  0: 392.96
     rank  1: 391.29
     rank  2: 393.28
     rank  3: 388.55
     rank  4: 31.17
     rank  5: 30.57
     rank  6: 31.18
     rank  7: 28.51
     rank  8: 21.04
     rank  9: 20.26
     rank 10: 20.56
     rank 11: 20.59
     rank 12: 10.47
     rank 13: 10.39
     rank 14: 10.34
     rank 15: 10.38
  backward-recv:
     rank  0: 1394.60
     rank  1: 1395.08
     rank  2: 1394.93
     rank  3: 1394.43
     rank  4: 600.06
     rank  5: 598.85
     rank  6: 599.85
     rank  7: 600.03
     rank  8: 394.63
     rank  9: 394.06
     rank 10: 393.79
     rank 11: 394.88
     rank 12: 197.98
     rank 13: 198.41
     rank 14: 199.20
     rank 15: 198.09
  backward-send:
     rank  4: 3.76
     rank  5: 4.68
     rank  6: 4.13
     rank  7: 3.55
     rank  8: 31.15
     rank  9: 31.11
     rank 10: 31.18
     rank 11: 30.67
     rank 12: 20.75
     rank 13: 20.39
     rank 14: 19.96
     rank 15: 20.78
     rank 16: 10.42
     rank 17: 10.34
     rank 18: 10.35
     rank 19: 10.56
  forward-send-backward-recv:
     rank  0: 4085.49
     rank  1: 4087.29
     rank  2: 4085.72
     rank  3: 4086.39
     rank  4: 810.58
     rank  5: 808.77
     rank  6: 810.23
     rank  7: 807.97
     rank  8: 719.96
     rank  9: 720.58
     rank 10: 718.18
     rank 11: 721.00
     rank 12: 546.78
     rank 13: 541.07
     rank 14: 547.37
     rank 15: 547.75
  backward-send-forward-recv:
     rank  4: 19.62
     rank  5: 20.35
     rank  6: 20.43
     rank  7: 19.42
     rank  8: 150.28
     rank  9: 148.53
     rank 10: 150.78
     rank 11: 145.16
     rank 12: 157.44
     rank 13: 154.38
     rank 14: 154.66
     rank 15: 159.77
     rank 16: 167.92
     rank 17: 161.43
     rank 18: 167.43
     rank 19: 167.27
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.08
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.07
  all-grads-sync:
     rank  0: 0.61
     rank  1: 0.57
     rank  2: 0.58
     rank  3: 0.61
     rank  4: 2.38
     rank  5: 2.42
     rank  6: 2.39
     rank  7: 2.44
     rank  8: 2.17
     rank  9: 2.28
     rank 10: 2.16
     rank 11: 2.30
     rank 12: 2.29
     rank 13: 2.17
     rank 14: 2.15
     rank 15: 2.28
     rank 16: 2.41
     rank 17: 2.39
     rank 18: 2.45
     rank 19: 2.38
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.18
     rank  3: 0.20
     rank  4: 0.04
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.05
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.07
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.06
     rank 16: 0.12
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.02
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.02
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.02
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.02
     rank 16: 0.04
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.03
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.54
     rank  1: 8.59
     rank  2: 8.59
     rank  3: 8.76
     rank  4: 48.12
     rank  5: 48.16
     rank  6: 48.21
     rank  7: 48.15
     rank  8: 43.07
     rank  9: 43.03
     rank 10: 43.05
     rank 11: 43.11
     rank 12: 43.15
     rank 13: 43.02
     rank 14: 43.02
     rank 15: 43.18
     rank 16: 46.84
     rank 17: 46.35
     rank 18: 46.44
     rank 19: 46.36
  optimizer:
     rank  0: 9.55
     rank  1: 9.60
     rank  2: 9.60
     rank  3: 9.77
     rank  4: 49.14
     rank  5: 49.17
     rank  6: 49.24
     rank  7: 49.16
     rank  8: 44.07
     rank  9: 44.03
     rank 10: 44.05
     rank 11: 44.12
     rank 12: 44.16
     rank 13: 44.03
     rank 14: 44.03
     rank 15: 44.19
     rank 16: 47.89
     rank 17: 47.36
     rank 18: 47.45
     rank 19: 47.37
 [2024-12-03 23:13:22] iteration        9/      10 | consumed samples:          288 | elapsed time per iteration (ms): 8058.7 | learning rate: 6.750000E-06 | global batch size:    32 | lm loss: 1.025088E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7968.06
     rank  1: 7968.03
     rank  2: 7968.07
     rank  3: 7968.03
     rank  4: 7971.89
     rank  5: 7971.93
     rank  6: 7971.91
     rank  7: 7971.89
     rank  8: 7971.44
     rank  9: 7971.48
     rank 10: 7971.44
     rank 11: 7971.44
     rank 12: 7971.39
     rank 13: 7971.39
     rank 14: 7971.43
     rank 15: 7971.35
     rank 16: 7971.79
     rank 17: 7971.79
     rank 18: 7971.79
     rank 19: 7971.77
  forward-compute:
     rank  0: 1065.50
     rank  1: 1067.83
     rank  2: 1066.54
     rank  3: 1070.95
     rank  4: 2964.49
     rank  5: 2968.55
     rank  6: 2962.74
     rank  7: 2969.82
     rank  8: 2780.23
     rank  9: 2781.17
     rank 10: 2779.97
     rank 11: 2789.64
     rank 12: 2794.73
     rank 13: 2801.23
     rank 14: 2799.35
     rank 15: 2793.34
     rank 16: 2932.22
     rank 17: 2940.67
     rank 18: 2933.55
     rank 19: 2931.73
  backward-compute:
     rank  0: 1014.80
     rank  1: 1015.73
     rank  2: 1011.79
     rank  3: 1016.86
     rank  4: 3385.68
     rank  5: 3388.24
     rank  6: 3385.97
     rank  7: 3391.07
     rank  8: 3275.22
     rank  9: 3277.68
     rank 10: 3277.88
     rank 11: 3274.01
     rank 12: 3275.70
     rank 13: 3280.99
     rank 14: 3277.19
     rank 15: 3275.10
     rank 16: 3512.50
     rank 17: 3513.73
     rank 18: 3511.85
     rank 19: 3516.56
  pure-backward-compute:
     rank  0: 1014.04
     rank  1: 1015.09
     rank  2: 1011.01
     rank  3: 1016.12
     rank  4: 3384.88
     rank  5: 3387.52
     rank  6: 3384.96
     rank  7: 3390.39
     rank  8: 3273.89
     rank  9: 3276.62
     rank 10: 3276.91
     rank 11: 3273.25
     rank 12: 3274.65
     rank 13: 3280.30
     rank 14: 3276.47
     rank 15: 3273.94
     rank 16: 3509.97
     rank 17: 3512.13
     rank 18: 3509.21
     rank 19: 3514.80
  batch-generator:
     rank  0: 61.85
     rank  1: 64.51
     rank  2: 67.04
     rank  3: 70.59
     rank  4: 57.57
     rank  5: 59.42
     rank  6: 64.87
     rank  7: 68.57
     rank  8: 55.03
     rank  9: 60.53
     rank 10: 58.42
     rank 11: 68.33
     rank 12: 54.57
     rank 13: 59.19
     rank 14: 59.39
     rank 15: 58.74
     rank 16: 58.33
     rank 17: 68.48
     rank 18: 64.34
     rank 19: 62.46
  forward-recv:
     rank  4: 69.81
     rank  5: 69.00
     rank  6: 69.78
     rank  7: 67.97
     rank  8: 286.19
     rank  9: 288.25
     rank 10: 286.50
     rank 11: 283.84
     rank 12: 456.78
     rank 13: 455.32
     rank 14: 457.23
     rank 15: 456.75
     rank 16: 631.14
     rank 17: 631.11
     rank 18: 630.80
     rank 19: 631.11
  forward-send:
     rank  0: 391.42
     rank  1: 389.57
     rank  2: 391.14
     rank  3: 386.44
     rank  4: 31.22
     rank  5: 30.71
     rank  6: 31.18
     rank  7: 28.43
     rank  8: 20.84
     rank  9: 19.10
     rank 10: 20.72
     rank 11: 20.53
     rank 12: 10.53
     rank 13: 10.39
     rank 14: 10.05
     rank 15: 10.52
  backward-recv:
     rank  0: 1412.59
     rank  1: 1412.30
     rank  2: 1412.84
     rank  3: 1412.16
     rank  4: 601.47
     rank  5: 601.54
     rank  6: 601.11
     rank  7: 601.30
     rank  8: 398.74
     rank  9: 398.99
     rank 10: 398.59
     rank 11: 399.54
     rank 12: 200.81
     rank 13: 201.07
     rank 14: 200.87
     rank 15: 200.86
  backward-send:
     rank  4: 4.13
     rank  5: 4.11
     rank  6: 4.42
     rank  7: 3.74
     rank  8: 30.84
     rank  9: 30.24
     rank 10: 30.92
     rank 11: 29.92
     rank 12: 20.79
     rank 13: 20.52
     rank 14: 20.54
     rank 15: 20.82
     rank 16: 10.40
     rank 17: 10.49
     rank 18: 10.18
     rank 19: 10.59
  forward-send-backward-recv:
     rank  0: 4067.49
     rank  1: 4067.98
     rank  2: 4069.49
     rank  3: 4066.77
     rank  4: 819.49
     rank  5: 817.78
     rank  6: 818.95
     rank  7: 818.68
     rank  8: 745.56
     rank  9: 745.55
     rank 10: 743.35
     rank 11: 746.29
     rank 12: 552.07
     rank 13: 547.57
     rank 14: 553.37
     rank 15: 553.98
  backward-send-forward-recv:
     rank  4: 19.88
     rank  5: 19.17
     rank  6: 19.75
     rank  7: 18.60
     rank  8: 146.01
     rank  9: 144.27
     rank 10: 145.87
     rank 11: 141.66
     rank 12: 157.11
     rank 13: 154.96
     rank 14: 153.82
     rank 15: 157.63
     rank 16: 167.29
     rank 17: 161.13
     rank 18: 167.29
     rank 19: 167.02
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.02
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.06
     rank  6: 0.05
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.06
     rank 17: 0.05
     rank 18: 0.05
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.57
     rank  1: 0.56
     rank  2: 0.61
     rank  3: 0.58
     rank  4: 2.39
     rank  5: 2.43
     rank  6: 2.41
     rank  7: 2.40
     rank  8: 2.16
     rank  9: 2.31
     rank 10: 2.17
     rank 11: 2.31
     rank 12: 2.18
     rank 13: 2.18
     rank 14: 2.17
     rank 15: 2.17
     rank 16: 2.38
     rank 17: 2.38
     rank 18: 2.46
     rank 19: 2.37
  optimizer-copy-to-main-grad:
     rank  0: 0.18
     rank  1: 0.19
     rank  2: 0.19
     rank  3: 0.20
     rank  4: 0.04
     rank  5: 0.05
     rank  6: 0.04
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.06
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.02
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.02
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.02
     rank 17: 0.01
     rank 18: 0.02
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.01
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.64
     rank  1: 8.40
     rank  2: 8.77
     rank  3: 8.60
     rank  4: 48.13
     rank  5: 48.20
     rank  6: 48.18
     rank  7: 48.08
     rank  8: 43.02
     rank  9: 43.10
     rank 10: 43.07
     rank 11: 43.05
     rank 12: 43.06
     rank 13: 43.08
     rank 14: 43.03
     rank 15: 43.03
     rank 16: 46.42
     rank 17: 46.32
     rank 18: 46.44
     rank 19: 46.37
  optimizer:
     rank  0: 9.33
     rank  1: 9.09
     rank  2: 9.46
     rank  3: 9.28
     rank  4: 48.83
     rank  5: 48.89
     rank  6: 48.89
     rank  7: 48.77
     rank  8: 43.71
     rank  9: 43.79
     rank 10: 43.76
     rank 11: 43.73
     rank 12: 43.75
     rank 13: 43.76
     rank 14: 43.72
     rank 15: 43.72
     rank 16: 47.11
     rank 17: 47.01
     rank 18: 47.13
     rank 19: 47.06
 [2024-12-03 23:13:30] iteration       10/      10 | consumed samples:          320 | elapsed time per iteration (ms): 8056.4 | learning rate: 7.500000E-06 | global batch size:    32 | lm loss: 1.434732E+00 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
times across ranks (ms):
  forward-backward:
     rank  0: 7972.93
     rank  1: 7972.93
     rank  2: 7973.24
     rank  3: 7972.94
     rank  4: 7976.78
     rank  5: 7976.79
     rank  6: 7977.08
     rank  7: 7976.79
     rank  8: 7976.31
     rank  9: 7976.32
     rank 10: 7976.59
     rank 11: 7976.31
     rank 12: 7976.30
     rank 13: 7976.25
     rank 14: 7976.56
     rank 15: 7976.28
     rank 16: 7976.67
     rank 17: 7976.70
     rank 18: 7977.42
     rank 19: 7976.68
  forward-compute:
     rank  0: 1095.47
     rank  1: 1098.26
     rank  2: 1096.37
     rank  3: 1100.72
     rank  4: 2969.04
     rank  5: 2973.28
     rank  6: 2968.40
     rank  7: 2973.88
     rank  8: 2775.90
     rank  9: 2777.65
     rank 10: 2775.60
     rank 11: 2783.85
     rank 12: 2794.09
     rank 13: 2800.12
     rank 14: 2798.47
     rank 15: 2793.06
     rank 16: 2925.07
     rank 17: 2932.60
     rank 18: 2926.05
     rank 19: 2925.00
  backward-compute:
     rank  0: 991.43
     rank  1: 992.09
     rank  2: 989.91
     rank  3: 993.39
     rank  4: 3369.00
     rank  5: 3372.34
     rank  6: 3369.63
     rank  7: 3373.39
     rank  8: 3276.08
     rank  9: 3277.89
     rank 10: 3278.54
     rank 11: 3275.34
     rank 12: 3275.61
     rank 13: 3280.99
     rank 14: 3276.52
     rank 15: 3275.42
     rank 16: 3508.13
     rank 17: 3508.89
     rank 18: 3506.88
     rank 19: 3511.14
  pure-backward-compute:
     rank  0: 990.74
     rank  1: 991.43
     rank  2: 989.24
     rank  3: 992.67
     rank  4: 3368.16
     rank  5: 3371.61
     rank  6: 3368.58
     rank  7: 3372.67
     rank  8: 3274.75
     rank  9: 3277.05
     rank 10: 3277.52
     rank 11: 3274.52
     rank 12: 3274.53
     rank 13: 3280.29
     rank 14: 3275.80
     rank 15: 3274.27
     rank 16: 3505.74
     rank 17: 3507.24
     rank 18: 3504.01
     rank 19: 3509.52
  batch-generator:
     rank  0: 61.33
     rank  1: 62.95
     rank  2: 63.35
     rank  3: 68.96
     rank  4: 58.36
     rank  5: 64.36
     rank  6: 64.72
     rank  7: 69.61
     rank  8: 55.17
     rank  9: 60.04
     rank 10: 58.59
     rank 11: 64.67
     rank 12: 54.92
     rank 13: 59.13
     rank 14: 59.60
     rank 15: 59.08
     rank 16: 54.55
     rank 17: 63.24
     rank 18: 58.89
     rank 19: 56.84
  forward-recv:
     rank  4: 86.23
     rank  5: 85.11
     rank  6: 86.32
     rank  7: 84.71
     rank  8: 304.91
     rank  9: 305.74
     rank 10: 305.01
     rank 11: 302.97
     rank 12: 474.65
     rank 13: 473.83
     rank 14: 474.82
     rank 15: 474.29
     rank 16: 648.20
     rank 17: 648.26
     rank 18: 648.15
     rank 19: 648.21
  forward-send:
     rank  0: 362.81
     rank  1: 360.68
     rank  2: 362.67
     rank  3: 357.96
     rank  4: 31.26
     rank  5: 30.58
     rank  6: 31.16
     rank  7: 28.45
     rank  8: 21.08
     rank  9: 20.03
     rank 10: 20.94
     rank 11: 20.32
     rank 12: 10.47
     rank 13: 10.40
     rank 14: 10.19
     rank 15: 10.49
  backward-recv:
     rank  0: 1413.95
     rank  1: 1414.24
     rank  2: 1414.33
     rank  3: 1413.78
     rank  4: 602.84
     rank  5: 602.55
     rank  6: 602.80
     rank  7: 602.99
     rank  8: 392.89
     rank  9: 394.13
     rank 10: 393.34
     rank 11: 394.21
     rank 12: 197.16
     rank 13: 197.66
     rank 14: 197.71
     rank 15: 196.97
  backward-send:
     rank  4: 4.09
     rank  5: 4.15
     rank  6: 4.22
     rank  7: 3.80
     rank  8: 31.30
     rank  9: 30.47
     rank 10: 31.23
     rank 11: 30.51
     rank 12: 20.90
     rank 13: 20.29
     rank 14: 20.46
     rank 15: 20.76
     rank 16: 10.54
     rank 17: 10.23
     rank 18: 10.53
     rank 19: 10.08
  forward-send-backward-recv:
     rank  0: 4092.40
     rank  1: 4092.75
     rank  2: 4092.74
     rank  3: 4092.07
     rank  4: 817.71
     rank  5: 816.32
     rank  6: 817.21
     rank  7: 817.97
     rank  8: 739.58
     rank  9: 740.37
     rank 10: 737.83
     rank 11: 740.19
     rank 12: 542.88
     rank 13: 538.38
     rank 14: 544.03
     rank 15: 544.50
  backward-send-forward-recv:
     rank  4: 19.90
     rank  5: 19.32
     rank  6: 20.60
     rank  7: 18.60
     rank  8: 147.39
     rank  9: 145.29
     rank 10: 147.26
     rank 11: 144.28
     rank 12: 157.55
     rank 13: 155.26
     rank 14: 154.71
     rank 15: 158.24
     rank 16: 168.27
     rank 17: 162.52
     rank 18: 167.88
     rank 19: 167.78
  layernorm-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.02
     rank  5: 0.02
     rank  6: 0.02
     rank  7: 0.02
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.02
     rank 17: 0.02
     rank 18: 0.06
     rank 19: 0.02
  embedding-grads-all-reduce:
     rank  0: 0.02
     rank  1: 0.02
     rank  2: 0.02
     rank  3: 0.02
     rank  4: 0.05
     rank  5: 0.05
     rank  6: 0.06
     rank  7: 0.05
     rank  8: 0.02
     rank  9: 0.02
     rank 10: 0.02
     rank 11: 0.02
     rank 12: 0.02
     rank 13: 0.02
     rank 14: 0.02
     rank 15: 0.02
     rank 16: 0.05
     rank 17: 0.05
     rank 18: 0.15
     rank 19: 0.05
  all-grads-sync:
     rank  0: 0.56
     rank  1: 0.57
     rank  2: 0.56
     rank  3: 0.56
     rank  4: 2.39
     rank  5: 2.41
     rank  6: 2.44
     rank  7: 2.40
     rank  8: 2.16
     rank  9: 2.28
     rank 10: 2.15
     rank 11: 2.28
     rank 12: 2.19
     rank 13: 2.20
     rank 14: 2.16
     rank 15: 2.19
     rank 16: 2.36
     rank 17: 2.39
     rank 18: 2.97
     rank 19: 2.39
  optimizer-copy-to-main-grad:
     rank  0: 0.17
     rank  1: 0.19
     rank  2: 0.17
     rank  3: 0.20
     rank  4: 0.04
     rank  5: 0.04
     rank  6: 0.05
     rank  7: 0.04
     rank  8: 0.04
     rank  9: 0.06
     rank 10: 0.04
     rank 11: 0.06
     rank 12: 0.06
     rank 13: 0.04
     rank 14: 0.04
     rank 15: 0.04
     rank 16: 0.06
     rank 17: 0.06
     rank 18: 0.13
     rank 19: 0.06
  optimizer-clip-main-grad:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.02
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.02
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.05
     rank 19: 0.01
  optimizer-count-zeros:
     rank  0: 0.01
     rank  1: 0.01
     rank  2: 0.01
     rank  3: 0.01
     rank  4: 0.01
     rank  5: 0.01
     rank  6: 0.01
     rank  7: 0.01
     rank  8: 0.01
     rank  9: 0.01
     rank 10: 0.01
     rank 11: 0.01
     rank 12: 0.01
     rank 13: 0.01
     rank 14: 0.01
     rank 15: 0.01
     rank 16: 0.01
     rank 17: 0.01
     rank 18: 0.04
     rank 19: 0.01
  optimizer-inner-step:
     rank  0: 8.55
     rank  1: 8.39
     rank  2: 8.59
     rank  3: 8.55
     rank  4: 48.13
     rank  5: 48.16
     rank  6: 48.22
     rank  7: 48.08
     rank  8: 43.10
     rank  9: 43.06
     rank 10: 43.03
     rank 11: 43.08
     rank 12: 43.02
     rank 13: 43.05
     rank 14: 43.01
     rank 15: 43.02
     rank 16: 46.35
     rank 17: 46.32
     rank 18: 47.16
     rank 19: 46.35
  optimizer:
     rank  0: 9.88
     rank  1: 9.73
     rank  2: 9.92
     rank  3: 9.88
     rank  4: 49.47
     rank  5: 49.51
     rank  6: 49.56
     rank  7: 49.43
     rank  8: 44.44
     rank  9: 44.40
     rank 10: 44.36
     rank 11: 44.42
     rank 12: 44.36
     rank 13: 44.39
     rank 14: 44.35
     rank 15: 44.36
     rank 16: 47.69
     rank 17: 47.65
     rank 18: 48.48
     rank 19: 47.68
